{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction 笔记 搬运计划： http://www.topgoer.com/go%E5%9F%BA%E7%A1%80/%E6%95%B0%E7%BB%84Array.html go news Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 16:39:57 "},"blog/algorithm/":{"url":"blog/algorithm/","title":"Algorithm","keywords":"","body":"Algorithm Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/100. 相同的树.html":{"url":"blog/algorithm/100. 相同的树.html","title":"100. 相同的树","keywords":"","body":"给你两棵二叉树的根节点 p 和 q ，编写一个函数来检验这两棵树是否相同。 如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func isSameTree(p *TreeNode, q *TreeNode) bool { if p.Left != q.Left && p.Right != q.Right { return false }else{ isSameTree(p, q) } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/11. 盛最多水的容器.html":{"url":"blog/algorithm/11. 盛最多水的容器.html","title":"11. 盛最多水的容器","keywords":"","body":" 盛最多水的容器 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 说明：你不能倾斜容器。 示例 1： 输入：[1,8,6,2,5,4,8,3,7] 输出：49 解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例 2： 输入：height = [1,1] 输出：1 提示： n == height.length 2 package stringset func maxArea(height []int) int { l, r := 0, len(height)-1 max := (r - l) * min(height[l], height[r]) for l max { max = min(height[l], height[r]) * (r - l) } } else { r-- if height[r] max { max = min(height[l], height[r]) * (r - l) } } } return max } func min(i, j int) int{ if i Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/15. 三个数之和.html":{"url":"blog/algorithm/15. 三个数之和.html","title":"15. 三个数之和","keywords":"","body":"15. 求三个数的和等于0 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。 注意：答案中不可以包含重复的三元组。 解题方法 该解法只是参照了另一个题解，Three Sum题解，本人只是将其可视化，方便大家理解 原理 先将数组进行排序 从左侧开始，选定一个值为 定值 ，右侧进行求解，获取与其相加为 00 的两个值 类似于快排，定义首和尾 首尾与 定值 相加 等于 00，记录这三个值 小于 00，首部右移 大于 00，尾部左移 定值右移，重复该步骤 作者：githber 链接：https://leetcode.cn/problems/3sum/solution/three-sum-giftu-jie-by-githber/ 复杂度分析： 时间复杂度 O(N^2)其中固定指针k循环复杂度 O(N)，双指针 i，j 复杂度 O(N)。 空间复杂度 O(1)：指针使用常数大小的额外空间。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/16. 最接近的三数之和.html":{"url":"blog/algorithm/16. 最接近的三数之和.html","title":"16. 最接近的三数之和","keywords":"","body":" 最接近的三数之和 给你一个长度为 n 的整数数组 nums 和 一个目标值 target。请你从 nums 中选出三个整数，使它们的和与 target 最接近。 返回这三个数的和。 假定每组输入只存在恰好一个解。 示例 1： 输入：nums = [-1,2,1,-4], target = 1 输出：2 解释：与 target 最接近的和是 2 (-1 + 2 + 1 = 2) 。 示例 2： 输入：nums = [0,0,0], target = 1 输出：0 提示： 3 解题思路： 标签：排序和双指针 本题目因为要计算三个数，如果靠暴力枚举的话时间复杂度会到 O(n^3)，需要降低时间复杂度 首先进行数组排序，时间复杂度 O(nlogn) 在数组 nums 中，进行遍历，每遍历一个值利用其下标i，形成一个固定值 nums[i] 再使用前指针指向 start = i + 1 处，后指针指向 end = nums.length - 1 处，也就是结尾处 根据 sum = nums[i] + nums[start] + nums[end] 的结果，判断 sum 与目标 target 的距离，如果更近则更新结果 ans 同时判断 sum 与 target 的大小关系，因为数组有序，如果 sum > target 则 end--，如果 sum 整个遍历过程，固定值为 n 次，双指针为 n 次，时间复杂度为 O(n^2) 总时间复杂度：O(nlogn) + O(n^2) = O(n^2) 作者：guanpengchn Code func threeSumClosest(nums []int, target int) int { result := make([]int,0) length := len(nums) if length 0 { r-- }else{ l++ } } } return result[0] } func abs(i int) int{ if i Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/2.两数之和.html":{"url":"blog/algorithm/2.两数之和.html","title":"2.两数之和","keywords":"","body":"2. 两数相加 给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。 请你将两个数相加，并以相同形式返回一个表示和的链表。 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例 1： 输入：l1 = [2,4,3], l2 = [5,6,4] 输出：[7,0,8] 解释：342 + 465 = 807. 示例 2： 输入：l1 = [0], l2 = [0] 输出：[0] 示例 3： 输入：l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9] 输出：[8,9,9,9,0,0,0,1] 提示： 每个链表中的节点数在范围 [1, 100] 内 0 题目数据保证列表表示的数字不含前导零 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/208. 前缀树.html":{"url":"blog/algorithm/208. 前缀树.html","title":"208. 前缀树","keywords":"","body":" 实现 Trie（前缀树） Trie（发音类似 \"try\"）或者说 前缀树 是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。 请你实现 Trie 类： Trie() 初始化前缀树对象。 void insert(String word) 向前缀树中插入字符串 word 。 boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。 boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。 示例： 输入 [\"Trie\", \"insert\", \"search\", \"search\", \"startsWith\", \"insert\", \"search\"] [[], [\"apple\"], [\"apple\"], [\"app\"], [\"app\"], [\"app\"], [\"app\"]] 输出 [null, null, true, false, true, null, true] 解释 Trie trie = new Trie(); trie.insert(\"apple\"); trie.search(\"apple\"); // 返回 True trie.search(\"app\"); // 返回 False trie.startsWith(\"app\"); // 返回 True trie.insert(\"app\"); trie.search(\"app\"); // 返回 True type Trie struct { children map[rune]*Trie match bool } func Constructor() Trie { return Trie{ children: make(map[rune]*Trie,0), match: false, } } func NewTrie(w rune) *Trie{ return &Trie{ children: make(map[rune]*Trie,0), match: false, } } func (this *Trie) Insert(word string) { for _, w := range word{ if this.children[w] == nil { this.children[w] = NewTrie(w) } this = this.children[w] } this.match = true } func (this *Trie) Search(word string) bool { for _, w := range word{ if _, ok := this.children[w]; ok{ this = this.children[w] }else{ return false } } return this.match } func (this *Trie) StartsWith(prefix string) bool { for _, w := range prefix { if _, ok := this.children[w]; ok{ this = this.children[w] }else{ return false } } return true } /** * Your Trie object will be instantiated and called as such: * obj := Constructor(); * obj.Insert(word); * param_2 := obj.Search(word); * param_3 := obj.StartsWith(prefix); */ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/21. merge_list.html":{"url":"blog/algorithm/21. merge_list.html","title":"21. Merge List","keywords":"","body":"21. 合并两个有序链表 难度简单2487收藏分享切换为英文接收动态反馈 将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例 1： 输入：l1 = [1,2,4], l2 = [1,3,4] 输出：[1,1,2,3,4,4] 示例 2： 输入：l1 = [], l2 = [] 输出：[] 示例 3： 输入：l1 = [], l2 = [0] 输出：[0] 提示： 两个链表的节点数目范围是 [0, 50] -100 l1 和 l2 均按 非递减顺序 排列 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/220. 桶排序-存在重复元素.html":{"url":"blog/algorithm/220. 桶排序-存在重复元素.html","title":"220. 桶排序 存在重复元素","keywords":"","body":"220. 存在重复元素 III 给你一个整数数组 nums 和两个整数 k 和 t 。请你判断是否存在 两个不同下标 i 和 j，使得 abs(nums[i] - nums[j]) ，同时又满足 abs(i - j) 。 如果存在则返回 true，不存在返回 false。 示例 1： 输入：nums = [1,2,3,1], k = 3, t = 0 输出：true 示例 2： 输入：nums = [1,0,1,1], k = 1, t = 2 输出：true 示例 3： 输入：nums = [1,5,9,1,5,9], k = 2, t = 3 输出：false 提示： 0 -231 0 0 O(n)神仙解法，桶，参考自https://leetcode.com/problems/contains-duplicate-iii/discuss/339421/Python-bucket-method-in-detail 首先，定义桶的大小是t+1, nums[i]//(t+1)决定放入几号桶,这样在一个桶里面的任意两个的绝对值差值都 先不考虑索引差值最大为K的限制，那么遍历nums每一个元素，并把他们放入相应的桶中，有两种情况会返回True 要放入的桶中已经有其他元素了，这时将nums[i]放进去满足差值 接着考虑限制桶中的索引差最大为K,当i>=k的时候： 我们就要去删除存放着nums[i-k]的那个桶(编号为nums[i-k]//(t+1)) 这样就能保证遍历到第i+1个元素时，全部桶中元素的索引最小值是i-k+1，就满足题目对索引的限制了 方法一：滑动窗口 + 有序集合 思路及算法 对于序列中每一个元素 xx 左侧的至多 kk 个元素，如果这 kk 个元素中存在一个元素落在区间 [x - t, x + t][x−t,x+t] 中，我们就找到了一对符合条件的元素。注意到对于两个相邻的元素，它们各自的左侧的 kk 个元素中有 k - 1k−1 个是重合的。于是我们可以使用滑动窗口的思路，维护一个大小为 kk 的滑动窗口，每次遍历到元素 xx 时，滑动窗口中包含元素 xx 前面的最多 kk 个元素，我们检查窗口中是否存在元素落在区间 [x - t, x + t][x−t,x+t] 中即可。 如果使用队列维护滑动窗口内的元素，由于元素是无序的，我们只能对于每个元素都遍历一次队列来检查是否有元素符合条件。如果数组的长度为 nn，则使用队列的时间复杂度为 O(nk)O(nk)，会超出时间限制。 因此我们希望能够找到一个数据结构维护滑动窗口内的元素，该数据结构需要满足以下操作： 支持添加和删除指定元素的操作，否则我们无法维护滑动窗口； 内部元素有序，支持二分查找的操作，这样我们可以快速判断滑动窗口中是否存在元素满足条件，具体而言，对于元素 xx，当我们希望判断滑动窗口中是否存在某个数 yy 落在区间 [x - t, x + t][x−t,x+t] 中，只需要判断滑动窗口中所有大于等于 x - tx−t 的元素中的最小元素是否小于等于 x + tx+t 即可。 我们可以使用有序集合来支持这些操作。 实现方面，我们在有序集合中查找大于等于 x - tx−t 的最小的元素 yy，如果 yy 存在，且 y \\leq x + ty≤x+t，我们就找到了一对符合条件的元素。完成检查后，我们将 xx 插入到有序集合中，如果有序集合中元素数量超过了 kk，我们将有序集合中最早被插入的元素删除即可。 注意 如果当前有序集合中存在相同元素，那么此时程序将直接返回 \\texttt{true}true。因此本题中的有序集合无需处理相同元素的情况。 为防止整型 \\texttt{int}int 溢出，我们既可以使用长整型 \\texttt{long}long，也可以对查找区间 [x - t, x + t][x−t,x+t] 进行限制，使其落在 \\texttt{int}int 范围内。 import \"math/rand\" type node struct { ch [2]*node priority int val int } func (o *node) cmp(b int) int { switch { case b o.val: return 1 default: return -1 } } func (o *node) rotate(d int) *node { x := o.ch[d^1] o.ch[d^1] = x.ch[d] x.ch[d] = o return x } type treap struct { root *node } func (t *treap) _put(o *node, val int) *node { if o == nil { return &node{priority: rand.Int(), val: val} } d := o.cmp(val) o.ch[d] = t._put(o.ch[d], val) if o.ch[d].priority > o.priority { o = o.rotate(d ^ 1) } return o } func (t *treap) put(val int) { t.root = t._put(t.root, val) } func (t *treap) _delete(o *node, val int) *node { if d := o.cmp(val); d >= 0 { o.ch[d] = t._delete(o.ch[d], val) return o } if o.ch[1] == nil { return o.ch[0] } if o.ch[0] == nil { return o.ch[1] } d := 0 if o.ch[0].priority > o.ch[1].priority { d = 1 } o = o.rotate(d) o.ch[d] = t._delete(o.ch[d], val) return o } func (t *treap) delete(val int) { t.root = t._delete(t.root, val) } func (t *treap) lowerBound(val int) (lb *node) { for o := t.root; o != nil; { switch c := o.cmp(val); { case c == 0: lb = o o = o.ch[0] case c > 0: o = o.ch[1] default: return o } } return } func containsNearbyAlmostDuplicate(nums []int, k, t int) bool { set := &treap{} for i, v := range nums { if lb := set.lowerBound(v - t); lb != nil && lb.val = k { set.delete(nums[i-k]) } } return false } 链接：https://leetcode.cn/problems/contains-duplicate-iii/solution/cun-zai-zhong-fu-yuan-su-iii-by-leetcode-bbkt/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/225. 队列实现栈.html":{"url":"blog/algorithm/225. 队列实现栈.html","title":"225. 队列实现栈","keywords":"","body":"225. 用队列实现栈 难度简单536收藏分享切换为英文接收动态反馈 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。 实现 MyStack 类： void push(int x) 将元素 x 压入栈顶。 int pop() 移除并返回栈顶元素。 int top() 返回栈顶元素。 boolean empty() 如果栈是空的，返回 true ；否则，返回 false 。 注意： 你只能使用队列的基本操作 —— 也就是 push to back、peek/pop from front、size 和 is empty 这些操作。 你所使用的语言也许不支持队列。 你可以使用 list （列表）或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 示例： 输入： [\"MyStack\", \"push\", \"push\", \"top\", \"pop\", \"empty\"] [[], [1], [2], [], [], []] 输出： [null, null, null, 2, 2, false] 解释： MyStack myStack = new MyStack(); myStack.push(1); myStack.push(2); myStack.top(); // 返回 2 myStack.pop(); // 返回 2 myStack.empty(); // 返回 False 提示： 1 最多调用100 次 push、pop、top 和 empty 每次调用 pop 和 top 都保证栈不为空 进阶：你能否仅用一个队列来实现栈。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/23. 合并K个升序链表.html":{"url":"blog/algorithm/23. 合并K个升序链表.html","title":"23. 合并K个升序链表","keywords":"","body":"23. 合并K个升序链表 难度困难2052收藏分享切换为英文接收动态反馈 给你一个链表数组，每个链表都已经按升序排列。 请你将所有链表合并到一个升序链表中，返回合并后的链表。 示例 1： 输入：lists = [[1,4,5],[1,3,4],[2,6]] 输出：[1,1,2,3,4,4,5,6] 解释：链表数组如下： [ 1->4->5, 1->3->4, 2->6 ] 将它们合并到一个有序链表中得到。 1->1->2->3->4->4->5->6 示例 2： 输入：lists = [] 输出：[] 示例 3： 输入：lists = [[]] 输出：[] 提示： k == lists.length 0 0 -10^4 lists[i] 按 升序 排列 lists[i].length 的总和不超过 10^4 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/26. 删除有序数组中的重复项.html":{"url":"blog/algorithm/26. 删除有序数组中的重复项.html","title":"26. 删除有序数组中的重复项","keywords":"","body":"26. 删除有序数组中的重复项 难度简单2721收藏分享切换为英文接收动态反馈 给你一个 升序排列 的数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。元素的 相对顺序 应该保持 一致 。 由于在某些语言中不能改变数组的长度，所以必须将结果放在数组nums的第一部分。更规范地说，如果在删除重复项之后有 k 个元素，那么 nums 的前 k 个元素应该保存最终结果。 将最终结果插入 nums 的前 k 个位置后返回 k 。 不要使用额外的空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。 判题标准: 系统会用下面的代码来测试你的题解: int[] nums = [...]; // 输入数组 int[] expectedNums = [...]; // 长度正确的期望答案 int k = removeDuplicates(nums); // 调用 assert k == expectedNums.length; for (int i = 0; i 如果所有断言都通过，那么您的题解将被 通过。 示例 1： 输入：nums = [1,1,2] 输出：2, nums = [1,2,_] 解释：函数应该返回新的长度 2 ，并且原数组 nums 的前两个元素被修改为 1, 2 。不需要考虑数组中超出新长度后面的元素。 示例 2： 输入：nums = [0,0,1,1,1,2,2,3,3,4] 输出：5, nums = [0,1,2,3,4] 解释：函数应该返回新的长度 5 ， 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4 。不需要考虑数组中超出新长度后面的元素。 提示： 1 -104 nums 已按 升序 排列 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/29. 两数相除.html":{"url":"blog/algorithm/29. 两数相除.html","title":"29. 两数相除","keywords":"","body":" 两数相除 给定两个整数，被除数 dividend 和除数 divisor。将两数相除，要求不使用乘法、除法和 mod 运算符。 返回被除数 dividend 除以除数 divisor 得到的商。 整数除法的结果应当截去（truncate）其小数部分，例如：truncate(8.345) = 8 以及 truncate(-2.7335) = -2 示例 1: 输入: dividend = 10, divisor = 3 输出: 3 解释: 10/3 = truncate(3.33333..) = truncate(3) = 3 示例 2: 输入: dividend = 7, divisor = -3 输出: -2 解释: 7/-3 = truncate(-2.33333..) = -2 题解： 前言 由于题目规定了「只能存储 32 位整数」，本题解的正文部分和代码中都不会使用任何 64 位整数。诚然，使用 64位整数可以极大地方便我们的编码，但这是违反题目规则的。 极限情况讨论： -2^31 ➗ -1， 产生溢出，此时返回2^31 -1 -2^31 ➗ 1, return -2 ^ 31 -2^31 ➗ -2^31， return1； xx ➗ -2^31，return 0 0 ➗ x, return 0 对于一般的情况，根据除数和被除数的符号，我们需要考虑 44 种不同的可能性。因此，为了方便编码，我们可以将被除数或者除数取相反数，使得它们符号相同。 如果我们将被除数和除数都变为正数，那么可能会导致溢出。例如当被除数为 -2^{31}时，它的相反数 2^{31}2 产生了溢出。因此，我们可以考虑将被除数和除数都变为负数，这样就不会有溢出的问题，在编码时只需要考虑 1种情况了。 如果我们将被除数和除数的其中（恰好）一个变为了正数，那么在返回答案之前，我们需要对答案也取相反数。 func divide(dividend int, divisor int) int { if dividend == 0{ return 0 } if dividend == math.MinInt32 { // 考虑被除数为最小值的情况 if divisor == 1 { return math.MinInt32 } if divisor == -1 { return math.MaxInt32 } } if divisor == math.MinInt32 { // 考虑除数为最小值的情况 if dividend == math.MinInt32 { return 1 } return 0 } sing := 1 if dividend > 0 && divisor 0 { sing = -1 } x, y := abs(dividend), abs(divisor) result :=0 for x >= y { i := 1 temp := y for x >= temp{ x -= temp result += i i = i 0{ return x } return x*-1 } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/30. 串联所有单词的子串.html":{"url":"blog/algorithm/30. 串联所有单词的子串.html","title":"30. 串联所有单词的子串","keywords":"","body":"30. 串联所有单词的子串 给定一个字符串 s 和一些 长度相同 的单词 words 。找出 s 中恰好可以由 words 中所有单词串联形成的子串的起始位置。 注意子串要与 words 中的单词完全匹配，中间不能有其他字符 ，但不需要考虑 words 中单词串联的顺序。 示例 1： 输入：s = \"barfoothefoobarman\", words = [\"foo\",\"bar\"] 输出：[0,9] 解释： 从索引 0 和 9 开始的子串分别是 \"barfoo\" 和 \"foobar\" 。 输出的顺序不重要, [9,0] 也是有效答案。 示例 2： 输入：s = \"wordgoodgoodgoodbestword\", words = [\"word\",\"good\",\"best\",\"word\"] 输出：[] 示例 3： 输入：s = \"barfoofoobarthefoobarman\", words = [\"bar\",\"foo\",\"the\"] 输出：[6,9,12] 提示： 1 s 由小写英文字母组成 1 1 words[i] 由小写英文字母组成 题目解： 记words的长度为m，words中的每个单词长度为n, s的长度为ls。 首先需要将s划分为单词组，每个单词的大小均为n（首位除外） func findSubstring(s string, words []string) (ans []int) { ls, m, n := len(s), len(words), len(words[0]) for i := 0; i Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/31. 下一个排列.html":{"url":"blog/algorithm/31. 下一个排列.html","title":"31. 下一个排列","keywords":"","body":"31. 下一个排列 难度中等1824 整数数组的一个 排列 就是将其所有成员以序列或线性顺序排列。 例如，arr = [1,2,3] ，以下这些都可以视作 arr 的排列：[1,2,3]、[1,3,2]、[3,1,2]、[2,3,1] 。 整数数组的 下一个排列 是指其整数的下一个字典序更大的排列。更正式地，如果数组的所有排列根据其字典顺序从小到大排列在一个容器中，那么数组的 下一个排列 就是在这个有序容器中排在它后面的那个排列。如果不存在下一个更大的排列，那么这个数组必须重排为字典序最小的排列（即，其元素按升序排列）。 例如，arr = [1,2,3] 的下一个排列是 [1,3,2] 。 类似地，arr = [2,3,1] 的下一个排列是 [3,1,2] 。 而 arr = [3,2,1] 的下一个排列是 [1,2,3] ，因为 [3,2,1] 不存在一个字典序更大的排列。 给你一个整数数组 nums ，找出 nums 的下一个排列。 必须 原地 修改，只允许使用额外常数空间。 示例 1： 输入：nums = [1,2,3] 输出：[1,3,2] 示例 2： 输入：nums = [3,2,1] 输出：[1,2,3] 示例 3： 输入：nums = [1,1,5] 输出：[1,5,1] 提示： 1 0 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/43. 字符串相乘.html":{"url":"blog/algorithm/43. 字符串相乘.html","title":"43. 字符串相乘","keywords":"","body":"43. 字符串相乘 难度中等983收藏分享切换为英文接收动态反馈 给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。 注意：不能使用任何内置的 BigInteger 库或直接将输入转换为整数。 示例 1: 输入: num1 = \"2\", num2 = \"3\" 输出: \"6\" 示例 2: 输入: num1 = \"123\", num2 = \"456\" 输出: \"56088\" 提示： 1 num1 和 num2 只能由数字组成。 num1 和 num2 都不包含任何前导零，除了数字0本身。 方法一： 做加法 情况1: 长度0， 返回0 情况2: 都不是0，【竖式乘法】的方法计算乘积。从右往左遍历乘数，将乘数的每一位与被乘数相乘得到对应的结果，再将每次得到的结果累加。这道题中，被乘数是num1，乘数是num2 注意：num2除了最低位以外，其余的每一位运算结果都要补0。 func multiply(num1 string, num2 string) string { if num1 == \"0\" || mum2 == \"0\"{ return \"0\" } ans := \"0\" m, n := len(num1), len(num2) // 被乘数 for i:=n-1;i>0;i--{ x := int(num2[i]-'0') * myPow(10,n-1-i) add := 0 // 乘数 for j:=m-1;j>0;j--{ y := int(num1[j]-'0') add += x*y*myPow(10, m-1-j) } ans += add } return ans } func myPow(x float64, n int) float64 { if n == 0 || x == 1{ return 1 } if n 方法二： Start from right to left, perform multiplication on every pair of digits, and add them together. Let's draw the process! From the following draft, we can immediately conclud `num1[i] * num2[j]` will be placed at indices `[i + j`, `i + j + 1]` java code public String multiply(String num1, String num2) { int m = num1.length(), n = num2.length(); int[] pos = new int[m + n]; for(int i = m - 1; i >= 0; i--) { for(int j = n - 1; j >= 0; j--) { int mul = (num1.charAt(i) - '0') * (num2.charAt(j) - '0'); int p1 = i + j, p2 = i + j + 1; int sum = mul + pos[p2]; pos[p1] += sum / 10; pos[p2] = (sum) % 10; } } StringBuilder sb = new StringBuilder(); for(int p : pos) if(!(sb.length() == 0 && p == 0)) sb.append(p); return sb.length() == 0 ? \"0\" : sb.toString(); } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/438. 找到字符串中所欲呕字母的异位词.html":{"url":"blog/algorithm/438. 找到字符串中所欲呕字母的异位词.html","title":"438. 找到字符串中所欲呕字母的异位词","keywords":"","body":"438. 找到字符串中所有字母异位词 给定两个字符串 s 和 p，找到 s 中所有 p 的 异位词 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。 异位词 指由相同字母重排列形成的字符串（包括相同的字符串）。 示例 1: 输入: s = \"cbaebabacd\", p = \"abc\" 输出: [0,6] 解释: 起始索引等于 0 的子串是 \"cba\", 它是 \"abc\" 的异位词。 起始索引等于 6 的子串是 \"bac\", 它是 \"abc\" 的异位词。 示例 2: 输入: s = \"abab\", p = \"ab\" 输出: [0,1,2] 解释: 起始索引等于 0 的子串是 \"ab\", 它是 \"ab\" 的异位词。 起始索引等于 1 的子串是 \"ba\", 它是 \"ab\" 的异位词。 起始索引等于 2 的子串是 \"ab\", 它是 \"ab\" 的异位词。 提示: 1 s 和 p 仅包含小写字母 方法一：滑动窗口 思路 根据题目要求，我们需要在字符串 ss 寻找字符串 pp 的异位词。因为字符串 pp 的异位词的长度一定与字符串 pp 的长度相同，所以我们可以在字符串 ss 中构造一个长度为与字符串 pp 的长度相同的滑动窗口，并在滑动中维护窗口中每种字母的数量；当窗口中每种字母的数量与字符串 pp 中每种字母的数量相同时，则说明当前窗口为字符串 pp 的异位词。 算法 在算法的实现中，我们可以使用数组来存储字符串 pp 和滑动窗口中每种字母的数量。 细节 当字符串 ss 的长度小于字符串 pp 的长度时，字符串 ss 中一定不存在字符串 pp 的异位词。但是因为字符串 ss 中无法构造长度与字符串 pp 的长度相同的窗口，所以这种情况需要单独处理。 作者：LeetCode-Solution 链接：https://leetcode.cn/problems/find-all-anagrams-in-a-string/solution/zhao-dao-zi-fu-chuan-zhong-suo-you-zi-mu-xzin/ 来源：力扣（LeetCode） 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 func findAnagrams(s string, p string) (res []int) { size := len(s) sizeP := len(p) if size = sizeP { sCount[s[i-sizeP]-'a']-- } if sCount == pCount{ res = append(res, i+1-sizeP) } } return res } 方法二：优化的滑动窗口 思路和算法 在方法一的基础上，我们不再分别统计滑动窗口和字符串 pp 中每种字母的数量，而是统计滑动窗口和字符串 pp 中每种字母数量的差；并引入变量 differ 来记录当前窗口与字符串 pp 中数量不同的字母的个数，并在滑动窗口的过程中维护它。 在判断滑动窗口中每种字母的数量与字符串 pp 中每种字母的数量是否相同时，只需要判断 differ 是否为零即可。 代码 func findAnagrams(s, p string) (ans []int) { sLen, pLen := len(s), len(p) if sLen Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/49. 字母异位词分组.html":{"url":"blog/algorithm/49. 字母异位词分组.html","title":"49. 字母异位词分组","keywords":"","body":"49. 字母异位词分组 难度中等1190收藏分享切换为英文接收动态反馈 给你一个字符串数组，请你将 字母异位词 组合在一起。可以按任意顺序返回结果列表。 字母异位词 是由重新排列源单词的字母得到的一个新单词，所有源单词中的字母通常恰好只用一次。 示例 1: 输入: strs = [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"] 输出: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]] 示例 2: 输入: strs = [\"\"] 输出: [[\"\"]] 示例 3: 输入: strs = [\"a\"] 输出: [[\"a\"]] 提示： 1 0 strs[i] 仅包含小写字母 同一组的的字符串，排序后的字符串相同 题解： 两个字符串互为字母异位词，当且仅当两个字符串包含的字母相同。同一组字母异位词中的字符串具备相同点，可以使用相同点作为一组字母异位词的标志，使用哈希表存储每一组字母异位词，哈希表的键为一组字母异位词的标志，哈希表的值为一组字母异位词列表。 遍历每个字符串，对于每个字符串，得到该字符串所在的一组字母异位词的标志，将当前字符串加入该组字母异位词的列表中。遍历全部字符串之后，哈希表中的每个键值对即为一组字母异位词。 以下的两种方法分别使用排序和计数作为哈希表的键。 方法一：排序 由于互为字母异位词的两个字符串包含的字母相同，因此对两个字符串分别进行排序之后得到的字符串一定是相同的，故可以将排序之后的字符串作为哈希表的键。 复杂度分析 时间复杂度：O(nk \\log k)O(nklogk)，其中 nn 是 \\textit{strs}strs 中的字符串的数量，kk 是 \\textit{strs}strs 中的字符串的的最大长度。需要遍历 nn 个字符串，对于每个字符串，需要 O(k \\log k)O(klogk) 的时间进行排序以及 O(1)O(1) 的时间更新哈希表，因此总时间复杂度是 O(nk \\log k)O(nklogk)。 空间复杂度：O(nk)O(nk)，其中 nn 是 \\textit{strs}strs 中的字符串的数量，kk 是 \\textit{strs}strs 中的字符串的的最大长度。需要用哈希表存储全部字符串 func groupAnagrams(strs []string) (res [][]string){ size := len(strs) used := map[int]bool{} for i:=0; i 1 { m[k]-- }else{ delete(m, k) } } return len(m) == 0 } func groupAnagrams(strs []string) (res [][]string){ m := make(map[string][]string,0) for _, str := range strs{ s := []byte(str) sort.Slice(s, func(i, j int) bool { return s[i] Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/50. Pow.html":{"url":"blog/algorithm/50. Pow.html","title":"50. Pow","keywords":"","body":"50. Pow(x, n) 难度中等995收藏分享切换为英文接收动态反馈 实现 pow(x, n) ，即计算 x 的整数 n 次幂函数（即，xn ）。 示例 1： 输入：x = 2.00000, n = 10 输出：1024.00000 示例 2： 输入：x = 2.10000, n = 3 输出：9.26100 示例 3： 输入：x = 2.00000, n = -2 输出：0.25000 解释：2-2 = 1/22 = 1/4 = 0.25 提示： -100.0 -231 -104 func myPow(x float64, n int) float64 { if n == 0 || x == 1{ return 1 } if n Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 08:41:49 "},"blog/algorithm/6. Z字形变换.html":{"url":"blog/algorithm/6. Z字形变换.html","title":"6. Z字形变换","keywords":"","body":"Z字形变换 [toc] 作者：LeetCode-Solution 方法一： 利用二维矩阵模拟 设n为字符串s的长度， r = numRows。对于r=1（只有一行）或者 r>=n(只有一列)的情况，答案与s相同，我们可以直接放回s，对于其余情况，考虑到创建一个二维矩阵，然后在矩阵上按Z字形填写字符串s，最后逐行扫描矩阵中的非空字符，组成答案。 根据题意，当我们在矩阵上填写字符串时，会向下填写r个字符，然后向上填写r-2个字符，最后回到第一行，因此Z字形变换周期t=r+r-2=2r-2,每个周期会占用矩阵上的1+r-2=r-1列。 因此我们有 n/t 个周期（最后一个周期视作完整周期），乘上每个周期的列数，得到矩阵的列数c=n/t * (r-1). 创建一个r行c列的矩阵，然后遍历字符串s并按Z字形填写。具体来说，设当前填写的位置（x,y),即举证的x行y列。 初始（x,y) = (0,0), 即举证的左上角。若当前字符下表i满足 i mod t func convert(s string, numRows int) string{ n, r := len(s), numRows if r == 1 || r >= n{ return s } t := r*2-2 c := (n+t-1) /t * (r-1) mat := make([][]byte, r) for i := range mat{ mat[i] = make([]byte, c) } x, y := 0, 0 for i, ch := range s{ mat[x][y] = byte(ch) if i%t 0{ ans = append(ans, ch) } } } return string(ans) } 复杂度分析： 时间复杂度： O（r * n), 其中 r == numRows, n为字符串s 的长度，时间主要消耗在矩阵的创建和遍历上，矩阵的行数为r，列数可以视为O(n) 空间复杂度：O（rn)。矩阵需要O（r n)的空间。 方法二： 压缩矩阵空间 方法一种的矩阵有大量的空间没有被使用，能否被优化 注意到每次矩阵的某一行添加字符时，都会添加到该行上一个字符的右侧，且最后组成答案时只会用到每行的非空字符。因此我们可以将矩阵的每行初始化为一个空列表，每次向某一行添加字符时，添加到该行的列表末尾即可。 func convert(s string, numRows int) string { r := numRows if r == 1 || r >= lens(s){ return s } mat := make([][]byte, r) t, x := r * 2 -2 , 0 for i, ch := range s { mat[x]=append(mat[x], byte(ch)) if i%t 方法三： 直接构造 矩阵中的每个非空字符和s的下标（记作idx），从而直接构造出答案 由于Z字形变换的周期为t = 2r -2， 因此对于矩阵第一行的非空字符，其对应的idx均为t的倍数，即idx=0（mod t); 同理，对于矩阵最后一行的非空字符，应满足 idx=r - 1 (mod t)。 对于矩阵的其余行（行号设为i），每个周期内有2个字符，第一个字符满足 idx = i （mod t), 第二个字符满足，idx = t - i (mod t) 数学规律 对于本题，我们可以不失一般性的将规律推导为「首项」和「公差公式」。 这通常能够有效减少一些判断。 分情况讨论： 对于第一行和最后一行：公差为 2 (n − 1) 的等差数列，首项是 i 对于其他行：两个公差为 2 (n − 1) 的等差数列交替排列，首项分别是 i 和 2 * n − i − 2 作者：AC_OIer func convert(s string, numRows int) string{ n, r := len(s), numRows if r == 1 || r >= n{ return s } t := r *2 -2 ans := make([]byte, 0, n) for i := 0; i 复杂度分析 时间复杂度：O(n)O(n)，其中 nn 为字符串 ss 的长度。ss 中的每个字符仅会被访问一次，因此时间复杂度为 O(n)O(n)。 空间复杂度：O(1)O(1)。返回值不计入空间复杂度。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/94 中序遍历二叉树.html":{"url":"blog/algorithm/94 中序遍历二叉树.html","title":"94 中序遍历二叉树","keywords":"","body":" 给定一个二叉树的根节点 root ，返回 它的 中序 遍历 。 解法一：通过递归实现 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func inorderTraversal(root *TreeNode) []int { var result []int if root == nil{ return nil } if root.Left != nil{ result = append(result, inorderTraversal(root.Left)...) } result = append(result, root.Val) if root.Right != nil{ result = append(result, inorderTraversal(root.Right)...) } return result } 效果 解法一的优化版本： /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func inorderTraversal(root *TreeNode) []int { var result []int var inorder func(root *TreeNode) inorder = func(root *TreeNode){ if root == nil{ return } if root.Left != nil{ inorder(root.Left) } result = append(result, root.Val) if root.Right != nil{ inorder(root.Right) } } inorder(root) return result } 解法二：通过栈实现 func inorderTraversal3(root *TreeNode){ array :=make([]*TreeNode,0) for root != nil || len(array)>0{ for root!=nil{ array = append(array, root) root = root.Left } if len(array)>0{ root = array[len(array)-1] array = array[:len(array)-1] print(root.Val, \"\") root = root.Right } } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:20:37 "},"blog/algorithm/95.不同二叉搜索树.html":{"url":"blog/algorithm/95.不同二叉搜索树.html","title":"95.不同二叉搜索树","keywords":"","body":"95. 不同的二叉搜索树 II 难度中等1234收藏分享切换为英文接收动态反馈 给你一个整数 n ，请你生成并返回所有由 n 个节点组成且节点值从 1 到 n 互不相同的不同 二叉搜索树 。可以按 任意顺序 返回答案。 示例 1： 输入：n = 3 输出：[[1,null,2,null,3],[1,null,3,2],[2,1,3],[3,1,null,null,2],[3,2,null,1]] 示例 2： 输入：n = 1 输出：[[1]] 提示： 1 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/98. 验证搜索二叉树.html":{"url":"blog/algorithm/98. 验证搜索二叉树.html","title":"98. 验证搜索二叉树","keywords":"","body":"方法一: 递归 思路和算法 要解决这道题首先我们要了解二叉搜索树有什么性质可以给我们利用，由题目给出的信息我们可以知道：如果该二叉树的左子树不为空，则左子树上所有节点的值均小于它的根节点的值； 若它的右子树不空，则右子树上所有节点的值均大于它的根节点的值；它的左右子树也为二叉搜索树。 这启示我们设计一个递归函数 helper(root, lower, upper) 来递归判断，函数表示考虑以 root 为根的子树，判断子树中所有节点的值是否都在 (l,r)(l,r) 的范围内（注意是开区间）。如果 root 节点的值 val 不在 (l,r)(l,r) 的范围内说明不满足条件直接返回，否则我们要继续递归调用检查它的左右子树是否满足，如果都满足才说明这是一棵二叉搜索树。 那么根据二叉搜索树的性质，在递归调用左子树时，我们需要把上界 upper 改为 root.val，即调用 helper(root.left, lower, root.val)，因为左子树里所有节点的值均小于它的根节点的值。同理递归调用右子树时，我们需要把下界 lower 改为 root.val，即调用 helper(root.right, root.val, upper)。 函数递归调用的入口为 helper(root, -inf, +inf)， inf 表示一个无穷大的值。 下图展示了算法如何应用在示例 2 上： 复杂度分析 时间复杂度：O(n)O(n)，其中 nn 为二叉树的节点个数。在递归调用的时候二叉树的每个节点最多被访问一次，因此时间复杂度为 O(n)O(n)。 空间复杂度：O(n)O(n)，其中 nn 为二叉树的节点个数。递归函数在递归过程中需要为每一层递归函数分配栈空间，所以这里需要额外的空间且该空间取决于递归的深度，即二叉树的高度。最坏情况下二叉树为一条链，树的高度为 nn ，递归最深达到 nn 层，故最坏情况下空间复杂度为 O(n)O(n) 。 package binary import \"math\" func isValidBST(root *TreeNode) bool { return helper(root, math.MinInt64, math.MaxInt64) } func helper(root *TreeNode, lower, upper int) bool { if root == nil{ return true } if root.Val = upper { return false } return helper(root.Left, lower, root.Val) && helper(root.Right, root.Val, upper) } 方法二：中序遍历 思路和算法 基于方法一中提及的性质，我们可以进一步知道二叉搜索树「中序遍历」得到的值构成的序列一定是升序的，这启示我们在中序遍历的时候实时检查当前节点的值是否大于前一个中序遍历到的节点的值即可。如果均大于说明这个序列是升序的，整棵树是二叉搜索树，否则不是，下面的代码我们使用栈来模拟中序遍历的过程。 可能有读者不知道中序遍历是什么，我们这里简单提及。中序遍历是二叉树的一种遍历方式，它先遍历左子树，再遍历根节点，最后遍历右子树。而我们二叉搜索树保证了左子树的节点的值均小于根节点的值，根节点的值均小于右子树的值，因此中序遍历以后得到的序列一定是升序序列。 func isValidBST2(root *TreeNode) bool { stack := []*TreeNode{} inorder := math.MinInt64 for len(stack) > 0 || root != nil { for root != nil { stack = append(stack, root) root = root.Left } root = stack[len(stack)-1] stack = stack[:len(stack)-1] if root.Val Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/99. 恢复搜索二叉树.html":{"url":"blog/algorithm/99. 恢复搜索二叉树.html","title":"99. 恢复搜索二叉树","keywords":"","body":"99. 恢复二叉搜索树 难度中等 给你二叉搜索树的根节点 root ，该树中的 恰好 两个节点的值被错误地交换。请在不改变其结构的情况下，恢复这棵树 。 示例 1： 输入：root = [1,3,null,null,2] 输出：[3,1,null,null,2] 解释：3 不能是 1 的左孩子，因为 3 > 1 。交换 1 和 3 使二叉搜索树有效。 示例 2： 输入：root = [3,1,4,null,null,2] 输出：[2,1,4,null,null,3] 解释：2 不能在 3 的右子树中，因为 2 提示： 树上节点的数目在范围 [2, 1000] 内 -231 进阶：使用 O(n) 空间复杂度的解法很容易实现。你能想出一个只使用 O(1) 空间的解决方案吗？ 解法一： 中序遍历 找出TreeNode x 和 y，a[i]>a[i+1]， a[j]>a[j+1] swap x, y 值 解法二： 解法三： 思路与算法 方法二中我们不再显示的用数组存储中序遍历的值序列，但是我们会发现我们仍需要 O(H)O(H) 的栈空间，无法满足题目的进阶要求，那么该怎么办呢？这里向大家介绍一种不同于平常递归或迭代的遍历二叉树的方法：Morris 遍历算法，该算法能将非递归的中序遍历空间复杂度降为 O(1)O(1)。 Morris 遍历算法整体步骤如下（假设当前遍历到的节点为 xx）： 如果 xx 无左孩子，则访问 xx 的右孩子，即 x = x.{right}x=x.right。 如果 xx 有左孩子，则找到 xx 左子树上最右的节点（即左子树中序遍历的最后一个节点，xx 在中序遍历中的前驱节点），我们记为 \\textit{predecessor}predecessor。根据 \\textit{predecessor}predecessor 的右孩子是否为空，进行如下操作。 如果 \\textit{predecessor}predecessor 的右孩子为空，则将其右孩子指向 xx，然后访问 xx 的左孩子，即 x = x.\\textit{left}x=x.left。 如果 \\textit{predecessor}predecessor 的右孩子不为空，则此时其右孩子指向 xx，说明我们已经遍历完 xx 的左子树，我们将 \\textit{predecessor}predecessor 的右孩子置空，然后访问 xx 的右孩子，即 x = x.\\textit{right}x=x.right。 重复上述操作，直至访问完整棵树。 其实整个过程我们就多做一步：将当前节点左子树中最右边的节点指向它，这样在左子树遍历完成后我们通过这个指向走回了 xx，且能再通过这个知晓我们已经遍历完成了左子树，而不用再通过栈来维护，省去了栈的空间复杂度。 了解完这个算法以后，其他地方与方法二并无不同，我们同样也是维护一个 \\textit{pred}pred 变量去比较即可，具体实现可以看下面的代码，这里不再赘述。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/LRU && LFU.html":{"url":"blog/algorithm/LRU && LFU.html","title":"LRU && LFU","keywords":"","body":"简述 LRU : 缓存中淘汰最久未使用的是数据 通过维护一个列表，将使用过的数据放在最前，按照这种队列头部插入的方法，形成按使用时间排序的队列。当数据达到上限后，淘汰队尾元素。 type LRUCache struct{ Capacility int keys map[string]*Element list *list.List } type Element struct{ pre, next *Element head *List value interface{} } LFU：缓存中淘汰最近使用次数最少的 维护一个元素队列，元素每使用一次， 计数器+1，按计数器使用次数排序，淘汰队尾元素。 链接 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/djst.html":{"url":"blog/algorithm/djst.html","title":"Djst","keywords":"","body":"最短路径—Dijkstra算法和Floyd算法 注意：以下代码 只是描述思路，没有测试过！！ Dijkstra算法 1.定义概览 Dijkstra(迪杰斯特拉)算法是典型的单源最短路径算法，用于计算一个节点到其他所有节点的最短路径。主要特点是以起始点为中心向外层层扩展，直到扩展到终点为止。Dijkstra算法是很有代表性的最短路径算法，在很多专业课程中都作为基本内容有详细的介绍，如数据结构，图论，运筹学等等。注意该算法要求图中不存在负权边。 问题描述：在无向图 G=(V,E) 中，假设每条边 E[i] 的长度为 w[i]，找到由顶点 V0 到其余各点的最短路径。（单源最短路径） 2.算法描述 1)算法思想：设G=(V,E)是一个带权有向图，把图中顶点集合V分成两组，第一组为已求出最短路径的顶点集合（用S表示，初始时S中只有一个源点，以后每求得一条最短路径 , 就将加入到集合S中，直到全部顶点都加入到S中，算法就结束了），第二组为其余未确定最短路径的顶点集合（用U表示），按最短路径长度的递增次序依次把第二组的顶点加入S中。在加入的过程中，总保持从源点v到S中各顶点的最短路径长度不大于从源点v到U中任何顶点的最短路径长度。此外，每个顶点对应一个距离，S中的顶点的距离就是从v到此顶点的最短路径长度，U中的顶点的距离，是从v到此顶点只包括S中的顶点为中间顶点的当前最短路径长度。 2)算法步骤： a.初始时，S只包含源点，即S＝{v}，v的距离为0。U包含除v外的其他顶点，即:U={其余顶点}，若v与U中顶点u有边，则正常有权值，若u不是v的出边邻接点，则权值为∞。 b.从U中选取一个距离v最小的顶点k，把k，加入S中（该选定的距离就是v到k的最短路径长度）。 c.以k为新考虑的中间点，修改U中各顶点的距离；若从源点v到顶点u的距离（经过顶点k）比原来距离（不经过顶点k）短，则修改顶点u的距离值，修改后的距离值的顶点k的距离加上边上的权。 d.重复步骤b和c直到所有顶点都包含在S中。 执行动画过程如下图 3.算法代码实现： ;) const int MAXINT = 32767; const int MAXNUM = 10; int dist[MAXNUM]; int prev[MAXNUM]; int A[MAXUNM][MAXNUM]; void Dijkstra(int v0) { 　　bool S[MAXNUM]; // 判断是否已存入该点到S集合中 int n=MAXNUM; 　　for(int i=1; i;) 4.算法实例 先给出一个无向图 用Dijkstra算法找出以A为起点的单源最短路径步骤如下 Floyd算法 1.定义概览 Floyd-Warshall算法（Floyd-Warshall algorithm）是解决任意两点间的最短路径的一种算法，可以正确处理有向图或负权的最短路径问题，同时也被用于计算有向图的传递闭包。Floyd-Warshall算法的时间复杂度为O(N3)，空间复杂度为O(N2)。 2.算法描述 1)算法思想原理： Floyd算法是一个经典的动态规划算法。用通俗的语言来描述的话，首先我们的目标是寻找从点i到点j的最短路径。从动态规划的角度看问题，我们需要为这个目标重新做一个诠释（这个诠释正是动态规划最富创造力的精华所在） 从任意节点i到任意节点j的最短路径不外乎2种可能，1是直接从i到j，2是从i经过若干个节点k到j。所以，我们假设Dis(i,j)为节点u到节点v的最短路径的距离，对于每一个节点k，我们检查Dis(i,k) + Dis(k,j) 2).算法描述： a.从任意一条单边路径开始。所有两点之间的距离是边的权，如果两点之间没有边相连，则权为无穷大。 　　 b.对于每一对顶点 u 和 v，看看是否存在一个顶点 w 使得从 u 到 w 再到 v 比己知的路径更短。如果是更新它。 3).Floyd算法过程矩阵的计算----十字交叉法 方法：两条线，从左上角开始计算一直到右下角 如下所示 给出矩阵，其中矩阵A是邻接矩阵，而矩阵Path记录u,v两点之间最短路径所必须经过的点 相应计算方法如下： 最后A3即为所求结果 3.算法代码实现 ;) typedef struct { char vertex[VertexNum]; //顶点表 int edges[VertexNum][VertexNum]; //邻接矩阵,可看做边表 int n,e; //图中当前的顶点数和边数 }MGraph; void Floyd(MGraph g) { 　　int A[MAXV][MAXV]; 　　int path[MAXV][MAXV]; 　　int i,j,k,n=g.n; 　　for(i=0;i(A[i][k]+A[k][j])) 　　{ 　　A[i][j]=A[i][k]+A[k][j]; 　　path[i][j]=k; 　 } 　} } ;) 算法时间复杂度:O(n3) Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/heap.html":{"url":"blog/algorithm/heap.html","title":"Heap","keywords":"","body":"heap 1.什么是堆 堆分为最大堆和最小堆，其实就是完全二叉树。 最大堆要求节点的元素都要不小于其孩子，最小堆要求节点元素都不大于其左右孩子，两者对左右孩子的大小关系不做任何要求，其实很好理解。 有了上面的定义，我们可以得知，处于最大堆的根节点的元素一定是这个堆中的最大值。 堆排序是利用堆这种数据结构而设计的一种排序算法，堆排序是一种选择排序，它的最坏，最好，平均时间复杂度均为O(nlogn)，它也是不稳定排序。首先简单了解下堆结构。 性质： 一个完全二叉树的第一个非叶子节点的索引 (maxIndex-1)/2 2.构建最小二叉堆 用随机生成的数组,以依次插入的方式构建一个最小二叉堆 package main import ( \"fmt\" \"math\" \"math/rand\" ) // 堆是一个完全二叉树，必须满足根节点大于两个叶子节点 // 从一个数组中构建堆：1.转二叉树 2.对半 3.siftDown type ElementType int type Heap struct { List []ElementType Size int Capacity int } func (h *Heap) size() int { return len(h.List) } // 入列, 从堆尾插入，然后向上移动 func (h *Heap) Push(data ElementType) { h.List = append(h.List, data) j := h.size() - 1 k := (j - 1) / 2 for j > 0 { if h.List[k] > h.List[j] { h.List[k], h.List[j] = h.List[j], h.List[k] j = k k = (j - 1) / 2 } else { break } } } // 出列 func (h *Heap) Pop() ElementType { minData := h.List[0] if h.size() == 1 { h.List = []ElementType{} return minData } lastData := h.List[h.size()-1] h.List = h.List[0 : h.size()-1] h.List[0] = lastData h.ShiftDown() return minData } // 向下移动 func (h *Heap) ShiftDown() { c := h.size() for x := 0; x = c { break } if min+1 = max { panic(\"min > max\") } x := rand.Intn(max-min) + min return ElementType(x) } func main() { // 无序数组 var heap Heap for i := 0; i 0; x-- { fmt.Print(\" \") } x += 1 } fmt.Print(heap.List[i], \" \") if math.Pow(2, float64(line))-2 == float64(i) { line += 1 fmt.Println() } } fmt.Println() fmt.Println(\"排序\") for heap.size() > 0 { fmt.Print(heap.Pop(), \" \") } } 3. 堆排序步骤 2.1 找到第一个非叶子节点 将一个数组假设成一个二叉堆，索引从0开始， 如下图first node is (8-1)/2=3 2.2 shift down 2.3 依次对其他非叶子节点重复第2步 算法复杂度: 将n个元素逐个插入到一个空堆中，算法复杂度是O(nlogn) heapify的过程， 算法复杂度为O(n) code package main import ( \"fmt\" \"math\" ) // 堆是一个完全二叉树，必须满足根节点大于两个叶子节点 // 从一个数组中构建堆：1.转二叉树 2.对半 3.siftDown type ElementType int type Heap struct { List []ElementType } // 入列, 从堆尾插入，然后向上移动 func (h *Heap)Push(data ElementType) { j := len(h.List) h.List = append(h.List, data) k := (j-1)/2 for k>-1 { if h.List[k] > h.List[j] { h.List[k], h.List[j] = h.List[j], h.List[k] j = k k = (j-1)/2 } else { break } } } // 出列 func (h *Heap)Pop() ElementType { minData := h.List[0] j := len(h.List) if j == 1{ h.List = []ElementType{} return minData } lastData := h.List[j-1] h.List = h.List[0:j-1] h.ShiftDown(lastData,0) return minData } // 向下移动 func (heap *Heap)ShiftDown(data ElementType, begin int){ // 从堆顶构建最小堆, 在堆顶放入一个元素 //循环条件 begin =0; root--{ heap.ShiftDown(list[root],root) } fmt.Println(\"往堆中插入新的数据: -100, 30, -15, -95\") heap.Push(-100) heap.Push(30) heap.Push(-15) heap.Push(-95) fmt.Println(\"打印堆\") level := 1 for i, data := range(heap.List){ maxIndex := 1* math.Pow(2,float64(level)) -1 if i+1 == int(maxIndex){ level += 1 fmt.Printf(\"%v\\r\\n\",data) }else{ fmt.Printf(\"%v \",data) } } fmt.Println(\"\\r\\n 从堆中取数据\") for i:=len(heap.List); i>0; i--{ fmt.Println(len(heap.List), \" \",heap.Pop()) } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/readme.html":{"url":"blog/algorithm/readme.html","title":"Readme","keywords":"","body":"Algorithm Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/二叉树最大距离的2个节点.html":{"url":"blog/algorithm/二叉树最大距离的2个节点.html","title":"二叉树最大距离的2个节点","keywords":"","body":"编程之美：求二叉树中节点的最大距离 1.问题描述 写一个程序求一棵二叉树相距最远的两个节点之间的距离 如下图： 2.分析与解法 对于任意一个节点，以该节点为根，假设这个根有k个孩子节点，那么距离最远的两个节点U与V之间的路径与这个根节点的关系有两种。 1).若路径经过Root，则U和V属于不同子树的，且它们都是该子树中到根节点最远的节点，否则跟它们的距离最远相矛盾 2).如果路径不经过Root，那么它们一定属于根的k个子树之一，并且它们也是该子树中相距最远的两个顶点 因此，问题就可以转化为在字数上的解，从而能够利用动态规划来解决。 设第K棵子树中相距最远的两个节点：Uk和Vk，其距离定义为d(Uk,Vk)，那么节点Uk或Vk即为子树K到根节点Rk距离最长的节点。不失一般性，我们设Uk为子树K中道根节点Rk距离最长的节点，其到根节点的距离定义为d(Uk,R)。取d(Ui,R)(1 3.代码实现 编程之美给出的代码如下： ;) //数据结构定义 struct NODE { NODE* pLeft; //左孩子 NODE* pRight; //右孩子 int nMaxLeft; //左孩子中的最长距离 int nMaxRight; //右孩子中的最长距离 char chValue; //该节点的值 }; int nMaxLen=0; //寻找树中最长的两段距离 void FindMaxLen(NODE* pRoot) { //遍历到叶子节点，返回 if(pRoot==NULL) { return; } //如果左子树为空，那么该节点的左边最长距离为0 if(pRoot->pLeft==NULL) { pRoot->nMaxLeft=0; } //如果右子树为空，那么该节点的右边最长距离为0 if(pRoot->pRight==NULL) { pRoot->nMaxRight=0; } //如果左子树不为空，递归寻找左子树最长距离 if(pRoot->pLeft!=NULL) { FindMaxLen(pRoot->pLeft); } //如果右子树不为空，递归寻找右子树最长距离 if(pRoot->pRight!=NULL) { FindMaxLen(pRoot->pRight); } if(pRoot->pLeft!=NULL) { int nTempMax=0; if(pRoot->pLeft->nMaxLeft > pRoot->pLeft->nMaxRight) { nTempMax=pRoot->pLeft->nMaxLeft; } else { nTempMax=pRoot->pLeft->nMaxRight; } pRoot->nMaxLeft=nTempMax+1; } //计算右子树最长节点距离 if(pRoot->pRight!=NULL) { int nTempMax=0; if(pRoot->pRight->nMaxLeft > pRoot->pRight->nMaxRight) { nTempMax= pRoot->pRight->nMaxLeft; } else { nTempMax= pRoot->pRight-> nMaxRight; } pRoot->nMaxRight=nTempMax+1; } //更新最长距离 if(pRoot->nMaxLeft+pRoot->nMaxRight > nMaxLen) { nMaxLen=pRoot->nMaxLeft+pRoot->nMaxRight; } } ;) 依据二叉树寻找最大深度的常规思想，又有代码如下： ;) struct BTNode { int data; BTNode* pLeft; BTNode* pRight; }; int maxDis = -1; int findMaxDis(BTNode* pRoot) { if(pRoot == NULL) return 0; int maxLeft = findMaxDis(pRoot->pLeft) ; int maxRight = findMaxDis(pRoot->pRight); if(maxLeft + maxRight > maxDis) { maxDis = maxLeft + maxRight; } return maxLeft > maxRight ? maxLeft+1 : maxRight+1; } ;) 后一段代码为自写 没有验证其正确性。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/回文.html":{"url":"blog/algorithm/回文.html","title":"回文","keywords":"","body":"O(n)回文子串（Manacher）算法 资料来源网络 参见：http://www.felix021.com/blog/read.php?2040 问题描述： 输入一个字符串，求出其中最大的回文子串。子串的含义是：在原串中连续出现的字符串片段。回文的含义是：正着看和倒着看相同，如abba和yyxyy。 解析： 这里介绍O(n)回文子串（Manacher）算法 算法基本要点：首先用一个非常巧妙的方式，将所有可能的奇数/偶数长度的回文子串都转换成了奇数长度：在每个字符的两边都插入一个特殊的符号。比如 abba 变成 #a#b#b#a#， aba变成 #a#b#a#。 为了进一步减少编码的复杂度，可以在字符串的开始加入另一个特殊字符，这样就不用特殊处理越界问题，比如$#a#b#a#。 下面以字符串12212321为例，经过上一步，变成了 S[] = \"$#1#2#2#1#2#3#2#1#\"; 然后用一个数组 P[i] 来记录以字符S[i]为中心的最长回文子串向左/右扩张的长度（包括S[i]），比如S和P的对应关系： S # 1 # 2 # 2 # 1 # 2 # 3 # 2 # 1 # P 1 2 1 2 5 2 1 4 1 2 1 6 1 2 1 2 1 (p.s. 可以看出，P[i]-1正好是原字符串中回文串的总长度） 下面计算P[i]，该算法增加两个辅助变量id和mx，其中id表示最大回文子串中心的位置，mx则为id+P[id]，也就是最大回文子串的边界。 这个算法的关键点就在这里了：如果mx > i，那么P[i] >= MIN(P[2 * id - i], mx - i)。 具体代码如下： ;) if(mx > i) { p[i] = (p[2*id - i] ;) 当 mx - i > P[j] 的时候，以S[j]为中心的回文子串包含在以S[id]为中心的回文子串中，由于 i 和 j 对称，以S[i]为中心的回文子串必然包含在以S[id]为中心的回文子串中，所以必有 P[i] = P[j]，见下图。 当 P[j] > mx - i 的时候，以S[j]为中心的回文子串不完全包含于以S[id]为中心的回文子串中，但是基于对称性可知，下图中两个绿框所包围的部分是相同的，也就是说以S[i]为中心的回文子串，其向右至少会扩张到mx的位置，也就是说 P[i] >= mx - i。至于mx之后的部分是否对称，就只能一个一个匹配了。 对于 mx 下面给出原文，进一步解释算法为线性的原因 源代码： ;) #include #include #include using namespace std; void findBMstr(string& str) { int *p = new int[str.size() + 1]; memset(p, 0, sizeof(p)); int mx = 0, id = 0; for(int i = 1; i i) { p[i] = (p[2*id - i] mx) { mx = i + p[i]; id = i; } } int max = 0, ii; for(int i = 1; i max) { ii = i; max = p[i]; } } max--; int start = ii - max ; int end = ii + max; for(int i = start; i ;) 执行结果： Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/回溯.html":{"url":"blog/algorithm/回溯.html","title":"回溯","keywords":"","body":"我所理解的回溯 回溯本质是暴力搜索，在问题的解空间树中，用 DFS 的方式，从根节点出发搜索整个解空间。 如果要找出所有的解，则要搜索整个子树，如果只用找出一个解，则搜到一个解就可以结束搜索。 「找出所有可能的组合」的问题，适合用回溯算法。 回溯算法有三个要点： 选择 决定了你每个节点有哪些分支，帮助你构建出解的空间树。 本题的选择就是，每个数字对应的多个字母，选择翻译成其中一个字母，就继续递归 约束条件 用来剪枝，剪去不满足约束条件的子树，避免无效的搜索。这题好像没怎么体现 目标 决定了何时捕获解，或者剪去得不到解的子树，提前回溯。扫描数字的指针到头了就可以将解加入解集了。 作者：xiao_ben_zhu 链接：https://leetcode.cn/problems/letter-combinations-of-a-phone-number/solution/shou-hua-tu-jie-liang-chong-jie-fa-dfshui-su-bfsya/ 来源：力扣（LeetCode） 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 概念 回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。许多复杂的，规模较大的问题都可以使用回溯法，有“通用解题方法”的美称。 基本思想 从一条路往前走，能进则进，不能进则退回来，换一条路再试。回溯在迷宫搜索中使用很常见，就是这条路走不通，然后返回前一个路口，继续下一条路。回溯算法说白了就是穷举法。不过回溯算法使用剪枝函数，剪去一些不可能到达 最终状态（即答案状态）的节点，从而减少状态空间树节点的生成。回溯法是一个既带有系统性又带有跳跃性的的搜索算法。它在包含问题的所有解的解空间树中，按照深度优先的策略，从根结点出发搜索解空间树。算法搜索至解空间树的任一结点时，总是先判断该结点是否肯定不包含问题的解。如果肯定不包含，则跳过对以该结点为根的子树的系统搜索，逐层向其祖先结点回溯。否则，进入该子树，继续按深度优先的策略进行搜索。回溯法在用来求问题的所有解时，要回溯到根，且根结点的所有子树都已被搜索遍才结束。而回溯法在用来求问题的任一解时，只要搜索到问题的一个解就可以结束。这种以深度优先的方式系统地搜索问题的解的算法称为回溯法，它适用于解一些组合数较大的问题。 解题步骤 1.定义一个解空间，它包含问题的解； 2.利用适于搜索的方法组织解空间； 3.利用深度优先法搜索解空间； 4.利用限界函数避免移动到不可能产生解的子空间。 练习 1.八皇后问题(递归与回溯) 该问题是国际西洋棋棋手马克斯·贝瑟尔于1848年提出：在8×8格的国际象棋上摆放八个皇后，使其不能互相攻击，即任意两个皇后都不能处于同一行、同一列或同一斜线上，问有多少种摆法。 思路： 用flag[n]=col表示第n行的皇后放在了第col列，这样只需要判断列和上下对角线有没有皇后就可以了，这里上下对角线都为15份，这样可以用两个一维数组来表示上下对角线是否有皇后 上对角线：n-col+7表示数组对角线 下角标：n+col表示数组下标 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/回溯算法/39. 组合数.html":{"url":"blog/algorithm/回溯算法/39. 组合数.html","title":"39. 组合数","keywords":"","body":"39. 组合总和 难度中等2060收藏分享切换为英文接收动态反馈 给你一个 无重复元素 的整数数组 candidates 和一个目标整数 target ，找出 candidates 中可以使数字和为目标数 target 的 所有 不同组合 ，并以列表形式返回。你可以按 任意顺序 返回这些组合。 candidates 中的 同一个 数字可以 无限制重复被选取 。如果至少一个数字的被选数量不同，则两种组合是不同的。 对于给定的输入，保证和为 target 的不同组合数少于 150 个。 示例 1： 输入：candidates = [2,3,6,7], target = 7 输出：[[2,2,3],[7]] 解释： 2 和 3 可以形成一组候选，2 + 2 + 3 = 7 。注意 2 可以使用多次。 7 也是一个候选， 7 = 7 。 仅有这两种组合。 示例 2： 输入: candidates = [2,3,5], target = 8 输出: [[2,2,2,2],[2,3,3],[3,5]] 示例 3： 输入: candidates = [2], target = 1 输出: [] 提示： 1 1 candidate 中的每个元素都 互不相同 1 思路分析：根据示例 1：输入: candidates = [2, 3, 6, 7]，target = 7。 候选数组里有 2，如果找到了组合总和为 7 - 2 = 5 的所有组合，再在之前加上 2 ，就是 7 的所有组合； 同理考虑 3，如果找到了组合总和为 7 - 3 = 4 的所有组合，再在之前加上 3 ，就是 7 的所有组合，依次这样找下去。 基于以上的想法，可以画出如下的树形图。建议大家自己在纸上画出这棵树，这一类问题都需要先画出树形图，然后编码实现。 编码通过 深度优先遍历 实现，使用一个列表，在 深度优先遍历 变化的过程中，遍历所有可能的列表并判断当前列表是否符合题目的要求，成为「回溯算法」（个人理解，非形式化定义）。 回溯算法的总结我写在了「力扣」第 46 题（全排列）的题解 《回溯算法入门级详解 + 经典例题列表（持续更新）》 中，如有需要请前往观看。 画出树形图 2020 年 9 月 9 日补充：以下给出的是一种树形图的画法。对于组合来说，还可以根据一个数选和不选画树形图，请参考 官方题解 或者 @elegant-pike 的 评论。 以输入：candidates = [2, 3, 6, 7], target = 7 为例： 说明： 以 target = 7 为 根结点 ，创建一个分支的时 做减法 ； 每一个箭头表示：从父亲结点的数值减去边上的数值，得到孩子结点的数值。边的值就是题目中给出的 candidate 数组的每个元素的值； 减到 00 或者负数的时候停止，即：结点 00 和负数结点成为叶子结点； 所有从根结点到结点 00 的路径（只能从上往下，没有回路）就是题目要找的一个结果。 这棵树有 44 个叶子结点的值 00，对应的路径列表是 [[2, 2, 3], [2, 3, 2], [3, 2, 2], [7]]，而示例中给出的输出只有 [[7], [2, 2, 3]]。即：题目中要求每一个符合要求的解是 不计算顺序 的。下面我们分析为什么会产生重复。 针对具体例子分析重复路径产生的原因（难点） 友情提示：这一部分我的描述是晦涩难懂的，建议大家先自己观察出现重复的原因，进而思考如何解决。 产生重复的原因是：在每一个结点，做减法，展开分支的时候，由于题目中说 每一个元素可以重复使用，我们考虑了 所有的 候选数，因此出现了重复的列表。 一种简单的去重方案是借助哈希表的天然去重的功能，但实际操作一下，就会发现并没有那么容易。 可不可以在搜索的时候就去重呢？答案是可以的。遇到这一类相同元素不计算顺序的问题，我们在搜索的时候就需要 按某种顺序搜索。具体的做法是：每一次搜索的时候设置 下一轮搜索的起点 begin，请看下图。 即：从每一层的第 22 个结点开始，都不能再搜索产生同一层结点已经使用过的 candidate 里的元素。 友情提示：如果题目要求，结果集不计算顺序，此时需要按顺序搜索，才能做到不重不漏。「力扣」第 47 题（ 全排列 II ）、「力扣」第 15 题（ 三数之和 ）也使用了类似的思想，使得结果集没有重复。 参考代码 1： 补充：参考代码 1 和参考代码 2 的 Python 部分，没有严格按照回溯算法来写，这里需要了解的知识点是： Python3 的 [1, 2] + [3] 语法生成了新的列表，一层一层传到根结点以后，直接 res.append(path) 就可以了； 基本类型变量在传参的时候，是复制，因此变量值的变化在参数里体现就行，所以 Python3 的代码看起来没有「回溯」这个步骤。 func combinationSum(candidates []int, target int) (res [][]int) { var dfs func(candidates []int, target int, rs []int) dfs = func(candidates []int, target int, rs []int){ if target Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/回溯算法/40. 组合数II.html":{"url":"blog/algorithm/回溯算法/40. 组合数II.html","title":"40. 组合数II","keywords":"","body":"40. 组合总和 II 作者：liweiwei1419 给定一个候选人编号的集合 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的每个数字在每个组合中只能使用 一次 。 注意：解集不能包含重复的组合。 示例 1: 输入: candidates = [10,1,2,7,6,1,5], target = 8, 输出: [ [1,1,6], [1,2,5], [1,7], [2,6] ] 示例 2: 输入: candidates = [2,5,2,1,2], target = 5, 输出: [ [1,2,2], [5] ] 提示: 1 1 1 解题思路： 一句话题解：按顺序搜索，设置合理的变量，在搜索的过程中判断是否会出现重复集结果。重点理解对输入数组排序的作用和 参考代码 中大剪枝和小剪枝 的意思。 与第 39 题（组合之和）的差别 这道题与上一问的区别在于： 第 39 题：candidates 中的数字可以无限制重复被选取； 第 40 题：candidates 中的每个数字在每个组合中只能使用一次。 相同点是：相同数字列表的不同排列视为一个结果。 如何去掉重复的集合（重点） 为了使得解集不包含重复的组合。有以下 2 种方案： 使用 哈希表 天然的去重功能，但是编码相对复杂； 这里我们使用和第 39 题和第 15 题（三数之和）类似的思路：不重复就需要按 顺序 搜索， 在搜索的过程中检测分支是否会出现重复结果 。注意：这里的顺序不仅仅指数组 candidates 有序，还指按照一定顺序搜索结果。 由第 39 题我们知道，数组 candidates 有序，也是 深度优先遍历 过程中实现「剪枝」的前提。 将数组先排序的思路来自于这个问题：去掉一个数组中重复的元素。很容易想到的方案是：先对数组 升序 排序，重复的元素一定不是排好序以后相同的连续数组区域的第 11 个元素。也就是说，剪枝发生在：同一层数值相同的结点第 22、33 ... 个结点，因为数值相同的第 11 个结点已经搜索出了包含了这个数值的全部结果，同一层的其它结点，候选数的个数更少，搜索出的结果一定不会比第 11 个结点更多，并且是第 11 个结点的子集。（说明：这段文字很拗口，大家可以结合具体例子，在纸上写写画画进行理解。） 说明： 解决这个问题可能需要解决 第 15 题（三数之和）、 第 47 题（全排列 II）、 第 39 题（组合之和）的经验； 对于如何去重还不太清楚的朋友，可以参考当前题解的 高赞置顶评论 。 感谢用户 @rmokerone 提供的 C++ 版本的参考代码。 func combinationSum2(candidates []int, target int) (res [][]int) { sort.Ints(candidates) x := target var dfs func(begin, target int, rs []int) dfs = func(begin, target int, rs []int){ if target 0 && candidates[i] ==candidates[i-1] && target == x{ continue } x := target-k if x Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/回溯算法/46. 全排列.html":{"url":"blog/algorithm/回溯算法/46. 全排列.html","title":"46. 全排列","keywords":"","body":"46. 全排列 难度中等2113 给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 示例 1： 输入：nums = [1,2,3] 输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] 示例 2： 输入：nums = [0,1] 输出：[[0,1],[1,0]] 示例 3： 输入：nums = [1] 输出：[[1]] 提示： 1 -10 nums 中的所有整数 互不相同 【回溯法】采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答时，它将取消上一步甚至是上几步的计算，在通过其他的可能的分步解答再次尝试寻找问题的答案。回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况： 找到一个可能存在的正确答案 在尝试了所有可能的分步方法后宣告问题没有答案。 【深度优先搜索】Depth-First-Search，DFS 是一种用于遍历或搜索树或图的算法。这个算法会尽可能深的搜索树的分支。当节点v的所在边都已被探寻过，搜索将 回溯 到发现节点v的那条边的起始节点。这一过程一直进行到已经发现从源结点可达的所有结点为止。如果还存在未被发现的结点，则选择其中一个作为源结点并重复以上过程，整个进程反复进行直到所有节点都被访问为止。 【与动态规划的区别】 共同点 求解一个问题分为很多步骤 每一个步骤可以有多种选择 不同点 动态规划只需要球我们评估最优解是多少，最优解对应的具体解是什么并不要求 回溯法可以搜索得到所有的方案，但是本质上它是一种遍历算法，时间复杂度很高。 【从全排列问题开始理解回溯算法】 我们尝试在纸上写 3个数字、44个数字、5个数字的全排列，相信不难找到这样的方法。以数组 [1, 2, 3] 的全排列为例。 每个结点表示了求解全排列问题的不同阶段，这些阶段通过变量的【不同的值】体现，这些变量的不同的值，称之为【状态】；使用深度优先遍历有【回头】的过程，在【回头】以后，状态变量需要设置成为和先前一样，因此在回到上一层结点的过程中，需要撤销上一次的选择，这个操作称为【状态重置】； 【深度优先遍历】借助系统栈空间，保存所需要的状态变量，在编码中只需要注意遍历到相应的结点的时候，状态变量的值是正确的，具体的做法是：往下走一层的时候，path 变量在尾部追加，而往回走的时候，需要撤销上一次的选择，也是在尾部操作，因此path变量是一个栈。 深度优先遍历通过「回溯」操作，实现了全局使用一份状态变量的效果。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/大神.html":{"url":"blog/algorithm/大神.html","title":"大神","keywords":"","body":"https://blog.csdn.net/fuxuemingzhu/article/details/105183554 大家好，我是负雪明烛。今天跟大家聊一聊「LeetCode应该怎么刷？」这个话题。 我是大二的时候开始接触 LeetCode 的，那时候 LeetCode 只有 400 题，我本来打算每天做 1 个题，但是由于当时觉得题目太难没坚持下去。在大四考研后和研一研二的两年左右的时间，我刷了 800 多道题（现在已经900多），并且大多数题目做了 2~3 遍。那么我是怎么做到的呢？ 一、入门篇 刷题姿势 刚开刷 LeetCode 时遇到二叉树翻转，想了一天也没明白，当时无比痛苦。因为我的方法不对，我总想着在脑子里面想明白再写，在白纸上不停地模拟二叉树树翻转的每一步，还想着用本地 IDE 写个二叉树结构进行Debug，现在看来都是走了弯路。 大部分新手应该是只学过课本上的一些数据结构和算法的知识，还没有实际刷题经验，因此非常痛苦。 对于新人而言，不应该自己死抠一个题目，如果想了一会没有任何思路，就应该果断看别人怎么写的。在理解了别人的做法之后，再凭理解和记忆在 LeetCode 的代码框里敲一遍。 学习 = 学 + 习。知识是学出来的，不是在自己脑子里蹦出来的；学过之后，还要自己动手练习。新手要勇敢地、经常地学习别人的解法和答案，然后凭理解敲代码练习。只要度过刷题初期的痛苦，后面就会越刷越快。 基础知识 需要掌握常用的数据结构和算法的思想和适用场景。 学习基础知识，我推荐 《算法（第4版）》。这个书不用全部细看，可以只看重点，比如前面的 Java 知识不用看，数学推导部分可以不用看。 再推荐一本侯捷的 《STL源码剖析》，这本书对理解C++ STL有重大帮助，看了之后绝对会对数据结构和算法有更深的理解，我看完这本书之后感觉相见恨晚啊。 刷题顺序 合理的刷题顺序能降低难度，帮助我们在有限的时间里获得最快的成长。 LeetCode 现在将近 2000 道题，基本没有人能够全部刷完，而且对于参加面试者来说也没有必要刷特别多的题。许多人在面试前刷了 200 道题，基本够了；准备更充分的人，大概会刷 400 道题；能刷 600 道题目以上的，基本上国内公司的 Offer 都能收获到一大堆。 我推荐的刷题顺序的规则是： 按分类刷；每个分类从 Easy 到 Medium 顺序刷； 优先刷 树、链表、二分查找、DFS、BFS 等面试常考类型； 优先刷题号靠前的题目； 优先刷点赞较多的题目； 如果基本上能做到 Easy 题 100% 能做对，Medium 题经过思考或与面试官交流后能做对，基本就能拿到 Offer。在实际面试过程中，很少出 Hard 题，视能力刷。 跟别人学习 向别人学习是非常必要的。 1）看别人的题解 主要看别人在解决这个题目的思路是什么。无论这个题你会不会，都要看下别人的解法，或许有新收获。 推荐的博客作者有： 负雪明烛：5 年在 CSDN 上更新了 800多道题解，收获 160万 阅读。在中文力扣日更题解。 李威威：中文力扣的大 V，对力扣题目掌握很全面，写得题解非常详细，对题目举一反三。 花花酱：基本每个题都有博客和视频，强烈推荐看他的视频。 Grandyang：在博客园更新了几乎所有力扣题目，收获了 1200万 阅读。 最近我在刷中文版的力扣，题解区的答案质量非常高。比较推荐的博主有：力扣官方题解，负雪明烛，李威威，zerotrac，Krahets。 我恬不知耻地推荐一下自己（负雪明烛）的题解，我最近已经连续在中文力扣日更「每日一题」题解 20 天。最近利用动图帮助大家理清做题思路，点赞和阅读数都比较高。 除了题解区以外，如果想看博客上面的题解，可以用搜索引擎搜题目和博主名。想看负雪明烛的「two sum」题解，那么搜索方式就是在关键词之后加上「fuxuemingzhu」： 2）看别人的总结 这部分包括算法讲解、套路整理、刷题模板等。 「算法题 = 思路 + 模板」，思路需要通过看别人的解答以及讲解获得，模板就是做题的套路，既可以自己总结，也可以看别人总结好的。 比如负雪明烛的【LeetCode】代码模板，刷题必会。 也比如说 AlgoWiki ： 当然推荐每个人在做题的过程中都整理一份自己的总结，用自己的方式总结好知识点和模板。 做好笔记 写作过程能更好地帮助我们理清思路，也能帮助我们再做此题时快速想起以前的做法，还能见证我们自己的成长。 在五六年前我刚开始刷题时，就把每个做过的题目记录在CSDN上，现在我的博客浏览量已经将近 161万 了。 任何题，无论难度，我都记录题目、想法、代码。虽然经常写博客的时间比写题的时间还多，但是把自己的想法讲解一遍才是真的懂了，更方便了自己之后看、以及大家交流。经常看到自己几年前写的愚蠢代码，然后感叹自己确实有进步了。 在 B站 有个小姐姐讲了小白如何上手LeetCode，也演示了如何用 iPad 做笔记，值得一看。 程序媛分享 | LeetCode小白如何上手刷题？iPad学习方法 | 刷题清单 | 新手指南 | 刷题找工作 | IT类 交流和监督 刷题最大的障碍是自己。特别是新手，很可能由于刚接触 LeetCode 感觉太难就没有毅力坚持下去，导致半途而废。而且，刷题更重要的是坚持，做题的感觉都需要手感进行保持的。 所以，如果能有个组织交流和监督就好了。 我组织了「每日一题交流群」的活动，并且做了个网站 https://ojeveryday.com 来监督大家打卡。在网站上提交力扣个人主页就能进群，群里的规则是每天同步力扣的每日一题，然后大家交流做法。群里还会组织模拟面试、周末直播讲题等活动。由于进群前需要提交自己的 LeetCode 个人主页，并且群主管理严格，所以群里没有任何广告。刷题群已经持续将近一年，欢迎大家加入。 事实证明这种大家一起做同一道题目，并且一起交流讨论的氛围非常好。 二、提高篇 如果你已经过了小白的阶段，那么应该做些提高项目。 周赛 所谓周赛，就是每周日上午，LeetCode 组织的一场比赛，总共 4 道题，一般是 Easy 一道，Medium 两道，Hard 一道。中英文网站同时开始，题目相同。每隔一个周六晚上有双周赛，题目和周赛类似。往届竞赛也可以点击参加做练习模拟。 做周赛的目的是检验我们的学习成果，毕竟这些题目都是新的，就像考试一样。 不要担心自己做不出来，只要尽力而为就好了，我一般的目标是解决前三道，第 4 道 Hard 做不出来也没有心理负担。 参加完比赛之后，看下别人的解答，因为这几个题目都是自己苦思冥想过的，因此学习和进步地都挺快。 我最好的一次周赛成绩是全球 28 名，当看到自己的 id 显示在了全球排名的第一页，我非常兴奋，开心了一整天。 模拟面试 对于大多数人来说，刷题的目的是找工作，那最终就要参加面试。一个人做题的过程是缺乏交流的，实际面试中会有交流互动。因此，推荐在面试前参加一下模拟面试。 另外，哪怕不参加模拟面试，给别人讲一下做题的思路和代码的实现过程也是大有裨益的。 需要参加模拟面试的也可以进「每日一题交流群」，我邀请了力扣全站排名第一的 storm 来做模拟面试官。 三、最后 上文总结了我想到的「LeetCode应该怎么刷？」的方法，最重要的还是坚持二字。做时间的朋友，努力付出就一定有收获。如果觉得刷题困难，就多多学习，多多交流，不要半途而废。 最后，希望大家都能够通过刷 LeetCode 获得成长，拿到自己满意的 Offer。 期待你的点赞、关注、分享。 欢迎加入刷题群 目前已经近 2000 人加入了每日一题打卡群。加入方式是通过每日一题打卡网站，该网站每天都会同步力扣每日一题，这是个互相帮助、互相监督的算法题打卡网站，其地址是 https://www.ojeveryday.com/。 想加入千人刷题群的朋友，可以打开上面的链接地址，然后在左侧点击「加入组织」，提交力扣个人主页，即可进入刷题群。期待你早日加入。 关于作者 我是本文的作者是负雪明烛，毕业于北京邮电大学，目前就职于阿里巴巴。坚持刷算法题 5 年，共计刷了 800 多道算法题。做过的每个算法题都在 CSDN 上写题解博客，获得好评无数，CSDN 的累计阅读量已经 161万 次！博客地址是：https://blog.csdn.net/fuxuemingzhu 「负雪明烛」公众号是负雪明烛维护的一个算法题解公众号，致力于帮助大家刷题、找工作。欢迎关注。 2021 年 2 月 13 日 负雪明烛 更新于 北京。 ———————————————— 版权声明：本文为CSDN博主「负雪明烛」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/fuxuemingzhu/article/details/105183554 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/宫水三叶大神.html":{"url":"blog/algorithm/宫水三叶大神.html","title":"宫水三叶大神","keywords":"","body":"GitHub: https://github.com/SharingSource/LogicStack-LeetCode/wiki 知乎：https://www.zhihu.com/column/c_1339974226623885312 leetcode： https://leetcode.cn/u/ac_oier/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/平和二叉树.html":{"url":"blog/algorithm/平和二叉树.html","title":"平和二叉树","keywords":"","body":"https://lyl0724.github.io/2020/01/25/1/ 2020-01-25更新： 说来惭愧，这是19年初写的文章了，那会的我还是不到50题的水平。当时是学了点后端的东西写了个博客网站，给它折腾上线后，就写了个文章放上去，顺便丢leetcode-cn上引流。没想到一年下来有好几万访问量，还有不少同学邮件联系我。 这一年来一直没有更新，最近过年回家没事，把博客还是给转到hexo上了(方便)。现在的我大概是poj+leetcode 600题水平了，对递归的看法也和去年写这篇博客的时候不太相同了。年后我一定会抽时间更新我的理解！ 尽管这篇博客可能略浅，但我相信一定是适用于初学者的，如果是完全写不出递归代码的同学，还是欢迎您阅读的，它可能很适合您入门～ 以下是正文： 递归解题三部曲 何为递归？程序反复调用自身即是递归。 我自己在刚开始解决递归问题的时候，总是会去纠结这一层函数做了什么，它调用自身后的下一层函数又做了什么…然后就会觉得实现一个递归解法十分复杂，根本就无从下手。 相信很多初学者和我一样，这是一个思维误区，一定要走出来。既然递归是一个反复调用自身的过程，这就说明它每一级的功能都是一样的，因此我们只需要关注一级递归的解决过程即可。 如上图所示，我们需要关心的主要是以下三点： 整个递归的终止条件。 一级递归需要做什么？ 应该返回给上一级的返回值是什么？ 因此，也就有了我们解递归题的三部曲： 找整个递归的终止条件：递归应该在什么时候结束？ 找返回值：应该给上一级返回什么信息？ 本级递归应该做什么：在这一级递归中，应该完成什么任务？ 一定要理解这3步，这就是以后递归秒杀算法题的依据和思路。 但这么说好像很空，我们来以题目作为例子，看看怎么套这个模版，相信3道题下来，你就能慢慢理解这个模版。之后再解这种套路递归题都能直接秒了。 例1：求二叉树的最大深度 先看一道简单的Leetcode题目： Leetcode 104. 二叉树的最大深度 题目很简单，求二叉树的最大深度，那么直接套递归解题三部曲模版： 找终止条件。 什么情况下递归结束？当然是树为空的时候，此时树的深度为0，递归就结束了。 找返回值。 应该返回什么？题目求的是树的最大深度，我们需要从每一级得到的信息自然是当前这一级对应的树的最大深度，因此我们的返回值应该是当前树的最大深度，这一步可以结合第三步来看。 本级递归应该做什么。 首先，还是强调要走出之前的思维误区，递归后我们眼里的树一定是这个样子的，看下图。此时就三个节点：root、root.left、root.right，其中根据第二步，root.left和root.right分别记录的是root的左右子树的最大深度。那么本级递归应该做什么就很明确了，自然就是在root的左右子树中选择较大的一个，再加上1就是以root为根的子树的最大深度了，然后再返回这个深度即可。 具体Java代码如下： class Solution { public int maxDepth(TreeNode root) { //终止条件：当树为空时结束递归，并返回当前深度0 if(root == null){ return 0; } //root的左、右子树的最大深度 int leftDepth = maxDepth(root.left); int rightDepth = maxDepth(root.right); //返回的是左右子树的最大深度+1 return Math.max(leftDepth, rightDepth) + 1; } } 当足够熟练后，也可以和Leetcode评论区一样，很骚的几行代码搞定问题，让之后的新手看的一脸懵逼(这道题也是我第一次一行代码搞定一道Leetcode题)： class Solution { public int maxDepth(TreeNode root) { return root == null ? 0 : Math.max(maxDepth(root.left), maxDepth(root.right)) + 1; } } 例2：两两交换链表中的节点 看了一道递归套路解决二叉树的问题后，有点套路搞定递归的感觉了吗？我们再来看一道Leetcode中等难度的链表的问题，掌握套路后这种中等难度的问题真的就是秒：Leetcode 24. 两两交换链表中的节点 直接上三部曲模版： 找终止条件。 什么情况下递归终止？没得交换的时候，递归就终止了呗。因此当链表只剩一个节点或者没有节点的时候，自然递归就终止了。 找返回值。 我们希望向上一级递归返回什么信息？由于我们的目的是两两交换链表中相邻的节点，因此自然希望交换给上一级递归的是已经完成交换处理，即已经处理好的链表。 本级递归应该做什么。 结合第二步，看下图！由于只考虑本级递归，所以这个链表在我们眼里其实也就三个节点：head、head.next、已处理完的链表部分。而本级递归的任务也就是交换这3个节点中的前两个节点，就很easy了。 附上Java代码： class Solution { public ListNode swapPairs(ListNode head) { //终止条件：链表只剩一个节点或者没节点了，没得交换了。返回的是已经处理好的链表 if(head == null || head.next == null){ return head; } //一共三个节点:head, next, swapPairs(next.next) //下面的任务便是交换这3个节点中的前两个节点 ListNode next = head.next; head.next = swapPairs(next.next); next.next = head; //根据第二步：返回给上一级的是当前已经完成交换后，即处理好了的链表部分 return next; } } 例3：平衡二叉树 相信经过以上2道题，你已经大概理解了这个模版的解题流程了。 那么请你先不看以下部分，尝试解决一下这道easy难度的Leetcode题（个人觉得此题比上面的medium难度要难）：Leetcode 110. 平衡二叉树 我觉得这个题真的是集合了模版的精髓所在，下面套三部曲模版： 找终止条件。 什么情况下递归应该终止？自然是子树为空的时候，空树自然是平衡二叉树了。 应该返回什么信息： 为什么我说这个题是集合了模版精髓？正是因为此题的返回值。要知道我们搞这么多花里胡哨的，都是为了能写出正确的递归函数，因此在解这个题的时候，我们就需要思考，我们到底希望返回什么值？ 何为平衡二叉树？平衡二叉树即左右两棵子树高度差不大于1的二叉树。而对于一颗树，它是一个平衡二叉树需要满足三个条件：它的左子树是平衡二叉树，它的右子树是平衡二叉树，它的左右子树的高度差不大于1。换句话说：如果它的左子树或右子树不是平衡二叉树，或者它的左右子树高度差大于1，那么它就不是平衡二叉树。 而在我们眼里，这颗二叉树就3个节点：root、left、right。那么我们应该返回什么呢？如果返回一个当前树是否是平衡二叉树的boolean类型的值，那么我只知道left和right这两棵树是否是平衡二叉树，无法得出left和right的高度差是否不大于1，自然也就无法得出root这棵树是否是平衡二叉树了。而如果我返回的是一个平衡二叉树的高度的int类型的值，那么我就只知道两棵树的高度，但无法知道这两棵树是不是平衡二叉树，自然也就没法判断root这棵树是不是平衡二叉树了。 因此，这里我们返回的信息应该是既包含子树的深度的int类型的值，又包含子树是否是平衡二叉树的boolean类型的值。可以单独定义一个ReturnNode类，如下： class ReturnNode{ boolean isB; int depth; //构造方法 public ReturnNode(boolean isB, int depth){ this.isB = isB; this.depth = depth; } } 本级递归应该做什么。 知道了第二步的返回值后，这一步就很简单了。目前树有三个节点：root，left，right。我们首先判断left子树和right子树是否是平衡二叉树，如果不是则直接返回false。再判断两树高度差是否不大于1，如果大于1也直接返回false。否则说明以root为节点的子树是平衡二叉树，那么就返回true和它的高度。 具体的Java代码如下： class Solution { //这个ReturnNode是参考我描述的递归套路的第二步：思考返回值是什么 //一棵树是BST等价于它的左、右俩子树都是BST且俩子树高度差不超过1 //因此我认为返回值应该包含当前树是否是BST和当前树的高度这两个信息 private class ReturnNode{ boolean isB; int depth; public ReturnNode(int depth, boolean isB){ this.isB = isB; this.depth = depth; } } //主函数 public boolean isBalanced(TreeNode root) { return isBST(root).isB; } //参考递归套路的第三部：描述单次执行过程是什么样的 //这里的单次执行过程具体如下： //是否终止?->没终止的话，判断是否满足不平衡的三个条件->返回值 public ReturnNode isBST(TreeNode root){ if(root == null){ return new ReturnNode(0, true); } //不平衡的情况有3种：左树不平衡、右树不平衡、左树和右树差的绝对值大于1 ReturnNode left = isBST(root.left); ReturnNode right = isBST(root.right); if(left.isB == false || right.isB == false){ return new ReturnNode(0, false); } if(Math.abs(left.depth - right.depth) > 1){ return new ReturnNode(0, false); } //不满足上面3种情况，说明平衡了，树的深度为左右俩子树最大深度+1 return new ReturnNode(Math.max(left.depth, right.depth) + 1, true); } } 一些可以用这个套路解决的题 暂时就写这么多啦，作为一个高考语文及格分，大学又学了工科的人，表述能力实在差因此啰啰嗦嗦写了一大堆，希望大家能理解这个很好用的套路。 下面我再列举几道我在刷题过程中遇到的也是用这个套路秒的题，真的太多了，大部分链表和树的递归题都能这么秒，因为树和链表天生就是适合递归的结构。 我会随时补充，正好大家可以看了上面三个题后可以拿这些题来练练手，看看自己是否能独立快速准确的写出递归解法了。 Leetcode 101. 对称二叉树 Leetcode 111. 二叉树的最小深度 Leetcode 226. 翻转二叉树：这个题的备注是最骚的。Mac OS下载神器homebrew的大佬作者去面试谷歌，没做出来这道算法题，然后被谷歌面试官怼了：”我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。” Leetcode 617. 合并二叉树 Leetcode 654. 最大二叉树 Leetcode 83. 删除排序链表中的重复元素 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/找出无序数组中第k小的数.html":{"url":"blog/algorithm/找出无序数组中第k小的数.html","title":"找出无序数组中第K小的数","keywords":"","body":"找出无序数组中第k小的数 题目描述： 给定一个无序整数数组，返回这个数组中第k小的数。 解析： 最平常的思路是将数组排序，最快的排序是快排，然后返回已排序数组的第k个数，算法时间复杂度为O（nlogn），空间复杂度为O（1）。使用快排的思想，但是每次只对patition之后的数组的一半递归，这样可以将时间复杂度将为O（n）。 代码实现： ;) #include #include #include #include #include using namespace std; void swap(int *p, int *q) { int t; t = *p; *p = *q; *q = t; } int findNumberK(vector &vec, int k, int s, int e) { int roll = vec[s], be = 0, j = s; for(int i = s+1 ; i a; int temp, k; cout > temp; while(temp != 0) { a.push_back(temp); cin >> temp; } cout > k; int re = findNumberK(a , k, 0 ,a.size() - 1); cout ()); cout ;) 执行效果： Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/无重复字符串.html":{"url":"blog/algorithm/无重复字符串.html","title":"无重复字符串","keywords":"","body":"3 无重复字符的最长子穿 解法一：暴力求解法 逐个生成子字符串 看他是否含有不重复的字符 时间复杂度分析： 找到所有的子串O(n2) 判断子字符串是否仅含有唯一字符： Hash Set O(n), 双指针O(n2) 整体的复杂度O（n3） 空间复杂度： Hash Set O(m) 我们先用一个例子考虑如何在较优的时间复杂度内通过本题。 我们不妨以示例一中的字符串 \\texttt{abcabcbb}abcabcbb 为例，找出从每一个字符开始的，不包含重复字符的最长子串，那么其中最长的那个字符串即为答案。对于示例一中的字符串，我们列举出这些结果，其中括号中表示选中的字符以及最长的字符串： 以 (a)bcabcbb 开始的最长字符串为 (abc)abcbb； 以 a(b)cabcbb 开始的最长字符串为 a(bca)bcbb； 以 ab(c)abcbb 开始的最长字符串为 ab(cab)cbb； 以 abc(a)bcbb 开始的最长字符串为 abc(abc)bb； 以 abca(b)cbb 开始的最长字符串为 abca(bc)bb； 以 abcab(c)bb 开始的最长字符串为 abcab(cb)b； 以 abcabc(b)b 开始的最长字符串为 abcabc(b)b； 以 abcabcb(b) 开始的最长字符串为 abcabcb(b)。 发现了什么？如果我们依次递增地枚举子串的起始位置，那么子串的结束位置也是递增的！这里的原因在于，假设我们选择字符串中的第 kk 个字符作为起始位置，并且得到了不包含重复字符的最长子串的结束位置为 r_kr 。那么当我们选择第 k+1k+1 个字符作为起始位置时，首先从 k+1k+1 到 r_kr 的字符显然是不重复的，并且由于少了原本的第 kk 个字符，我们可以尝试继续增大 r_kr ,直到右侧出现了重复字符为止。 解法二：移动窗口 func lengthOfLongestSubstring(s string) int { // 哈希集合，记录每个字符是否出现过 m := map[byte]int{} n := len(s) // 右指针，初始值为 -1，相当于我们在字符串的左边界的左侧，还没有开始移动 rk, ans := -1, 0 for i := 0; i Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/最小生成树.html":{"url":"blog/algorithm/最小生成树.html","title":"最小生成树","keywords":"","body":"最小生成树-Prim算法和Kruskal算法 Prim算法 1.概览 普里姆算法（Prim算法），图论中的一种算法，可在加权连通图里搜索最小生成树。意即由此算法搜索到的边子集所构成的树中，不但包括了连通图里的所有顶点（英语：Vertex (graph theory)），且其所有边的权值之和亦为最小。该算法于1930年由捷克数学家沃伊捷赫·亚尔尼克（英语：Vojtěch Jarník）发现；并在1957年由美国计算机科学家罗伯特·普里姆（英语：Robert C. Prim）独立发现；1959年，艾兹格·迪科斯彻再次发现了该算法。因此，在某些场合，普里姆算法又被称为DJP算法、亚尔尼克算法或普里姆－亚尔尼克算法。 2.算法简单描述 1).输入：一个加权连通图，其中顶点集合为V，边集合为E； 2).初始化：Vnew = {x}，其中x为集合V中的任一节点（起始点），Enew = {},为空； 3).重复下列操作，直到Vnew = V： a.在集合E中选取权值最小的边，其中u为集合Vnew中的元素，而v不在Vnew集合当中，并且v∈V（如果存在有多条满足前述条件即具有相同权值的边，则可任意选取其中之一）； b.将v加入集合Vnew中，将边加入集合Enew中； 4).输出：使用集合Vnew和Enew来描述所得到的最小生成树。 下面对算法的图例描述 图例 说明 不可选 可选 已选（Vnew） 此为原始的加权连通图。每条边一侧的数字代表其权值。 - - - 顶点D被任意选为起始点。顶点A、B、E和F通过单条边与D相连。A是距离D最近的顶点，因此将A及对应边AD以高亮表示。 C, G A, B, E, F D 下一个顶点为距离D或A最近的顶点。B距D为9，距A为7，E为15，F为6。因此，F距D或A最近，因此将顶点F与相应边DF以高亮表示。 C, G B, E, F A, D 算法继续重复上面的步骤。距离A为7的顶点B被高亮表示。 C B, E, G A, D, F 在当前情况下，可以在C、E与G间进行选择。C距B为8，E距B为7，G距F为11。E最近，因此将顶点E与相应边BE高亮表示。 无 C, E, G A, D, F, B 这里，可供选择的顶点只有C和G。C距E为5，G距E为9，故选取C，并与边EC一同高亮表示。 无 C, G A, D, F, B, E 顶点G是唯一剩下的顶点，它距F为11，距E为9，E最近，故高亮表示G及相应边EG。 无 G A, D, F, B, E, C 现在，所有顶点均已被选取，图中绿色部分即为连通图的最小生成树。在此例中，最小生成树的权值之和为39。 无 无 A, D, F, B, E, C, G 3.简单证明prim算法 反证法：假设prim生成的不是最小生成树 1).设prim生成的树为G0 2).假设存在Gmin使得cost(Gmin)不属于G0 3).将加入G0中可得一个环，且不是该环的最长边(这是因为∈Gmin) 4).这与prim每次生成最短边矛盾 5).故假设不成立，命题得证. 4.算法代码实现(未检验) ;) #define MAX 100000 #define VNUM 10+1 //这里没有ID为0的点,so id号范围1~10 int edge[VNUM][VNUM]={/*输入的邻接矩阵*/}; int lowcost[VNUM]={0}; //记录Vnew中每个点到V中邻接点的最短边 int addvnew[VNUM]; //标记某点是否加入Vnew int adjecent[VNUM]={0}; //记录V中与Vnew最邻近的点 void prim(int start) { int sumweight=0; int i,j,k=0; for(i=1;i;) 5.时间复杂度 这里记顶点数v，边数e 邻接矩阵:O(v2) 邻接表:O(elog2v) Kruskal算法 1.概览 Kruskal算法是一种用来寻找最小生成树的算法，由Joseph Kruskal在1956年发表。用来解决同样问题的还有Prim算法和Boruvka算法等。三种算法都是贪婪算法的应用。和Boruvka算法不同的地方是，Kruskal算法在图中存在相同权值的边时也有效。 2.算法简单描述 1).记Graph中有v个顶点，e个边 2).新建图Graphnew，Graphnew中拥有原图中相同的e个顶点，但没有边 3).将原图Graph中所有e个边按权值从小到大排序 4).循环：从权值最小的边开始遍历每条边 直至图Graph中所有的节点都在同一个连通分量中 ​ if 这条边连接的两个节点于图Graphnew中不在同一个连通分量中 ​ 添加这条边到图Graphnew中 图例描述： 首先第一步，我们有一张图Graph，有若干点和边 将所有的边的长度排序，用排序的结果作为我们选择边的依据。这里再次体现了贪心算法的思想。资源排序，对局部最优的资源进行选择，排序完成后，我们率先选择了边AD。这样我们的图就变成了右图 在剩下的变中寻找。我们找到了CE。这里边的权重也是5 依次类推我们找到了6,7,7，即DF，AB，BE。 下面继续选择， BC或者EF尽管现在长度为8的边是最小的未选择的边。但是现在他们已经连通了（对于BC可以通过CE,EB来连接，类似的EF可以通过EB,BA,AD,DF来接连）。所以不需要选择他们。类似的BD也已经连通了（这里上图的连通线用红色表示了）。 最后就剩下EG和FG了。当然我们选择了EG。最后成功的图就是右： 3.简单证明Kruskal算法 对图的顶点数n做归纳，证明Kruskal算法对任意n阶图适用。 归纳基础： n=1，显然能够找到最小生成树。 归纳过程： 假设Kruskal算法对n≤k阶图适用，那么，在k+1阶图G中，我们把最短边的两个端点a和b做一个合并操作，即把u与v合为一个点v'，把原来接在u和v的边都接到v'上去，这样就能够得到一个k阶图G'(u,v的合并是k+1少一条边)，G'最小生成树T'可以用Kruskal算法得到。 我们证明T'+{}是G的最小生成树。 用反证法，如果T'+{}不是最小生成树，最小生成树是T，即W(T)})。显然T应该包含，否则，可以用加入到T中，形成一个环，删除环上原有的任意一条边，形成一棵更小权值的生成树。而T-{}，是G'的生成树。所以W(T-{}))=W(T'+{})，产生了矛盾。于是假设不成立，T'+{}是G的最小生成树，Kruskal算法对k+1阶图也适用。 由数学归纳法，Kruskal算法得证。 4.代码算法实现 ;) typedef struct { char vertex[VertexNum]; //顶点表 int edges[VertexNum][VertexNum]; //邻接矩阵,可看做边表 int n,e; //图中当前的顶点数和边数 }MGraph; typedef struct node { int u; //边的起始顶点 int v; //边的终止顶点 int w; //边的权值 }Edge; void kruskal(MGraph G) { int i,j,u1,v1,sn1,sn2,k; int vset[VertexNum]; //辅助数组，判定两个顶点是否连通 int E[EdgeNum]; //存放所有的边 k=0; //E数组的下标从0开始 for (i=0;i %d, %d\",E[j].u,E[j].v,E[j].w); k++; for (i=0;i;) 时间复杂度：elog2e e为图中的边数 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/求二叉树中节点的最大距离.html":{"url":"blog/algorithm/求二叉树中节点的最大距离.html","title":"求二叉树中节点的最大距离","keywords":"","body":"编程之美：求二叉树中节点的最大距离 1.问题描述 写一个程序求一棵二叉树相距最远的两个节点之间的距离 如下图： 2.分析与解法 对于任意一个节点，以该节点为根，假设这个根有k个孩子节点，那么距离最远的两个节点U与V之间的路径与这个根节点的关系有两种。 1).若路径经过Root，则U和V属于不同子树的，且它们都是该子树中到根节点最远的节点，否则跟它们的距离最远相矛盾 2).如果路径不经过Root，那么它们一定属于根的k个子树之一，并且它们也是该子树中相距最远的两个顶点 因此，问题就可以转化为在字数上的解，从而能够利用动态规划来解决。 设第K棵子树中相距最远的两个节点：Uk和Vk，其距离定义为d(Uk,Vk)，那么节点Uk或Vk即为子树K到根节点Rk距离最长的节点。不失一般性，我们设Uk为子树K中道根节点Rk距离最长的节点，其到根节点的距离定义为d(Uk,R)。取d(Ui,R)(1 3.代码实现 编程之美给出的代码如下： ;) //数据结构定义 struct NODE { NODE* pLeft; //左孩子 NODE* pRight; //右孩子 int nMaxLeft; //左孩子中的最长距离 int nMaxRight; //右孩子中的最长距离 char chValue; //该节点的值 }; int nMaxLen=0; //寻找树中最长的两段距离 void FindMaxLen(NODE* pRoot) { //遍历到叶子节点，返回 if(pRoot==NULL) { return; } //如果左子树为空，那么该节点的左边最长距离为0 if(pRoot->pLeft==NULL) { pRoot->nMaxLeft=0; } //如果右子树为空，那么该节点的右边最长距离为0 if(pRoot->pRight==NULL) { pRoot->nMaxRight=0; } //如果左子树不为空，递归寻找左子树最长距离 if(pRoot->pLeft!=NULL) { FindMaxLen(pRoot->pLeft); } //如果右子树不为空，递归寻找右子树最长距离 if(pRoot->pRight!=NULL) { FindMaxLen(pRoot->pRight); } if(pRoot->pLeft!=NULL) { int nTempMax=0; if(pRoot->pLeft->nMaxLeft > pRoot->pLeft->nMaxRight) { nTempMax=pRoot->pLeft->nMaxLeft; } else { nTempMax=pRoot->pLeft->nMaxRight; } pRoot->nMaxLeft=nTempMax+1; } //计算右子树最长节点距离 if(pRoot->pRight!=NULL) { int nTempMax=0; if(pRoot->pRight->nMaxLeft > pRoot->pRight->nMaxRight) { nTempMax= pRoot->pRight->nMaxLeft; } else { nTempMax= pRoot->pRight-> nMaxRight; } pRoot->nMaxRight=nTempMax+1; } //更新最长距离 if(pRoot->nMaxLeft+pRoot->nMaxRight > nMaxLen) { nMaxLen=pRoot->nMaxLeft+pRoot->nMaxRight; } } ;) 依据二叉树寻找最大深度的常规思想，又有代码如下： ;) struct BTNode { int data; BTNode* pLeft; BTNode* pRight; }; int maxDis = -1; int findMaxDis(BTNode* pRoot) { if(pRoot == NULL) return 0; int maxLeft = findMaxDis(pRoot->pLeft) ; int maxRight = findMaxDis(pRoot->pRight); if(maxLeft + maxRight > maxDis) { maxDis = maxLeft + maxRight; } return maxLeft > maxRight ? maxLeft+1 : maxRight+1; } ;) 后一段代码为自写 没有验证其正确性。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/线索二叉树.html":{"url":"blog/algorithm/线索二叉树.html","title":"线索二叉树","keywords":"","body":"线索二叉树原理 作者：简Cloud 链接：https://www.jianshu.com/p/deb1d2f2549a 来源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 遍历二叉树的其实就是以一定规则将二叉树中的结点排列成一个线性序列，得到二叉树中结点的先序序列、中序序列或后序序列。这些线性序列中的每一个元素都有且仅有一个前驱结点和后继结点。 但是当我们希望得到二叉树中某一个结点的前驱或者后继结点时，普通的二叉树是无法直接得到的，只能通过遍历一次二叉树得到。每当涉及到求解前驱或者后继就需要将二叉树遍历一次，非常不方便。 于是是否能够改变原有的结构，将结点的前驱和后继的信息存储进来。 二叉树结构 观察二叉树的结构，我们发现指针域并没有充分的利用，有很多“NULL”，也就是存在很多空指针。 对于一个有n个结点的二叉链表，每个节点都有指向左右孩子的两个指针域，一共有2n个指针域。而n个结点的二叉树又有n-1条分支线数(除了头结点，每一条分支都指向一个结点)，也就是存在2n-(n-1)=n+1个空指针域。这些指针域只是白白的浪费空间。因此, 可以用空链域来存放结点的前驱和后继。线索二叉树就是利用n+1个空链域来存放结点的前驱和后继结点的信息。 线索二叉树 如图以中序二叉树为例，我们可以把这颗二叉树中所有空指针域的lchild，改为指向当前结点的前驱（灰色箭头），把空指针域中的rchild，改为指向结点的后继（绿色箭头）。我们把指向前驱和后继的指针叫做线索 \\，加上线索的二叉树就称之为*线索二叉树*。 线索二叉树结点结构 如果只是在原二叉树的基础上利用空结点，那么就存在着这么一个问题：我们如何知道某一结点的lchild是指向他的左孩子还是指向前驱结点？rchild是指向右孩子还是后继结点？显然我们要对他的指向增设标志来加以区分。 因此，我们在每一个结点都增设两个标志域LTag和RTag，它们只存放0或1的布尔型变量，占用的空间很小。于是结点的结构如图所示。 结点结构 其中： LTag为0是指向该结点的左孩子，为1时指向该结点的前驱 RTag为0是指向该结点的右孩子，为1时指向该结点的后继 因此实际的二叉链表图为 际的二叉链表图 线索二叉树的结构实现 二叉树的线索存储结构定义如下： typedef char TElemType; typedef enum { Link, Thread } PointerTag; //Link==0,表示指向左右孩子指针 //Thread==1,表示指向前驱或后继的线索 //二叉树线索结点存储结构 typedef struct BiThrNode { TElemType data; //结点数据 struct BiThrNode *lchild, *rchild; //左右孩子指针 PointerTag LTag; PointerTag RTag; //左右标志 }BiThrNode, *BiThrTree; 二叉树线索化 线索化 对普通二叉树以某种次序遍历使其成为线索二叉树的过程就叫做线索化。因为前驱和后继结点只有在二叉树的遍历过程中才能得到，所以线索化的具体过程就是在二叉树的遍历中修改空指针。 线索化具体实现 以中序二叉树的线索化为例，线索化的具体实现就是将中序二叉树的遍历进行修改，把原本打印函数的代码改为指针修改的代码就可以了。 我们设置一个pre指针，永远指向遍历当前结点的前一个结点。若遍历的当前结点左指针域为空，也就是无左孩子，则把左孩子的指针指向pre(相对当前结点的前驱结点)。 右孩子同样的，当pre的右孩子为空，则把pre右孩子的指针指向当前结点(相对pre结点为后继结点)。 最后把当前结点赋给pre，完成后续的递归遍历线索化。 中序遍历线索化的递归函数代码如下： void InThreading(BiThrTree B,BiThrTree *pre) { if(!B) return; InThreading(B->lchild,pre); //--------------------中间为修改空指针代码--------------------- if(!B->lchild){ //没有左孩子 B->LTag = Thread; //修改标志域为前驱线索 B->lchild = *pre; //左孩子指向前驱结点 } if(!(*pre)->rchild){ //没有右孩子 (*pre)->RTag = Thread; //修改标志域为后继线索 (*pre)->rchild = B; //前驱右孩子指向当前结点 } *pre = B; //保持pre指向p的前驱 //--------------------------------------------------------- InThreading(B->rchild,pre); } 增设头结点 线索化后的二叉树，就如同操作一个双向链表。于是我们想到为二叉树增设一个头结点，这样就和双向链表一样，即能够从第一个结点正向开始遍历，也可以从最后一个结点逆向遍历。 上图，在线索二叉链表上添加一个head结点，并令其lchild域的指针指向二叉树的根结点(A)，其rchild域的指针指向中序遍历访问的最后一个结点(G)。同样地，二叉树中序序列的第一个结点中，lchild域指针指向头结点，中序序列的最后一个结点rchild域指针也指向头结点。 于是从头结点开始，我们既可以从第一个结点顺后继结点遍历，也可以从最后一个结点起顺前驱遍历。就和双链表一样。 增设头结点并线索化的代码实现 //为线索二叉树添加头结点，使之可以双向操作 Status InOrderThreading(BiThrTree *Thrt,BiThrTree T){ if(!(*Thrt = (BiThrTree)malloc(sizeof(BiThrNode)))) exit(OVERFLOW); //开辟结点 (*Thrt)->LTag = Link; (*Thrt)->RTag = Thread; //设置标志域 (*Thrt)->rchild = (*Thrt); //右结点指向本身 if(!T) { (*Thrt)->lchild = (*Thrt); return OK; //若根结点不存在,则该二叉树为空,让该头结点指向自身. } BiThrTree pre; //设置前驱结点 //令头结点的左指针指向根结点 pre = (*Thrt); (*Thrt)->lchild = T; //开始递归输入线索化 InThreading(T,&pre); //此时结束了最后一个结点的线索化了,下面的代码把头结点的后继指向了最后一个结点. //并把最后一个结点的后继也指向头结点,此时树成为了一个类似双向链表的循环. pre->rchild = *Thrt; pre->RTag = Thread; (*Thrt)->rchild = pre; return OK; } 遍历线索二叉树 线索二叉树的遍历就可以通过之前建立好的线索，沿着后继线索依依访问下去就行。 //非递归遍历线索二叉树 Status InOrderTraverse(BiThrTree T) { BiThrNode *p = T->lchild; while(p!=T){ while(p->LTag==Link) p = p->lchild; //走向左子树的尽头 printf(\"%c\",p->data ); while(p->RTag==Thread && p->rchild!=T) { //访问该结点的后续结点 p = p->rchild; printf(\"%c\",p->data ); } p = p->rchild; } return OK; } 完整代码 #include #include //函数状态结果代码 #define TRUE 1 #define FALSE 0 #define OK 1 #define ERROR 0 #define INFEASIBLE -1 #define OVERFLOW -2 //Status是函数的类型，其值是函数结果状态代码 typedef int Status; typedef char TElemType; typedef enum { Link, Thread } PointerTag; typedef struct BiThrNode { TElemType data; struct BiThrNode *lchild, *rchild; PointerTag LTag; PointerTag RTag; }BiThrNode, *BiThrTree; //线索二叉树初始化 Status CreateBiThrNode(BiThrTree * B) { char ch; scanf(\"%c\", &ch); if(ch=='#') *B = NULL; else{ if(!((*B) = (BiThrNode *)malloc(sizeof(BiThrNode)))) exit(OVERFLOW); (*B)->data = ch; (*B)->LTag = Link; (*B)->RTag = Link; CreateBiThrNode(&(*B)->lchild); CreateBiThrNode(&(*B)->rchild); } return OK; } //线索二叉树线索化 void InThreading(BiThrTree B,BiThrTree *pre) { if(!B) return; InThreading(B->lchild,pre); if(!B->lchild){ B->LTag = Thread; B->lchild = *pre; } if(!(*pre)->rchild){ (*pre)->RTag = Thread; (*pre)->rchild = B; } *pre = B; InThreading(B->rchild,pre); } //为线索二叉树添加头结点，使之可以双向操作 Status InOrderThreading(BiThrTree *Thrt,BiThrTree T){ if(!(*Thrt = (BiThrTree)malloc(sizeof(BiThrNode)))) exit(OVERFLOW); (*Thrt)->LTag = Link; (*Thrt)->RTag = Thread; (*Thrt)->rchild = (*Thrt); if(!T) { (*Thrt)->lchild = (*Thrt); return OK; //若根结点不存在,则该二叉树为空,让该头结点指向自身. } BiThrTree pre; //令头结点的左指针指向根结点 pre = (*Thrt); (*Thrt)->lchild = T; //开始递归输入线索化 InThreading(T,&pre); //此时结束了最后一个结点的线索化了,下面的代码把头结点的后继指向了最后一个结点. //并把最后一个结点的后继也指向头结点,此时树成为了一个类似双向链表的循环. pre->rchild = *Thrt; pre->RTag = Thread; (*Thrt)->rchild = pre; return OK; } //非递归遍历线索二叉树 Status InOrderTraverse(BiThrTree T) { BiThrNode *p = T->lchild; while(p!=T){ while(p->LTag==Link) p = p->lchild; //走向左子树的尽头 printf(\"%c\",p->data ); while(p->RTag==Thread && p->rchild!=T) { //访问该结点的后续结点 p = p->rchild; printf(\"%c\",p->data ); } p = p->rchild; } return OK; } int main() { BiThrTree B,T; CreateBiThrNode(&B); InOrderThreading(&T,B); printf(\"中序遍历二叉树的结果为：\"); InOrderTraverse(T); printf(\"\\n\"); } //测试数据:abc##de#g##f### Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/贪心算法.html":{"url":"blog/algorithm/贪心算法.html","title":"贪心算法","keywords":"","body":"贪心算法 1.定义概览 贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的仅是在某种意义上的局部最优解。贪心算法不是对所有问题都能得到整体最优解，但对范围相当广泛的许多问题他能产生整体最优解或者是整体最优解的近似解。 贪心算法在有最优子结构的问题中尤为有效。最优子结构的意思是局部最优解能决定全局最优解。简单地说，问题能够分解成子问题来解决，子问题的最优解能递推到最终问题的最优解 算法思想：从问题的某一个初始解出发逐步逼近给定的目标，以尽可能快的地求得更好的解。当达到某算法中的某一步不能再继续前进时，算法停止。 该算法存在问题： 1). 不能保证求得的最后解是最佳的； 2). 不能用来求最大或最小解问题； 3). 只能求满足某些约束条件的可行解的范围。 Dijkstra算法、Prim算法和Kruskal算法都属于典型的贪心算法 2.活动安排问题 设有n个活动的集合E={1,2,…,n}，其中每个活动都要求使用同一资源，如演讲会场等，而在同一时间内只有一个活动能使用这一资源。每个活动i都有一个要求使用该资源的起始时间si和一个结束时间fi,且si 在下面所给出的解活动安排问题的贪心算法greedySelector : int greedySelector(int s[MAXNUM] , int f[MAXNUM], bool a[]) //s数组记录着相应活动开始时间，f数组记录着相应活动结束时间 { //各个活动按结束时间的非减序排列 int n=MAXNUM-1; a[1]=true; //安排第一个活动 int j=1,count=1; for (int i=2;i=f[j]) //检验当前最早结束的活动的开始时间是否晚于前一个活动的结束结束时间 { //如果晚于，则表示两个活动相互兼容 a[i]=true; //标记为true，表示已经安排 j=i; //记已经安排活动的个数 count++; } else a[i]=false; //与已安排活动不兼容，标记此活动未安排 } return count; } 由于输入的活动以其完成时间的非减序排列，所以算法greedySelector每次总是选择具有最早完成时间的相容活动加入集合A中。直观上，按这种方法选择相容活动为未安排活动留下尽可能多的时间。也就是说，该算法的贪心选择的意义是使剩余的可安排时间段极大化，以便安排尽可能多的相容活动。 算法greedySelector的效率极高。当输入的活动已按结束时间的非减序排列，算法只需O(n)的时间安排n个活动，使最多的活动能相容地使用公共资源。如果所给出的活动未按非减序排列，可以用O(nlogn)的时间重排。 例：设待安排的11个活动的开始时间和结束时间按结束时间的非减序排列如下： 算法greedySelector 的计算过程如左图所示。图中每行相应于算法的一次迭代。阴影长条表示的活动是已选入集合A的活动，而空白长条表示的活动是当前正在检查相容性的活动。 若被检查的活动i的开始时间Si小于最近选择的活动j的结束时间fi，则不选择活动i，否则选择活动i加入集合A中。 贪心算法并不总能求得问题的整体最优解。但对于活动安排问题，贪心算法greedySelector却总能求得的整体最优解，即它最终所确定的相容活动集合A的规模最大。这个结论可以用数学归纳法证明。 3.贪心算法基本要素 1).贪心选择性质 贪心选择性质是指所求问题的整体最优解可以通过一系列局部最优的选择，即贪心选择来达到。这是贪心算法可行的第一个基本要素,也是贪心算法与动态规划算法的主要区别。 在动态规划算法中，每步所作的选择往往依赖于相关子问题的解。因而只有在解出相关子问题后，才能作出选择。而在贪心算法中，仅在当前状态下作出最好选择，即局部最优选择。然后再去解作出这个选择后产生的相应的子问题。贪心算法所作的贪心选择可以依赖于以往所作过的选择，但决不依赖于将来所作的选择，也不依赖于子问题的解。正是由于这种差别，动态规划算法通常以自底向上的方式解各子问题，而贪心算法则通常以自顶向下的方式进行,以迭代的方式作出相继的贪心选择，每作一次贪心选择就将所求问题简化为一个规模更小的子问题。 对于一个具体问题，要确定它是否具有贪心选择性质，我们必须证明每一步所作的贪心选择最终导致问题的一个整体最优解。通常可以用我们在证明活动安排问题的贪心选择性质时所采用的方法来证明。首先考察问题的一个整体最优解，并证明可修改这个最优解，使其以贪心选择开始。而且作了贪心选择后，原问题简化为一个规模更小的类似子问题。然后，用数学归纳法证明，通过每一步作贪心选择，最终可得到问题的一个整体最优解。其中，证明贪心选择后的问题简化为规模更小的类似子问题的关键在于利用该问题的最优子结构性质。 2).最优子结构性质 当一个问题的最优解包含着它的子问题的最优解时，称此问题具有最优子结构性质。问题所具有的这个性质是该问题可用动态规划算法或贪心算法求解的一个关键特征。在活动安排问题中，其最优子结构性质表现为：若a是对于正的活动安排问题包含活动1的一个最优解,则相容活动集合a’=a—{1}是对于e’={i∈e:si≥f1}的活动安排问题的一个最优解。 3).贪心算法与动态规划算法的差异 贪心算法和动态规划算法都要求问题具有最优子结构性质，这是两类算法的一个共同点。大多数时候，能用贪心算法求解的问题，都可以用动态规划算法求解。但是能用动态规划求解的，不一定能用贪心算法进行求解。 4.0-1背包问题和背包问题 1).两个问题的描述 0-1背包问题： 给定n种物品和一个背包。物品i的重量是Wi，其价值为Vi，背包的容量为C。应如何选择装入背包的物品，使得装入背包中物品的总价值最大? 在选择装入背包的物品时，对每种物品i只有2种选择，即装入背包或不装入背包。不能将物品i装入背包多次，也不能只装入部分的物品i。 背包问题： 与0-1背包问题类似，所不同的是在选择物品i装入背包时，可以选择物品i的一部分，而不一定要全部装入背包，1≤i≤n。 这2类问题都具有最优子结构性质，极为相似，但背包问题可以用贪心算法求解，而0-1背包问题却不能用贪心算法求解。 2).用贪心算法解背包问题的基本步骤： 首先计算每种物品单位重量的价值Vi/Wi，然后，依贪心选择策略，将尽可能多的单位重量价值最高的物品装入背包。若将这种物品全部装入背包后，背包内的物品总重量未超过C，则选择单位重量价值次高的物品并尽可能多地装入背包。依此策略一直地进行下去，直到背包装满为止。 代码实现： float knapsack(float c,float w[MAXNUM], float v[MAXNUM],float con[MAXNUM]) { int n=MAXNUM; float d[n],hascon=0,remain=c; int i; for (i = 0; i remain ) //如果第i个物品无法整体装进保 则跳出循环 break; con[i]=1; hascon+=v[i] //累加装进包的物品总价值 remain-=w[i]; } if (i 算法knapsack的主要计算时间在于将各种物品依其单位重量的价值从大到小排序。因此，算法的计算时间上界为O（nlogn）。当然，为了证明算法的正确性，还必须证明背包问题具有贪心选择性质。 对于0-1背包问题，贪心选择之所以不能得到最优解是因为在这种情况下，它无法保证最终能将背包装满，部分闲置的背包空间使每公斤背包空间的价值降低了。事实上，在考虑0-1背包问题时，应比较选择该物品和不选择该物品所导致的最终方案，然后再作出最好选择。由此就导出许多互相重叠的子问题。这正是该问题可用动态规划算法求解的另一重要特征。 实际上也是如此，动态规划算法的确可以有效地解0-1背包问题。 5.贪心算法的适用范围: 贪心算法并不能总求得问题的整体最优解。但对于某些问题，却总能求得整体最优解，这要看问题时什么了。只要能满足贪心算法的两个性质：贪心选择性质和最优子结构性质，贪心算法就可以出色地求出问题的整体最优解。即使某些问题，贪心算法不能求得整体的最优解，贪心算法也能求出大概的整体最优解。如果你的要求不是太高，贪心算法是一个很好的选择。最优子结构性质是比较容易看出来的，但是贪心选择性质就没那么容易了，这个时候需要证明。证明往往使用数学归纳法。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 09:05:40 "},"blog/algorithm/递归三部曲.html":{"url":"blog/algorithm/递归三部曲.html","title":"递归三部曲","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/angular/":{"url":"blog/angular/","title":"Angular","keywords":"","body":"每篇一句 面试高大上，面试造飞机，工作拧螺丝 因此不能以为自己工作觉得还OK，就觉得自己技术还不错了 如题，指的是在restful风格的url设计中，怎么实现批量删除呢？ 这里指的删除是真删除，不是逻辑删除。如果是逻辑删除，其实就是update，使用put方法即可 如果是需要删除一个条目，可以直接将需要删除的条目的id放进url里面，比如http://example.com/posts/2016，但是如果需要再一次请求里面**删除多个条目**，应该如何设计比较合理呢？我现在想到的是以下两种方法： 用逗号分隔放进url里面：http://example.com/posts/2016,2017； 将需要删除的一系列id放进请求体里面，但是似乎没有这样的标准（DELETE请求）。 先说说方法1，如果删除的数据非常多，比如超过1000个id，那很可能就超过URL的长度限制了。 Url长度限制： IE7.0 :url最大长度2083个字符，超过最大长度后仍然能提交，但是只能传过去2083个字符。 firefox 3.0.3 :url最大长度7764个字符，超过最大长度后无法提交。 Google Chrome 2.0.168 :url最大长度7713个字符，超过最大长度后无法提交 从上面可以看出，这是有风险的可能提交不了的。但是话说回来，你是什么需求，需要一次性删除1000条记录，这是多么危险的操作，怎么可能通过API暴露出来呢？所以综合考虑，我个人认为，使用url的方式传递删除的值，是没有任何问题的。毕竟我们99%的情况，都是非常少量多额删除操作。 再说说方法2，其实我是不太建议的。因为我们删除操作，肯定使用DELETE请求，但是奈何我们并不建议在DELETE请求里放body体，原因在于：根据RFC标准文档，DELETE请求的body在语义上没有任何意义。事实上一些网关、代理、防火墙在收到DELETE请求后，会把请求的body直接剥离掉。 所以，万一你要放在body体里传参，请使用POST请求 这里介绍一种比较优雅，但是比较麻烦点的方法： 分成2步完成，第一步发送POST请求，集合所有要删除的IDs然后返回一个header,然后在利用这个header调用DELETE请求。具体步骤如下: 发送POST请求，集中所有的IDs (可以存到Redis或者普通数据库) http://example.com/posts/deletes 成功后可以返回一个唯一的头文件： HTTP/1.1 201 created, and a Location header to: http://example.com/posts/deletes/KJHJS675 然后可以利用Ajax直接发送DELETE请求: DELETE http://example.com/posts/deletes/KJHJS675 这样就可以在不暴露IDs的情况下更加安全的删除相关条目。 最后如果要获得一个资源，一定要用GET方法么？ 在一些文章中，看到获取资源的时候，一般用GET方法。我的问题是，我要获取的资源是一个账户的信息，需要实用token，我一般把token放在POST请求里面，当然也可以将token放在连接中使用GET。 其实，restful只是一种理想的情。你是否完全遵循Restful设计原则了 如果完全遵循的话, 获取账户信息应当是GET请求, 但是token通常是会放在header中, 不在url中体现 针对我们的token这个事情，在我项目中会使用post请求根据用户信息获取一个token，然后拿着token用get方法请求资源。另外，我也会将token放到http请求头中。以上是个人工作经验，希望对各位有帮助 最后 restful风格的url我们可以尽量去遵守，因为它对运维或者监控都非常友好。但是不要一根经，它只是理想情况，有的时候并不满足我们的需求，我们可以变通的看问题。 简明的一幅图，rest接口的命名规范： API must has version use token rather than cookie url is not case。。，not use 大写字母 use - rather then _ remeber doc 为什么会推荐用 -，而不是 _？ -叫做分词符，顾名思义用作分开不同词的。这个最佳实践来自于针对Google为首的SEO（搜索引擎优化）需要，Google搜索引擎会把url中出现的-当做空格对待，这样url “/it-is-crazy” 会被搜索引擎识别为与“it\",“is”,“crazy\"关键词或者他们的组合关键字相关。 当用户搜索”it”,“crazy”, \"it is crazy\"时，很容易检索到这个url，排名靠前。 _ 这个符号如果出现在url中，会自动被Google忽略，“/it_is_crazy”被识别为与关键词 “itIsCrazy”相关。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/angular/1-start.html":{"url":"blog/angular/1-start.html","title":"Start","keywords":"","body":"[toc] 1-start 搭建开发环境 要想搭建开发环境，请遵循搭建本地环境中的步骤进行操作。 创建新的工作区和一个初始应用 Angular 的工作区就是你开发应用所在的上下文环境。一个工作区包含一个或多个项目所需的文件。 每个项目都是一组由应用、库或端到端（e2e）测试组成的文件集合。 在本教程中，你将创建一个新的工作区。 要想创建一个新的工作区和一个初始应用项目，需要： 确保你现在没有位于 Angular 工作区的文件夹中。例如，如果你之前已经创建过 \"快速上手\" 工作区，请回到其父目录中。 运行 CLI 命令 ng new，空间名请使用 angular-tour-of-heroes，如下所示： ng new angular-tour-of-heroes ng new 命令会提示你输入要在初始应用项目中包含哪些特性，请按 Enter 或 Return 键接受其默认值。 启动应用服务器 进入工作区目录，并启动这个应用。 cd angular-tour-of-heroes ng serve --open ng serve 命令会构建本应用、启动开发服务器、监听源文件，并且当那些文件发生变化时重新构建本应用。 --open 标志会打开浏览器，并访问 http://localhost:4200/。 Angular 组件 你所看到的这个应用就是一个外壳。这个外壳是被一个名叫AppComponent的Angular组件控制的。 组件是 Angular 应用中的基本构造块。 它们在屏幕上显示数据，监听用户输入，并且根据这些输入执行相应的动作。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/angular/2-component.html":{"url":"blog/angular/2-component.html","title":"Component","keywords":"","body":"[toc] 2-component 应用程序现在有了基本的标题。 接下来你要创建一个新的组件来显示英雄信息并且把这个组件放到应用程序的外壳里去。 创建英雄列表组件 使用 Angular CLI 创建一个名为 heroes 的新组件。 ng generate component heroes CLI 创建了一个新的文件夹 src/app/heroes/，并生成了 HeroesComponent 的四个文件。 HeroesComponent 的类文件如下： import { Component, OnInit } from '@angular/core'; @Component({ selector: 'app-heroes', templateUrl: './heroes.component.html', styleUrls: ['./heroes.component.css'] }) export class HeroesComponent implements OnInit { constructor() { } ngOnInit() { } } 你要从Angular核心库中导入component符号，并为组件类加上 @Component装饰器 @Component是个装饰器函数，用于为该组件指定Angular所需要的元数据。 CLI自动生成了三个元数据属性： Selector - 组件的选择器（CSS元素选择器） templateUrl - 组件模版文件的位置 styleUrls - 组件私有CSS样式表文件的位置 CSS 元素选择器 app-heroes 用来在父组件的模板中匹配 HTML 元素的名称，以识别出该组件。 ngOnInit() 是一个生命周期钩子，Angular 在创建完组件后很快就会调用 ngOnInit()。这里是放置初始化逻辑的好地方。 始终要 export 这个组件类，以便在其它地方（比如 AppModule）导入它。 添加 hero 属性 往 HeroesComponent 中添加一个 hero 属性，用来表示一个名叫 “Windstorm” 的英雄。 heroes.component.ts (hero property) hero = 'Windstorm'; 显示英雄 打开模板文件 heroes.component.html。删除 Angular CLI 自动生成的默认内容，改为到 hero 属性的数据绑定。 heroes.component.html {{hero}} 显示 HeroesComponent 视图 要显示 HeroesComponent 你必须把它加到壳组件 AppComponent 的模板中。 别忘了，app-heroes 就是 HeroesComponent 的 元素选择器。 所以，只要把 元素添加到 AppComponent 的模板文件中就可以了，就放在标题下方。 src/app/app.component.html {{title}} 如果 CLI 的 ng serve 命令仍在运行，浏览器就会自动刷新，并且同时显示出应用的标题和英雄的名字。 创建 Hero 类 真实的英雄当然不止一个名字。 在 src/app 文件夹中为 Hero 类创建一个文件，并添加 id 和 name 属性。 src/app/hero.ts port interface Hero { id: number; name: string; } 回到 HeroesComponent 类，并且导入这个 Hero 类。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/angular/3-form.html":{"url":"blog/angular/3-form.html","title":"Form","keywords":"","body":"构建模版驱动表单 本教程将为你演示如何创建一个模板驱动表单，它的控件元素绑定到数据属性，并通过输入验证来保持数据的完整性和样式，以改善用户体验。 当在模板中进行更改时，模板驱动表单会使用双向数据绑定来更新组件中的数据模型，反之亦然。 Template-driven forms use two-way data binding to update the data model in the component as changes are made in the template and vice versa. 效果 步骤： 建立基本表单 定义一个范例数据模型 包括必须的基础设施，比如FormModule 使用ngModel 指令和双向数据绑定语法把表单控件绑定到数据属性 检查ngModel如何使用CSS类报告控件状态 为控件命名，以便让ngModel可以访问他们 用ngModel跟踪输入的有效和控制的状态 添加自定义 CSS 来根据状态提供可视化反馈。 显示和隐藏验证错误信息。 通过添加到模型数据来响应原生HTML按钮的单击事件 使用表单的ngSubmit 输出属性来处理表单提交 在表单生效之前，先禁用Submit按钮 在提交完成后，把已完成的表单替换成页面上不同的内容 建立表单 1. Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/angular/4-dynamic-form.html":{"url":"blog/angular/4-dynamic-form.html","title":"Dynamic Form","keywords":"","body":"Angular: 动态创建Form表单 作者： limei 连接：https://limeii.github.io/2019/05/angular-dynamic-form/ 在web应用里通常会有这样一种场景：比如支付宝信用卡还款，假设支付宝收费标准如下： 普通用户，2000元以内免费，2000-50000收费10元，50000元以上收费15元。 砖石会员，5000元以内免费，5000-50000收费5元，50000元以上收费10元。 现在需要做一个页面，用来专门用来收集这样的收费标准，以后可能需要增加新的收费标准或者修改现有的收费标准。 这个页面可以设计成这样： 在angular用dynamic form可以很容易实现这种动态加载表单的效果，并且可以轻松实现对每一个field进行校验。接下来介绍如何在angular里实现上面的表单。 开发环境如下： 项目结构如下： 第一步，需要在app.module.ts引入FormsModule和ReactiveFormsModule //app.module.ts import { FormsModule, ReactiveFormsModule } from '@angular/forms' 第二步，创建DynamicFeeComponent，这个是每次动态添加的form表单 详细代码： angular-dynamic-form-dynamic-fee.component.html angular-dynamic-form-dynamic-fee.component.ts 需要注意的是，formarray中每一项都是一个独立的formgroup，本质上来说在app.componnet中就是有两层嵌套的form，只不过这里的被嵌套的是一个formarray. formarray是多个formgroup数组集合。在formarray formgroup的命名需要用数字表示： 在DynamicFeeComponent里需要用到每一个formControl的时候，通过[group]=\"feeForm.controls.feeArray.controls[i]\"把每个formgroup传给DynamicFeeComponent。 否则的话一直会报类似这样的错:cannot access formcontrol 第四步，在DynamicFeeComponent里，为每一个字段添加require的校验，formarray中只要有一个字段校验不对，整个form.valid就为false 在每添加一个feeItem的时候，给每个字段设置require校验： //app.component.ts addFeeItem() { this.feeArray.push( this.fb.group({ userlevel: ['', Validators.required], tierMin: ['', Validators.required], tierMax: ['', Validators.required], amount: ['', Validators.required] }) ) } 在每添加一个feeItem的时候，给每个字段设置require校验： //app.component.ts addFeeItem() { this.feeArray.push( this.fb.group({ userlevel: ['', Validators.required], tierMin: ['', Validators.required], tierMax: ['', Validators.required], amount: ['', Validators.required] }) ) } 校验方法如下： //dynamic-fee.component.ts onValidate() { if (this.isSubmitted) { const form = this.group; for (const field in this.feeItemErrors) { if (form.get(field).errors) { const error = Object.keys(form.get(field).errors); this.feeItemErrors[field] = this.feeItemErrorsMessage[field][error[0]]; } else { this.feeItemErrors[field] = null; } } } } 需要注意的是，在钩子函数ngAfterContentChecked里需要重新调用onValidate方法，保证在点击submit button以后，每次更改页面里的值后能够实时校验。 //dynamic-fee.component.ts ngAfterContentChecked() { if (this.isSubmitted) { this.onValidate(); } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/angular/readme.html":{"url":"blog/angular/readme.html","title":"Readme","keywords":"","body":"每篇一句 面试高大上，面试造飞机，工作拧螺丝 因此不能以为自己工作觉得还OK，就觉得自己技术还不错了 如题，指的是在restful风格的url设计中，怎么实现批量删除呢？ 这里指的删除是真删除，不是逻辑删除。如果是逻辑删除，其实就是update，使用put方法即可 如果是需要删除一个条目，可以直接将需要删除的条目的id放进url里面，比如http://example.com/posts/2016，但是如果需要再一次请求里面**删除多个条目**，应该如何设计比较合理呢？我现在想到的是以下两种方法： 用逗号分隔放进url里面：http://example.com/posts/2016,2017； 将需要删除的一系列id放进请求体里面，但是似乎没有这样的标准（DELETE请求）。 先说说方法1，如果删除的数据非常多，比如超过1000个id，那很可能就超过URL的长度限制了。 Url长度限制： IE7.0 :url最大长度2083个字符，超过最大长度后仍然能提交，但是只能传过去2083个字符。 firefox 3.0.3 :url最大长度7764个字符，超过最大长度后无法提交。 Google Chrome 2.0.168 :url最大长度7713个字符，超过最大长度后无法提交 从上面可以看出，这是有风险的可能提交不了的。但是话说回来，你是什么需求，需要一次性删除1000条记录，这是多么危险的操作，怎么可能通过API暴露出来呢？所以综合考虑，我个人认为，使用url的方式传递删除的值，是没有任何问题的。毕竟我们99%的情况，都是非常少量多额删除操作。 再说说方法2，其实我是不太建议的。因为我们删除操作，肯定使用DELETE请求，但是奈何我们并不建议在DELETE请求里放body体，原因在于：根据RFC标准文档，DELETE请求的body在语义上没有任何意义。事实上一些网关、代理、防火墙在收到DELETE请求后，会把请求的body直接剥离掉。 所以，万一你要放在body体里传参，请使用POST请求 这里介绍一种比较优雅，但是比较麻烦点的方法： 分成2步完成，第一步发送POST请求，集合所有要删除的IDs然后返回一个header,然后在利用这个header调用DELETE请求。具体步骤如下: 发送POST请求，集中所有的IDs (可以存到Redis或者普通数据库) http://example.com/posts/deletes 成功后可以返回一个唯一的头文件： HTTP/1.1 201 created, and a Location header to: http://example.com/posts/deletes/KJHJS675 然后可以利用Ajax直接发送DELETE请求: DELETE http://example.com/posts/deletes/KJHJS675 这样就可以在不暴露IDs的情况下更加安全的删除相关条目。 最后如果要获得一个资源，一定要用GET方法么？ 在一些文章中，看到获取资源的时候，一般用GET方法。我的问题是，我要获取的资源是一个账户的信息，需要实用token，我一般把token放在POST请求里面，当然也可以将token放在连接中使用GET。 其实，restful只是一种理想的情。你是否完全遵循Restful设计原则了 如果完全遵循的话, 获取账户信息应当是GET请求, 但是token通常是会放在header中, 不在url中体现 针对我们的token这个事情，在我项目中会使用post请求根据用户信息获取一个token，然后拿着token用get方法请求资源。另外，我也会将token放到http请求头中。以上是个人工作经验，希望对各位有帮助 最后 restful风格的url我们可以尽量去遵守，因为它对运维或者监控都非常友好。但是不要一根经，它只是理想情况，有的时候并不满足我们的需求，我们可以变通的看问题。 简明的一幅图，rest接口的命名规范： API must has version use token rather than cookie url is not case。。，not use 大写字母 use - rather then _ remeber doc 为什么会推荐用 -，而不是 _？ -叫做分词符，顾名思义用作分开不同词的。这个最佳实践来自于针对Google为首的SEO（搜索引擎优化）需要，Google搜索引擎会把url中出现的-当做空格对待，这样url “/it-is-crazy” 会被搜索引擎识别为与“it\",“is”,“crazy\"关键词或者他们的组合关键字相关。 当用户搜索”it”,“crazy”, \"it is crazy\"时，很容易检索到这个url，排名靠前。 _ 这个符号如果出现在url中，会自动被Google忽略，“/it_is_crazy”被识别为与关键词 “itIsCrazy”相关。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/database/":{"url":"blog/database/","title":"Database","keywords":"","body":"Database Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/database/backup.html":{"url":"blog/database/backup.html","title":"Backup","keywords":"","body":"mongodump --uri=\"mongodb://8.16.0.162:27017;8.16.0.163:27017;8.16.0.54:27017/?replicaSet=rs\" [additional options] template: keystone/templates/job-fernet-setup.yaml:65:62 at : keystone/charts/helm-toolkit/templates/utils/_hash.tpl:20:4: executing \"helm-toolkit.utils.hash\" at : error calling include: template: keystone/templates/configmap-etc.yaml:66:4: at (dict \"envAll\" $envAll \"template\" .Values.conf.mpm_event \"key\" \"mpm_event.conf\" \"format\" \"Secret\")>: keystone/charts/helm-toolkit/templates/snippets/_values_template_renderer.tpl:70:25: at : error calling tpl: cannot retrieve Template.Basepath from values inside tpl function: | Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/database/mongo-query.html":{"url":"blog/database/mongo-query.html","title":"Mongo Query","keywords":"","body":"mongo-query db.articles.aggregate( [ { $match: { $or: [ { score: { $gt: 70, $lt: 90 } }, { views: { $gte: 1000 } } ] } }, { $group: { _id: null, count: { $sum: 1 } } } ] ); Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/database/mongodb.html":{"url":"blog/database/mongodb.html","title":"Mongodb","keywords":"","body":"mongodb mongoDB是目前比较流行的一个基于分布式文件存储的数据库，它是一个介于关系数据库和非关系数据库(NoSQL)之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 mongoDB介绍 mongoDB是目前比较流行的一个基于分布式文件存储的数据库，它是一个介于关系数据库和非关系数据库(NoSQL)之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 mongoDB中将一条数据存储为一个文档（document），数据结构由键值（key-value）对组成。 其中文档类似于我们平常编程中用到的JSON对象。 文档中的字段值可以包含其他文档，数组及文档数组 相关概念 MongoDB术语/概念 说明 对比SQL术语/概念 database 数据库 database collection 集合 table document 文档 row field 字段 column index index 索引 primary key 主键 MongoDB自动将_id字段设置为主键 primary key BSON MongoDB中的JSON文档存储在名为BSON(二进制编码的JSON)的二进制表示中。与其他将JSON数据存储为简单字 符串和数字的数据库不同，BSON编码扩展了JSON表示，使其包含额外的类型，如int、long、date、浮点数和 decimal128。这使得应用程序更容易可靠地处理、排序和比较数据。 连接MongoDB的Go驱动程序中有两大类型表示BSON数据：D和Raw。 类型D家族被用来简洁地构建使用本地Go类型的BSON对象。这对于构造传递给MongoDB的命令特别有用。D家族包括四类: D：一个BSON文档。这种类型应该在顺序重要的情况下使用，比如MongoDB命令。 M：一张无序的map。它和D是一样的，只是它不保持顺序。 A：一个BSON数组。 E：D里面的一个元素。 实践出真理 start one container and connect docker run -d -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=root -e MONGO_INITDB_ROOT_PASSWORD=123456 registry.cn-hangzhou.aliyuncs.com/launcher/mongo:4.2.1 docker exec -it $container_id sh #mongo 192.168.1.200:27017/test -u user -p password mongo -u root -p 123456 base operation show dbs; use test; db.createCollection(\"hello\"); show collections; db.hello.insert({\"name\":\"testdb\"}); db.hello.drop() Index 在hello集合上，建立对ID字段的索引，1代表升序。 >db.hello.ensureIndex({ID:1}) 在hello集合上，建立对ID字段、Name字段和Gender字段建立索引 >db.hello.ensureIndex({ID:1,Name:1,Gender:-1}) 查看hello集合上的所有索引 >db.hello.getIndexes() 删除索引用db.collection.dropIndex()，有一个参数，可以是建立索引时指定的字段，也可以是getIndex看到的索引名称。 >db.hello.dropIndex( \"IDIdx\" ) >db.hello.dropIndex({ID:1}) 插入 db.inventory.insertMany([ { item: \"journal\", qty: 25, size: { h: 14, w: 21, uom: \"cm\" }, status: \"A\" }, { item: \"notebook\", qty: 50, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"A\" }, { item: \"paper\", qty: 100, size: { h: 8.5, w: 11, uom: \"in\" }, status: \"D\" }, { item: \"planner\", qty: 75, size: { h: 22.85, w: 30, uom: \"cm\" }, status: \"D\" }, { item: \"postcard\", qty: 45, size: { h: 10, w: 15.25, uom: \"cm\" }, status: \"A\" } ]); 查询 一般查询 db.inventory.find( { status: \"D\" } ) SELECT * FROM inventory WHERE status = \"D\" db.inventory.find( { status: { $in: [ \"A\", \"D\" ] } } ) db.inventory.find( { status: \"A\", qty: { $lt: 30 } } ) db.inventory.find( { status: \"A\", qty: { $lt: 30 } } ) SELECT * FROM inventory WHERE status = \"A\" AND qty db.inventory.find( { $or: [ { status: \"A\" }, { qty: { $lt: 30 } } ] } ) SELECT * FROM inventory WHERE status = \"A\" OR qty db.inventory.find( { status: \"A\", $or: [ { qty: { $lt: 30 } }, { item: /^p/ } ] } ) SELECT * FROM inventory WHERE status = \"A\" AND ( qty Query on Embedded/Nested Documents db.inventory.find( { size: { w: 21, h: 14, uom: \"cm\" } } ) db.inventory.find( { \"size.h\": { $lt: 15 } } ) db.inventory.find( { \"size.h\": { $lt: 15 }, \"size.uom\": \"in\", status: \"D\" } ) Query an Array db.inventory.insertMany([ { item: \"journal\", qty: 25, tags: [\"blank\", \"red\"], dim_cm: [ 14, 21 ] }, { item: \"notebook\", qty: 50, tags: [\"red\", \"blank\"], dim_cm: [ 14, 21 ] }, { item: \"paper\", qty: 100, tags: [\"red\", \"blank\", \"plain\"], dim_cm: [ 14, 21 ] }, { item: \"planner\", qty: 75, tags: [\"blank\", \"red\"], dim_cm: [ 22.85, 30 ] }, { item: \"postcard\", qty: 45, tags: [\"blue\"], dim_cm: [ 10, 15.25 ] } ]); 有顺序要求 db.inventory.find( { tags: [\"red\", \"blank\"] } ) # 只能查询到一条 没有顺序要求 db.inventory.find( { tags: { $all: [\"red\", \"blank\"] } } ) # 可以查询到3条 Query an Array for an Element To query if the array field contains at least one element with the specified value, use the filter { : } where is the element value.db.inventory.find( { tags: \"red\" } ) 更新 syntax: db.collection.update(criteria, objNew, upsert, multi ) criteria:update的查询条件，类似sql update查询内where后面的 objNew:update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的。 upsert : 如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。 multi : mongodb默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 匹配数组中单个字段 将第一个文档中grade字段中值为85更新为1000，如果不知道数组中元素的位置，可以使用位置$操作符 请记住，位置$操作符充当更新文档查询中第一个匹配的占位符, 只修改匹配到的第一个，假设有2个85， 需要执行两次才能全部更新。 db.collection.update( { }, { : { \"array.$.field\" : value } } ) db.students.insert({_id:1,grades:[80,85,90]}) db.students.insert({_id:2,grades:[88,90,92]}) db.students.insert({_id:3,grades:[85,100,90,85]}) db.students.update({_id:3, grades:85},{$set:{'grades.$':1000}}) 匹配数组中多个字段 位置操作符$能够更新第一个匹配的数组元素通过$elemMatch()操作符匹配多个内嵌文档的查询条件 在golang中 的eleMatch 是match 如下语句会更新嵌套文档中的std值为6，条件是文档的主键是4，字段grades的嵌套文档字段grade字段值小于等于90mean字段值大于80 db.students.update( { _id: 4, grades: { $elemMatch: { grade: { $lte: 90 }, mean: { $gt: 80 } } } }, { $set: { \"grades.$.std\" : 6 } } ) 根据数组特定位置的元素 大小来查询 Using dot notation, you can specify query conditions for an element at a particular index or position of the array. The array uses zero-based indexing.db.inventory.find( { \"dim_cm.1\": { $gt: 25 } } ) Query an Array by Array Length Use the $size operator to query for arrays by number of elements. For example, the following selects documents where the array tags has 3 elements.db.inventory.find( { \"tags\": { $size: 3 } } ) Query an Array of Embedded Documents Query for a Document Nested in an Array The following example selects all documents where an element in the instock array matches the specified document: ``` db.inventory.insertMany( [ { item: \"journal\", instock: [ { warehouse: \"A\", qty: 5 }, { warehouse: \"C\", qty: 15 } ] }, { item: \"notebook\", instock: [ { warehouse: \"C\", qty: 5 } ] }, { item: \"paper\", instock: [ { warehouse: \"A\", qty: 60 }, { warehouse: \"B\", qty: 15 } ] }, { item: \"planner\", instock: [ { warehouse: \"A\", qty: 40 }, { warehouse: \"B\", qty: 5 } ] }, { item: \"postcard\", instock: [ { warehouse: \"B\", qty: 15 }, { warehouse: \"C\", qty: 35 } ] } ]); db.inventory.find( { \"instock\": { warehouse: \"A\", qty: 5 } } ) #### Equality matches on the whole embedded/nested document require an exact match of the specified document, including the field order. For example, the following query does not match any documents in the inventory collection: db.inventory.find( { \"instock\": { qty: 5, warehouse: \"A\" } } ) #### Specify a Query Condition on a Field in an Array of Documents db.inventory.find( { 'instock.qty': { $lte: 20 } } ) #### Use the Array Index to Query for a Field in the Embedded Document db.inventory.find( { 'instock.0.qty': { $lte: 20 } } ) #### Specify Multiple Conditions for Array of Document > * When specifying conditions on more than one field nested in an array of documents, you can specify the query such that either a single document meets these condition or any combination of documents (including a single document) in the array meets the conditions. db.inventory.find( { \"instock\": { $elemMatch: { qty: 5, warehouse: \"A\" } } } ) db.inventory.find( { \"instock\": { $elemMatch: { qty: { $gt: 10, $lte: 20 } } } } ) #### Combination of Elements Satisfies the Criteria If the compound query conditions on an array field do not use the $elemMatch operator, the query selects those documents whose array contains any combination of elements that satisfies the conditions. For example, the following query matches documents where any document nested in the instock array has the qty field greater than 10 and any document (but not necessarily the same embedded document) in the array has the qty field less than or equal to 20: db.inventory.find( { \"instock.qty\": { $gt: 10, $lte: 20 } } ) db.inventory.find( { \"instock.qty\": 5, \"instock.warehouse\": \"A\" } ) ### Project Fields to Return from Query #### Return the Specified Fields and the _id Field Only db.inventory.insertMany( [ { item: \"journal\", status: \"A\", size: { h: 14, w: 21, uom: \"cm\" }, instock: [ { warehouse: \"A\", qty: 5 } ] }, { item: \"notebook\", status: \"A\", size: { h: 8.5, w: 11, uom: \"in\" }, instock: [ { warehouse: \"C\", qty: 5 } ] }, { item: \"paper\", status: \"D\", size: { h: 8.5, w: 11, uom: \"in\" }, instock: [ { warehouse: \"A\", qty: 60 } ] }, { item: \"planner\", status: \"D\", size: { h: 22.85, w: 30, uom: \"cm\" }, instock: [ { warehouse: \"A\", qty: 40 } ] }, { item: \"postcard\", status: \"A\", size: { h: 10, w: 15.25, uom: \"cm\" }, instock: [ { warehouse: \"B\", qty: 15 }, { warehouse: \"C\", qty: 35 } ] } ]); db.inventory.find( { status: \"A\" }, { item: 1, status: 1 } ) ``` 原文链接 mongodb_doc group_aggregate Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/database/readme.html":{"url":"blog/database/readme.html","title":"Readme","keywords":"","body":"Database Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/database/statefulset.html":{"url":"blog/database/statefulset.html","title":"Statefulset","keywords":"","body":"准备工作 几个副本需要几个node节点 设置pod 亲和度 设置 node label 初始化 set password apiVersion: v1 kind: Service metadata: name: mongo-rs-svc labels: name: mongo namespace: 'default' spec: ports: - port: 27017 targetPort: 27017 selector: app: mongo-rs --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mongo-rs namespace: 'default' spec: serviceName: mongo-rs-svc replicas: 3 selector: matchLabels: app: mongo-rs template: metadata: labels: app: mongo-rs spec: hostNetwork: true terminationGracePeriodSeconds: 10 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - mongo-rs topologyKey: kubernetes.io/hostname containers: - env: - name: MONGO_INITDB_DATABASE value: admin - name: MONGO_INITDB_ROOT_USERNAME value: root - name: MONGO_INITDB_ROOT_PASSWORD value: \"123456\" name: mongod image: registry.cn-hangzhou.aliyuncs.com/launcher/mongo:4.2.1 command: [\"sh\", \"-c\", \"numactl --interleave=all mongod -f /etc/conf.d/mongodb --replSet rs\"] resources: requests: cpu: 500m memory: 500m volumeMounts: - name: mongodb-data mountPath: /var/lib/mongodb - name: mongo-config mountPath: /etc/conf.d ports: - containerPort: 27017 volumes: - name: mongo-config configMap: name: mongodb-rs-config - hostPath: path: /usr/local/mongodb type: \"\" name: mongodb-data --- apiVersion: v1 kind: ConfigMap metadata: name: mongodb-rs-config namespace: 'default' data: mongodb: | processManagement: fork: false net: port: 27017 bindIp: 0.0.0.0 storage: dbPath: /var/lib/mongodb --- apiVersion: v1 kind: Service metadata: name: lsh-mcp-mongo namespace: 'default' spec: ports: - name: http nodePort: 32436 port: 27017 protocol: TCP targetPort: 27017 selector: statefulset.kubernetes.io/pod-name: mongo-rs-0 sessionAffinity: None type: NodePort 初始化 rs.initiate({ _id: \"rs\", version: 1, members: [ { _id: 0, host : \"10.168.12.208:27017\" }, { _id: 1, host : \"10.168.12.117:27017\" }, { _id: 2, host : \"10.168.12.236:27017\" }, ]}); rs.initiate({ _id: \"rs\", version: 1, members: [ { _id: 0, host : \"10.168.12.176:27017\" }, { _id: 1, host : \"mongo-rs-1.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\" }, { _id: 2, host : \"mongo-rs-2.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\" }, ]}); rs.reconfig({_id: \"rs\", version: 1, protocolVersion: 1, members: [ { _id: 0, host : \"10.168.12.176:27017\" }, { _id: 1, host : \"mongo-rs-1.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\" }, { _id: 2, host : \"mongo-rs-2.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\" }, {\"force\":true}); use admin db.createUser( { user: \"root\", pwd: \"123456\", roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } ) set password use admin db.createUser( { user: \"root\", pwd: \"password\", roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } ) db.auth(\"root\",\"password\") mongorestore --host localhost:27017 -u root -p 123456 --authenticationDatabase=admin --dir=./dump mongodump --host 10.66.187.127:27017 -u mongouser -p thepasswordA1 --authenticationDatabase=admin --db=testdb -o /data/dump_testdb Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/docker/":{"url":"blog/docker/","title":"Docker","keywords":"","body":"Docker Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/docker/CentOS.html":{"url":"blog/docker/CentOS.html","title":"CentOS","keywords":"","body":"CentOS-8 警告：切勿在没有配置 Docker YUM 源的情况下直接使用 yum 命令安装 Docker. [toc] 准备工作 系统要求 Docker 支持 64 位版本 CentOS 7/8，并且要求内核版本不低于 3.10。 CentOS 7 满足最低内核的要求，但由于内核版本比较低，部分功能（如 overlay2 存储层驱动）无法使用，并且部分功能可能不太稳定。 卸载旧版本 旧版本的 Docker 称为 docker 或者 docker-engine，使用以下命令卸载旧版本： sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 使用 yum 安装 执行以下命令安装依赖包： sudo yum install -y yum-utils 鉴于国内网络问题，强烈建议使用国内源，官方源请在注释中查看。 执行下面的命令添加 yum 软件源： sudo yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sudo sed -i 's/download.docker.com/mirrors.aliyun.com\\/docker-ce/g' /etc/yum.repos.d/docker-ce.repo # 官方源 # $ sudo yum-config-manager \\ # --add-repo \\ # https://download.docker.com/linux/centos/docker-ce.repo 如果需要测试版本的 Docker 请执行以下命令： $ sudo yum-config-manager --enable docker-ce-test 安装 Docker 更新 yum 软件源缓存，并安装 docker-ce。 $ sudo yum install docker-ce docker-ce-cli containerd.io CentOS8 额外设置 由于 CentOS8 防火墙使用了 nftables，但 Docker 尚未支持 nftables， 我们可以使用如下设置使用 iptables： 更改 /etc/firewalld/firewalld.conf # FirewallBackend=nftables FirewallBackend=iptables 或者执行如下命令： $ firewall-cmd --permanent --zone=trusted --add-interface=docker0 $ firewall-cmd --reload 使用脚本自动安装 在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，CentOS 系统上可以使用这套脚本安装，另外可以通过 --mirror 选项使用国内源进行安装： 若你想安装测试版的 Docker, 请从 test.docker.com 获取脚本 # $ curl -fsSL test.docker.com -o get-docker.sh $ curl -fsSL get.docker.com -o get-docker.sh $ sudo sh get-docker.sh --mirror Aliyun # $ sudo sh get-docker.sh --mirror AzureChinaCloud 执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker 的稳定(stable)版本安装在系统中。 启动 Docker $ sudo systemctl enable docker $ sudo systemctl start docker 建立 docker 用户组 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。 建立 docker 组： $ sudo groupadd docker 将当前用户加入 docker 组： $ sudo usermod -aG docker $USER 退出当前终端并重新登录，进行如下测试。 测试 Docker 是否安装正确 $ docker run --rm hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b8dfde127a29: Pull complete Digest: sha256:308866a43596e83578c7dfa15e27a73011bdd402185a84c5cd7f32a88b501a24 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 若能正常输出以上信息，则说明安装成功。 镜像加速 如果在使用过程中发现拉取 Docker 镜像十分缓慢，可以配置 Docker 国内镜像加速。 添加内核参数 如果在 CentOS 使用 Docker 看到下面的这些警告信息： WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled 请添加内核配置参数以启用这些功能。 $ sudo tee -a /etc/sysctl.conf 然后重新加载 sysctl.conf 即可 $ sudo sysctl -p 参考文档 Docker 官方 CentOS 安装文档。 https://firewalld.org/2018/07/nftables-backend https://github.com/moby/libnetwork/issues/2496 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:22:34 "},"blog/docker/install.html":{"url":"blog/docker/install.html","title":"Install","keywords":"","body":"安装 docker shell 脚本 #!/usr/bin/env bash echo \"clean env\" yum remove -y docker docker-common container-selinux docker-selinux docker-engine rm -rf /var/lib/docker echo \"install docker 18.09.8\" sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum clean packages #查看docker-ce版本并且安装 yum list docker-ce --showduplicates | sort -r sudo yum install -y docker-ce-19.03.14 echo \"config docker daemon\" mkdir -p /etc/docker cat > /etc/docker/daemon.json #修改启动文件 echo \"[Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network.target firewalld.service [Service] Type=notify #ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:6071 --insecure-registry=0.0.0.0/0 ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock --insecure-registry=0.0.0.0/0 ExecReload=/bin/kill -s HUP LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TimeoutStartSec=0 Delegate=yes KillMode=process [Install] WantedBy=multi-user.target\" > /usr/lib/systemd/system/docker.service # default native.cgroupdriver=systemd ##如果要开启tls # https://blog.csdn.net/laodengbaiwe0838/article/details/79340805 # --service # -H=tcp://0.0.0.0:2376 # 修改端口号为2376 # -H=unix:///var/run/docker.sock # --tlsverify # --tlscacert=/etc/docker/ca.pem # --tlscert=/etc/docker/server-cert.pem # --tlskey=/etc/docker/server-key.pem #校验 export http_proxy='' export https_proxy='' curl localhost:6071/info Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/docker/readme.html":{"url":"blog/docker/readme.html","title":"Readme","keywords":"","body":"Docker Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/docker/官方shell.html":{"url":"blog/docker/官方shell.html","title":"官方Shell","keywords":"","body":"docker 官方 install shell #!/bin/sh set -e # Docker CE for Linux installation script # # See https://docs.docker.com/install/ for the installation steps. # # This script is meant for quick & easy install via: # $ curl -fsSL https://get.docker.com -o get-docker.sh # $ sh get-docker.sh # # For test builds (ie. release candidates): # $ curl -fsSL https://test.docker.com -o test-docker.sh # $ sh test-docker.sh # # NOTE: Make sure to verify the contents of the script # you downloaded matches the contents of install.sh # located at https://github.com/docker/docker-install # before executing. # # Git commit from https://github.com/docker/docker-install when # the script was uploaded (Should only be modified by upload job): SCRIPT_COMMIT_SHA=\"26ff363bcf3b3f5a00498ac43694bf1c7d9ce16c\" # The channel to install from: # * nightly # * test # * stable # * edge (deprecated) DEFAULT_CHANNEL_VALUE=\"test\" if [ -z \"$CHANNEL\" ]; then CHANNEL=$DEFAULT_CHANNEL_VALUE fi DEFAULT_DOWNLOAD_URL=\"https://download.docker.com\" if [ -z \"$DOWNLOAD_URL\" ]; then DOWNLOAD_URL=$DEFAULT_DOWNLOAD_URL fi DEFAULT_REPO_FILE=\"docker-ce.repo\" if [ -z \"$REPO_FILE\" ]; then REPO_FILE=\"$DEFAULT_REPO_FILE\" fi mirror='' DRY_RUN=${DRY_RUN:-} while [ $# -gt 0 ]; do case \"$1\" in --mirror) mirror=\"$2\" shift ;; --dry-run) DRY_RUN=1 ;; --*) echo \"Illegal option $1\" ;; esac shift $(( $# > 0 ? 1 : 0 )) done case \"$mirror\" in Aliyun) DOWNLOAD_URL=\"https://mirrors.aliyun.com/docker-ce\" ;; AzureChinaCloud) DOWNLOAD_URL=\"https://mirror.azure.cn/docker-ce\" ;; esac command_exists() { command -v \"$@\" > /dev/null 2>&1 } is_dry_run() { if [ -z \"$DRY_RUN\" ]; then return 1 else return 0 fi } is_wsl() { case \"$(uname -r)\" in *microsoft* ) true ;; # WSL 2 *Microsoft* ) true ;; # WSL 1 * ) false;; esac } is_darwin() { case \"$(uname -s)\" in *darwin* ) true ;; *Darwin* ) true ;; * ) false;; esac } deprecation_notice() { distro=$1 date=$2 echo echo \"DEPRECATION WARNING:\" echo \" The distribution, $distro, will no longer be supported in this script as of $date.\" echo \" If you feel this is a mistake please submit an issue at https://github.com/docker/docker-install/issues/new\" echo sleep 10 } get_distribution() { lsb_dist=\"\" # Every system that we officially support has /etc/os-release if [ -r /etc/os-release ]; then lsb_dist=\"$(. /etc/os-release && echo \"$ID\")\" fi # Returning an empty string here should be alright since the # case statements don't act unless you provide an actual value echo \"$lsb_dist\" } add_debian_backport_repo() { debian_version=\"$1\" backports=\"deb http://ftp.debian.org/debian $debian_version-backports main\" if ! grep -Fxq \"$backports\" /etc/apt/sources.list; then (set -x; $sh_c \"echo \\\"$backports\\\" >> /etc/apt/sources.list\") fi } echo_docker_as_nonroot() { if is_dry_run; then return fi if command_exists docker && [ -e /var/run/docker.sock ]; then ( set -x $sh_c 'docker version' ) || true fi your_user=your-user [ \"$user\" != 'root' ] && your_user=\"$user\" # intentionally mixed spaces and tabs here -- tabs are stripped by \" /dev/null 2>&1 lsb_release_exit_code=$? set -e # Check if the command has exited successfully, it means we're in a forked distro if [ \"$lsb_release_exit_code\" = \"0\" ]; then # Print info about current distro cat &1 | tr '[:upper:]' '[:lower:]' | grep -E 'id' | cut -d ':' -f 2 | tr -d '[:space:]') dist_version=$(lsb_release -a -u 2>&1 | tr '[:upper:]' '[:lower:]' | grep -E 'codename' | cut -d ':' -f 2 | tr -d '[:space:]') # Print info about upstream distro cat &2 &2 &2 &2 /dev/null || true)\" sh_c='sh -c' if [ \"$user\" != 'root' ]; then if command_exists sudo; then sh_c='sudo -E sh -c' elif command_exists su; then sh_c='su -c' else cat >&2 &2 /dev/null; then pre_reqs=\"$pre_reqs gnupg\" fi apt_repo=\"deb [arch=$(dpkg --print-architecture)] $DOWNLOAD_URL/linux/$lsb_dist $dist_version $CHANNEL\" ( if ! is_dry_run; then set -x fi $sh_c 'apt-get update -qq >/dev/null' $sh_c \"DEBIAN_FRONTEND=noninteractive apt-get install -y -qq $pre_reqs >/dev/null\" $sh_c \"curl -fsSL \\\"$DOWNLOAD_URL/linux/$lsb_dist/gpg\\\" | apt-key add -qq - >/dev/null\" $sh_c \"echo \\\"$apt_repo\\\" > /etc/apt/sources.list.d/docker.list\" $sh_c 'apt-get update -qq >/dev/null' ) pkg_version=\"\" if [ -n \"$VERSION\" ]; then if is_dry_run; then echo \"# WARNING: VERSION pinning is not supported in DRY_RUN\" else # Will work for incomplete versions IE (17.12), but may not actually grab the \"latest\" if in the test channel pkg_pattern=\"$(echo \"$VERSION\" | sed \"s/-ce-/~ce~.*/g\" | sed \"s/-/.*/g\").*-0~$lsb_dist\" search_command=\"apt-cache madison 'docker-ce' | grep '$pkg_pattern' | head -1 | awk '{\\$1=\\$1};1' | cut -d' ' -f 3\" pkg_version=\"$($sh_c \"$search_command\")\" echo \"INFO: Searching repository for VERSION '$VERSION'\" echo \"INFO: $search_command\" if [ -z \"$pkg_version\" ]; then echo echo \"ERROR: '$VERSION' not found amongst apt-cache madison results\" echo exit 1 fi search_command=\"apt-cache madison 'docker-ce-cli' | grep '$pkg_pattern' | head -1 | awk '{\\$1=\\$1};1' | cut -d' ' -f 3\" # Don't insert an = for cli_pkg_version, we'll just include it later cli_pkg_version=\"$($sh_c \"$search_command\")\" pkg_version=\"=$pkg_version\" fi fi ( if ! is_dry_run; then set -x fi if [ -n \"$cli_pkg_version\" ]; then $sh_c \"apt-get install -y -qq --no-install-recommends docker-ce-cli=$cli_pkg_version >/dev/null\" fi $sh_c \"apt-get install -y -qq --no-install-recommends docker-ce$pkg_version >/dev/null\" ) echo_docker_as_nonroot exit 0 ;; centos|fedora|rhel) yum_repo=\"$DOWNLOAD_URL/linux/$lsb_dist/$REPO_FILE\" if ! curl -Ifs \"$yum_repo\" > /dev/null; then echo \"Error: Unable to curl repository file $yum_repo, is it valid?\" exit 1 fi if [ \"$lsb_dist\" = \"fedora\" ]; then pkg_manager=\"dnf\" config_manager=\"dnf config-manager\" enable_channel_flag=\"--set-enabled\" disable_channel_flag=\"--set-disabled\" pre_reqs=\"dnf-plugins-core\" pkg_suffix=\"fc$dist_version\" else pkg_manager=\"yum\" config_manager=\"yum-config-manager\" enable_channel_flag=\"--enable\" disable_channel_flag=\"--disable\" pre_reqs=\"yum-utils\" pkg_suffix=\"el\" fi ( if ! is_dry_run; then set -x fi $sh_c \"$pkg_manager install -y -q $pre_reqs\" $sh_c \"$config_manager --add-repo $yum_repo\" if [ \"$CHANNEL\" != \"stable\" ]; then $sh_c \"$config_manager $disable_channel_flag docker-ce-*\" $sh_c \"$config_manager $enable_channel_flag docker-ce-$CHANNEL\" fi $sh_c \"$pkg_manager makecache\" ) pkg_version=\"\" if [ -n \"$VERSION\" ]; then if is_dry_run; then echo \"# WARNING: VERSION pinning is not supported in DRY_RUN\" else pkg_pattern=\"$(echo \"$VERSION\" | sed \"s/-ce-/\\\\\\\\.ce.*/g\" | sed \"s/-/.*/g\").*$pkg_suffix\" search_command=\"$pkg_manager list --showduplicates 'docker-ce' | grep '$pkg_pattern' | tail -1 | awk '{print \\$2}'\" pkg_version=\"$($sh_c \"$search_command\")\" echo \"INFO: Searching repository for VERSION '$VERSION'\" echo \"INFO: $search_command\" if [ -z \"$pkg_version\" ]; then echo echo \"ERROR: '$VERSION' not found amongst $pkg_manager list results\" echo exit 1 fi search_command=\"$pkg_manager list --showduplicates 'docker-ce-cli' | grep '$pkg_pattern' | tail -1 | awk '{print \\$2}'\" # It's okay for cli_pkg_version to be blank, since older versions don't support a cli package cli_pkg_version=\"$($sh_c \"$search_command\" | cut -d':' -f 2)\" # Cut out the epoch and prefix with a '-' pkg_version=\"-$(echo \"$pkg_version\" | cut -d':' -f 2)\" fi fi ( if ! is_dry_run; then set -x fi # install the correct cli version first if [ -n \"$cli_pkg_version\" ]; then $sh_c \"$pkg_manager install -y -q docker-ce-cli-$cli_pkg_version\" fi $sh_c \"$pkg_manager install -y -q docker-ce$pkg_version\" ) echo_docker_as_nonroot exit 0 ;; *) if [ -z \"$lsb_dist\" ]; then if is_darwin; then echo echo \"ERROR: Unsupported operating system 'macOS'\" echo \"Please get Docker Desktop from https://www.docker.com/products/docker-desktop\" echo exit 1 fi fi echo echo \"ERROR: Unsupported distribution '$lsb_dist'\" echo exit 1 ;; esac exit 1 } # wrapped up in a function so that we have some protection against only getting # half the file during \"curl | sh\" do_install Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/":{"url":"blog/golang/","title":"Golang","keywords":"","body":"Golang Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/aop.html":{"url":"blog/golang/aop.html","title":"Aop","keywords":"","body":"golang apply AOP AOP 的核心概念 切面（Aspect） ：通常是一个类，在里面可以定义切入点和通知。 连接点（Joint Point） ：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中连接点指的就是被拦截的到的方法，实际上连接点还可以是字段或者构造器。 切入点（Pointcut） ：对连接点进行拦截的定义。 通知（Advice） ：拦截到连接点之后所要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类。 AOP 代理 ：AOP 框架创建的对象，代理就是目标对象的加强。Spring 中的 AOP 代理可以使 JDK 动态代理，也可以是 CGLIB 代理，前者基于接口，后者基于子类。 golang code package main import ( \"errors\" \"fmt\" ) // User type User struct { Name string Pass string } // Auth 验证 func (u *User) Auth() { // 实际业务逻辑 fmt.Printf(\"register user:%s, use pass:%s\\n\", u.Name, u.Pass) } // UserAdvice type UserAdvice interface { // Before 前置通知 Before(user *User) error // After 后置通知 After(user *User) } // ValidatePasswordAdvice 用户名验证 type ValidateNameAdvice struct { } // ValidatePasswordAdvice 密码验证 type ValidatePasswordAdvice struct { MinLength int MaxLength int } func (ValidateNameAdvice) Before(user *User) error { fmt.Println(\"ValidateNameAdvice before\") if user.Name == \"admin\" { return errors.New(\"admin can't be used\") } return nil } func (ValidateNameAdvice) After(user *User) { fmt.Println(\"ValidateNameAdvice after\") fmt.Printf(\"username:%s validate sucess\\n\", user.Name) } // Before 前置校验 func (advice ValidatePasswordAdvice) Before(user *User) error { fmt.Println(\"ValidatePasswordAdvice before\") if user.Pass == \"123456\" { return errors.New(\"pass isn't strong\") } if len(user.Pass) > advice.MaxLength { return fmt.Errorf(\"len of pass must less than:%d\", advice.MaxLength) } if len(user.Pass) Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/atomic.html":{"url":"blog/golang/atomic.html","title":"Atomic","keywords":"","body":"atomic 背景 高并发是个很常见的场景，为了确保数据计算的准确性，我们要求事物进行原子操作。golang 中sync/atomic就是解决这个问题的。 什么是原子性、原子操作 原子(atomic)本意是\"不能被进一步分割的最小粒子\"，而原子操作(atomic operation)意为\"不可中断的一个或一系列操作\"。其实用大白话说出来就是让多个线程对同一块内存的操作是串行的，不会因为并发操作把内存写的不符合预期。我们来看这样一个例子：假设现在是一个银行账户系统，用户A想要自己从自己的账户中转1万元到用户B的账户上，直到转帐成功完成一个事务，主要做这两件事： 从A的账户中减去1万元，如果A的账户原来就有2万元，现在就变成了1万元 给B的账户添加1万元，如果B的账户原来有2万元，那么现在就变成了3万元 假设在操作一的时候，系统发生了故障，导致给B账户添加款项失败了，那么就要进行回滚。回滚就是回到事务之前的状态，我们把这种要么一起成功的操作叫做原子操作，而原子性就是要么完整的被执行、要么完全不执行。 如何保证原子性 锁机制 在处理器层面，可以采用总线加锁或者对缓存加锁的方式来实现多处理器之间的原子操作。通过加锁保证从系统内存中读取或写入一个字节是原子的，也就是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。 总线锁：处理器提供一个Lock#信号，当一个处理器的总线上输出此 信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。总线锁会把CPU和内存之间的通信锁住了，在锁定期间，其他处理就不能操作其他内存地址的数据，所以总线锁定的开销比较大，所以处理会在某些场合使用缓存锁进行优化。缓存锁：内存区域如果被缓存在处理器上的缓存行中，并且在Lock#操作期间，那么当它执行操作回写到内存时，处理不在总线上声言Lock#信号，而是修改内部的内存地址，并允许它的缓存一致机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域的数据，其他处理器回写已被锁定的缓存行的数据时，就会使缓存无效。 锁机制虽然可以保证原子性，但是锁机制会存在以下问题： 多线程竞争的情况下，频繁的加锁、释放锁会导致较多的上下文切换和调度延时，性能会很差 当一个线程占用时间比较长时，就导致其他需要此锁的线程挂起. 上面我们说的都是悲观锁，要解决这种低效的问题，我们可以采用乐观锁，每次不加锁，而是假设没有冲突去完成某项操作，如果因为冲突失败就重试，直到成功为止。也就是我们接下来要说的CAS(compare and swap). CAS(compare and swap) CAS的全称为Compare And Swap，直译就是比较交换。是一条CPU的原子指令，其作用是让CPU先进行比较两个值是否相等，然后原子地更新某个位置的值，其实现方式是给予硬件平台的汇编指令，在intel的CPU中，使用的cmpxchg指令，就是说CAS是靠硬件实现的，从而在硬件层面提升效率。简述过程是这样： 假设包含3个参数内存位置(V)、预期原值(A)和新值(B)。V表示要更新变量的值，E表示预期值，N表示新值。仅当V值等于E值时，才>会将V的值设为N，如果V值和E值不同，则说明已经有其他线程在做更新，则当前线程什么都不做，最后CAS返回当前V的真实值。CAS操作时抱着乐观的态度进行的，它总是认为自己可以成功完成操作。基于这样的原理，CAS操作即使没有锁，也可以发现其他线程对于当前线程的干扰。 Codes func CompareAndSwap(int *addr,int oldValue,int newValue) bool{ if *addr == nil{ return false } if *addr == oldValue { *addr = newValue return true } return false } 不过上面的代码可能会发生一个问题，也就是ABA问题，因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就会变成1A-2B－3A。 goalng 标准库原子操作 在Go语言标准库中，sync/atomic包将底层硬件提供的原子操作封装成了Go的函数，主要分为5个系列的函数，分别是： func SwapXXXX(addr int32, new int32) (old int32)系列：其实就是原子性的将new值保存到`addr`并返回旧值。代码表示： old = *addr *addr = new return old func CompareAndSwapXXXX((addr int64, old, new int64) (swapped bool)系列：其就是原子性的比较`addr和old的值，如果相同则将new赋值给*addr`并返回真，代码表示： if *addr == old{ *addr = new return ture } return false func AddXXXX(addr int64, delta int64) (new int64)系列：原子性的将val的值添加到`addr`并返回新值。代码表示： *addr += delta return *addr func LoadXXXX(addr uint32) (val uint32)系列：原子性的获取`addr`的值 func StoreXXXX(addr int32, val int32)原子性的将val值保存到`addr` Go语言在1.4版本时添加一个新的类型Value，此类型的值就相当于一个容器，可以被用来\"原子地\"存储(store)和加载(Load)任意类型的值。这些使用起来都还比较简单，就不写例子了，接下来我们一起看一看这些方法是如何实现的。 源码解析 由于系列比较多。底层实现的方法也大同小异，这里就主要分析一下Value的实现方法吧。为什么不分析其他系列的呢？因为原子操作由底层硬件支持，所以看其他系列实现都要看汇编，Go的汇编是基于Plan9的，这个汇编语言真的资料甚少，我也是真的不懂，水平不够，也不自讨苦吃了，等后面真的能看懂这些汇编了，再来分析吧。这个网站有一些关于plan9汇编的知识，有兴趣可以看一看：http://doc.cat-v.org/plan_9/4th_edition/papers/asm。 Value结构 我们先来看一下Value的结构： type Value struct { v interface{} } Value结构里就只有一个字段，是interface类型，虽然这里是interface类型，但是这里要注意，第一次Store写入的类型就确定了之后写入的类型，否则会发生panic。因为这里是interface类型，所以为了之后写入与读取操作方便，又在这个包里定义了一个ifaceWords结构，其实他就是一个空interface，他的作用就是将interface分解成类型和数值。结构如下： // ifaceWords is interface{} internal representation. type ifaceWords struct { typ unsafe.Pointer data unsafe.Pointer } Value的写入操作 我们一起来看一看他是如何实现写入操作的： // Store sets the value of the Value to x. // All calls to Store for a given Value must use values of the same concrete type. // Store of an inconsistent type panics, as does Store(nil). func (v *Value) Store(x interface{}) { if x == nil { panic(\"sync/atomic: store of nil value into Value\") } vp := (*ifaceWords)(unsafe.Pointer(v)) xp := (*ifaceWords)(unsafe.Pointer(&x)) for { typ := LoadPointer(&vp.typ) if typ == nil { // Attempt to start first store. // Disable preemption so that other goroutines can use // active spin wait to wait for completion; and so that // GC does not see the fake type accidentally. runtime_procPin() if !CompareAndSwapPointer(&vp.typ, nil, unsafe.Pointer(^uintptr(0))) { runtime_procUnpin() continue } // Complete first store. StorePointer(&vp.data, xp.data) StorePointer(&vp.typ, xp.typ) runtime_procUnpin() return } if uintptr(typ) == ^uintptr(0) { // First store in progress. Wait. // Since we disable preemption around the first store, // we can wait with active spinning. continue } // First store completed. Check type and overwrite data. if typ != xp.typ { panic(\"sync/atomic: store of inconsistently typed value into Value\") } StorePointer(&vp.data, xp.data) return } } // Disable/enable preemption, implemented in runtime. func runtime_procPin() func runtime_procUnpin() 这段代码中的注释集已经告诉了我们，调用Store方法写入的类型必须与原类型相同，不一致便会发生panic。接下来分析代码实现： 首先判断条件写入参数不能为nil，否则触发panic 通过使用unsafe.Pointer将oldValue和newValue转换成ifaceWords类型。方便我们获取他的原始类型(typ)和值(data). 为了保证原子性，所以这里使用一个for换来处理，当已经有Store正在进行写入时，会进行等待. 如果还没写入过数据，那么获取不到原始类型，就会开始第一次写入操作，这里会把先调用runtime_procPin()方法禁止调度器对当前 goroutine 的抢占（preemption），这样也可以防止GC线程看到假类型。 调用CAS方法来判断当前地址是否有被抢占，这里大家可能对unsafe.Pointer(^uintptr(0))这一句话有点不明白，因为是第一个写入数据，之前是没有数据的，所以通过这样一个中间值来做判断，如果失败就会解除抢占锁，解除禁止调度器，继续循环等待. 设置中间值成功后，我们接下来就可以安全的把v设为传入的新值了，这里会先写入值，在写入类型(typ)，因为我们会根据ty来做完成判断。 第一次写入没完成，我们还会通过uintptr(typ) == ^uintptr(0)来进行判断，因为还是第一次放入的中间类型，他依然会继续等待第一次完成。 如果第一次写入完成，会检查上一次写入的类型与这次写入的类型是否一致，不一致则会抛出panic. 这里代码量没有多少，相信大家一定看懂了吧～。 Value的读操作 先看一下代码： // Load returns the value set by the most recent Store. // It returns nil if there has been no call to Store for this Value. func (v *Value) Load() (x interface{}) { vp := (*ifaceWords)(unsafe.Pointer(v)) typ := LoadPointer(&vp.typ) if typ == nil || uintptr(typ) == ^uintptr(0) { // First store not yet completed. return nil } data := LoadPointer(&vp.data) xp := (*ifaceWords)(unsafe.Pointer(&x)) xp.typ = typ xp.data = data return } 读取操作的代码就很简单了： 1.第一步使用unsafe.Pointer将oldValue转换成ifaceWords类型，然后获取他的类型，如果没有类型或者类型出去中间值，那么说明现在还没数据或者第一次写入还没有完成。 通过检查后，调用LoadPointer方法可以获取他的值，然后构造一个新interface的typ和data返回。 小彩蛋 前面我们在说CAS时，说到了ABA问题，所以我就写了demo试一试Go标准库atomic.CompareAndSwapXXX方法是否有解决这个问题，看运行结果是没有，所以这里大家使用的时候要注意一下(虽然我也没想到什么现在什么业务场景会出现这个问题，但是还是要注意一下，需要自己评估)。 func main() { var share uint64 = 1 wg := sync.WaitGroup{} wg.Add(3) // 协程1，期望值是1,欲更新的值是2 go func() { defer wg.Done() swapped := atomic.CompareAndSwapUint64(&share,1,2) fmt.Println(\"goroutine 1\",swapped) }() // 协程2，期望值是1，欲更新的值是2 go func() { defer wg.Done() time.Sleep(5 * time.Millisecond) swapped := atomic.CompareAndSwapUint64(&share,1,2) fmt.Println(\"goroutine 2\",swapped) }() // 协程3，期望值是2，欲更新的值是1 go func() { defer wg.Done() time.Sleep(1 * time.Millisecond) swapped := atomic.CompareAndSwapUint64(&share,2,1) fmt.Println(\"goroutine 3\",swapped) }() wg.Wait() fmt.Println(\"main exit\") } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/base/":{"url":"blog/golang/base/","title":"Base","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/base/copy.html":{"url":"blog/golang/base/copy.html","title":"Copy","keywords":"","body":"go-clone：极致优化深拷贝效率与拷贝私有字段 关于提升深拷贝效率的思考 前文所说的思路和方案，对于熟悉反射的开发者来说非常容易想到，属于常规解法了。不过我们都知道，反射的执行效率有限，对于字段较多的结构来说，深拷贝的效率会远低于浅拷贝。考虑到 Go 数据结构并不存在副作用，对于普通的数值类型可以直接浅拷贝，对于指针、接口、map 等类型也可以先浅拷贝再替换成新内容，这种拷贝方法会比通过反射来拷贝高效很多。 考虑以下数据结构。 type T1 struct { A int B string C []float64 d uint } 如果我们手动进行深拷贝，最高效的方法如下所示： t := &T1{ A: 123, B: \"test\", C: []float64{1.2, 3.4}, d: 321, } var cloned T1 cloned = *t // 先做一次浅拷贝。 cloned.C = make([]float64, len(t.C)) // 申请新的内存空间。 copy(cloned.T, t.C) // 将 t.C 内容拷贝过来。 这里需要注意，B string 是可以直接浅拷贝的，在 Go 里面约定 string 是不可变（immutable）的，其中引用的字符串不会被轻易修改，甚至字面常量都放在了只读的内存空间中来确保真正的不可变。 深拷贝私有字段 在这个例子里面暗含了一个「彩蛋」：本来不可见的私有字段（unexported field）d 也被「顺便」深拷贝了。 如果按照反射的方法，所有私有字段都不可写（私有字段的 reflect.Value 中 CanSet 方法始终返回 false）， 从而也不能在深拷贝的时候写入数据，但现在使用这种先浅拷贝再深拷贝的方法会造成私有字段也被拷贝，假设其中包含指针之类类型，那么这个指针就会还指向老的数据结构，并没有真正达成深拷贝的目标，会造成潜在的问题。 想要提升拷贝效率，就得考虑怎么样才能完美的拷贝所有私有字段才行，要想做到这点就得了解 Go 数据结构的内存布局细节。 Go 为了能方便的与底层系统进行互操作，在数据结构的内存布局方面保持了非常严格的顺序性，使得我们可以有机会直接通过偏移量来得知每个字段，包括私有字段，在内存中的真实位置，借助 unsafe 库提供的不安全内存访问的能力就可以修改任意的私有字段。 下面这个例子展示了如何纯粹通过 reflect 和 unsafe 来深拷贝一个私有字段的指针。 type T2 struct { A int p []int } t := &T2{ A: 123, p: []int{1}, } tt := reflect.TypeOf(t).Elem() // 拿到类型 T。 fieldP := tt.FieldByName(\"p\") // 拿到 p 的字段信息。 tv := reflect.ValueOf(t).Elem() // 拿到 t 的反射值。 p := tv.FieldByIndex(fieldP.Index) // 拿到 t.p 的反射值，虽然不让写，但是可以读。 num := p.Len() c := p.Cap() clonedP := reflect.MakeSlice(fieldP.Type, num, c) // 构造一个新的 slice。 src := unsafe.Pointer(p.Pointer()) // 拿到 p 数据指针。 dst := unsafe.Pointer(clonedP.Pointer()) // 拿到 clonedP 数据指针。 sz := int(p.Type().Elem().Size()) // 计算出 []int 单个数组元素的长度，即 int 的长度。 l := num * sz // 得到 p 的数据真实内存字节数。 cc := c * sz // 得到 p 的 cap 真实内存字节数。 // 直接无视类型进行内存拷贝，相当于 C 语言里面的 memcpy。 copy((*[math.MaxInt32]byte)(dst)[:l:cc], (*[math.MaxInt32]byte)(src)[:l:cc]) var cloned T2 cloned = *t // 先做一次浅拷贝。 ptr := unsafe.Pointer(uintptr(unsafe.Pointer(&cloned)) + fieldP.Offset) // 拿到 p 的真实内存位置。 // 这里已知 p 是一个 slice，用 `SliceHeader` 进行强制拷贝，相当于做了 cloned.p = clonedP。 *(*reflect.SliceHeader)(p) = reflect.SliceHeader{ Data: dst, Len: num, Cap: c, } 很显然上面这段代码非常的折腾，涉及不少 Go runtime 层面上的概念和技巧，我们来逐段仔细看一下。为了方便叙述，上面的代码里直接假定我们已经知道 p 是个 []int，这样就不用写大量 switch...case 判断 Kind，让本来就挺难理解的代码变得更难读了。如果希望看到完整的类型判断逻辑，可以参考源码 https://github.com/huandu/go-clone/blob/v1.1.2/clone.go#L291。 首先，能拷贝私有字段的前提是，我们可以通过 reflect 库读到私有字段的类型定义和数据，只读不能写，假如读都读不到，那就一点办法都没有了。 其次，在拿到字段 p 的类型信息 fieldP 之后，我们就可以轻松通过 field.Type 得知 p 的类型，从而可以通过 Kind 来区分不同类型的不同代码逻辑。在 fieldP 里面有个非常关键点字段 fieldP.Offset，它表示 p 相对结构指针的头部的偏移量。 下面这个等式是始终正确的。 unsafe.Pointer(&t.p) == unsafe.Pointer(uintptr(unsafe.Pointer(t)) + fieldP.Offset) 知道 p 真实内存位置之后就能做很多事情了，比如直接进行内存拷贝。同理，由于 slice 数据的内存是连续的，一旦知道了真实的内存地址之后也可以直接进行数据拷贝，完成 slice 内容的复制。 具体内存拷贝的方法就是下面这段代码。 copy((*[math.MaxInt32]byte)(dst)[:l:cc], (*[math.MaxInt32]byte)(src)[:l:cc]) 它的原理是：先将 dst 和 src 这样的 unsafe.Pointer 强制转成 [math.MaxInt32]byte 类型，然后再对这个伪装的数组进行 slice 操作，将要拷贝的内容切出来生成合法的 []byte，最后交给 copy 来拷贝数据。 最后，还需要注意 slice 结构本身并不是一个指针，而是包含了几个字段的结构，具体定义放在 reflect.SliceHeader 这里。 减少反射使用的次数 反射使用过多就影响效率，我们可以看到，业务中大多数 Go 数据结构的字段类型都是数值类型（比如各种 int、float 等），特别是那些只包含数值类型的 Go 数据结构，简单的做一次浅拷贝就能完成所有工作，这样处理肯定比每次都用 reflect 遍历所有字段进行逐一拷贝来得快很多。 可以想到，如果能预先缓存类型信息，仅仅标记出类型中必须进行深拷贝的字段就好了，这样每个类型至多只做一次反射，剩下的拷贝就可以完全交给各种 unsafe 内存操作就好了。 在当前实现中，我们定义了一个类型 type structType，用于记录结构里面需要进行深拷贝的字段信息，没有记录在内的字段信息就是可以浅拷贝的数值字段。 type structType struct { PointerFields []structFieldType } type structFieldType struct { Offset uintptr // The offset from the beginning of the struct. Index int // The index of the field. } 具体生成这个类型数据的代码放在 https://github.com/huandu/go-clone/blob/v1.1.2/structtype.go#L51 这里，思路比较简单直接：遍历结构的每个字段，判断是否是数值类型，如果不是，生成 structFieldType 并放入到 structType 的 PointerFields 里面。 由于 Go 数据结构的类型定义不会在运行时进行修改，为了避免经常重复的分析一个类型，实际中我们用了一个 sync.Map 类型的全局变量 cachedStructTypes 类记录历史分析结果。 定义特殊的数值类型结构体 有一些 Go 结构体，看起来像是包含了指针需要深拷贝，但实际上应该始终当做值类型来使用。 这里面有下面几个典型的类型，我们正常使用的时候在函数中都是传值，而不是传指针： time.Time 表示时间，这里面虽然有一个 loc *time.Location 字段，但实际上不需要拷贝，这个 loc 指向的是一段只读的内容。 reflect.Value 表示反射值，这里面有比较复杂的指针信息，但由于这个值仅仅是一个实际类型的「代理」，深度拷贝这个数据并无实际意义。 相信在业务代码中也可能会有类似的数值类型结构体，为了能争取处理这些情况，代码里提供了一个 MarkAsScalar 的函数，将这些类型统统加入到一个全局白名单里面，凡是这些类型的字段都会被看做简单的数值进行浅拷贝。 此外，还有 reflect.Type 这个特殊的 interface 也需要单独处理，它实际是 *reflect.rtype 类型，这个类型指向程序的只读内存空间，也不应该深度拷贝。不过由于它的独特性，没有任何一个类型与之相似，代码中就直接进行了特殊处理。 拷贝函数指针 经过上面的探索，基本解决了大部分的深拷贝问题，但实际中发现还有一个非常难啃的硬骨头，即函数指针的拷贝。 如果我们有下面的数据结构： type T3 struct { fn func() } 这个 fn 本质上是一个指针，考虑到函数本身在运行时是只读的，一般情况下简单当做 uintptr 拷贝值就好了。 但凡是都有万一，当遇到下面这种非常复杂的情况时，浅拷贝并不可行。 type T4 struct { fn func() int m map[int]interface{} } t := T4{ m: map[int]interface{}{ 0: T4{ fn: bytes.NewBuffer(nil).Len, // fn 指向一个绑定了 receiver 的方法。 }, }, } 很显然，在拷贝 m 的时候必须遍历这个 map 所有元素，拿到 interface{} 具体值再进行拷贝。由于 reflect 接口的限制，在这个场景中我们无法拿到 T4 的内存位置，只能用老方法去遍历每个字段进行逐一拷贝，正常情况下一切都没问题，但很不幸的是，唯独只有当 fn 指向一个绑定了 receiver 的方法的时候，reflect.Value 的 Pointer 方法返回的地址是个假地址，这导致我们无法正确拷贝 fn 的值。 Go runtime 里面相关代码如下： func (v Value) Pointer() uintptr { k := v.kind() switch k { // 省略... case Func: if v.flag&flagMethod != 0 { // As the doc comment says, the returned pointer is an // underlying code pointer but not necessarily enough to // identify a single function uniquely. All method expressions // created via reflect have the same underlying code pointer, // so their Pointers are equal. The function used here must // match the one used in makeMethodValue. f := methodValueCall return **(**uintptr)(unsafe.Pointer(&f)) } // 省略... } 可以看到，当 flagMethod 标记设上时，即上面所说的这种 fn 的值， Pointer 方法不会诚实的返回函数指针。 Go 这样设计很可能是为了掩盖 runtime 在函数指针上做的 trick：一般来说，fn 指向一个函数指针，即代码段的内存位置，但 Go 为了能让函数指针绑定 receiver， 在这种情况下 fn 会指向一个包含了上下文信息的结构，reflect 为了在这种状况下依然让调用者感觉 fn 是一个代码段地址就做了这个伪装。 关于 Go runtime 怎么实现函数指针，详见官方文档 https://golang.org/s/go11func。 为了解决这个问题，我们重新思考了这个细节的拷贝策略。如果拿不到真实的值同时又不想过度依赖 Go runtime 的具体实现，还有一个简单可行的方法是使用 reflect.Value 的 Set 方法直接设置值（回到最原始的方案），但这里面有个前提是字段必须 CanSet，且设置进去的值不能是私有字段。 以下是 Go reflect 库的相关代码： // Set assigns x to the value v. // It panics if CanSet returns false. // As in Go, x's value must be assignable to v's type. func (v Value) Set(x Value) { v.mustBeAssignable() // v 必须 CanSet。 x.mustBeExported() // x 必须不能是私有字段。 // 省略... } // mustBeExported panics if f records that the value was obtained using // an unexported field. func (f flag) mustBeExported() { if f == 0 || f&flagRO != 0 { // 调用 panic... } } 因此，为了能够正常的调用 Set 方法，我们不得不拿出终极大招，直接篡改 reflect.Value 里面的标志位，关键就是去掉 fn 这个私有字段的 flagRO 标志位。 直接去 hack reflect.Value 的私有字段是不靠谱的，未来太难以维护，考虑到我们可以相对安全的浅拷贝 interface{} ，可以通过一个空接口进行中转而间接的去掉这个标记位且不破坏其他的标记。 // typeOfInterface 是 interface{} 这种类型本身。 var typeOfInterface = reflect.TypeOf((*interface{})(nil)).Elem() // forceClearROFlag clears all RO flags in v to make v accessible. // It's a hack based on the fact that InterfaceData is always available on RO data. // This hack can be broken in any Go version. // Don't use it unless we have no choice, e.g. copying func in some edge cases. func forceClearROFlag(v reflect.Value) reflect.Value { var i interface{} v = v.Convert(typeOfInterface) // 将任意的类型强制转成 interface{}，确保 v 内存布局可控。 nv := reflect.ValueOf(&i) // i 是局部变量，不会设置 flagRO。 *(*[2]uintptr)(unsafe.Pointer(nv.Pointer())) = v.InterfaceData() // 浅拷贝 v 的内容到 i. return nv.Elem().Elem() // 返回原来 v 指向的真实内容。 } 至此，我们就可以正常的通过 Set 来设置这个反射值了，最后再将复制出来的值通过内存操作拷贝到结构里面去即可。 小结 通过利用 Go 数据结构内存布局的特点，先进行浅拷贝再对有需要的字段进行深拷贝，并且预先将结构字段的类型信息缓存起来方便处理，这样可以极大的提升拷贝性能。对于字段都是纯数值类型来说，可以提升超过一个数量级的性能；对于本身就很复杂的类型来说，也会因为减少了反射调用而提升几倍性能。具体的测试结果可以看项目的 README。 基于这种非常彻底和高效的深拷贝，使得我们甚至于可以用这种技术来实现一些原来不敢想的功能，比如 Go 的 immutable data，这也就是 clone.Wrap、clone.Unwrap 和 clone.Undo 实现的功能了，通过这些 API 我们可以做到数据透明的保存和重置，模拟其他语言的 immutable 特性。这方面的实现细节也挺多的，未来有机会再分享吧。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/base/new_and_make.html":{"url":"blog/golang/base/new_and_make.html","title":"New And Make","keywords":"","body":"golang：new and make Go 语言中的 new 和 make 一直是新手比较容易混淆的东西, 咋一看很相似.不过解释两者之间的不同也非常容易. 他们所做的事情,和应用的类型也不相同. 二者都是用来分配空间. Go语言中new和make是内建的两个函数,主要用来创建分配类型内存. 在我们定义生成变量的时候,可能会觉得有点迷惑,其实他们的规则很简单,下面我们就通过一些示例说明他们的区别和使用. new new(T) 为一个 T 类型新值分配空间并将此空间初始化为 T 的零值,返回的是新值的地址,也就是 T 类型的指针 *T,该指针指向 T 的新分配的零值. new要点： 内置函数 new 分配空间. 传递给new 函数的是一个类型,不是一个值. 返回值是 指向这个新分配的零值的指针. make make(T, args) 返回的是初始化之后的 T 类型的值,这个新值并不是 T 类型的零值,也不是指针 *T,是经过初始化之后的 T 的引用. make 也是内建函数,你可以从 http://golang.org/pkg/builtin/#make 看到它, 它的函数原型 比 new 多了一个（长度）参数,返回值也不同. make 只能用于 slice,map,channel 三种类型, 并且只能是这三种对象. 和 new 一样,第一个参数是 类型,不是一个值. 但是make 的返回值就是这个类型（即使一个引用类型）,而不是指针.具体的返回值,依赖具体传入的类型. diff new(T) 返回 T 的指针 *T 并指向 T 的零值. make(T) 返回的初始化的 T,只能用于 slice,map,channel,要获得一个显式的指针，使用new进行分配，或者显式地使用一个变量的地址. new 函数分配内存,make函数初始化； package main import \"fmt\" func main() { p := new([]int) //p == nil; with len and cap 0 fmt.Println(p) v := make([]int, 10, 50) // v is initialed with len 10, cap 50 fmt.Println(v) /*********Output**************** &[] [0 0 0 0 0 0 0 0 0 0] *********************************/ (*p)[0] = 18 // panic: runtime error: index out of range // because p is a nil pointer, with len and cap 0 v[1] = 18 // ok } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/base/readme.html":{"url":"blog/golang/base/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/base/reflect.html":{"url":"blog/golang/base/reflect.html","title":"Reflect","keywords":"","body":"golang: reflect 在计算机科学领域，反射是指一类应用，它们能够自描述和自控制。也就是说，这类应用通过采用某种机制来实现对自己行为的描述（self-representation）和监测（examination），并能根据自身行为的状态和结果，调整或修改应用所描述行为的状态和相关的语义。 每种语言的反射模型都不同，并且有些语言根本不支持反射。Golang语言实现了反射，反射机制就是在运行时动态的调用对象的方法和属性，官方自带的reflect包就是反射相关的，只要包含这个包就可以使用。 多插一句，Golang的gRPC也是通过反射实现的。 TypeAndValue 通过运行结果可以得知获取未知类型的interface的具体变量及其类型的步骤为： 先获取interface的reflect.Type，然后通过NumField进行遍历 再通过reflect.Type的Field获取其Field 最后通过Field的Interface()得到对应的value get method name and type 通过运行结果可以得知获取未知类型的interface的所属方法（函数）的步骤为： 先获取interface的reflect.Type，然后通过NumMethod进行遍历 再分别通过reflect.Type的Method获取对应的真实的方法（函数） 最后对结果取其Name和Type得知具体的方法名 也就是说反射可以将“反射类型对象”再重新转换为“接口类型变量” struct 或者 struct 的嵌套都是一样的判断处理方式 package main import ( \"fmt\" \"reflect\" ) type User struct { Id int Name string Age int } func (u User) ReflectCallFunc() { fmt.Println(\"Allen.Wu ReflectCallFunc\") } func main() { user := User{1, \"Allen.Wu\", 25} DoFiledAndMethod(user) } // 通过接口来获取任意参数，然后一一揭晓 func DoFiledAndMethod(input interface{}) { getType := reflect.TypeOf(input) fmt.Println(\"get Type is :\", getType.Name()) getValue := reflect.ValueOf(input) fmt.Println(\"get all Fields is:\", getValue) // 获取方法字段 // 1. 先获取interface的reflect.Type，然后通过NumField进行遍历 // 2. 再通过reflect.Type的Field获取其Field // 3. 最后通过Field的Interface()得到对应的value for i := 0; i 运行结果： get Type is : User get all Fields is: {1 Allen.Wu 25} Id: int = 1 Name: string = Allen.Wu Age: int = 25 ReflectCallFunc: func(main.User) set variable reflect.Value是通过reflect.ValueOf(X)获得的，只有当X是指针的时候， 才可以通过reflec.Value修改实际变量X的值， 即：要修改反射类型的对象就一定要保证其值是“addressable”的 需要传入的参数是* float64这个指针，然后可以通过pointer.Elem()去获取所指向的Value，注意一定要是指针。 如果传入的参数不是指针，而是变量，那么 通过Elem获取原始值对应的对象则直接panic 通过CanSet方法查询是否可以设置返回false newValue.CantSet()表示是否可以重新设置其值，如果输出的是true则可修改，否则不能修改，修改完之后再进行打印发现真的已经修改了。 reflect.Value.Elem() 表示获取原始值对应的反射对象，只有原始对象才能修改，当前反射对象是不能修改的 也就是说如果要修改反射类型对象，其值必须是“addressable”【对应的要传入的是指针，同时要通过Elem方法获取原始值对应的反射对象】 struct 或者 struct 的嵌套都是一样的判断处理方式 package main import ( \"fmt\" \"reflect\" ) func main() { var num float64 = 1.2345 fmt.Println(\"old value of pointer:\", num) // 通过reflect.ValueOf获取num中的reflect.Value，注意，参数必须是指针才能修改其值 pointer := reflect.ValueOf(&num) newValue := pointer.Elem() fmt.Println(\"type of pointer:\", newValue.Type()) fmt.Println(\"settability of pointer:\", newValue.CanSet()) // 重新赋值 newValue.SetFloat(77) fmt.Println(\"new value of pointer:\", num) //////////////////// // 如果reflect.ValueOf的参数不是指针，会如何？ pointer = reflect.ValueOf(num) //newValue = pointer.Elem() // 如果非指针，这里直接panic，“panic: reflect: call of reflect.Value.Elem on float64 Value” } 运行结果： old value of pointer: 1.2345 type of pointer: float64 settability of pointer: true new value of pointer: 77 通过发射做结构体参数校验 type Student struct { Name string `json:\"name\"` Age string `json:\"log_path\"` Grade string `json:\"grade,omitempty\"` } func ValidObjectByTag(obj interface{}) error { t := reflect.TypeOf(obj) value := reflect.ValueOf(obj) for i := 0; i Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/base/string.html":{"url":"blog/golang/base/string.html","title":"String","keywords":"","body":"golang string usage strings 包中有一些常用的字符串操作方法，记录一下，方便查阅。 1、strings.HasPrefix (s string, prefix string) bool：判断字符串 s 是否以 prefix 开头 /*输出：true*/ ok := strings.HasPrefix(\"beijing\", \"bei\") 2、strings.HasSuffix (s string, suffix string) bool：判断字符串 s 是否以 suffix 结尾 /*输出：true*/ ok := strings.HasSuffix(\"beijing\", \"ing\") 3、strings.Index (s string, str string) int：判断 str 在 s 中首次出现的位置，如果没有出现，则返回 - 1 /*输出：3*/ index := strings.Index(\"I love Golang\", \"ov\") 4、strings.LastIndex (s string, str string) int：判断 str 在 s 中最后出现的位置，如果没有出现，则返回 - 1 /*输出：8*/ index := strings.LastIndex(\"I love Golang\", \"o\") 5、strings.Replace (str string, old string, new string, n int)：将字符串 str 中的 old，替换成 new，n 表示替换 n 个 /*输出：I leve Golang*/ newString := strings.Replace(\"I love Golang\", \"o\", \"e\", 1) 6、strings.ReplaceAll (str string, old string, new string)：将字符串 str 中的 old，全部替换成 new /*输出：I leve Gelang*/ newString := strings.ReplaceAll(\"I love Golang\", \"o\", \"e\") 7、strings.Count (str string, substr string) int：计算 str 字符串中总共出现多少次 substr /*输出：2*/ index := strings.Count(\"I love love Golang\", \"ov\") 8、strings.Repeat (str string, count int) string：重复 count 次 str /*输出：love love love*/ str := strings.Repeat(\"love \", 3) 9、strings.ToLower (str string) string：转为小写 /*输出：love*/ str := strings.ToLower(\"LOVE\") 10、strings.ToUpper (str string) string：转为大写 /*输出：LOVE*/ str := strings.ToUpper(\"love\") 11、strings.TrimSpace (str string)：去掉字符串首尾空白字符 /*输出：love*/ str := strings.TrimSpace(\" love \") 12、strings.Trim (str string, cut string)：去掉字符串首尾 cut 字符 /*输出：love*/ str := strings.Trim(\"@love@\", \"@\") 13、strings.TrimLeft (str string, cut string)：去掉字符串首 cut 字符 /*输出：love@*/ str := strings.TrimLeft(\"@love@\", \"@\") 14、strings.TrimRight (str string, cut string)：去掉字符串右边 cut 字符 /*输出：@love*/ str := strings.TrimRight(\"@love@\", \"@\") 15、strings.Fields (str string)：返回 str 空格分隔的所有子串的 slice /*输出：[I love beijing]*/ sli := strings.Fields(\"I love beijing\") 16、strings.Split (str string, split string)：返回 str split 分隔的所有子串的 slice /*输出：[I love beijing]*/ sli := strings.Split(\"I@love@beijing\", \"@\") 17、strings.Join (s1 [] string, sep string)：用 sep 把 s1 中的所有元素链接起来 /*输出：I@love@beijing*/ str := strings.Join([]string{\"I\", \"love\", \"beijing\"}, \"@\") 18、strconv.Itoa (i int)：把一个整数 i 转成字符串 /*输出：string*/ str := strconv.Itoa(22) fmt.Printf(\"%T\", str) 19、strconv.Atoi (str string)(int, error)：把一个字符串转成整数 /*输出：int*/ i, _ := strconv.Atoi(\"22\") fmt.Printf(\"%T\", i) ———————————————— 原文作者：一根毛毛闯天下 转自链接：https://learnku.com/articles/43150 版权声明：著作权归作者所有。商业转载请联系作者获得授权，非商业转载请保留以上作者信息和原文链接。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/base/tag.html":{"url":"blog/golang/base/tag.html","title":"Tag","keywords":"","body":"Go结构体标签 结构体的字段除了名字和类型外，还可以有一个可选的标签（tag）：它是一个附属于字段的字符串，可以是文档或其他的重要标记。比如在我们解析json或生成json文件时，常用到encoding/json包，它提供一些默认标签，例如：omitempty标签可以在序列化的时候忽略0值或者空值。而-标签的作用是不进行序列化，其效果和和直接将结构体中的字段写成小写的效果一样。 type Info struct { Name string Age int `json:\"age,omitempty\"` Sex string } type Info struct { Name string Age int `json:\"age,string\"` //这样生成的json对象中，age就为字符串 Sex string } 现在来了解下如何设置自定义的标签，以及如何像官方包一样，可以通过标签，对字段进行自定义处理。要实现这些，我们要用到reflect包。 package main import ( \"fmt\" \"reflect\" ) const tagName = \"Testing\" type Info struct { Name string `Testing:\"-\"` Age int `Testing:\"age,min=17,max=60\"` Sex string `Testing:\"sex,required\"` } func main() { info := Info{ Name: \"benben\", Age: 23, Sex: \"male\", } //通过反射，我们获取变量的动态类型 t := reflect.TypeOf(info) fmt.Println(\"Type:\", t.Name()) fmt.Println(\"Kind:\", t.Kind()) for i := 0; i Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/clouser.html":{"url":"blog/golang/clouser.html","title":"Clouser","keywords":"","body":"闭包 一般情况下 一个函数 是无法直接使用 其他函数 内的局部变量的，由此产生了必包的概念。 定义： 在函数嵌套的前提下，内函数使用外函数的变量，返回的内函数叫闭包 作用 将函数用作值传递 词法作用域是静态作用域 一句话总结: 重复使用函数内的局部变量, 在重复使用的过程中保护这个局部变量不被污染的一种机制 构造闭包 用外层函数包裹要保护的变量和内层函数。 外层函数将内层函数返回到外部。 调用外层函数，获得内层函数的对象，保存在外部的变量中——形成了闭包。 闭包形成的原因: 外层函数调用后，外层函数的函数作用域（AO）对象无法释放，被内层函数引用着。 闭包的缺点： 1.比普通函数占用更多的内存。 解决： 1.闭包不在使用时，要及时释放。 2.将引用内层函数对象的变量赋值为null。 source doc 视频解释 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/debug.html":{"url":"blog/golang/debug.html","title":"Debug","keywords":"","body":"golang DEBUG Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/err 优雅处理.html":{"url":"blog/golang/err 优雅处理.html","title":"Err 优雅处理","keywords":"","body":"error 优雅处理 问题： golang 一个方法里面有多个方法要执行， 每个方法都要判断，当有一个内方法发生error 的时候停止往下执行，直接跳出外方法并返回错误。有什么简单的方法可以避免多个if。。。else 。。。。 判断？ func extranlMethod() error{ interMethod_1() interMethod_2() interMethod_3() interMethod_4() } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/flag.html":{"url":"blog/golang/flag.html","title":"Flag","keywords":"","body":"flag Dash - shorthand -- normalized name 2. Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/gin/gin.html":{"url":"blog/golang/gin/gin.html","title":"Gin","keywords":"","body":"https://xiaonuo.top/articles/2020/12/14/1607931833092.html 路由 golang 默认路由 middle 日志打印 engine.Use(Logger(), Recovery()) // Logger instances a Logger middleware that will write the logs to gin.DefaultWriter. // By default gin.DefaultWriter = os.Stdout. func Logger() HandlerFunc { return LoggerWithConfig(LoggerConfig{}) } // LoggerConfig defines the config for Logger middleware. type LoggerConfig struct { // Optional. Default value is gin.defaultLogFormatter Formatter LogFormatter // Output is a writer where logs are written. // Optional. Default value is gin.DefaultWriter. Output io.Writer // SkipPaths is a url path array which logs are not written. // Optional. SkipPaths []string } 文件服务 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/go module 2.html":{"url":"blog/golang/go module 2.html","title":"Go Module 2","keywords":"","body":"18 Essential Go Module tidbits for a newbie Written by Zara Cooper on February 24, 2020 In 2018, Go 1.11 was released and with it came Go module support. Since then many Go developers have created, used, and published modules. Creating a module is pretty easy, but figuring out how to version your module, manage its dependencies and understanding what all the numbers and words in go.mod and go.sum files mean can be confusing. In this article, I’ll walk you through 18 tidbits that will make creating and managing your Go modules easier. In this article ⌲ 1. So what’s a module anyway? ⌲ 2. Go versions that support modules ⌲ 3. Enabling modules on the go command ⌲ 4. SemVer ⌲ 5. Module structure ⌲ 6. Creating a new module ⌲ 7. The go.mod file ⌲ 8. The types of dependencies in the go.mod file ⌲ 9. Module queries ⌲ 10. The go.sum file ⌲ 11. Adding dependencies to your module ⌲ 12. How the go command interacts with modules ⌲ 13. Semantic import versioning ⌲ 14. Versioning and releasing your module ⌲ 15. Vendoring ⌲ 16. Tips on migrating non-module projects to modules ⌲ 17. The go mod command ⌲ 18. Private modules 1. So what’s a module anyway? Packages, modules, and repositories are all ways Go source code is managed and distributed but how do you distinguish them from one another? Here’s the difference: ⌲ A package is a collection of one or more Go source (.go extension) files that reside in one directory. ⌲ A module is a collection of one or more packages and has a version associated with it. ⌲ A repository is a collection of one or more modules, at least in the context of Go modules. Now that we’ve cleared that up, let’s examine the benefits of using modules that a package and repository don’t provide. They facilitate reproducible builds. This means that if you build your Go code given a set of constant conditions such as dependencies among other things, you can reproduce that same build the next day with the same code and conditions. A module manages your dependencies and eliminates the need to use external vendor tools like glide, dep, govendor, etc. A module allows you to write code outside the GOPATH. 2. Go versions that support modules As mentioned above, module support came with the release of Go 1.11. All Go 1.11+ versions continue to improve upon and add more features to module support. However, if you’re still using Go versions earlier than 1.11, it’s still possible to use modules as dependencies in your codebase. Using version 0 and 1 module dependencies is uncomplicated. It’s more complex using version 2+ module dependencies with these later versions. So updates were made to Go versions 1.9.7+, 1.10.3+ and 1.11 to allow codebases using those versions to rely on v2+ module dependencies. For that reason, no changes have to be made to existing code to support module dependencies in these kinds of projects. This only works if modules are disabled for these kinds of codebases. This feature is known as minimal module compatibility. 3. Enabling modules on the go command To enable Go modules in your codebase, you need to be running Go 1.11 or higher. Once a compatible Go version is installed, there are two ways to enables module support: Add a go.mod file to your project. Set the temporary environment variable GO111MODULE to on. The GO111MODULE variable can take any of these three values: on: use of modules is required by the go command. off: module support is disabled for the go command. auto: is the default value. Module support is enabled if a go.mod file is present within your project and is disabled if there is no go.mod file in your directory. Depending on the value of GO111MODULE, the go command takes on two kinds of modes: Module-aware mode: Takes this mode when GO111MODULE is on or on auto with a project using modules. In this mode, the go command searches for dependencies in the module cache or the vendor directory. GOPATH mode: Takes this mode when GO111MODULE is off or on auto with a project not using modules. In this mode, the go command looks in the GOPATH and vendor directories for dependencies. 4. SemVer SemVer or in full semantic versioning is the main versioning system that Go modules use. A semantic version number takes the form v major.minor.patch where: major: denotes API changes that make the current version backward-incompatible with the former version. minor: connotes the addition of backward-compatible features/functionalities to the version. patch: denotes the addition of backward-compatible bug fixes. All Go modules should be versioned as above with the addition of the prefix v e.g. v2.4.3. 5. Module structure A module consists of and is defined by: a. Module Path The module path defines the location of a module. It can be derived from VCS (like Git) metadata by the go command or set explicitly by the creator of the module. b. Module Root The module root corresponds to the module path. It is the directory in which your Go module exists. It is the root directory in which the go.mod and go.sum files are located and contains a tree of Go source files. c. go.mod go.mod is a file that is located within your module root. It is automatically created when a module is initialized using the Go command and is auto-populated with: the module path the version of your module the version of Go your module is using required dependencies replacement dependencies excluded dependencies d. go.sum The go.sum file is used to authenticate your dependencies to ensure that no unexpected changes were introduced to them. This guarantees that builds are repeatable i.e. given the same source code and versioned dependencies (and other constant factors), consecutive builds will result in the same build all the time. The go command creates cryptographic checksums of a module’s dependencies and go.mod files. It then stores them in the go.sum file. You can read more about the contents of a go.sum file at tidbit 10. e. Source packages A module, in essence, is just a collection of Go packages. All other features of modules like versioning and dependency management exist to facilitate the building of these packages and their consumption. f. Dependencies Dependencies are the set of all modules that are required to build and test the main module (module in which go command is run). g. Build List A build list is the set of the main module and all its required dependencies that provide packages to a build of the main module. A build list is the result of the go command converting the go.mod file to a list of specific modules when it builds the main module. Only specific versions of dependencies are used. If multiple versions of a dependency exist, then the go command only adds the most recent version of it to the build list. 6. Creating a new module To create a new module, you need to decide what the module path and module root are going to be. Your module’s path will depend on where your module is in relation to the GOPATH or if you’re using a VCS. You also need to consider what the version of your module will be as it affects the path of your module. Note that where you run the below commands to create your module is where your module will reside (module root). a. If your module will be located within GOPATH, all you have to do is: go mod init The go command will infer the module path using VCS metadata. For example, if your module is located at $GOPATH/src/github.com/x/y then your module’s path will be github.com/x/y. b. If your module is located outside of GOPATH, and you already have a repo initialized for it, the same approach as (a) above will work. c. If your module is located outside of GOPATH and has no repo initialized for it or if you would like to overwrite its inferred module path, you’ll need to provide a path. go mod init [path] If your module is v2+ you will need to edit its path in the go.mod file to reflect the version. This is because of Semantic Import Versioning that is touched on at tidbit 13. 7. The go.mod file The go.mod file describes the path of a module, the version of Go the module is using and the dependencies of the module. The five verbs used in go.mod directives are: a. module: describes the path of the module and its version. b. go: used to set the version of Go used when compiling the module. The resulting directive does not affect build tags. c. require: specifies a dependency of a precise version that is required by the module. d. replace: used to replace one version of a dependency with another dependency eg. replace buggy/dependency v1.3.6 => stable/dependency/v2 v2.9.4. e. exclude: used to omit a particular version of a dependency from use. Here’s an example of how these verbs are used in a go.mod file: module my/module/v2 go 1.13 require additional/dependency/v4 v4.7.3 require another/dependency v1.5.3 exclude outdated/dependency v3.4.1 The go command creates the go.mod file when a module is initialized. It then populates it with the latest versions of dependencies referenced in your source code. These dependencies can also come from a list created by a dependency management tool like godep. At least 7 other dependency management tools are supported. This file can be modified by the go mod command but other go commands like go build, go test, etc. can add to the file but never remove from it. go.mod is line-oriented and each line should feature a directive unless creating blocks from adjacent lines with the same verb. For example: require( additional/dependency/v4 v4.7.3 another/dependency v1.5.3 ) Comments can be written on the go.mod file but only single-line comments with \\\\ can be used. Multiline comments with \\**\\ cannot be used. 8. The types of dependencies in the go.mod file In the go.mod file, different kinds of dependencies are listed differently depending on how they relate to the module. These are: a.Direct Dependencies These are dependencies that are directly imported by the current module. They are unmarked in the go.mod file. b. Indirect Dependencies These are unlisted dependencies of the direct dependencies. Some dependencies won’t list their dependencies. So the main module has to list them instead. Indirect dependencies are marked in the go.mod with an //indirect comment adjacent to its listing. Some indirect dependencies are listed when a direct dependency’s dependencies are upgraded because this goes against its stated requirements. This is what an indirect dependency line would look like in a go.mod file: require indirect/dependency/v3 v3.3.1 //indirect 9. Module queries A module query is a request for a module at a precise or condition-matching version. It can be requested using the go command or on the go.mod file. The version requested corresponds to some stated conditions or could be an exact version. If no exact version is requested, the go command translates the module query into specific module versions then updates the go.mod file with the results. The two primary reasons module queries are used are: Some dependencies do not have SemVer tags. It could be that the dependency has not been tagged at any point in time or a change introduced by a commit has not been tagged but needs to be used. Module developers may require some flexibility when defining what dependencies to use in their builds. Stating a specific version for a dependency does not provide this flexibility. It’s important to note that when using module queries release versions are given priority over pre-release versions. The four types of module queries are pseudo-versions, semantic versions, strings, and revision identifiers. a. Pseudo-versions Some dependencies have no SemVer tags associated with them. Newly committed changes that have not yet been tagged may sometimes be required before an actual tagged release is published. To reference an untagged dependency, a pseudo-version is created for it. Pseudo-versions are defined by three parts: i. A prefix: This is usually v0.0.0 when no tags whatsoever have been created for the dependency. However, in cases where there are tags associated with the dependency but you’d like to use newly committed changes outside the most recent tagged version, the prefix would be the most recent tagged version before the change. For example, if you have a dependency with the most recent tagged version being v5.9.2 but would like to use a version with later untagged committed changes, then v5.9.2 would be the prefix. ii. A time portion: this is the timestamp of when the pseudo-version was created and is used to compare two pseudo-versions to determine which is the most recent. iii. A commit hash: this is the underlying commit that marks the changes in the dependency that you’d like to use. Here’s an illustration of what a pseudo-version looks like: Pseudo-versions come in 3 forms: A dependency with absolutely no tags. Example, v0.0.0-yyyymmddhhmmss-abcdefabcdef A dependency with its most recent tag being a fully-specified semantic version. Example, vX.Y.Z-yyyymmddhhmmss-abcdefabcdef A dependency with its most recent tag being a fully-specified pre-release candidate semantic version. Example, vX.Y.Z-pre.0-yyyymmddhhmmss-abcdefabcdef b. Semantic versions Semantic versions as covered in tidbit 4 can provide flexibility when you’d like to receive patches or features as soon as they are made available. Types of SemVer module queries include: Fully specified semantic version like v4.5.8 which resolves to a specific version of a dependency. Semantic prefixes which can include just the major version or a major and minor version. For example, v8 or v7.3. Semantic version comparisons resolve to the closest version to a comparison target. The four operators than can be used include >, >=, and . Examples of these versions are , >=7.9.1 etc. These do not work with go get. c. Strings These include: latest which corresponds to a repository’s latest untagged revision (if no tags exist) or the latest available tagged version. upgrade which corresponds to a version later than latest. This can include pre-release revisions, for example. patch which corresponds to the latest tagged version of a dependency with the same major and minor version. d. Revision identifiers These include: commit hash prefixes (8 to 12 first characters of a commit hash are adequate for most repositories). revision tags that are not SemVer compliant eg. 3.0-beta6. branch names. If a revision identifier matches another query syntax then that syntax is given priority over it. Examples of module queries in a go.mod file: require( github.com/a/b v1.5.8 github.com/c/d/v3 v3 github.com/e/f v1.8 github.com/g/h/v7 Examples of module queries used with the go get command: go get github.com/a/b@v1.5.8 go get github.com/c/d@v3 go get github.com/e/f@v1.8 go get github.com/i/j@latest go get github.com/k/l@upgrade go get github.com/m/n@patch go get github.com/o/p@5ce8005990f77d06 go get github.com/q/r@3.0-beta6 go get github.com/s/t@staging 10. The go.sum file The purpose of the go.sum file is to help the go command authenticate a module’s dependencies. It helps ensure that the source code of a dependency remains the same for a specified version. This is important because: a. it ensures that builds are repeatable. b. no malicious or accidental changes have been added to the dependencies. The go.sum stores two types of cryptographic checksums for each dependency: a. A checksum of the dependency file tree (source code files). b. A checksum of the dependency’s go.mod file, if it has opted into module use. If the dependency is not a module, then this is not generated. This checksum is needed when generating the dependency graph (list of dependencies required to build the main module). So for each dependency, two lines are added to the go.sum file unless the dependency is not a module. Otherwise, only one line is added. Here’s what it looks like: The go.sum file resides with the go.mod file at the module root. Unlike the go.mod file which is generated when a module is initialized, the go.sum file is generated when the module is built for the first time. 11. Adding dependencies to your module You can add dependencies to your module in these ways: a. Adding your dependency to your source code. The go command automatically determines what requirements are requested in your source code but are missing. It then adds them to the go.mod and downloads them when the main module is built. b. A dependency can also be added to your module’s go.mod using go get on the command line. c. Test dependencies are added to the module’s go.mod when running go test. d. Several other go commands scan your source code for missing requirements then add them to your go.mod when running. Commands like these include go list etc. More about this is touched on at tidbit 12. e. Use go mod download to add a module dependency to your local cache before you reference it to your source code. By default, module dependencies are stored in GOPATH/pkg/mod but if you opt to vend your dependencies, they are stored in a vendor directory at the module root. 12. How the go command interacts with modules The various go commands interact with modules in different ways. Here’s a general categorization of how they behave in relation to modules: i. Some go commands when run, search through the source code and add missing build dependencies to the go.mod file and download them to the local cache or vendor directory. They, however, do not remove unused dependencies or delete anything from the go.mod or go.sum. These commands are: go build, go clean, go fix, go fmt, go generate, go get, go install, go list, and go run. ii. Other go commands not only add missing build dependencies but also add missing test dependencies to the go.mod, the go.sum, and the local cache or vendor directory. These commands are go test and go vet. They do not remove any unused dependencies if they happen upon them. iii. The go mod command was created for module operations. Out of the eight go mod operations only three directly add missing module dependencies to the local cache/vendor directory when run. These are go mod tidy, go mod vendor, and go mod why. go mod download adds dependencies to the local cache when specifically requested. 13. Semantic Import Versioning Go 1.11+ module versions adhere to semantic import versioning. Semantic import versioning involves versioning a module following SemVer and the import compatibility rule\\. You can read up on Semver at tidbit 4. The import compatibility rule states that: If an old package and a new package have the same import path, the new package must be backward compatible with the old package. So according to the import compatibility rule, if any compatibility breaking changes are introduced to your package, the package’s import path should change. The change in import path is accomplished by adding the major version of the module to which the package belongs to the module path. However, this only applies to releases that have opted into the use of modules. Releases that use modules follow 3 rules to comply with semantic import versioning. These are: a. Adhere to SemVer. b. v0 and v1 releases should not include their major version numbers in their module and import paths. v0 releases omit v0 because these versions are considered to be initial, unstable and still under development as progress is made towards v1. v1 releases just omit v1 by default. c. v2+ releases should include their major version numbers in their module and import paths. This is required by the go command to preserve import compatibility as mentioned above. Below are some examples: Version Major Version Module Path Import Path v0.4.2 v0 github.com/a/module github.com/a/module/package v1.3.9 v1 github.com/a/module github.com/a/module/package v2.8.3 v2 github.com/a/module/v2 github.com/a/module/v2/package v5.6.1 v5 github.com/a/module/v5 github.com/a/module/v5/package v9.4.5 v9 github.com/a/module/v9 github.com/a/module/v9/package v13.5.6 v13 github.com/a/module/v13 github.com/a/module/v13/package There are 2 exceptions to this rule which in time will be unnecessary as more projects adopt the use of modules. These are: Projects using gopkg.in. gopkg.in is an earlier versioning system. It concatenated a package’s import path to its major version using a period. Projects that have not opted in to the use of modules yet. There are some benefits to semantic import versioning: It prevents the diamond dependency import problem.. It’s possible to use two versions of a module with different major version numbers within the same codebase. It helps package developers easily identify dependencies early that may be problematic to work with in the future. This is because using v0, pre-release, incompatible or pseudo versions does not guarantee backward compatibility. 14. Versioning and releasing your module To prepare your module for versioning, you need to ensure that you’ve: initialized a repository for your module. Git, Mercurial, Bazaar among others are supported. initialized your module and have a go.mod file in your module root. added a license if you intend to publish your module. If this is the first time your code is opting into module use, two things need to be considered before proceeding: Is your codebase new or pre-existing? What version would you like to initially tag your module? If your codebase is new, creating a module is straightforward. However, if you have a pre-existing codebase and are just now opting into modules, you will need to adopt SemVer if you haven’t already. Once that’s done, increment the major version because opting into modules is considered a compatibility breaking change. Deciding what initial version to use comes with its own set of considerations. Tagging your module as v0 means your module is still under development, is not stable and cannot guarantee backward compatibility. Tagging your module as v1+ and beyond means, your module is stable and enforces backward compatibility within each major version. Pre-release versions behave similarly to v0 modules. After you’ve opted in to the use of modules, you will need to follow SemVer. So if you’re: introducing a backward-incompatible change such as an API change, deleting exported types, etc. you have to increment your major version. You also need to change import and module paths in your source code and go.mod. making backward-compatible changes that affect the API like adding types, methods, functions, etc. increment the minor version. adding changes that do not alter the API like fixing bugs, increment the patch version. creating a pre-release version, append a hyphen after the patch version then a series of dot separated identifiers e.g 1.0.0-alpha.7.1. a. Versioning your V0 and V1 modules Versioning your v0 and v1 modules is a 4-stage process: Remove unnecessary dependencies that are not required in your build or tests. go mod tidy Test your code. go test ./... Tag your module with the version you’ve come up with according to the above SemVer rules. For example with git: git tag [your version] Push your brand-new tag to your origin repository: git push origin [your tag] b. Versioning your v2+ modules As per the import compatibility rule, module paths for v2+ modules have a major version suffix to indicate backward incompatibility and prevent the diamond dependency hell problem. To accommodate this suffix, there are two ways to help version and tag your v2+ modules: The major branch method: create a new branch for every new major version The major subdirectory method: create a new directory in the module root for every new major version The major subdirectory method is preferred over the major branch method because: with the subdirectory method, tools that are module-unaware can work with different versions of a module. With branches, it gets tricky since these tools are not module aware and can pull from master which may be a different version than requested. with the branch method, module-unaware tools have a hard time working with module paths and it’s difficult to locate files associated with those file paths. makes migration to other versions of a dependency easier in non-module codebases. Large non-module codebases can depend on multiple versions of the same dependency when doing phased migrations to newer versions with the major subdirectory method. Major branch method Create a new branch based off of the most recent version of your module. Make it your current operating branch. Name it the next major version you’d like to create eg. v2. git branch v2 git checkout v2 In your go.mod file, change the suffix of your module path to reflect the new version in the module directive. For example, when moving from v1 to v2, module github.com/user/module would change to module github.com/user/module/v2. go mod edit -module github.com/user/module/v2 go.mod Update import paths in your source code to reflect the change in your module path eg. github.com/user/module/package in v1 would change to github.com/user/module/v2/package in v2. You could use your editor for this step or find and sed on the command line if you’d prefer. Tidy your dependencies. go mod tidy Test your code. go test ./... Tag your module. To make sure all changes made are stable, it’s recommended to first create a pre-release version as some instability is understandable with pre-releases. Once all new changes have been tested and found to be stable, you could tag the next stable release. git tag [tag] Push your new branch and tag. git push origin [branch] git push origin [tag] Major subdirectory method Navigate to the module root when creating a v2 or the parent directory that hosts all other versions for v3+. Create a new directory. Name it the next major version you’d like to create eg. v3. mkdir v3 Copy all your source code and the go.mod from the previous version into the new directory you created. cp *.go v3/ cp go.mod v3/go.mod Change your current directory to the one you just created. On the go.mod file, change the suffix of your module path to reflect the new version. For example: go mod edit -module github.com/user/module/v3 go.mod Update import paths in your source code to reflect the change in your module path eg. github.com/user/module/package in v1 would change to github.com/user/module/v2/package in v2. You could use your editor for this step or find and sed on the command line if you’d prefer. Tidy your dependencies. go mod tidy Test your code. go test ./... Tag your module. To make sure all changes made are stable, it’s recommended to first create a pre-release version as some instability is understandable with pre-releases. Once all new changes have been tested and found to be stable, you could tag the next stable release. git tag [tag] Push your tag. git push origin [tag] Some best practices to follow when versioning and releasing your v2+ modules: Always consider your module’s users before deciding to break compatibility. It’s additional work for your users to adopt changes you make to your codebase. Consider the maintainers of your codebase since it’s also more work for them to constantly maintain new and older versions of your dependency. Continually maintain your module to keep its consumers from using outdated code. Always have a good reason to make breaking changes to justify reasons 1 through 3. 15. Vendoring Vendoring is the process of fetching packages that your codebase depends on and storing them usually in the codebase directory. In Go, vendored packages are stored in the vendor directory at the module root. However by default, when using modules the vendor directory is ignored by the go command. To vendor dependencies for your module, you run: go mod vendor This will create a vendor directory in your module root with all your dependencies stored in them. When using the go command with your module, pass the -mod=vendor flag to it if you would like the command to use dependencies from the vendor directory. 16. Tips on migrating non-module projects to modules When migrating your non-module project to a module, it’s important to consider two things: Users of the project Maintainers of the project Tips for maintainers: You could migrate your projects using either the major subdirectory or the major branch strategy. When initially migrating your project to modules, increment your major version since it’s considered a breaking change. Because of the import compatibility rule, the new major version is added to the module path and this means you also have to modify your codebase’s import paths as well. To migrate your dependencies, just run go mod init as this will auto-populate the dependencies in the go.mod file. You will need to modify the import paths for v2+ module dependencies within the codebase and in the go.mod. Tips for handling users’ needs: Encourage users to import packages directly in code as this is easier than using go get. The go command automatically adds new imports to go.mod as dependencies. Have options for your non-module users who may want to consume your module. If your module is in v2+, you can go one of two ways: the major subdirectory strategy which is good for most Go versions or the major branch strategy which works for Go 1.9.7+, 1.10.3+, and 1.11+ codebases. Non-module users have very few problems using v0 and v1 module dependencies so there’s not much to worry about if your module is in v0 or v1. Your users can use multiple versions of your module in their codebase. This is especially helpful if your users are slowly migrating dependencies in a large codebase or to fill deficiencies between different versions of your module. 17. The go mod command The go mod command is used to perform operations on modules. The eight go mod commands are: Command Operation Usage download Downloads a module to local cache (GOPATH/pkg/mod/cache). A specific module version can be requested using the query path@version. This is mostly used when you’d like to preload dependencies. To get a more detailed overview of the module you are downloading use the -json flag. go mod download [flags] [path]@[version]... edit Used to edit a go.mod file. It reads the go.mod file then writes changes to the same file or another specified file. It is mostly useful for tools or scripts. The command does not do any module lookup so determining any errors related to the modified file contents is up to you. With this command, you can:- format the go.mod (-fmt) - change the module path (-module newPath)- require a dependency (-require=path@version)- drop a required dependency (-droprequire=path@version)- replace a dependency with another different dependency (-replace oldpath@version=newpath@version)- drop a replacement dependency (-dropreplace=module@version)- exclude a dependency (-exclude=path@version)- drop an excluded dependency(-dropexclude=path@version)- change the Go version (-go=version)- print the modified contents of the go.mod without writing the results back to the source go.mod (-print)- print the modified contents of the go.mod in a JSON format without writing the results back to the source go.mod (-json) go mod edit [flags] [target go.mod] graph prints a text version of the module requirement graph which is a list of your module’s direct and indirect dependencies. go mod graph init initializes a new module by creating a go.mod and populating it with the module path, a Go version, and a list of dependencies. If you are outside GOPATH or not within a repository you need to provide a module path as it’s not possible to infer one and this operation will fail without it. The resulting go.mod is written to the current directory. go mod init [module path] tidy determines missing and unused module dependencies then adds or removes them from your go.mod and go.sum. Use the -v flag for a detailed overview of this command’s results. go mod tidy [-v] vendor adds your module’s build and test dependencies to a vendor directory. Use the -v flag for a detailed overview of this command’s results. go mod vendor [-v] verify checks that the module’s dependencies in the source cache have not been modified since being downloaded. go mod verify why shows how and where packages or modules are needed in your main module. It does this by showing you the shortest path in your module’s dependency graph between your module and a specified package or module. By default, the arguments are considered to be packages. If you use the -m flag, they are considered to be modules. The -vendor flag excludes test dependencies from the results. go mod why [-m] [-vendor] packages... 18. Private Modules To host private modules it’s important to first know and understand the services that help the go command fetch modules. The Go Team introduced three free public services in 2019 that make it easier to fetch and work with module dependencies. These include: 1. The Module Mirror It is served at proxy.golang.org. It is a proxy that caches Go module source code and metadata in its storage system. The module mirror is used by default by the Go command as of Go 1.13. Some core reasons to have the module mirror include: To cater to the specific needs of the go command. The go command needs to fetch very precise metadata or source code at a time. For example, fetching a list of available versions of a module, metadata on a distinct version, the go.mod file for one exact version, source code for a precise version, etc. But with source control services, getting this kind of individual information is not possible. To reduce latency when fetching modules. When trying to resolve the version of a dependency that does not have a fully specified semantic versioned tag, the go command may have to pull down a whole repository from a source control server. This is slower. The module mirror allows the go command to pull only distinct metadata and source code in such cases making it faster to resolve these kinds of dependencies. To reduce storage on developers’ systems since only specific source code and metadata is fetched by the go command. Provides source code that is unavailable from its primary location, for example in cases where a module has been mistakenly deleted or maliciously taken down. 2. The Checksum Database It is served at sum.golang.org. It is a global tamper-proof database that hosts go.sum lines (SHA-256 hashes of the source code and go.mod files). It’s designed in a way that makes tampering easily detectable and evident. It also provides endpoints that the go command uses to fetch go.sum files and verify local go.sum files. One advantage of the checksum database is that it provides a layer of security on top of a module proxy or source control service or server that can be audited and reports instances where incorrect source code is provided. 3. The Module Index It is served at index.golang.org. It’s a feed of newly published module versions that proxy.golang.org makes available. It’s helpful when separately caching what’s available to proxy.golang.org on your own. When working with private modules, you can instruct the go command to not use the module mirror or checksum database. Alternatively, you could host your versions of the above services on a private server. Substitutes you could use are like Thumbai, Athens, Goproxy, and JFrog Artifactory. To change what the go command defaults to for the module mirror and checksum DB, you will need to change some or all of the below environment variables: Environment Variable Purpose Format Example GOPRIVATE Controls which modules are private and cannot be accessed and validated by the public module proxy and checksum database. Comma-separated list of glob patterns of module path prefixes GOPRIVATE=``*.privateorg.com,``*.secretorg.com,``somecorp.com/confidential GONOPROXY Controls which modules should not be fetched from the public module mirror. Can override GOPRIVATE. Comma-separated list of glob patterns of module path prefixes GONOPROXY=``*.privateorg.com,``*.secretorg.com,``somecorp.com/confidential GONOSUMDB Controls which modules should not be validated against the public checksum database. Can override GOPRIVATE. Comma-separated list of glob patterns of module path prefixes GONOSUMDB=``*.privateorg.com,``*.secretorg.com,``somecorp.com/confidential GOPROXY Controls which module mirror proxies the go command should use and in what order, or whether or not to use a module proxy. Comma-separated list that takes any of these values:- module proxy URLs- direct for a direct connection to source control server (proxies listed after this are never consulted)- off to disallow downloading from any source. GOPROXY=``proxy.privateorg.com,``otherproxy.privateorg.com,``direct GOSUMDB Controls which checksum database the go command should use or whether or not to use a checksum database. - Name of the checksum database with an optional public key and URL- off to disallow checksum database consultation. GOSUMDB=``sum.privateorg.com``+ ``https://sum.privateorg.com Conclusion Modules have vastly simplified dependency management in Go. It’s important to have a detailed understanding of how they work to take advantage of all the features they provide. I hope that this article provided some useful information to gophers who are new to modules. Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/go_mod.html":{"url":"blog/golang/go_mod.html","title":"Go Mod","keywords":"","body":"Go mod Go modules是官方推出推荐的GOPATH的一个替代方案，同时集成了对版本控制和包分发的支持。 随着go1.11推出后，还是有很多的问题。官网不断在社区收集开发者反馈的问题，继续进行优化修bug。 到go1.12 gomod还是未默认将gomod设置为包管理工具取代GOPATH。在go1.11和go1.12版本中， 因为没有将gomod设置为默认包管理方式，增加了一个临时环境变量GO111MODULE来控制使用方式。 这个环境变量有三种设置方式：off、on、auto off，则go命令从不使用go modules的功能，在执行go命令时将继续在GOPATH中查找依赖包，继续使用老的GOPATH模式； auto，当go源码不在GOPATH路径下且当前目录或者上层目录存在go.mod文件时，启用gomod模式，否则将使用GOPATH模式。 on，则go命令使用go mod模式，命令执行过程中将忽略GOPATH的设置，按照gomod的方式管理go程序； 在gomod模式下，开发的项目下载的依赖包还是会存储到GOPATH/pkg/mod目录下，编译生成的二进制文件也将会存放到GOPATH/bin/ 目录下。 go.mod：依赖列表和版本约束。 go.sum：记录module文件hash值，用于安全校验。 use go mod build project set env export GO111MODULE=on export GOPROXY=https://goproxy.io create init file: go.mod go mod init go build auto write require package go build main.go go mod replace 将 go.mod 中的所有依赖下载到vendor包下, old是要被替换的package，new就是用于替换的package。 go mod edit -replace=old[@v]=new[@v] 这里有几点要注意： replace应该在引入新的依赖后立即执行，以免go tools自动更新mod文件时使用了old package导致可能的失败 package后面的version不可省略。（edit所有操作都需要版本tag） version不能是master或者latest，这两者go get可用，但是go mod edit不可识别，会报错。（不知道是不是bug，虽然文档里表示可以这么用，希望go1.12能做点完善措施） download package to vendor go mod vendor go get 下载/升级依赖 go mod不再下载源码进$GOPATH/src go mod的下载目录在$GOPATH/pkg/mod，并且是文件权限是只读的-r--r--r-- vendor 模式 go mod是不推荐使用vendor目录的，而是直接使用source或cache中的包。 module mode下默认忽略vendor目录。通过flag-mod=vendor设置vendor模式，依赖只从顶层的vendor中查找。可以通过环境变量GOFLAGS=-mod=vendor来设置flag 清除缓存 go clean -modcache 最佳实践 go mod不推荐使用vendor，不要将vendor提交到版本控制。 提交go.mod，可以忽略go.sum，因为会根据校验sum跨平台可能报错 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/mvc.html":{"url":"blog/golang/mvc.html","title":"Mvc","keywords":"","body":" register （注册路由） handler （某一个种功能集合的interface） model （业务逻辑，数据CRUD） Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/oauth.html":{"url":"blog/golang/oauth.html","title":"Oauth","keywords":"","body":"OAuth2 OAuth2 是一种身份验证协议，用于使用其他服务提供商来对应用程序中的用户进行身份验证和授权。 github Register App 向github申请注册一个application,一个application对应一个项目，我们需要拿到一个client id和secret来用于后续的登陆认证 https://github.com/settings/developers fill out form Application Name： 这个是GitHub 用来标识我们的APP的 Authorization callback url:就是上面我特意用红色字体标识的的，很关键 Homepage url 这个是展示用的，在我们接下来的登录中用不到，随便写就行了 get result data cliendt_id 这是GitHub用来标识我们的APP的接下来我们需要通过这个字段来构建我们的登录url client Secret 这个很关键，等会我们就靠它来认证的，要好好保存。我这个只是演示教程，用完就销毁了，所以直接公开了。 login with clientID link && return code 构造相关的登陆连接，引导用户点击登陆， 这一步需要用到上面的clientID request url: https://github.com/login/oauth/authorize?client_id=xxxxxxxxxxxxxxxx response: response: http://localhost:8080/?code=xxxxxxxxxxxx use code to get token 用户同意登陆后，third-side可以拿到一个code，通过这个code可以向Github拿到用户的token https://github.com/login/oauth/access_token?client_id=%s&client_secret=%s&code=%s http://localhost:8080/welcome.html?access_token=xxxxxxxxxxxxx example code https://github.com/sohamkamani/go-oauth-example.git Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/prometheus.html":{"url":"blog/golang/prometheus.html","title":"Prometheus","keywords":"","body":"Prometheus [toc] metrics https://segmentfault.com/a/1190000024467720 为了能够帮助用户理解和区分这些不同监控指标之间的差异，Prometheus定义了4种不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要）。 在Exporter返回的样本数据中，其注释中也包含了该样本的类型。例如： # HELP node_cpu Seconds the cpus spent in each mode. # TYPE node_cpu counter node_cpu{cpu=\"cpu0\",mode=\"idle\"} 362812.7890625 Counter：只增不减的计数器 Counter类型的指标其工作方式和计数器一样，只增不减（除非系统发生重置）。常见的监控指标，如http_requests_total，node_cpu都是Counter类型的监控指标。 一般在定义Counter类型指标的名称时推荐使用_total作为后缀。 Counter是一个简单但有强大的工具，例如我们可以在应用程序中记录某些事件发生的次数，通过以时序的形式存储这些数据，我们可以轻松的了解该事件产生速率的变化。 PromQL内置的聚合操作和函数可以让用户对这些数据进行进一步的分析： 例如，通过rate()函数获取HTTP请求量的增长率： rate(http_requests_total[5m]) 查询当前系统中，访问量前10的HTTP地址： topk(10, http_requests_total) Gauge：可增可减的仪表盘 与Counter不同，Gauge类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。常见指标如：node_memory_MemFree（主机当前空闲的内容大小）、node_memory_MemAvailable（可用内存大小）都是Gauge类型的监控指标。 通过Gauge指标，用户可以直接查看系统的当前状态： node_memory_MemFree 对于Gauge类型的监控指标，通过PromQL内置函数delta()可以获取样本在一段时间返回内的变化情况。例如，计算CPU温度在两个小时内的差异： delta(cpu_temp_celsius{host=\"zeus\"}[2h]) 还可以使用deriv()计算样本的线性回归模型，甚至是直接使用predict_linear()对数据的变化趋势进行预测。例如，预测系统磁盘空间在4个小时之后的剩余情况： predict_linear(node_filesystem_free{job=\"node\"}[1h], 4 * 3600) 使用Histogram和Summary分析数据分布情况 除了Counter和Gauge类型的监控指标以外，Prometheus还定义了Histogram和Summary的指标类型。Histogram和Summary主用用于统计和分析样本的分布情况。 在大多数情况下人们都倾向于使用某些量化指标的平均值，例如CPU的平均使用率、页面的平均响应时间。这种方式的问题很明显，以系统API调用的平均响应时间为例：如果大多数API请求都维持在100ms的响应时间范围内，而个别请求的响应时间需要5s，那么就会导致某些WEB页面的响应时间落到中位数的情况，而这种现象被称为长尾问题。 为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组。例如，统计延迟在0~10ms之间的请求数有多少而10~20ms之间的请求数又有多少。通过这种方式可以快速分析系统慢的原因。Histogram和Summary都是为了能够解决这样问题的存在，通过Histogram和Summary类型的监控指标，我们可以快速了解监控样本的分布情况。 例如，指标prometheus_tsdb_wal_fsync_duration_seconds的指标类型为Summary。 它记录了Prometheus Server中wal_fsync处理的处理时间，通过访问Prometheus Server的/metrics地址，可以获取到以下监控样本数据： # HELP prometheus_tsdb_wal_fsync_duration_seconds Duration of WAL fsync. # TYPE prometheus_tsdb_wal_fsync_duration_seconds summary prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.5\"} 0.012352463 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.9\"} 0.014458005 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.99\"} 0.017316173 prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002 prometheus_tsdb_wal_fsync_duration_seconds_count 216 从上面的样本中可以得知当前Prometheus Server进行wal_fsync操作的总次数为216次，耗时2.888716127000002s。其中中位数（quantile=0.5）的耗时为0.012352463，9分位数（quantile=0.9）的耗时为0.014458005s。 在Prometheus Server自身返回的样本数据中，我们还能找到类型为Histogram的监控指标prometheus_tsdb_compaction_chunk_range_bucket。 # HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction # TYPE prometheus_tsdb_compaction_chunk_range histogram prometheus_tsdb_compaction_chunk_range_bucket{le=\"100\"} 0 prometheus_tsdb_compaction_chunk_range_bucket{le=\"400\"} 0 prometheus_tsdb_compaction_chunk_range_bucket{le=\"1600\"} 0 prometheus_tsdb_compaction_chunk_range_bucket{le=\"6400\"} 0 prometheus_tsdb_compaction_chunk_range_bucket{le=\"25600\"} 0 prometheus_tsdb_compaction_chunk_range_bucket{le=\"102400\"} 0 prometheus_tsdb_compaction_chunk_range_bucket{le=\"409600\"} 0 prometheus_tsdb_compaction_chunk_range_bucket{le=\"1.6384e+06\"} 260 prometheus_tsdb_compaction_chunk_range_bucket{le=\"6.5536e+06\"} 780 prometheus_tsdb_compaction_chunk_range_bucket{le=\"2.62144e+07\"} 780 prometheus_tsdb_compaction_chunk_range_bucket{le=\"+Inf\"} 780 prometheus_tsdb_compaction_chunk_range_sum 1.1540798e+09 prometheus_tsdb_compaction_chunk_range_count 780 与Summary类型的指标相似之处在于Histogram类型的样本同样会反应当前指标的记录的总数(以_count作为后缀)以及其值的总量（以_sum作为后缀）。不同在于Histogram指标直接反应了在不同区间内样本的个数，区间通过标签len进行定义。 同时对于Histogram的指标，我们还可以通过histogram_quantile()函数计算出其值的分位数。不同在于Histogram通过histogram_quantile函数是在服务器端计算的分位数。 而Sumamry的分位数则是直接在客户端计算完成。因此对于分位数的计算而言，Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。反之对于客户端而言Histogram消耗的资源更少。在选择这两种方式时用户应该按照自己的实际场景进行选择。 install prometheus helm https://artifacthub.io/packages/helm/prometheus-community/prometheus helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add kube-state-metrics https://kubernetes.github.io/kube-state-metrics helm repo update helm install prometheus prometheus-community/prometheus --set alertmanager.enabled=false --set persistentVolume.enabled=false helm show values prometheus-community/prometheus > value.yaml golang project 集成 prometheus Installation How Go exposition works Adding your own metrics Other Go client features Summary Prometheus has an official Go client library that you can use to instrument Go applications. In this guide, we'll create a simple Go application that exposes Prometheus metrics via HTTP. NOTE: For comprehensive API documentation, see the GoDoc for Prometheus' various Go libraries. Installation You can install the prometheus, promauto, and promhttp libraries necessary for the guide using go get: go get github.com/prometheus/client_golang/prometheus go get github.com/prometheus/client_golang/prometheus/promauto go get github.com/prometheus/client_golang/prometheus/promhttp How Go exposition works To expose Prometheus metrics in a Go application, you need to provide a /metrics HTTP endpoint. You can use the prometheus/promhttp library's HTTP Handler as the handler function. This minimal application, for example, would expose the default metrics for Go applications via http://localhost:2112/metrics: package main import ( \"net/http\" \"github.com/prometheus/client_golang/prometheus/promhttp\" ) func main() { http.Handle(\"/metrics\", promhttp.Handler()) http.ListenAndServe(\":2112\", nil) } To start the application: go run main.go To access the metrics: curl http://localhost:2112/metrics Adding your own metrics The application above exposes only the default Go metrics. You can also register your own custom application-specific metrics. This example application exposes a myapp_processed_ops_total counter that counts the number of operations that have been processed thus far. Every 2 seconds, the counter is incremented by one. package main import ( \"net/http\" \"time\" \"github.com/prometheus/client_golang/prometheus\" \"github.com/prometheus/client_golang/prometheus/promauto\" \"github.com/prometheus/client_golang/prometheus/promhttp\" ) func recordMetrics() { go func() { for { opsProcessed.Inc() time.Sleep(2 * time.Second) } }() } var ( opsProcessed = promauto.NewCounter(prometheus.CounterOpts{ Name: \"myapp_processed_ops_total\", Help: \"The total number of processed events\", }) ) func main() { recordMetrics() http.Handle(\"/metrics\", promhttp.Handler()) http.ListenAndServe(\":2112\", nil) } To run the application: go run main.go To access the metrics: curl http://localhost:2112/metrics In the metrics output, you'll see the help text, type information, and current value of the myapp_processed_ops_total counter: 我们利用promauto包提供的NewCounter方法定义了一个Counter类型的监控指标，只需要填充名字以及帮助信息，该指标就创建完成了。需要注意的是，Counter类型数据的名字要尽量以_total作为后缀。否则当Prometheus与其他系统集成时，可能会出现指标无法识别的问题。每当有请求访问根目录时，该指标就会调用Inc()方法加一，当然，我们也可以调用Add()方法累加任意的非负数。 再次运行修改后的程序，先对根路径进行多次访问，再对/metrics路径进行访问，可以看到新定义的指标已经成功暴露了： # HELP myapp_processed_ops_total The total number of processed events # TYPE myapp_processed_ops_total counter myapp_processed_ops_total 5 You can configure a locally running Prometheus instance to scrape metrics from the application. Here's an example prometheus.yml configuration: scrape_configs: - job_name: myapp scrape_interval: 10s static_configs: - targets: - localhost:2112 Gauge example 监控累积的请求处理显然还是不够的，通常我们还想知道当前正在处理的请求的数量。Prometheus中的Gauge类型数据，与Counter不同，它既能增大也能变小。将正在处理的请求数量定义为Gauge类型是合适的。因此，我们新增的代码块如下： ... var ( ... http_request_in_flight = promauto.NewGauge( prometheus.GaugeOpts{ Name: \"http_request_in_flight\", Help: \"Current number of http requests in flight\", }, ) ) ... http.HandleFunc(\"/\", func(http.ResponseWriter, *http.Request){ http_request_in_flight.Inc() defer http_request_in_flight.Dec() http_request_total.Inc() }) ... Gauge和Counter类型的数据操作起来的差别并不大，唯一的区别是Gauge支持Dec()或者Sub()方法减小指标的值。 Histogram example 对于一个网络服务来说，能够知道它的平均时延是重要的，不过很多时候我们更想知道响应时间的分布状况。Prometheus中的Histogram类型就对此类需求提供了很好的支持。具体到需要新增的代码如下： ... var ( ... http_request_duration_seconds = promauto.NewHistogram( prometheus.HistogramOpts{ Name: \"http_request_duration_seconds\", Help: \"Histogram of lantencies for HTTP requests\", // Buckets: []float64{.1, .2, .4, 1, 3, 8, 20, 60, 120}, }, ) ) ... http.HandleFunc(\"/\", func(http.ResponseWriter, *http.Request){ now := time.Now() http_request_in_flight.Inc() defer http_request_in_flight.Dec() http_request_total.Inc() time.Sleep(time.Duration(rand.Intn(1000)) * time.Millisecond) http_request_duration_seconds.Observe(time.Since(now).Seconds()) }) ... 在访问了若干次上述HTTP Server的根路径之后，从/metrics路径得到的响应如下： # HELP http_request_duration_seconds Histogram of lantencies for HTTP requests # TYPE http_request_duration_seconds histogram http_request_duration_seconds_bucket{le=\"0.005\"} 0 http_request_duration_seconds_bucket{le=\"0.01\"} 0 http_request_duration_seconds_bucket{le=\"0.025\"} 0 http_request_duration_seconds_bucket{le=\"0.05\"} 0 http_request_duration_seconds_bucket{le=\"0.1\"} 3 http_request_duration_seconds_bucket{le=\"0.25\"} 3 http_request_duration_seconds_bucket{le=\"0.5\"} 5 http_request_duration_seconds_bucket{le=\"1\"} 8 http_request_duration_seconds_bucket{le=\"2.5\"} 8 http_request_duration_seconds_bucket{le=\"5\"} 8 http_request_duration_seconds_bucket{le=\"10\"} 8 http_request_duration_seconds_bucket{le=\"+Inf\"} 8 http_request_duration_seconds_sum 3.238809838 http_request_duration_seconds_count 8 Histogram类型暴露的监控数据要比Counter和Gauge复杂得多，最后以_sum和_count开头的指标分别表示总的响应时间以及对于响应时间的计数。而它们之上的若干行表示：时延在0.005秒内的响应数目，0.01秒内的响应次数，0.025秒内的响应次数...最后的+Inf表示响应时间无穷大的响应次数，它的值和_count的值是相等的。显然，Histogram类型的监控数据很好地呈现了数据的分布状态。当然，Histogram默认的边界设置，例如0.005,0.01这类数值一般是用来衡量一个网络服务的时延的。对于具体的应用场景，我们也可以对它们进行自定义，类似于上述代码中被注释掉的那一行（最后的+Inf会自动添加）。 与Histogram类似，Prometheus中定义了一种类型Summary，从另一个角度描绘了数据的分布状况。对于响应时延，我们可能想知道它们的中位数是多少？九分位数又是多少？对于Summary类型数据的定义及使用如下： ... var ( ... http_request_summary_seconds = promauto.NewSummary( prometheus.SummaryOpts{ Name: \"http_request_summary_seconds\", Help: \"Summary of lantencies for HTTP requests\", // Objectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001, 0.999, 0.0001}, }, ) ) ... http.HandleFunc(\"/\", func(http.ResponseWriter, *http.Request){ now := time.Now() http_request_in_flight.Inc() defer http_request_in_flight.Dec() http_request_total.Inc() time.Sleep(time.Duration(rand.Intn(1000)) * time.Millisecond) http_request_duration_seconds.Observe(time.Since(now).Seconds()) http_request_summary_seconds.Observe(time.Since(now).Seconds()) }) ... Summary的定义和使用与Histogram是类似的，最终我们得到的结果如下： $ curl http://127.0.0.1:8080/metrics | grep http_request_summary # HELP http_request_summary_seconds Summary of lantencies for HTTP requests # TYPE http_request_summary_seconds summary http_request_summary_seconds{quantile=\"0.5\"} 0.31810446 http_request_summary_seconds{quantile=\"0.9\"} 0.887116164 http_request_summary_seconds{quantile=\"0.99\"} 0.887116164 http_request_summary_seconds_sum 3.2388269649999994 http_request_summary_seconds_count 8 同样，_sum和_count分别表示请求的总时延以及请求的数目，与Histogram不同的是，Summary其余的部分分别表示，响应时间的中位数是0.31810446秒，九分位数位0.887116164等等。我们也可以根据具体的需求对Summary呈现的分位数进行自定义，如上述程序中被注释的Objectives字段。令人疑惑的是，它是一个map类型，其中的key表示的是分位数，而value表示的则是误差。例如，上述的0.31810446秒是分布在响应数据的0.45~0.55之间的，而并非完美地落在0.5。 事实上，上述的Counter，Gauge，Histogram，Summary就是Prometheus能够支持的全部监控数据类型了（其实还有一种类型Untyped，表示未知类型）。一般使用最多的是Counter和Gauge这两种基本类型，结合PromQL对基础监控数据强大的分析处理能力，我们就能获取极其丰富的监控信息。 不过，有的时候，我们可能希望从更多的特征维度去衡量一个指标。例如，对于接收到的HTTP请求的数目，我们可能希望知道具体到每个路径接收到的请求数目。假设当前能够访问/和/foo目录，显然定义两个不同的Counter，比如http_request_root_total和http_request_foo_total，并不是一个很好的方法。一方面扩展性比较差：如果定义更多的访问路径就需要创建更多新的监控指标，同时，我们定义的特征维度往往不止一个，可能我们想知道某个路径且返回码为XXX的请求数目是多少，这种方法就无能为力了；另一方面，PromQL也无法很好地对这些指标进行聚合分析。 Prometheus对于此类问题的方法是为指标的每个特征维度定义一个label，一个label本质上就是一组键值对。一个指标可以和多个label相关联，而一个指标和一组具体的label可以唯一确定一条时间序列。对于上述分别统计每条路径的请求数目的问题，标准的Prometheus的解决方法如下： ... var ( http_request_total = promauto.NewCounterVec( prometheus.CounterOpts{ Name: \"http_request_total\", Help: \"The total number of processed http requests\", }, []string{\"path\"}, ) ... http.HandleFunc(\"/\", func(http.ResponseWriter, *http.Request){ ... http_request_total.WithLabelValues(\"root\").Inc() ... }) http.HandleFunc(\"/foo\", func(http.ResponseWriter, *http.Request){ ... http_request_total.WithLabelValues(\"foo\").Inc() ... }) ) 此处以Counter类型的数据举例，对于其他另外三种数据类型的操作是完全相同的。此处我们在调用NewCounterVec方法定义指标时，我们定义了一个名为path的label，在/和/foo的Handler中，WithLabelValues方法分别指定了label的值为root和foo，如果该值对应的时间序列不存在，则该方法会新建一个，之后的操作和普通的Counter指标没有任何不同。而最终通过/metrics暴露的结果如下： $ curl http://127.0.0.1:8080/metrics | grep http_request_total # HELP http_request_total The total number of processed http requests # TYPE http_request_total counter http_request_total{path=\"foo\"} 9 http_request_total{path=\"root\"} 5 可以看到，此时指标http_request_total对应两条时间序列，分别表示path为foo和root时的请求数目。那么如果我们反过来想统计，各个路径的请求总和呢？我们是否需要定义个path的值为total，用来表示总体的计数情况？显然是不必的，PromQL能够轻松地对一个指标的各个维度的数据进行聚合，通过如下语句查询Prometheus就能获得请求总和： sum(http_request_total) label在Prometheus中是一个简单而强大的工具，理论上，Prometheus没有限制一个指标能够关联的label的数目。但是，label的数目也并不是越多越好，因为每增加一个label，用户在使用PromQL的时候就需要额外考虑一个label的配置。一般来说，我们要求添加了一个label之后，对于指标的求和以及求均值都是有意义的 2. 进阶 基于上文所描述的内容，我们就能很好地在自己的应用程序里面定义各种监控指标并且保证它能被Prometheus接收处理了。但是有的时候我们可能需要更强的定制化能力，尽管使用高度封装的API确实很方便，不过它附加的一些东西可能不是我们想要的，比如默认的Handler提供的Golang运行时相关以及进程相关的一些监控指标。另外，当我们自己编写Exporter的时候，该如何利用已有的组件，将应用原生的监控指标转化为符合Prometheus标准的指标。为了解决上述问题，我们有必要对Prometheus SDK内部的实现机理了解地更为深刻一些。 在Prometheus SDK中，Register和Collector是两个核心对象。Collector里面可以包含一个或者多个Metric，它事实上是一个Golang中的interface，提供如下两个方法： type Collector interface { Describe(chan简单地说，Describe方法通过channel能够提供该Collector中每个Metric的描述信息，Collect方法则通过channel提供了其中每个Metric的具体数据。单单定义Collector还是不够的，我们还需要将其注册到某个Registry中，Registry会调用它的Describe方法保证新添加的Metric和之前已经存在的Metric并不冲突。而Registry则需要和具体的Handler相关联，这样当用户访问/metrics路径时，Handler中的Registry会调用已经注册的各个Collector的Collect方法，获取指标数据并返回。 在上文中，我们定义一个指标如此方便，根本原因是promauto为我们做了大量的封装，例如，对于我们使用的promauto.NewCounter方法，其具体实现如下： http_request_total = promauto.NewCounterVec( prometheus.CounterOpts{ Name: \"http_request_total\", Help: \"The total number of processed http requests\", }, []string{\"path\"}, ) --- // client_golang/prometheus/promauto/auto.go func NewCounterVec(opts prometheus.CounterOpts, labelNames []string) *prometheus.CounterVec { c := prometheus.NewCounterVec(opts, labelNames) prometheus.MustRegister(c) return c } --- // client_golang/prometheus/counter.go func NewCounterVec(opts CounterOpts, labelNames []string) *CounterVec { desc := NewDesc( BuildFQName(opts.Namespace, opts.Subsystem, opts.Name), opts.Help, labelNames, opts.ConstLabels, ) return &CounterVec{ metricVec: newMetricVec(desc, func(lvs ...string) Metric { if len(lvs) != len(desc.variableLabels) { panic(makeInconsistentCardinalityError(desc.fqName, desc.variableLabels, lvs)) } result := &counter{desc: desc, labelPairs: makeLabelPairs(desc, lvs)} result.init(result) // Init self-collection. return result }), } } 一个Counter（或者CounterVec，即包含label的Counter）其实就是一个Collector的具体实现，它的Describe方法提供的描述信息，无非就是指标的名字，帮助信息以及定义的Label的名字。promauto在对它完成定义之后，还调用prometheus.MustRegister(c)进行了注册。事实上，prometheus默认提供了一个Default Registry，prometheus.MustRegister会将Collector直接注册到Default Registry中。如果我们直接使用了promhttp.Handler()来处理/metrics路径的请求，它会直接将Default Registry和Handler相关联并且向Default Registry注册Golang Collector和Process Collector。所以，假设我们不需要这些自动注入的监控指标，只要构造自己的Handler就可以。 当然，Registry和Collector也都是能自定义的，特别在编写Exporter的时候，我们往往会将所有的指标定义在一个Collector中，根据访问应用原生监控接口的结果对所需的指标进行填充并返回结果。基于上述对于Prometheus SDK的实现机制的理解，我们可以实现一个最简单的Exporter框架如下所示： package main import ( \"net/http\" \"math/rand\" \"time\" \"github.com/prometheus/client_golang/prometheus\" \"github.com/prometheus/client_golang/prometheus/promhttp\" ) type Exporter struct { up *prometheus.Desc } func NewExporter() *Exporter { namespace := \"exporter\" up := prometheus.NewDesc(prometheus.BuildFQName(namespace, \"\", \"up\"), \"If scrape target is healthy\", nil, nil) return &Exporter{ up: up, } } func (e *Exporter) Describe(ch chan在这个Exporter的最简实现中，我们创建了新的Registry，手动对exporter这个Collector完成了注册并且基于这个Registry自己构建了一个Handler并且与/metrics相关联。在初始exporter的时候，我们仅仅需要调用NewDesc()方法填充需要监控的指标的描述信息。当用户访问/metrics路径时，经过完整的调用链，最后在进行Collect的时候，我们才会对应用的原生监控接口进行访问，获取监控数据。在真实的Exporter实现中，该步骤应该在Scrape()方法中完成。最后，根据返回的原生监控数据，利用MustNewConstMetric()构造出我们所需的Metric，返回给channel即可。访问该Exporter的/metrics得到的结果如下： $ curl http://127.0.0.1:8080/metrics # HELP exporter_up If scrape target is healthy # TYPE exporter_up gauge exporter_up 1 Other Go client features In this guide we covered just a small handful of features available in the Prometheus Go client libraries. You can also expose other metrics types, such as gauges and histograms, non-global registries, functions for pushing metrics to Prometheus PushGateways, bridging Prometheus and Graphite, and more. Summary In this guide, you created two sample Go applications that expose metrics to Prometheus---one that exposes only the default Go metrics and one that also exposes a custom Prometheus counter---and configured a Prometheus instance to scrape metrics from those applications. 添加数据采集任务 当服务运行起来之后，需要进行如下操作让 Prometheus 监控服务发现并采集监控指标： 使用lens 选择对应 Prometheus 实例进入管理页面。 点击 【Custom Resource】 ---> 【monitoring.coreos.com】----> 通过服务发现添加 Service Monitor，目前支持基于 Labels 发现对应的目标实例地址，因此可以对一些服务添加特定的 K8S Labels，可以使 Labels 下的服务都会被 Prometheus 服务自动识别出来，不需要再为每个服务一一添加采取任务，以上面的例子配置信息如下： apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: go-demo # 填写一个唯一名称 namespace: cm-prometheus # namespace固定，不要修改 spec: endpoints: - interval: 30s # 填写service yaml中Prometheus Exporter对应的Port的Name port: 2112 # 填写Prometheus Exporter对应的Path的值，不填默认/metrics path: /metrics relabelings: # ** 必须要有一个 label 为 application，这里假设 k8s 有一个 label 为 app， # 我们通过 relabel 的 replace 动作把它替换成了 application - action: replace sourceLabels: [__meta_kubernetes_pod_label_app] targetLabel: application # 选择要监控service所在的namespace namespaceSelector: matchNames: - golang-demo # 填写要监控service的Label值，以定位目标service selector: matchLabels: app: golang-app-demo 项目实战 github address: https://github.com/chenjiandongx/ginprom label package prometheus import ( \"fmt\" \"net/http\" \"regexp\" \"time\" \"github.com/gin-gonic/gin\" \"github.com/prometheus/client_golang/prometheus\" ) const namespace = \"service\" var ( labels = []string{\"status\", \"endpoint\", \"method\"} uptime = prometheus.NewCounterVec( prometheus.CounterOpts{ Namespace: namespace, Name: \"uptime\", Help: \"HTTP service uptime.\", }, nil, ) reqCount = prometheus.NewCounterVec( prometheus.CounterOpts{ Namespace: namespace, Name: \"http_request_count_total\", Help: \"Total number of HTTP requests made.\", }, labels, ) reqDuration = prometheus.NewHistogramVec( prometheus.HistogramOpts{ Namespace: namespace, Name: \"http_request_duration_seconds\", Help: \"HTTP request latencies in seconds.\", }, labels, ) reqSizeBytes = prometheus.NewSummaryVec( prometheus.SummaryOpts{ Namespace: namespace, Name: \"http_request_size_bytes\", Help: \"HTTP request sizes in bytes.\", }, labels, ) respSizeBytes = prometheus.NewSummaryVec( prometheus.SummaryOpts{ Namespace: namespace, Name: \"http_response_size_bytes\", Help: \"HTTP response sizes in bytes.\", }, labels, ) ) // init registers the prometheus metrics func init() { prometheus.MustRegister(uptime, reqCount, reqDuration, reqSizeBytes, respSizeBytes) go recordUptime() } // recordUptime increases service uptime per second. func recordUptime() { for range time.Tick(time.Second) { uptime.WithLabelValues().Inc() } } // calcRequestSize returns the size of request object. func calcRequestSize(r *http.Request) float64 { size := 0 if r.URL != nil { size = len(r.URL.String()) } size += len(r.Method) size += len(r.Proto) for name, values := range r.Header { size += len(name) for _, value := range values { size += len(value) } } size += len(r.Host) // r.Form and r.MultipartForm are assumed to be included in r.URL. if r.ContentLength != -1 { size += int(r.ContentLength) } return float64(size) } type RequestLabelMappingFn func(c *gin.Context) string // PromOpts represents the Prometheus middleware Options. // It is used for filtering labels by regex. type PromOpts struct { ExcludeRegexStatus string ExcludeRegexEndpoint string ExcludeRegexMethod string EndpointLabelMappingFn RequestLabelMappingFn } // NewDefaultOpts return the default ProOpts func NewDefaultOpts() *PromOpts { return &PromOpts{ EndpointLabelMappingFn: func(c *gin.Context) string { //by default do nothing, return URL as is return c.Request.URL.Path }, } } // checkLabel returns the match result of labels. // Return true if regex-pattern compiles failed. func (po *PromOpts) checkLabel(label, pattern string) bool { if pattern == \"\" { return true } matched, err := regexp.MatchString(pattern, label) if err != nil { return true } return !matched } // PromMiddleware returns a gin.HandlerFunc for exporting some Web metrics func PromMiddleware(promOpts *PromOpts) gin.HandlerFunc { // make sure promOpts is not nil if promOpts == nil { promOpts = NewDefaultOpts() } // make sure EndpointLabelMappingFn is callable if promOpts.EndpointLabelMappingFn == nil { promOpts.EndpointLabelMappingFn = func(c *gin.Context) string { return c.Request.URL.Path } } return func(c *gin.Context) { start := time.Now() c.Next() status := fmt.Sprintf(\"%d\", c.Writer.Status()) endpoint := promOpts.EndpointLabelMappingFn(c) method := c.Request.Method lvs := []string{status, endpoint, method} isOk := promOpts.checkLabel(status, promOpts.ExcludeRegexStatus) && promOpts.checkLabel(endpoint, promOpts.ExcludeRegexEndpoint) && promOpts.checkLabel(method, promOpts.ExcludeRegexMethod) if !isOk { return } // no response content will return -1 respSize := c.Writer.Size() if respSize Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/readme.html":{"url":"blog/golang/readme.html","title":"Readme","keywords":"","body":"Golang Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/spdy.html":{"url":"blog/golang/spdy.html","title":"Spdy","keywords":"","body":"spdy Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/swagger.html":{"url":"blog/golang/swagger.html","title":"Swagger","keywords":"","body":"Swagger start edit ui web docker run -d -p 8081:8080 swaggerapi/swagger-editor sample yaml swagger: \"2.0\" info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specification schemes: - https host: simple.api basePath: /openapi101 paths: {} 懒人必备 Generate a spec from source,即通过源码生成文档，很符合我的需求。 Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务的接口文档。 目前的项目基本都是前后端分离，后端为前端提供接口的同时，还需同时提供接口的说明文档。但我们的代码总是会根据实际情况来实时更新，这个时候有可能会忘记更新接口的说明文档，造成一些不必要的问题。 用人话说，swagger就是帮你写接口说明文档的。更具体地，可以看下面的图片，swagger官方建议使用下面的红字部分，这篇博客主要是记录如何，使用swagger自动生成Api文档的，所以只介绍swagger-ui，其他的…以后我用到会再整理。 auto generate doc swagger generate spec -o ./swagger.json Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/tls.html":{"url":"blog/golang/tls.html","title":"Tls","keywords":"","body":"Golfing TLS coding golang server package main import ( \"crypto/tls\" \"crypto/x509\" \"fmt\" \"io/ioutil\" \"net/http\" ) type myhandler struct { } func (h *myhandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hi, This is an example of http service in golang!\\n\") } func main() { pool := x509.NewCertPool() caCertPath := \"ca.crt\" caCrt, err := ioutil.ReadFile(caCertPath) if err != nil { fmt.Println(\"ReadFile err:\", err) return } pool.AppendCertsFromPEM(caCrt) s := &http.Server{ Addr: \":8081\", Handler: &myhandler{}, TLSConfig: &tls.Config{ ClientCAs: pool, ClientAuth: tls.RequireAndVerifyClientCert, }, } err = s.ListenAndServeTLS(\"server.crt\", \"server.key\") if err != nil { fmt.Println(\"ListenAndServeTLS err:\", err) } } golang client package main import ( \"crypto/tls\" \"crypto/x509\" \"fmt\" \"io/ioutil\" \"net/http\" ) func main() { pool := x509.NewCertPool() caCertPath := \"ca.crt\" caCrt, err := ioutil.ReadFile(caCertPath) if err != nil { fmt.Println(\"ReadFile err:\", err) return } pool.AppendCertsFromPEM(caCrt) cliCrt, err := tls.LoadX509KeyPair(\"client.crt\", \"client.key\") if err != nil { fmt.Println(\"Loadx509keypair err:\", err) return } tr := &http.Transport{ TLSClientConfig: &tls.Config{ RootCAs: pool, Certificates: []tls.Certificate{cliCrt}, }, } client := &http.Client{Transport: tr} resp, err := client.Get(\"https://localhost:8081\") if err != nil { fmt.Println(\"Get error:\", err) return } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) fmt.Println(string(body)) } # normal func GetHttp(url string) (data []byte, err error) { klog.Infof(\"GetHttp url: %s\", url) req, err := http.NewRequest(http.MethodGet, url, nil) if err != nil { return } req.Header.Set(\"Content-Type\", \"application/json\") req.Header.Set(\"charset\", \"UTF-8\") clt := http.Client{} resp, err := clt.Do(req) if err != nil { return } body, err := ioutil.ReadAll(resp.Body) defer resp.Body.Close() klog.V(4).Infof(\"resp: %+v\", resp) return body, err } # pass through nginx type Response struct { ErrMsg string `json:\"errMsg\"` Body string `json:\"body\"` Status int `json:\"status\"` } func wrap(err error, resp *http.Response) Response { var r Response if err != nil { r.ErrMsg = err.Error() } if resp != nil { reader := resp.Body if resp.Header.Get(\"Content-Encoding\") == \"gzip\" { reader, err = gzip.NewReader(resp.Body) if err != nil { klog.Errorf(\"gzip error: %v\", err) } } body, err := ioutil.ReadAll(reader) defer resp.Body.Close() if err != nil { klog.Errorf(\"read resp.Body err: %v\", err) return r } klog.Infof(\"body: %v\", string(body)) r.Body = string(body) r.Status = resp.StatusCode } return r } trouble shouting 1.x509: cannot validate certificate for 1.1.1.1 because it doesn't contain any IP SANs 解决方案一： echo \"1.1.1.1 test.com\" > /etc/hosts client use 域名调用 解决方案二： 关键点在于，服务端证书生成时，需要设置subjectAltName = IP:10.30.0.163，设置方式如下（通过在证书生成语句中添加-extfile extfile.cnf，实现将extfile.cnf中的内容写入到证书中） openssl genrsa -out server.key 2048 openssl req -new -key server.key -subj \"/CN=1.1.1.1\" -out server.csr echo subjectAltName = IP:1.1.1.1 > extfile.cnf openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile extfile.cnf -out server.crt -days 5000 可以使用下面命令查看服务证书内容是否有设置的服务端IP地址，如图： openssl x509 -in ./server.crt -noout -text link http://singlecool.com/2017/10/21/TLS-Go/ https://blog.csdn.net/min19900718/article/details/87920254?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/代码走读/2020.5.26.html":{"url":"blog/golang/代码走读/2020.5.26.html","title":"2020.5.26","keywords":"","body":"5.26号 pipeline代码走读 发现的问题： pipeline resource： git 和 image 仓库都应该可以支持多个，即多个代码源和多个镜像push task 也是同样的，可以有多个镜像构建任务以及部署任务 Client 初始化 放到 main.go 中，避免项目无法启动 K8s client 放到统一的位置 多字符串拼接 方案String.Builder. https://juejin.im/entry/5ad0696751882555784e60d3 1.1 “+” 拼接方式 缺陷 与许多支持string类型的语言一样，golang中的string类型也是只读且不可变的。因此，这种拼接字符串的方式会导致大量的string创建、销毁和内存分配。如果你拼接的字符串比较多的话，这显然不是一个正确的姿 与byte.Buffer思路类似，既然 string 在构建过程中会不断的被销毁重建，那么就尽量避免这个问题，底层使用一个 buf []byte 来存放字符串的内容。 对于写操作，就是简单的将byte写入到 buf 即可。 为了解决bytes.Buffer.String()存在的[]byte -> string类型转换和内存拷贝问题，这里使用了一个unsafe.Pointer的存指针转换操作，实现了直接将buf []byte转换为 string类型，同时避免了内存充分配的问题。 如果我们自己来实现strings.Builder, 大部分情况下我们完成前3步就觉得大功告成了。但是标准库做得要更近一步。我们知道Golang的堆栈在大部分情况下是不需要开发者关注的，如果能够在栈上完成的工作逃逸到了堆上，性能就大打折扣了。因此，copyCheck 加入了一行比较hack的代码来避免buf逃逸到堆上。关于这部分内容，你可以进一步阅读Dave Cheney的关于Go’s hidden #pragmas. 总结： \"+\" 拼接 存在多次内存分配 bytes.Buffer 在最后调用String()方法 存在类型转换和内存拷贝 string.Builder 直接指针转换，避免内存分配 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/代码走读/2021.12.24.html":{"url":"blog/golang/代码走读/2021.12.24.html","title":"2021.12.24","keywords":"","body":"http 工具 功能 代码模块 设计思想 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/代码走读/2022.1.06.html":{"url":"blog/golang/代码走读/2022.1.06.html","title":"2022.1.06","keywords":"","body":"Idp-agent 代码走读 idp-agent 代码分服务端和客户端 服务端功能： ws server forward request from idp-base 客户端连接到服务端 1. upgrade http to ws connect 1. limit client key 1. Save to Default Transport 1. Read pool 1. WritePool 服务端转发Request To client json Decode Request extract connect key from Request Struct get PersConnect from DefaultTransport pconn.roundTrip(req), send message and receive message Question： 客户端每次连接 如何服用一条 websocket 通道 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/代码走读/2022.3.25-log-agent.html":{"url":"blog/golang/代码走读/2022.3.25-log-agent.html","title":"2022.3.25 Log Agent","keywords":"","body":"log-anget 问题： 删除 datadog 相关代码 const ( // DefaultConfPath points to the folder containing lstack.yaml DefaultConfPath = \"/etc/datadog-agent\" // DefaultLogFile points to the log file that will be used if not configured DefaultLogFile = \"/var/log/datadog/agent.log\" ) // logs-intake endpoints depending on the site and environment. var logsEndpoints = map[string]int{ \"agent-intake.logs.lstack.com\": 10516, \"agent-intake.logs.lstack.eu\": 443, \"agent-intake.logs.datad0g.com\": 10516, \"agent-intake.logs.datad0g.eu\": 443, } 删除失败测试 模块 autodiscovery logs tagger telemetry metadata serializer configResolver 架构图 数据流图 poll pods，轮询kubelet接口获取pods，转为[]config配置 ac.schedule configs 配置为services（service 即为pod） i.addSource 找到service对应的pod，containers，生成sources 为文件创建tailer，读取文件内容 将文件送给pipeline处理 p.applyRedactingRules,过滤日志 sender发送给服务端存储日 Logs-agent 启动流程 1. config.SetupLogger，配置日志 2. http.ListenAndServe，监听5011端口 3. err := logs.Start，启动logs-agent 1. scheduler.CreateScheduler(sources, services), 2. config.BuildEndpoints， 3. config.GlobalProcessingRules()，配置全局过滤规则 4. agent = NewAgent，初始化agent 5. agent.Start()，启动agent 1. pipeline.NewProvider，初始化pipelineProvider. pipeline启动后，监听来自inputChan的消息，并经过processor处理发送到sender的inputChan，sender调用s.destinations.Main.Send发功给destination，同时发送给p.outputChan = p.auditor.Channel() 2. file.NewScanner，监听来自sources的变化，创建tailer读取文件，并forwardMessages送给tailer的outputChan 3. container.NewLauncher，监听services的变化，当给pod添加了一个注解，将转位source添加到sources，后面就交给scannere去处理 4. common.LoadComponents 1. tagger.SetDefaultTagger 2. metaScheduler := scheduler.NewMetaScheduler() 3. AC = setupAutoDiscovery，将配置转为intergration.Config。初始化了2个listeners，ad.AddListeners 5. common.StartAutoConfig() 模块功能 configResolver 获取被监听对象的host， pid，port， hostname，extra var templateVariables = map[string]variableGetter{ \"host\": getHost, \"pid\": getPid, \"port\": getPort, \"hostname\": getHostname, \"extra\": getExtra, } Autodiscovery integration This package is responsible of defining the types representing an integration which can be used by several components of the agent to configure checks or logs collectors for example. // Config is a generic container for configuration files // When a new field is added to this struct, please evaluate whether it should be computed in the config Digest // and update the field's documentation and the Digest method accordingly type Config struct { Name string `json:\"check_name\"` // the name of the check (include in digest: true) Instances []Data `json:\"instances\"` // the list of instances in Yaml (include in digest: true) InitConfig Data `json:\"init_config\"` // the init_config in Yaml (include in digest: true) MetricConfig Data `json:\"metric_config\"` // the metric config in Yaml (jmx check only) (include in digest: false) LogsConfig Data `json:\"logs\"` // the logs config in Yaml (logs-agent only) (include in digest: true) ADIdentifiers []string `json:\"ad_identifiers\"` // the list of AutoDiscovery identifiers (optional) (include in digest: true) Provider string `json:\"provider\"` // the provider that issued the config (include in digest: false) Entity string `json:\"-\"` // the entity ID (optional) (include in digest: true) TaggerEntity string `json:\"-\"` // the tagger entity ID (optional) (include in digest: false) ClusterCheck bool `json:\"cluster_check\"` // cluster-check configuration flag (include in digest: false) NodeName string `json:\"node_name\"` // node name in case of an endpoint check backed by a pod (include in digest: true) CreationTime CreationTime `json:\"-\"` // creation time of service (include in digest: false) Source string `json:\"source\"` // the source of the configuration (include in digest: false) IgnoreAutodiscoveryTags bool `json:\"ignore_autodiscovery_tags\"` // used to ignore tags coming from autodiscovery (include in digest: true) MetricsExcluded bool `json:\"-\"` // whether metrics collection is disabled (set by container listeners only) (include in digest: false) LogsExcluded bool `json:\"-\"` // whether logs collection is disabled (set by container listeners only) (include in digest: false) } Providers Scheduler： ​ activeSchedulers map[string]Scheduler // Scheduler is the interface that should be implemented if you want to schedule and // unschedule integrations type Scheduler interface { Schedule([]integration.Config) Unschedule([]integration.Config) Stop() } tagger 启动流程 文件： cmd/log-agent/app/run.go StartAgent() config.SetupLogger(...) // 配置全局变量Lstack // start logs-agent if err := logs.Start(func() *autodiscovery.AutoConfig { return common.AC }); err != nil { log.Error(\"Could not start logs-agent: \", err) } 文件 pkg/log-agent/tag/logs.go // LogSources stores a list of log sources. type LogSources struct { mu sync.Mutex sources []*LogSource addedByType map[string]chan *LogSource removedByType map[string]chan *LogSource } // LogSource holds a reference to an integration name and a log configuration, and allows to track errors and // successful operations on it. Both name and configuration are static for now and determined at creation time. // Changing the status is designed to be thread safe. type LogSource struct { // Put expvar Int first because it's modified with sync/atomic, so it needs to // be 64-bit aligned on 32-bit systems. See https://golang.org/pkg/sync/atomic/#pkg-note-BUG BytesRead expvar.Int Name string Config *LogsConfig Status *LogStatus inputs map[string]bool lock *sync.Mutex Messages *Messages // sourceType is the type of the source that we are tailing whereas Config.Type is the type of the tailer // that reads log lines for this source. E.g, a sourceType == containerd and Config.Type == file means that // the agent is tailing a file to read logs of a containerd container sourceType SourceType info map[string]string // In the case that the source is overridden, keep a reference to the parent for bubbling up information about the child ParentSource *LogSource // LatencyStats tracks internal stats on the time spent by messages from this source in a processing pipeline, i.e. // the duration between when a message is decoded by the tailer/listener/decoder and when the message is handled by a sender LatencyStats *util.StatsTracker } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:20:37 "},"blog/golang/任务队列/":{"url":"blog/golang/任务队列/","title":"任务队列","keywords":"","body":"gocraft/work 工作队列源码简介简介功能特性注册Job注册Job 流程发送JobWoker Fetch JobWorker handle Job创建消费任务的2种方法执行消费任务的真正 gocraft/work 工作队列源码简介 ubeadm init phase kubeconfig kubelet --node-name=cn-hangzhou.i-bp1bq96d1zohe28czs47 --kubeconfig-dir=/tmp/ --a piserver-advertise-address=172.16.112.134 --apiserver-bind-port=6443 简介 gocraft/work 是一款使用go开发的任务处理软件，通过redis 存储任务队列，可以使用工作池同时处理多个任务。本文主要介绍任务注册和任务消费的源代码。 功能特性 Fast and efficient. Faster than this, this, and this. See below for benchmarks. Reliable - don't lose jobs even if your process crashes. Middleware on jobs -- good for metrics instrumentation, logging, etc. If a job fails, it will be retried a specified number of times. Schedule jobs to happen in the future. Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. Web UI to manage failed jobs and observe the system. Periodically enqueue jobs on a cron-like schedule. Pause / unpause jobs and control concurrency within and across processes 注册Job 注册Job 流程 创建redis client pool 创建对象，定义 任务处理函数 创建 任务工作池，需要传入 被处理对象结构体， 最大并发数， 命名空间， redis client pool 创建Job， 需要传入 job 名称和 job 处理函数， job 在redis 中使用列表存储，key的组成：nameSapce:job:jobName， 同一namespace支持多种类型任务处理 这里使用任务名称作为key存入redis， 任务处理参数存放到列表中 func main() { // Make a new pool. Arguments: // Context{} is a struct that will be the context for the request. // 10 is the max concurrency // \"my_app_namespace\" is the Redis namespace // redisPool is a Redis pool pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) // Add middleware that will be executed for each job pool.Middleware((*Context).Log) // Map the name of jobs to handler functions // pool 中的 jobTypes是一个字典，key 是任务名称， value 是 任务处理函数 // 当有任务的时候，会将任务需要的参数 放入到redis key 为jobName的列表中 // 第二个参数必须是 工作池对象的方法 pool.Job(\"send_email\", (*Context).SendEmail) // Customize options: pool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export) // Start processing jobs pool.Start() ... } 发送Job 发送job 其实调用NewEnqueuer方法向redis 的列表中压入元素（具体的内容是任务参数） package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } // Make an enqueuer with a particular namespace var enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool) func main() { // Enqueue a job named \"send_email\" with the specified parameters. _, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4}) if err != nil { log.Fatal(err) } } Woker Fetch Job 在New WrokPool的时候会根据并法参数concurrency，创建同等个数的woker。 Worker 是一个job处理者，通过永久for循环，不间断的从redis 的任务队列中获取任务，在处理任务的时候，协程阻塞，等待一个任务处理完，再继续下一个。 下面的代码是worker 在for循环中的重要操作（1） fetch job （2） process job func (w *worker) loop() { for { select { 。。。 case fetchJob 本质是redis 的 pop，push 操作。首先将redis 列表中的任务 移除，然后再放入到处理队列中，这个操作必须是原子操作（原子性是指事务是一个不可再分割的工作单元，事务中的操作要么都发生，要么都不发生），作者使用了lua脚本完成。最后返回一个job 对象，里面有后面任务处理函数需要的args，即这里的rawJson func (w *worker) fetchJob() (*Job, error) { scriptArgs = append(scriptArgs, w.poolID) // ARGV[1] ... values, err := redis.Values(w.redisFetchScript.Do(conn, scriptArgs...)) ... job, err := newJob(rawJSON, dequeuedFrom, inProgQueue) .. return job, nil } Worker handle Job Pool.JobWithOptions(InstallMasterJob, work.JobOptions{Priority: 1, MaxFails: 1}, ConsumeJob) workpool 注册任务ConsumeJob后， 该任务ConsumeJob会被赋值给 worker.jobTypes[job.Name].GenericHandler, 他的反射类型被赋值给了jobType.DynamicHandler。如果该消费任务使用了上下文参数。 创建消费任务的2种方法 If you don't need context: func YourFunctionName(job *work.Job) error If you want your handler to accept a context: func (c Context) YourFunctionName(job work.Job) error // or, func YourFunctionName(c Context, job work.Job) error func (wp *WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) *WorkerPool { jobOpts = applyDefaultsAndValidate(jobOpts) vfn := reflect.ValueOf(fn) validateHandlerType(wp.contextType, vfn) jt := &jobType{ Name: name, //vfn 任务消费方法的反射类型， 如果消费方法中有ctx 参数，那么会调用反射执行 DynamicHandler: vfn, JobOptions: jobOpts, } if gh, ok := fn.(func(*Job) error); ok { // 用户的任务消费函数，被赋值给了jobType的GenericHandler， 如果消费方法只有一个job参数，则执行GenericHandler jt.IsGeneric = true jt.GenericHandler = gh } wp.jobTypes[name] = jt for _, w := range wp.workers { w.updateMiddlewareAndJobTypes(wp.middleware, wp.jobTypes) } return wp } 执行消费任务的gocraft/work 工作队列源码简介简介功能特性注册Job注册Job 流程发送JobWoker Fetch JobWorker handle Job创建消费任务的2种方法执行消费任务的真正 gocraft/work 工作队列源码简介 ubeadm init phase kubeconfig kubelet --node-name=cn-hangzhou.i-bp1bq96d1zohe28czs47 --kubeconfig-dir=/tmp/ --a piserver-advertise-address=172.16.112.134 --apiserver-bind-port=6443 简介 gocraft/work 是一款使用go开发的任务处理软件，通过redis 存储任务队列，可以使用工作池同时处理多个任务。本文主要介绍任务注册和任务消费的源代码。 功能特性 Fast and efficient. Faster than this, this, and this. See below for benchmarks. Reliable - don't lose jobs even if your process crashes. Middleware on jobs -- good for metrics instrumentation, logging, etc. If a job fails, it will be retried a specified number of times. Schedule jobs to happen in the future. Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. Web UI to manage failed jobs and observe the system. Periodically enqueue jobs on a cron-like schedule. Pause / unpause jobs and control concurrency within and across processes 注册Job 注册Job 流程 创建redis client pool 创建对象，定义 任务处理函数 创建 任务工作池，需要传入 被处理对象结构体， 最大并发数， 命名空间， redis client pool 创建Job， 需要传入 job 名称和 job 处理函数， job 在redis 中使用列表存储，key的组成：nameSapce:job:jobName， 同一namespace支持多种类型任务处理 这里使用任务名称作为key存入redis， 任务处理参数存放到列表中 func main() { // Make a new pool. Arguments: // Context{} is a struct that will be the context for the request. // 10 is the max concurrency // \"my_app_namespace\" is the Redis namespace // redisPool is a Redis pool pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) // Add middleware that will be executed for each job pool.Middleware((*Context).Log) // Map the name of jobs to handler functions // pool 中的 jobTypes是一个字典，key 是任务名称， value 是 任务处理函数 // 当有任务的时候，会将任务需要的参数 放入到redis key 为jobName的列表中 // 第二个参数必须是 工作池对象的方法 pool.Job(\"send_email\", (*Context).SendEmail) // Customize options: pool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export) // Start processing jobs pool.Start() ... } 发送Job 发送job 其实调用NewEnqueuer方法向redis 的列表中压入元素（具体的内容是任务参数） package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } // Make an enqueuer with a particular namespace var enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool) func main() { // Enqueue a job named \"send_email\" with the specified parameters. _, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4}) if err != nil { log.Fatal(err) } } Woker Fetch Job 在New WrokPool的时候会根据并法参数concurrency，创建同等个数的woker。 Worker 是一个job处理者，通过永久for循环，不间断的从redis 的任务队列中获取任务，在处理任务的时候，协程阻塞，等待一个任务处理完，再继续下一个。 下面的代码是worker 在for循环中的重要操作（1） fetch job （2） process job func (w *worker) loop() { for { select { 。。。 case fetchJob 本质是redis 的 pop，push 操作。首先将redis 列表中的任务 移除，然后再放入到处理队列中，这个操作必须是原子操作（原子性是指事务是一个不可再分割的工作单元，事务中的操作要么都发生，要么都不发生），作者使用了lua脚本完成。最后返回一个job 对象，里面有后面任务处理函数需要的args，即这里的rawJson func (w *worker) fetchJob() (*Job, error) { scriptArgs = append(scriptArgs, w.poolID) // ARGV[1] ... values, err := redis.Values(w.redisFetchScript.Do(conn, scriptArgs...)) ... job, err := newJob(rawJSON, dequeuedFrom, inProgQueue) .. return job, nil } Worker handle Job Pool.JobWithOptions(InstallMasterJob, work.JobOptions{Priority: 1, MaxFails: 1}, ConsumeJob) workpool 注册任务ConsumeJob后， 该任务ConsumeJob会被赋值给 worker.jobTypes[job.Name].GenericHandler, 他的反射类型被赋值给了jobType.DynamicHandler。如果该消费任务使用了上下文参数。 创建消费任务的2种方法 If you don't need context: func YourFunctionName(job *work.Job) error If you want your handler to accept a context: func (c Context) YourFunctionName(job work.Job) error // or, func YourFunctionName(c Context, job work.Job) error func (wp *WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) *WorkerPool { jobOpts = applyDefaultsAndValidate(jobOpts) vfn := reflect.ValueOf(fn) validateHandlerType(wp.contextType, vfn) jt := &jobType{ Name: name, //vfn 任务消费方法的反射类型， 如果消费方法中有ctx 参数，那么会调用反射执行 DynamicHandler: vfn, JobOptions: jobOpts, } if gh, ok := fn.(func(*Job) error); ok { // 用户的任务消费函数，被赋值给了jobType的GenericHandler， 如果消费方法只有一个job参数，则执行GenericHandler jt.IsGeneric = true jt.GenericHandler = gh } wp.jobTypes[name] = jt for _, w := range wp.workers { w.updateMiddlewareAndJobTypes(wp.middleware, wp.jobTypes) } return wp } 执行消费任务的真正代码 worker对象的processJob（job * Job） 方法 调用了runJob方法执行GenericHandler or DynamicHandler.Call func runJob(job *Job, ctxType reflect.Type, middleware []*middlewareHandler, jt *jobType) (returnCtx reflect.Value, returnError error) { 。。。。 next = func() error { 。。。。 if jt.IsGeneric { // 任务消费方法没有ctx时候执行 return jt.GenericHandler(job) } // 任务消费方法有ctx时执行 res := jt.DynamicHandler.Call([]reflect.Value{returnCtx, reflect.ValueOf(job)}) x := res[0].Interface() if x == nil { return nil } return x.(error) } ... returnError = next() return } 真正代码 worker对象的processJob（job * Job） 方法 调用了runJob方法执行GenericHandler or DynamicHandler.Call func runJob(job *Job, ctxType reflect.Type, middleware []*middlewareHandler, jt *jobType) (returnCtx reflect.Value, returnError error) { 。。。。 next = func() error { 。。。。 if jt.IsGeneric { // 任务消费方法没有ctx时候执行 return jt.GenericHandler(job) } // 任务消费方法有ctx时执行 res := jt.DynamicHandler.Call([]reflect.Value{returnCtx, reflect.ValueOf(job)}) x := res[0].Interface() if x == nil { return nil } return x.(error) } ... returnError = next() return } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/任务队列/celery-任务队列.html":{"url":"blog/golang/任务队列/celery-任务队列.html","title":"Celery 任务队列","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/任务队列/gocraft-worker.html":{"url":"blog/golang/任务队列/gocraft-worker.html","title":"Gocraft Worker","keywords":"","body":"gocraft-wroker what gocraft/work lets you enqueue and processes background jobs in Go. Jobs are durable and backed by Redis. Very similar to Sidekiq for Go. feature Fast and efficient. Faster than this, this, and this. See below for benchmarks. Reliable - don't lose jobs even if your process crashes. Middleware on jobs -- good for metrics instrumentation, logging, etc. If a job fails, it will be retried a specified number of times. Schedule jobs to happen in the future. Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. Web UI to manage failed jobs and observe the system. Periodically enqueue jobs on a cron-like schedule. Pause / unpause jobs and control concurrency within and across processes use example Pass namespace and json argument to workPool Define Consumer function Start workPool 注册job package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } // Make an enqueuer with a particular namespace var enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool) func main() { // Enqueue a job named \"send_email\" with the specified parameters. _, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4}) if err != nil { log.Fatal(err) } } 消费job package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" \"os\" \"os/signal\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } type Context struct{ customerID int64 } func main() { // Make a new pool. Arguments: // Context{} is a struct that will be the context for the request. // 10 is the max concurrency // \"my_app_namespace\" is the Redis namespace // redisPool is a Redis pool pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) // Add middleware that will be executed for each job pool.Middleware((*Context).Log) pool.Middleware((*Context).FindCustomer) // Map the name of jobs to handler functions pool.Job(\"send_email\", (*Context).SendEmail) // Customize options: pool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export) // Start processing jobs pool.Start() // Wait for a signal to quit: signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, os.Interrupt, os.Kill) coding job retry job， dead job， schedule job client get scheduledJob get retryJob get deadJob get workpool heartbeats get delete zsetJob heartbeat workpool enque run watch available other job productor Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/任务队列/gocraftwork-2.html":{"url":"blog/golang/任务队列/gocraftwork-2.html","title":"Gocraftwork 2","keywords":"","body":"[toc] gocraft/work 工作队列源码简介 ubeadm init phase kubeconfig kubelet --node-name=cn-hangzhou.i-bp1bq96d1zohe28czs47 --kubeconfig-dir=/tmp/ --a piserver-advertise-address=172.16.112.134 --apiserver-bind-port=6443 简介 gocraft/work 是一款使用go开发的任务处理软件，通过redis 存储任务队列，可以使用工作池同时处理多个任务。本文主要介绍任务注册和任务消费的源代码。 功能特性 Fast and efficient. Faster than this, this, and this. See below for benchmarks. Reliable - don't lose jobs even if your process crashes. Middleware on jobs -- good for metrics instrumentation, logging, etc. If a job fails, it will be retried a specified number of times. Schedule jobs to happen in the future. Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. Web UI to manage failed jobs and observe the system. Periodically enqueue jobs on a cron-like schedule. Pause / unpause jobs and control concurrency within and across processes 注册Job 注册Job 流程 创建redis client pool 创建对象，定义 任务处理函数 创建 任务工作池，需要传入 被处理对象结构体， 最大并发数， 命名空间， redis client pool 创建Job， 需要传入 job 名称和 job 处理函数， job 在redis 中使用列表存储，key的组成：nameSapce:job:jobName， 同一namespace支持多种类型任务处理 这里使用任务名称作为key存入redis， 任务处理参数存放到列表中 func main() { // Make a new pool. Arguments: // Context{} is a struct that will be the context for the request. // 10 is the max concurrency // \"my_app_namespace\" is the Redis namespace // redisPool is a Redis pool pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) // Add middleware that will be executed for each job pool.Middleware((*Context).Log) // Map the name of jobs to handler functions // pool 中的 jobTypes是一个字典，key 是任务名称， value 是 任务处理函数 // 当有任务的时候，会将任务需要的参数 放入到redis key 为jobName的列表中 // 第二个参数必须是 工作池对象的方法 pool.Job(\"send_email\", (*Context).SendEmail) // Customize options: pool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export) // Start processing jobs pool.Start() ... } 发送Job 发送job 其实调用NewEnqueuer方法向redis 的列表中压入元素（具体的内容是任务参数） package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } // Make an enqueuer with a particular namespace var enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool) func main() { // Enqueue a job named \"send_email\" with the specified parameters. _, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4}) if err != nil { log.Fatal(err) } } Woker Fetch Job 在New WrokPool的时候会根据并法参数concurrency，创建同等个数的woker。 Worker 是一个job处理者，通过永久for循环，不间断的从redis 的任务队列中获取任务，在处理任务的时候，协程阻塞，等待一个任务处理完，再继续下一个。 下面的代码是worker 在for循环中的重要操作（1） fetch job （2） process job func (w *worker) loop() { for { select { 。。。 case fetchJob 本质是redis 的 pop，push 操作。首先将redis 列表中的任务 移除，然后再放入到处理队列中，这个操作必须是原子操作（原子性是指事务是一个不可再分割的工作单元，事务中的操作要么都发生，要么都不发生），作者使用了lua脚本完成。最后返回一个job 对象，里面有后面任务处理函数需要的args，即这里的rawJson func (w *worker) fetchJob() (*Job, error) { scriptArgs = append(scriptArgs, w.poolID) // ARGV[1] ... values, err := redis.Values(w.redisFetchScript.Do(conn, scriptArgs...)) ... job, err := newJob(rawJSON, dequeuedFrom, inProgQueue) .. return job, nil } Worker handle Job Pool.JobWithOptions(InstallMasterJob, work.JobOptions{Priority: 1, MaxFails: 1}, ConsumeJob) workpool 注册任务ConsumeJob后， 该任务ConsumeJob会被赋值给 worker.jobTypes[job.Name].GenericHandler, 他的反射类型被赋值给了jobType.DynamicHandler。如果该消费任务使用了上下文参数。 创建消费任务的2种方法 If you don't need context: func YourFunctionName(job *work.Job) error If you want your handler to accept a context: func (c Context) YourFunctionName(job work.Job) error // or, func YourFunctionName(c Context, job work.Job) error func (wp *WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) *WorkerPool { jobOpts = applyDefaultsAndValidate(jobOpts) vfn := reflect.ValueOf(fn) validateHandlerType(wp.contextType, vfn) jt := &jobType{ Name: name, //vfn 任务消费方法的反射类型， 如果消费方法中有ctx 参数，那么会调用反射执行 DynamicHandler: vfn, JobOptions: jobOpts, } if gh, ok := fn.(func(*Job) error); ok { // 用户的任务消费函数，被赋值给了jobType的GenericHandler， 如果消费方法只有一个job参数，则执行GenericHandler jt.IsGeneric = true jt.GenericHandler = gh } wp.jobTypes[name] = jt for _, w := range wp.workers { w.updateMiddlewareAndJobTypes(wp.middleware, wp.jobTypes) } return wp } 执行消费任务的真正代码 worker对象的processJob（job * Job） 方法 调用了runJob方法执行GenericHandler or DynamicHandler.Call func runJob(job *Job, ctxType reflect.Type, middleware []*middlewareHandler, jt *jobType) (returnCtx reflect.Value, returnError error) { 。。。。 next = func() error { 。。。。 if jt.IsGeneric { // 任务消费方法没有ctx时候执行 return jt.GenericHandler(job) } // 任务消费方法有ctx时执行 res := jt.DynamicHandler.Call([]reflect.Value{returnCtx, reflect.ValueOf(job)}) x := res[0].Interface() if x == nil { return nil } return x.(error) } ... returnError = next() return } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:20:37 "},"blog/golang/任务队列/gocraftwork.html":{"url":"blog/golang/任务队列/gocraftwork.html","title":"Gocraftwork","keywords":"","body":"gocraft/work gocraft/work lets you enqueue and processes background jobs in Go. Jobs are durable and backed by Redis. Very similar to Sidekiq for Go. Fast and efficient. Faster than this, this, and this. See below for benchmarks. Reliable - don't lose jobs even if your process crashes. Middleware on jobs -- good for metrics instrumentation, logging, etc. If a job fails, it will be retried a specified number of times. Schedule jobs to happen in the future. Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. Web UI to manage failed jobs and observe the system. Periodically enqueue jobs on a cron-like schedule. Pause / unpause jobs and control concurrency within and across processes Enqueue new jobs To enqueue jobs, you need to make an Enqueuer with a redis namespace and a redigo pool. Each enqueued job has a name and can take optional arguments. Arguments are k/v pairs (serialized as JSON internally). package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } // Make an enqueuer with a particular namespace var enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool) func main() { // Enqueue a job named \"send_email\" with the specified parameters. _, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4}) if err != nil { log.Fatal(err) } } Process jobs In order to process jobs, you'll need to make a WorkerPool. Add middleware and jobs to the pool, and start the pool. package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" \"os\" \"os/signal\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } type Context struct{ customerID int64 } func main() { // Make a new pool. Arguments: // Context{} is a struct that will be the context for the request. // 10 is the max concurrency // \"my_app_namespace\" is the Redis namespace // redisPool is a Redis pool pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) // Add middleware that will be executed for each job pool.Middleware((*Context).Log) pool.Middleware((*Context).FindCustomer) // Map the name of jobs to handler functions pool.Job(\"send_email\", (*Context).SendEmail) // Customize options: pool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export) // Start processing jobs pool.Start() // Wait for a signal to quit: signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, os.Interrupt, os.Kill) Special Features Contexts Just like in gocraft/web, gocraft/work lets you use your own contexts. Your context can be empty or it can have various fields in it. The fields can be whatever you want - it's your type! When a new job is processed by a worker, we'll allocate an instance of this struct and pass it to your middleware and handlers. This allows you to pass information from one middleware function to the next, and onto your handlers. Custom contexts aren't really needed for trivial example applications, but are very important for production apps. For instance, one field in your context can be your tagged logger. Your tagged logger augments your log statements with a job-id. This lets you filter your logs by that job-id. Check-ins Since this is a background job processing library, it's fairly common to have jobs that that take a long time to execute. Imagine you have a job that takes an hour to run. It can often be frustrating to know if it's hung, or about to finish, or if it has 30 more minutes to go. To solve this, you can instrument your jobs to \"checkin\" every so often with a string message. This checkin status will show up in the web UI. For instance, your job could look like this: func (c *Context) Export(job *work.Job) error { rowsToExport := getRows() for i, row := range rowsToExport { exportRow(row) if i % 1000 == 0 { job.Checkin(\"i=\" + fmt.Sprint(i)) // Here's the magic! This tells gocraft/work our status } } } Then in the web UI, you'll see the status of the worker: Name Arguments Started At Check-in At Check-in export {\"account_id\": 123} 2016/07/09 04:16:51 2016/07/09 05:03:13 i=335000 Scheduled Jobs You can schedule jobs to be executed in the future. To do so, make a new Enqueuer and call its EnqueueIn method: enqueuer := work.NewEnqueuer(\"my_app_namespace\", redisPool) secondsInTheFuture := 300 _, err := enqueuer.EnqueueIn(\"send_welcome_email\", secondsInTheFuture, work.Q{\"address\": \"test@example.com\"}) Unique Jobs You can enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. For instance, you might have a worker that expires the cache of an object. It doesn't make sense for multiple such jobs to exist at once. Also note that unique jobs are supported for normal enqueues as well as scheduled enqueues. enqueuer := work.NewEnqueuer(\"my_app_namespace\", redisPool) job, err := enqueuer.EnqueueUnique(\"clear_cache\", work.Q{\"object_id_\": \"123\"}) // job returned job, err = enqueuer.EnqueueUnique(\"clear_cache\", work.Q{\"object_id_\": \"123\"}) // job == nil -- this duplicate job isn't enqueued. job, err = enqueuer.EnqueueUniqueIn(\"clear_cache\", 300, work.Q{\"object_id_\": \"789\"}) // job != nil (diff id) Periodic Enqueueing (Cron) You can periodically enqueue jobs on your gocraft/work cluster using your worker pool. The scheduling specification uses a Cron syntax where the fields represent seconds, minutes, hours, day of the month, month, and week of the day, respectively. Even if you have multiple worker pools on different machines, they'll all coordinate and only enqueue your job once. pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) pool.PeriodicallyEnqueue(\"0 0 * * * *\", \"calculate_caches\") // This will enqueue a \"calculate_caches\" job every hour pool.Job(\"calculate_caches\", (*Context).CalculateCaches) // Still need to register a handler for this job separately Run the Web UI The web UI provides a view to view the state of your gocraft/work cluster, inspect queued jobs, and retry or delete dead jobs. Building an installing the binary: go get github.com/gocraft/work/cmd/workwebui go install github.com/gocraft/work/cmd/workwebui Then, you can run it: workwebui -redis=\"redis:6379\" -ns=\"work\" -listen=\":5040\" Navigate to http://localhost:5040/. You'll see a view that looks like this: Design and concepts Enqueueing jobs When jobs are enqueued, they're serialized with JSON and added to a simple Redis list with LPUSH. Jobs are added to a list with the same name as the job. Each job name gets its own queue. Whereas with other job systems you have to design which jobs go on which queues, there's no need for that here. Scheduling algorithm Each job lives in a list-based queue with the same name as the job. Each of these queues can have an associated priority. The priority is a number from 1 to 100000. Each time a worker pulls a job, it needs to choose a queue. It chooses a queue probabilistically based on its relative priority. If the sum of priorities among all queues is 1000, and one queue has priority 100, jobs will be pulled from that queue 10% of the time. Obviously if a queue is empty, it won't be considered. The semantics of \"always process X jobs before Y jobs\" can be accurately approximated by giving X a large number (like 10000) and Y a small number (like 1). Processing a job To process a job, a worker will execute a Lua script to atomically move a job its queue to an in-progress queue. A job is dequeued and moved to in-progress if the job queue is not paused and the number of active jobs does not exceed concurrency limit for the job type The worker will then run the job and increment the job lock. The job will either finish successfully or result in an error or panic. If the process completely crashes, the reaper will eventually find it in its in-progress queue and requeue it. If the job is successful, we'll simply remove the job from the in-progress queue. If the job returns an error or panic, we'll see how many retries a job has left. If it doesn't have any, we'll move it to the dead queue. If it has retries left, we'll consume a retry and add the job to the retry queue. Workers and WorkerPools WorkerPools provide the public API of gocraft/work. You can attach jobs and middleware to them. You can start and stop them. Based on their concurrency setting, they'll spin up N worker goroutines. Each worker is run in a goroutine. It will get a job from redis, run it, get the next job, etc. Each worker is independent. They are not dispatched work -- they get their own work. Retry job, scheduled jobs, and the requeuer In addition to the normal list-based queues that normal jobs live in, there are two other types of queues: the retry queue and the scheduled job queue. Both of these are implemented as Redis z-sets. The score is the unix timestamp when the job should be run. The value is the bytes of the job. The requeuer will occasionally look for jobs in these queues that should be run now. If they should be, they'll be atomically moved to the normal list-based queue and eventually processed. Dead jobs After a job has failed a specified number of times, it will be added to the dead job queue. The dead job queue is just a Redis z-set. The score is the timestamp it failed and the value is the job. To retry failed jobs, use the UI or the Client API. The reaper If a process crashes hard (eg, the power on the server turns off or the kernal freezes), some jobs may be in progress and we won't want to lose them. They're safe in their in-progress queue. The reaper will look for worker pools without a heartbeat. It will scan their in-progress queues and requeue anything it finds. Unique jobs You can enqueue unique jobs such that a given name/arguments are on the queue at once. Both normal queues and the scheduled queue are considered. When a unique job is enqueued, we'll atomically set a redis key that includes the job name and arguments and enqueue the job. When the job is processed, we'll delete that key to permit another job to be enqueued. Periodic jobs You can tell a worker pool to enqueue jobs periodically using a cron schedule. Each worker pool will wake up every 2 minutes, and if jobs haven't been scheduled yet, it will schedule all the jobs that would be executed in the next five minutes. Each periodic job that runs at a given time has a predictable byte pattern. Since jobs are scheduled on the scheduled job queue (a Redis z-set), if the same job is scheduled twice for a given time, it can only exist in the z-set once. Paused jobs You can pause jobs from being processed from a specific queue by setting a \"paused\" redis key (see redisKeyJobsPaused) Conversely, jobs in the queue will resume being processed once the paused redis key is removed Job concurrency You can control job concurrency using JobOptions{MaxConcurrency: }. Unlike the WorkerPool concurrency, this controls the limit on the number jobs of that type that can be active at one time by within a single redis instance This works by putting a precondition on enqueuing function, meaning a new job will not be scheduled if we are at or over a job's MaxConcurrency limit A redis key (see redisKeyJobsLock) is used as a counting semaphore in order to track job concurrency per job type The default value is 0, which means \"no limit on job concurrency\" Note: if you want to run jobs \"single threaded\" then you can set the MaxConcurrency accordingly: worker_pool.JobWithOptions(jobName, JobOptions{MaxConcurrency: 1}, (*Context).WorkFxn) Terminology reference \"worker pool\" - a pool of workers \"worker\" - an individual worker in a single goroutine. Gets a job from redis, does job, gets next job... \"heartbeater\" or \"worker pool heartbeater\" - goroutine owned by worker pool that runs concurrently with workers. Writes the worker pool's config/status (aka \"heartbeat\") every 5 seconds. \"heartbeat\" - the status written by the heartbeater. \"observer\" or \"worker observer\" - observes a worker. Writes stats. makes \"observations\". \"worker observation\" - A snapshot made by an observer of what a worker is working on. \"periodic enqueuer\" - A process that runs with a worker pool that periodically enqueues new jobs based on cron schedules. \"job\" - the actual bundle of data that constitutes one job \"job name\" - each job has a name, like \"create_watch\" \"job type\" - backend/private nomenclature for the handler+options for processing a job \"queue\" - each job creates a queue with the same name as the job. only jobs named X go into the X queue. \"retry jobs\" - if a job fails and needs to be retried, it will be put on this queue. \"scheduled jobs\" - jobs enqueued to be run in th future will be put on a scheduled job queue. \"dead jobs\" - if a job exceeds its MaxFails count, it will be put on the dead job queue. \"paused jobs\" - if paused key is present for a queue, then no jobs from that queue will be processed by any workers until that queue's paused key is removed \"job concurrency\" - the number of jobs being actively processed of a particular type across worker pool processes but within a single redis instance Benchmarks The benches folder contains various benchmark code. In each case, we enqueue 100k jobs across 5 queues. The jobs are almost no-op jobs: they simply increment an atomic counter. We then measure the rate of change of the counter to obtain our measurement. Library Speed gocraft/work 20944 jobs/s jrallison/go-workers 19945 jobs/s benmanns/goworker 10328.5 jobs/s albrow/jobs 40 jobs/s gocraft gocraft offers a toolkit for building web apps. Currently these packages are available: gocraft/web - Go Router + Middleware. Your Contexts. gocraft/dbr - Additions to Go's database/sql for super fast performance and convenience. gocraft/health - Instrument your web apps with logging and metrics. gocraft/work - Process background jobs in Go. These packages were developed by the engineering team at UserVoice and currently power much of its infrastructure and tech stack. Authors Jonathan Novak -- https://github.com/cypriss Tai-Lin Chu -- https://github.com/taylorchu Sponsored by UserVoice Collapse ▴ Documentation ¶ Index ¶ Variables type BackoffCalculator type Client func NewClient(namespace string, pool redis.Pool) Client func (c Client) DeadJobs(page uint) ([]DeadJob, int64, error) func (c *Client) DeleteAllDeadJobs() error func (c *Client) DeleteDeadJob(diedAt int64, jobID string) error func (c *Client) DeleteRetryJob(retryAt int64, jobID string) error func (c *Client) DeleteScheduledJob(scheduledFor int64, jobID string) error func (c Client) Queues() ([]Queue, error) func (c *Client) RetryAllDeadJobs() error func (c *Client) RetryDeadJob(diedAt int64, jobID string) error func (c Client) RetryJobs(page uint) ([]RetryJob, int64, error) func (c Client) ScheduledJobs(page uint) ([]ScheduledJob, int64, error) func (c Client) WorkerObservations() ([]WorkerObservation, error) func (c Client) WorkerPoolHeartbeats() ([]WorkerPoolHeartbeat, error) type DeadJob type Enqueuer func NewEnqueuer(namespace string, pool redis.Pool) Enqueuer func (e Enqueuer) Enqueue(jobName string, args map[string]interface{}) (Job, error) func (e Enqueuer) EnqueueIn(jobName string, secondsFromNow int64, args map[string]interface{}) (ScheduledJob, error) func (e Enqueuer) EnqueueUnique(jobName string, args map[string]interface{}) (Job, error) func (e Enqueuer) EnqueueUniqueIn(jobName string, secondsFromNow int64, args map[string]interface{}) (ScheduledJob, error) type GenericHandler type GenericMiddlewareHandler type Job func (j *Job) ArgBool(key string) bool func (j *Job) ArgError() error func (j *Job) ArgFloat64(key string) float64 func (j *Job) ArgInt64(key string) int64 func (j *Job) ArgString(key string) string func (j *Job) Checkin(msg string) type JobOptions type NextMiddlewareFunc type Q type Queue type RetryJob type ScheduledJob type WorkerObservation type WorkerPool func NewWorkerPool(ctx interface{}, concurrency uint, namespace string, pool redis.Pool) WorkerPool func (wp *WorkerPool) Drain() func (wp WorkerPool) Job(name string, fn interface{}) WorkerPool func (wp WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) WorkerPool func (wp WorkerPool) Middleware(fn interface{}) WorkerPool func (wp WorkerPool) PeriodicallyEnqueue(spec string, jobName string) WorkerPool func (wp *WorkerPool) Start() func (wp *WorkerPool) Stop() type WorkerPoolHeartbeat Constants ¶ This section is empty. Variables ¶ View Source var ErrNotDeleted = fmt.Errorf(\"nothing deleted\") ErrNotDeleted is returned by functions that delete jobs to indicate that although the redis commands were successful, no object was actually deleted by those commmands. View Source var ErrNotRetried = fmt.Errorf(\"nothing retried\") ErrNotRetried is returned by functions that retry jobs to indicate that although the redis commands were successful, no object was actually retried by those commmands. Functions ¶ This section is empty. Types ¶ type BackoffCalculator ¶ type BackoffCalculator func(job *Job) int64 You may provide your own backoff function for retrying failed jobs or use the builtin one. Returns the number of seconds to wait until the next attempt. The builtin backoff calculator provides an exponentially increasing wait function. type Client ¶ type Client struct { // contains filtered or unexported fields } Client implements all of the functionality of the web UI. It can be used to inspect the status of a running cluster and retry dead jobs. func NewClient ¶ func NewClient(namespace string, pool *redis.Pool) *Client NewClient creates a new Client with the specified redis namespace and connection pool. func (*Client) DeadJobs ¶ func (c *Client) DeadJobs(page uint) ([]*DeadJob, int64, error) DeadJobs returns a list of DeadJob's. The page param is 1-based; each page is 20 items. The total number of items (not pages) in the list of dead jobs is also returned. func (*Client) DeleteAllDeadJobs ¶ func (c *Client) DeleteAllDeadJobs() error DeleteAllDeadJobs deletes all dead jobs. func (*Client) DeleteDeadJob ¶ func (c *Client) DeleteDeadJob(diedAt int64, jobID string) error DeleteDeadJob deletes a dead job from Redis. func (*Client) DeleteRetryJob ¶ func (c *Client) DeleteRetryJob(retryAt int64, jobID string) error DeleteRetryJob deletes a job in the retry queue. func (*Client) DeleteScheduledJob ¶ func (c *Client) DeleteScheduledJob(scheduledFor int64, jobID string) error DeleteScheduledJob deletes a job in the scheduled queue. func (*Client) Queues ¶ func (c *Client) Queues() ([]*Queue, error) Queues returns the Queue's it finds. func (*Client) RetryAllDeadJobs ¶ func (c *Client) RetryAllDeadJobs() error RetryAllDeadJobs requeues all dead jobs. In other words, it puts them all back on the normal work queue for workers to pull from and process. func (*Client) RetryDeadJob ¶ func (c *Client) RetryDeadJob(diedAt int64, jobID string) error RetryDeadJob retries a dead job. The job will be re-queued on the normal work queue for eventual processing by a worker. func (*Client) RetryJobs ¶ func (c *Client) RetryJobs(page uint) ([]*RetryJob, int64, error) RetryJobs returns a list of RetryJob's. The page param is 1-based; each page is 20 items. The total number of items (not pages) in the list of retry jobs is also returned. func (*Client) ScheduledJobs ¶ func (c *Client) ScheduledJobs(page uint) ([]*ScheduledJob, int64, error) ScheduledJobs returns a list of ScheduledJob's. The page param is 1-based; each page is 20 items. The total number of items (not pages) in the list of scheduled jobs is also returned. func (*Client) WorkerObservations ¶ func (c *Client) WorkerObservations() ([]*WorkerObservation, error) WorkerObservations returns all of the WorkerObservation's it finds for all worker pools' workers. func (*Client) WorkerPoolHeartbeats ¶ func (c *Client) WorkerPoolHeartbeats() ([]*WorkerPoolHeartbeat, error) WorkerPoolHeartbeats queries Redis and returns all WorkerPoolHeartbeat's it finds (even for those worker pools which don't have a current heartbeat). type DeadJob ¶ type DeadJob struct { DiedAt int64 `json:\"died_at\"` *Job } DeadJob represents a job in the dead queue. type Enqueuer ¶ type Enqueuer struct { Namespace string // eg, \"myapp-work\" Pool *redis.Pool // contains filtered or unexported fields } Enqueuer can enqueue jobs. func NewEnqueuer ¶ func NewEnqueuer(namespace string, pool *redis.Pool) *Enqueuer NewEnqueuer creates a new enqueuer with the specified Redis namespace and Redis pool. func (*Enqueuer) Enqueue ¶ func (e *Enqueuer) Enqueue(jobName string, args map[string]interface{}) (*Job, error) Enqueue will enqueue the specified job name and arguments. The args param can be nil if no args ar needed. Example: e.Enqueue(\"send_email\", work.Q{\"addr\": \"test@example.com\"}) func (*Enqueuer) EnqueueIn ¶ func (e *Enqueuer) EnqueueIn(jobName string, secondsFromNow int64, args map[string]interface{}) (*ScheduledJob, error) EnqueueIn enqueues a job in the scheduled job queue for execution in secondsFromNow seconds. func (*Enqueuer) EnqueueUnique ¶ func (e *Enqueuer) EnqueueUnique(jobName string, args map[string]interface{}) (*Job, error) EnqueueUnique enqueues a job unless a job is already enqueued with the same name and arguments. The already-enqueued job can be in the normal work queue or in the scheduled job queue. Once a worker begins processing a job, another job with the same name and arguments can be enqueued again. Any failed jobs in the retry queue or dead queue don't count against the uniqueness -- so if a job fails and is retried, two unique jobs with the same name and arguments can be enqueued at once. In order to add robustness to the system, jobs are only unique for 24 hours after they're enqueued. This is mostly relevant for scheduled jobs. EnqueueUnique returns the job if it was enqueued and nil if it wasn't func (*Enqueuer) EnqueueUniqueIn ¶ func (e *Enqueuer) EnqueueUniqueIn(jobName string, secondsFromNow int64, args map[string]interface{}) (*ScheduledJob, error) EnqueueUniqueIn enqueues a unique job in the scheduled job queue for execution in secondsFromNow seconds. See EnqueueUnique for the semantics of unique jobs. type GenericHandler ¶ type GenericHandler func(*Job) error GenericHandler is a job handler without any custom context. type GenericMiddlewareHandler ¶ type GenericMiddlewareHandler func(*Job, NextMiddlewareFunc) error GenericMiddlewareHandler is a middleware without any custom context. type Job ¶ type Job struct { // Inputs when making a new job Name string `json:\"name,omitempty\"` ID string `json:\"id\"` EnqueuedAt int64 `json:\"t\"` Args map[string]interface{} `json:\"args\"` Unique bool `json:\"unique,omitempty\"` // Inputs when retrying Fails int64 `json:\"fails,omitempty\"` // number of times this job has failed LastErr string `json:\"err,omitempty\"` FailedAt int64 `json:\"failed_at,omitempty\"` // contains filtered or unexported fields } Job represents a job. func (*Job) ArgBool ¶ func (j *Job) ArgBool(key string) bool ArgBool returns j.Args[key] typed to a bool. If the key is missing or of the wrong type, it sets an argument error on the job. This function is meant to be used in the body of a job handling function while extracting arguments, followed by a single call to j.ArgError(). func (*Job) ArgError ¶ func (j *Job) ArgError() error ArgError returns the last error generated when extracting typed params. Returns nil if extracting the args went fine. func (*Job) ArgFloat64 ¶ func (j *Job) ArgFloat64(key string) float64 ArgFloat64 returns j.Args[key] typed to a float64. If the key is missing or of the wrong type, it sets an argument error on the job. This function is meant to be used in the body of a job handling function while extracting arguments, followed by a single call to j.ArgError(). func (*Job) ArgInt64 ¶ func (j *Job) ArgInt64(key string) int64 ArgInt64 returns j.Args[key] typed to an int64. If the key is missing or of the wrong type, it sets an argument error on the job. This function is meant to be used in the body of a job handling function while extracting arguments, followed by a single call to j.ArgError(). func (*Job) ArgString ¶ func (j *Job) ArgString(key string) string ArgString returns j.Args[key] typed to a string. If the key is missing or of the wrong type, it sets an argument error on the job. This function is meant to be used in the body of a job handling function while extracting arguments, followed by a single call to j.ArgError(). func (*Job) Checkin ¶ func (j *Job) Checkin(msg string) Checkin will update the status of the executing job to the specified messages. This message is visible within the web UI. This is useful for indicating some sort of progress on very long running jobs. For instance, on a job that has to process a million records over the course of an hour, the job could call Checkin with the current job number every 10k jobs. type JobOptions ¶ type JobOptions struct { Priority uint // Priority from 1 to 10000 MaxFails uint // 1: send straight to dead (unless SkipDead) SkipDead bool // If true, don't send failed jobs to the dead queue when retries are exhausted. MaxConcurrency uint // Max number of jobs to keep in flight (default is 0, meaning no max) Backoff BackoffCalculator // If not set, uses the default backoff algorithm } JobOptions can be passed to JobWithOptions. type NextMiddlewareFunc ¶ type NextMiddlewareFunc func() error NextMiddlewareFunc is a function type (whose instances are named 'next') that you call to advance to the next middleware. type Q ¶ type Q map[string]interface{} Q is a shortcut to easily specify arguments for jobs when enqueueing them. Example: e.Enqueue(\"send_email\", work.Q{\"addr\": \"test@example.com\", \"track\": true}) type Queue ¶ type Queue struct { JobName string `json:\"job_name\"` Count int64 `json:\"count\"` Latency int64 `json:\"latency\"` } Queue represents a queue that holds jobs with the same name. It indicates their name, count, and latency (in seconds). Latency is a measurement of how long ago the next job to be processed was enqueued. type RetryJob ¶ type RetryJob struct { RetryAt int64 `json:\"retry_at\"` *Job } RetryJob represents a job in the retry queue. type ScheduledJob ¶ type ScheduledJob struct { RunAt int64 `json:\"run_at\"` *Job } ScheduledJob represents a job in the scheduled queue. type WorkerObservation ¶ type WorkerObservation struct { WorkerID string `json:\"worker_id\"` IsBusy bool `json:\"is_busy\"` // If IsBusy: JobName string `json:\"job_name\"` JobID string `json:\"job_id\"` StartedAt int64 `json:\"started_at\"` ArgsJSON string `json:\"args_json\"` Checkin string `json:\"checkin\"` CheckinAt int64 `json:\"checkin_at\"` } WorkerObservation represents the latest observation taken from a worker. The observation indicates whether the worker is busy processing a job, and if so, information about that job. type WorkerPool ¶ type WorkerPool struct { // contains filtered or unexported fields } WorkerPool represents a pool of workers. It forms the primary API of gocraft/work. WorkerPools provide the public API of gocraft/work. You can attach jobs and middlware to them. You can start and stop them. Based on their concurrency setting, they'll spin up N worker goroutines. func NewWorkerPool ¶ func NewWorkerPool(ctx interface{}, concurrency uint, namespace string, pool *redis.Pool) *WorkerPool NewWorkerPool creates a new worker pool. ctx should be a struct literal whose type will be used for middleware and handlers. concurrency specifies how many workers to spin up - each worker can process jobs concurrently. func (*WorkerPool) Drain ¶ func (wp *WorkerPool) Drain() Drain drains all jobs in the queue before returning. Note that if jobs are added faster than we can process them, this function wouldn't return. func (*WorkerPool) Job ¶ func (wp *WorkerPool) Job(name string, fn interface{}) *WorkerPool Job registers the job name to the specified handler fn. For instance, when workers pull jobs from the name queue they'll be processed by the specified handler function. fn can take one of these forms: (ContextType).func(Job) error, (ContextType matches the type of ctx specified when creating a pool) func(*Job) error, for the generic handler format. func (*WorkerPool) JobWithOptions ¶ func (wp *WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) *WorkerPool JobWithOptions adds a handler for 'name' jobs as per the Job function, but permits you specify additional options such as a job's priority, retry count, and whether to send dead jobs to the dead job queue or trash them. func (*WorkerPool) Middleware ¶ func (wp *WorkerPool) Middleware(fn interface{}) *WorkerPool Middleware appends the specified function to the middleware chain. The fn can take one of these forms: (ContextType).func(Job, NextMiddlewareFunc) error, (ContextType matches the type of ctx specified when creating a pool) func(*Job, NextMiddlewareFunc) error, for the generic middleware format. func (*WorkerPool) PeriodicallyEnqueue ¶ func (wp *WorkerPool) PeriodicallyEnqueue(spec string, jobName string) *WorkerPool PeriodicallyEnqueue will periodically enqueue jobName according to the cron-based spec. The spec format is based on https://godoc.org/github.com/robfig/cron, which is a relatively standard cron format. Note that the first value is the seconds! If you have multiple worker pools on different machines, they'll all coordinate and only enqueue your job once. func (*WorkerPool) Start ¶ func (wp *WorkerPool) Start() Start starts the workers and associated processes. func (*WorkerPool) Stop ¶ func (wp *WorkerPool) Stop() Stop stops the workers and associated processes. type WorkerPoolHeartbeat ¶ type WorkerPoolHeartbeat struct { WorkerPoolID string `json:\"worker_pool_id\"` StartedAt int64 `json:\"started_at\"` HeartbeatAt int64 `json:\"heartbeat_at\"` JobNames []string `json:\"job_names\"` Concurrency uint `json:\"concurrency\"` Host string `json:\"host\"` Pid int `json:\"pid\"` WorkerIDs []string `json:\"worker_ids\"` } WorkerPoolHeartbeat represents the heartbeat from a worker pool. WorkerPool's write a heartbeat every 5 seconds so we know they're alive and includes config information. Source Files View all client.go dead_pool_reaper.go enqueue.go heartbeater.go identifier.go job.go log.go observer.go periodic_enqueuer.go priority_sampler.go redis.go requeuer.go run.go time.go worker.go worker_pool.go Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/任务队列/kubernetes-任务队列.html":{"url":"blog/golang/任务队列/kubernetes-任务队列.html","title":"Kubernetes 任务队列","keywords":"","body":"https://zhuanlan.zhihu.com/p/273471213 https://time.geekbang.org/column/article/f071a3bd9fea412629deaa2d4b00b078/share?code=SD%2Fh271kOXJQ6cqIthaz8kseca2KlgQydhWLf6aGVQk%3D&oss_token= 美图 https://zhuanlan.zhihu.com/p/94082947 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/任务队列/readme.html":{"url":"blog/golang/任务队列/readme.html","title":"Readme","keywords":"","body":"gocraft/work 工作队列源码简介简介功能特性注册Job注册Job 流程发送JobWoker Fetch JobWorker handle Job创建消费任务的2种方法执行消费任务的真正 gocraft/work 工作队列源码简介 ubeadm init phase kubeconfig kubelet --node-name=cn-hangzhou.i-bp1bq96d1zohe28czs47 --kubeconfig-dir=/tmp/ --a piserver-advertise-address=172.16.112.134 --apiserver-bind-port=6443 简介 gocraft/work 是一款使用go开发的任务处理软件，通过redis 存储任务队列，可以使用工作池同时处理多个任务。本文主要介绍任务注册和任务消费的源代码。 功能特性 Fast and efficient. Faster than this, this, and this. See below for benchmarks. Reliable - don't lose jobs even if your process crashes. Middleware on jobs -- good for metrics instrumentation, logging, etc. If a job fails, it will be retried a specified number of times. Schedule jobs to happen in the future. Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. Web UI to manage failed jobs and observe the system. Periodically enqueue jobs on a cron-like schedule. Pause / unpause jobs and control concurrency within and across processes 注册Job 注册Job 流程 创建redis client pool 创建对象，定义 任务处理函数 创建 任务工作池，需要传入 被处理对象结构体， 最大并发数， 命名空间， redis client pool 创建Job， 需要传入 job 名称和 job 处理函数， job 在redis 中使用列表存储，key的组成：nameSapce:job:jobName， 同一namespace支持多种类型任务处理 这里使用任务名称作为key存入redis， 任务处理参数存放到列表中 func main() { // Make a new pool. Arguments: // Context{} is a struct that will be the context for the request. // 10 is the max concurrency // \"my_app_namespace\" is the Redis namespace // redisPool is a Redis pool pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) // Add middleware that will be executed for each job pool.Middleware((*Context).Log) // Map the name of jobs to handler functions // pool 中的 jobTypes是一个字典，key 是任务名称， value 是 任务处理函数 // 当有任务的时候，会将任务需要的参数 放入到redis key 为jobName的列表中 // 第二个参数必须是 工作池对象的方法 pool.Job(\"send_email\", (*Context).SendEmail) // Customize options: pool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export) // Start processing jobs pool.Start() ... } 发送Job 发送job 其实调用NewEnqueuer方法向redis 的列表中压入元素（具体的内容是任务参数） package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } // Make an enqueuer with a particular namespace var enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool) func main() { // Enqueue a job named \"send_email\" with the specified parameters. _, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4}) if err != nil { log.Fatal(err) } } Woker Fetch Job 在New WrokPool的时候会根据并法参数concurrency，创建同等个数的woker。 Worker 是一个job处理者，通过永久for循环，不间断的从redis 的任务队列中获取任务，在处理任务的时候，协程阻塞，等待一个任务处理完，再继续下一个。 下面的代码是worker 在for循环中的重要操作（1） fetch job （2） process job func (w *worker) loop() { for { select { 。。。 case fetchJob 本质是redis 的 pop，push 操作。首先将redis 列表中的任务 移除，然后再放入到处理队列中，这个操作必须是原子操作（原子性是指事务是一个不可再分割的工作单元，事务中的操作要么都发生，要么都不发生），作者使用了lua脚本完成。最后返回一个job 对象，里面有后面任务处理函数需要的args，即这里的rawJson func (w *worker) fetchJob() (*Job, error) { scriptArgs = append(scriptArgs, w.poolID) // ARGV[1] ... values, err := redis.Values(w.redisFetchScript.Do(conn, scriptArgs...)) ... job, err := newJob(rawJSON, dequeuedFrom, inProgQueue) .. return job, nil } Worker handle Job Pool.JobWithOptions(InstallMasterJob, work.JobOptions{Priority: 1, MaxFails: 1}, ConsumeJob) workpool 注册任务ConsumeJob后， 该任务ConsumeJob会被赋值给 worker.jobTypes[job.Name].GenericHandler, 他的反射类型被赋值给了jobType.DynamicHandler。如果该消费任务使用了上下文参数。 创建消费任务的2种方法 If you don't need context: func YourFunctionName(job *work.Job) error If you want your handler to accept a context: func (c Context) YourFunctionName(job work.Job) error // or, func YourFunctionName(c Context, job work.Job) error func (wp *WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) *WorkerPool { jobOpts = applyDefaultsAndValidate(jobOpts) vfn := reflect.ValueOf(fn) validateHandlerType(wp.contextType, vfn) jt := &jobType{ Name: name, //vfn 任务消费方法的反射类型， 如果消费方法中有ctx 参数，那么会调用反射执行 DynamicHandler: vfn, JobOptions: jobOpts, } if gh, ok := fn.(func(*Job) error); ok { // 用户的任务消费函数，被赋值给了jobType的GenericHandler， 如果消费方法只有一个job参数，则执行GenericHandler jt.IsGeneric = true jt.GenericHandler = gh } wp.jobTypes[name] = jt for _, w := range wp.workers { w.updateMiddlewareAndJobTypes(wp.middleware, wp.jobTypes) } return wp } 执行消费任务的gocraft/work 工作队列源码简介简介功能特性注册Job注册Job 流程发送JobWoker Fetch JobWorker handle Job创建消费任务的2种方法执行消费任务的真正 gocraft/work 工作队列源码简介 ubeadm init phase kubeconfig kubelet --node-name=cn-hangzhou.i-bp1bq96d1zohe28czs47 --kubeconfig-dir=/tmp/ --a piserver-advertise-address=172.16.112.134 --apiserver-bind-port=6443 简介 gocraft/work 是一款使用go开发的任务处理软件，通过redis 存储任务队列，可以使用工作池同时处理多个任务。本文主要介绍任务注册和任务消费的源代码。 功能特性 Fast and efficient. Faster than this, this, and this. See below for benchmarks. Reliable - don't lose jobs even if your process crashes. Middleware on jobs -- good for metrics instrumentation, logging, etc. If a job fails, it will be retried a specified number of times. Schedule jobs to happen in the future. Enqueue unique jobs so that only one job with a given name/arguments exists in the queue at once. Web UI to manage failed jobs and observe the system. Periodically enqueue jobs on a cron-like schedule. Pause / unpause jobs and control concurrency within and across processes 注册Job 注册Job 流程 创建redis client pool 创建对象，定义 任务处理函数 创建 任务工作池，需要传入 被处理对象结构体， 最大并发数， 命名空间， redis client pool 创建Job， 需要传入 job 名称和 job 处理函数， job 在redis 中使用列表存储，key的组成：nameSapce:job:jobName， 同一namespace支持多种类型任务处理 这里使用任务名称作为key存入redis， 任务处理参数存放到列表中 func main() { // Make a new pool. Arguments: // Context{} is a struct that will be the context for the request. // 10 is the max concurrency // \"my_app_namespace\" is the Redis namespace // redisPool is a Redis pool pool := work.NewWorkerPool(Context{}, 10, \"my_app_namespace\", redisPool) // Add middleware that will be executed for each job pool.Middleware((*Context).Log) // Map the name of jobs to handler functions // pool 中的 jobTypes是一个字典，key 是任务名称， value 是 任务处理函数 // 当有任务的时候，会将任务需要的参数 放入到redis key 为jobName的列表中 // 第二个参数必须是 工作池对象的方法 pool.Job(\"send_email\", (*Context).SendEmail) // Customize options: pool.JobWithOptions(\"export\", work.JobOptions{Priority: 10, MaxFails: 1}, (*Context).Export) // Start processing jobs pool.Start() ... } 发送Job 发送job 其实调用NewEnqueuer方法向redis 的列表中压入元素（具体的内容是任务参数） package main import ( \"github.com/gomodule/redigo/redis\" \"github.com/gocraft/work\" ) // Make a redis pool var redisPool = &redis.Pool{ MaxActive: 5, MaxIdle: 5, Wait: true, Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", \":6379\") }, } // Make an enqueuer with a particular namespace var enqueuer = work.NewEnqueuer(\"my_app_namespace\", redisPool) func main() { // Enqueue a job named \"send_email\" with the specified parameters. _, err := enqueuer.Enqueue(\"send_email\", work.Q{\"address\": \"test@example.com\", \"subject\": \"hello world\", \"customer_id\": 4}) if err != nil { log.Fatal(err) } } Woker Fetch Job 在New WrokPool的时候会根据并法参数concurrency，创建同等个数的woker。 Worker 是一个job处理者，通过永久for循环，不间断的从redis 的任务队列中获取任务，在处理任务的时候，协程阻塞，等待一个任务处理完，再继续下一个。 下面的代码是worker 在for循环中的重要操作（1） fetch job （2） process job func (w *worker) loop() { for { select { 。。。 case fetchJob 本质是redis 的 pop，push 操作。首先将redis 列表中的任务 移除，然后再放入到处理队列中，这个操作必须是原子操作（原子性是指事务是一个不可再分割的工作单元，事务中的操作要么都发生，要么都不发生），作者使用了lua脚本完成。最后返回一个job 对象，里面有后面任务处理函数需要的args，即这里的rawJson func (w *worker) fetchJob() (*Job, error) { scriptArgs = append(scriptArgs, w.poolID) // ARGV[1] ... values, err := redis.Values(w.redisFetchScript.Do(conn, scriptArgs...)) ... job, err := newJob(rawJSON, dequeuedFrom, inProgQueue) .. return job, nil } Worker handle Job Pool.JobWithOptions(InstallMasterJob, work.JobOptions{Priority: 1, MaxFails: 1}, ConsumeJob) workpool 注册任务ConsumeJob后， 该任务ConsumeJob会被赋值给 worker.jobTypes[job.Name].GenericHandler, 他的反射类型被赋值给了jobType.DynamicHandler。如果该消费任务使用了上下文参数。 创建消费任务的2种方法 If you don't need context: func YourFunctionName(job *work.Job) error If you want your handler to accept a context: func (c Context) YourFunctionName(job work.Job) error // or, func YourFunctionName(c Context, job work.Job) error func (wp *WorkerPool) JobWithOptions(name string, jobOpts JobOptions, fn interface{}) *WorkerPool { jobOpts = applyDefaultsAndValidate(jobOpts) vfn := reflect.ValueOf(fn) validateHandlerType(wp.contextType, vfn) jt := &jobType{ Name: name, //vfn 任务消费方法的反射类型， 如果消费方法中有ctx 参数，那么会调用反射执行 DynamicHandler: vfn, JobOptions: jobOpts, } if gh, ok := fn.(func(*Job) error); ok { // 用户的任务消费函数，被赋值给了jobType的GenericHandler， 如果消费方法只有一个job参数，则执行GenericHandler jt.IsGeneric = true jt.GenericHandler = gh } wp.jobTypes[name] = jt for _, w := range wp.workers { w.updateMiddlewareAndJobTypes(wp.middleware, wp.jobTypes) } return wp } 执行消费任务的真正代码 worker对象的processJob（job * Job） 方法 调用了runJob方法执行GenericHandler or DynamicHandler.Call func runJob(job *Job, ctxType reflect.Type, middleware []*middlewareHandler, jt *jobType) (returnCtx reflect.Value, returnError error) { 。。。。 next = func() error { 。。。。 if jt.IsGeneric { // 任务消费方法没有ctx时候执行 return jt.GenericHandler(job) } // 任务消费方法有ctx时执行 res := jt.DynamicHandler.Call([]reflect.Value{returnCtx, reflect.ValueOf(job)}) x := res[0].Interface() if x == nil { return nil } return x.(error) } ... returnError = next() return } 真正代码 worker对象的processJob（job * Job） 方法 调用了runJob方法执行GenericHandler or DynamicHandler.Call func runJob(job *Job, ctxType reflect.Type, middleware []*middlewareHandler, jt *jobType) (returnCtx reflect.Value, returnError error) { 。。。。 next = func() error { 。。。。 if jt.IsGeneric { // 任务消费方法没有ctx时候执行 return jt.GenericHandler(job) } // 任务消费方法有ctx时执行 res := jt.DynamicHandler.Call([]reflect.Value{returnCtx, reflect.ValueOf(job)}) x := res[0].Interface() if x == nil { return nil } return x.(error) } ... returnError = next() return } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/参数校验.html":{"url":"blog/golang/参数校验.html","title":"参数校验","keywords":"","body":"golang validator 在web开发中，基本上每个入参都需要做校验，如果分别对每个参数做校验，那么代码将会十分冗余。那么有什么好的解决方案呢。 Package validator implements value validations for structs and individual fields based on tags It has the following unique features: Cross Field and Cross Struct validations by using validation tags or custom validators. Slice, Array and Map diving, which allows any or all levels of a multidimensional field to be validated. Ability to dive into both map keys and values for validation Handles type interface by determining it's underlying type prior to validation. Handles custom field types such as sql driver Valuer see Valuer Alias validation tags, which allows for mapping of several validations to a single tag for easier defining of validations on structs Extraction of custom defined Field Name e.g. can specify to extract the JSON name while validating and have it available in the resulting FieldError Customizable i18n aware error messages. Default validator for the gin web framework; upgrading from v8 to v9 in gin see here 使用示范 字段验证 跨字段以及跨Struct验证 枚举 name string json:\"pluginGroups\" validate:\"oneof=1,2,3\" 最小值 type User struct { Name string `validate:\"contains=tom\"` Age int `validate:\"min=1\"` } link 利器 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/日志收集-datalog/config.html":{"url":"blog/golang/日志收集-datalog/config.html","title":"Config","keywords":"","body":"config intergration_config.go // LogsConfig represents a log source config, which can be for instance // a file to tail or a port to listen to. type LogsConfig struct { Type string Port int // Network Path string // File, Journald Encoding string `mapstructure:\"encoding\" json:\"encoding\"` // File ExcludePaths []string `mapstructure:\"exclude_paths\" json:\"exclude_paths\"` // File TailingMode string `mapstructure:\"start_position\" json:\"start_position\"` // File IncludeUnits []string `mapstructure:\"include_units\" json:\"include_units\"` // Journald ExcludeUnits []string `mapstructure:\"exclude_units\" json:\"exclude_units\"` // Journald ContainerMode bool `mapstructure:\"container_mode\" json:\"container_mode\"` // Journald Image string // Docker Label string // Docker // Name contains the container name Name string // Docker // Identifier contains the container ID Identifier string // Docker ChannelPath string `mapstructure:\"channel_path\" json:\"channel_path\"` // Windows Event Query string // Windows Event // used as input only by the Channel tailer. // could have been unidirectional but the tailer could not close it in this case. // TODO(remy): strongly typed to an AWS Lambda LogMessage, we should probably use // a more generic type here. //Channel chan aws.LogMessage Service string Source string SourceCategory string Tags []string ProcessingRules []*ProcessingRule `mapstructure:\"log_processing_rules\" json:\"log_processing_rules\"` } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/日志收集-datalog/log-agent-数据结构.html":{"url":"blog/golang/日志收集-datalog/log-agent-数据结构.html","title":"Log Agent 数据结构","keywords":"","body":"数据结构 LogSource 存放 LogSource 信道，和LogSource数组 LogsConfig LogStatus Messages SourceType LatencyStats: StatusTracker: LatencyStats跟踪来自此源的消息在处理管道中花费的时间的内部统计信息，即: //消息被尾/侦听器/解码器解码到消息被发送者处理之间的时间间隔 Scheduler： 在不同输入类型的日志收集上创建一个新的源和服务用户启动或停止 一个“源”代表了一个文件配置、docker 标签或pod注解的 日志配置。 一个服务代表了一个进程，例如一个运行在主机上的容器 // Schedule creates new sources and services from a list of integration configs. // An integration config can be mapped to a list of sources when it contains a Provider, // while an integration config can be mapped to a service when it contains an Entity. // An entity represents a unique identifier for a process that be reused to query logs. func (s *Scheduler) Schedule(configs []integration.Config) {} schedule 从一系列的集成配置中创建新的源和服务。 当他包含一个供应商的时候一个集成配置能个映射到一系列的源。 当包含一个实体的时候，集成配置可以映射到一个服务 一个实体代表一个进程的唯一识别号，其能够被用来重复查询日志 // LogsConfig represents a log source config, which can be for instance // a file to tail or a port to listen to. type LogsConfig struct { Type string Port int // Network Path string // File, Journald Encoding string `mapstructure:\"encoding\" json:\"encoding\"` // File ExcludePaths []string `mapstructure:\"exclude_paths\" json:\"exclude_paths\"` // File TailingMode string `mapstructure:\"start_position\" json:\"start_position\"` // File IncludeUnits []string `mapstructure:\"include_units\" json:\"include_units\"` // Journald ExcludeUnits []string `mapstructure:\"exclude_units\" json:\"exclude_units\"` // Journald ContainerMode bool `mapstructure:\"container_mode\" json:\"container_mode\"` // Journald Image string // Docker Label string // Docker // Name contains the container name Name string // Docker // Identifier contains the container ID Identifier string // Docker ChannelPath string `mapstructure:\"channel_path\" json:\"channel_path\"` // Windows Event Query string // Windows Event // used as input only by the Channel tailer. // could have been unidirectional but the tailer could not close it in this case. // TODO(remy): strongly typed to an AWS Lambda LogMessage, we should probably use // a more generic type here. //Channel chan aws.LogMessage Service string Source string SourceCategory string Tags []string ProcessingRules []*ProcessingRule `mapstructure:\"log_processing_rules\" json:\"log_processing_rules\"` } // Messages holds messages and warning that can be displayed in the status // Warnings are display at the top of the log section in the status and // messages are displayed in the log source that generated the message type Messages struct { messages map[string]string lock *sync.Mutex } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/日志收集-datalog/大纲.html":{"url":"blog/golang/日志收集-datalog/大纲.html","title":"大纲","keywords":"","body":"思路 [toc] 代码规划 autodiscovery 该包负责从不同的源收集检查配置， # package `autodiscovery` This package is a core piece of the agent. It is responsible for collecting check configurations from different sources (see package [config providers](https://logs-agent/tree/master/pkg/autodiscovery/providers)) and then schedule or unschedule integration configurations with the help of the schedulers. It is also responsible for listening to container-related events and resolve template configurations that would match them. ## `AutoConfig` As a central component, `AutoConfig` owns and orchestrates several key modules: - it owns a reference to a [`MetaScheduler`](https://logs-agent/blob/master/pkg/autodiscovery/scheduler) that dispatches integrations configs for scheduling or unscheduling to all registered schedulers. There are 3 scheduler implementations: checks scheduler and logs scheduler in the agent, and cluster checks dispatcher in the cluster agent. - it stores a list of [`ConfigProviders`](https://logs-agent/blob/master/pkg/autodiscovery/providers) and poll them according to their poll policy via [`configPollers`](https://logs-agent/blob/master/pkg/autodiscovery/config_poller.go) - it owns [`ServiceListener`](https://logs-agent/blob/master/pkg/autodiscovery/listeners) used to listen to lifecycle events of containers and other kind of services like network devices, kubernetes Endpoints and Service objects - it uses the `ConfigResolver` that resolves a configuration template to an actual configuration based on a service matching the template. A template matches a service if they have in common at least one AD identifier element - it uses a `store` component to safely store and retrieve all data and mappings needed for the autodiscovery lifecycle common configresolver integration listeners providers scheduler config errors logs metadata serializer status tagger telemetry util retry 包介绍 这个包实现了错误重试机制，能够嵌入到任意一个需要错误尝试的系统类中。 它足够灵活地支持任何公开func() err 方法的进程，并且可以扩张到其他重试策略，而不是默认的策略 支持的策略 不重试：第一个错误时失败 重试计数： 回滚 version Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/golang/日志收集-datalog/架构.html":{"url":"blog/golang/日志收集-datalog/架构.html","title":"架构","keywords":"","body":"架构 整体逻辑介绍 tailer 首先从log 文件读取，将读取的内容发送到decoder的 input channel中 decoder 从自身的input channel读取数据 ，判断数据是否需要截断，将数据写入line parser的 input channel line parser从自身的input channel读取数据，解析内容、status、时间戳等，写入line handler的 input channel line handler从自身的input channel 读取数据，去除空格，发送到自身的output channel tailer forwardMessage 从decoder的output channel（与line handler共享）读取数据，添加tag后， 发送给pipeline的 input channel processor 从自身的input channel（pipe line的input channel）读取数据，encode后（比如encode为json/pb格式），发送到sender的input channel sender 从input channel读取数据，最后又将message写入pipeline的output channle sender将message的content发送给datadog 后台，发送时默认不压缩传输，http支持gzip压缩传输，tcp不支持压缩。 pipeline的output channel初始传入的是auditor的 input channel。 auditor从input channel 读取数据，写入内存 ，定时器从buffer刷入磁盘，另外一个定时器定期清理内存过期数据 input channel用白色的小长条表示， output channel 用灰色的小长条表示。 input channel或者outputchannel 都是暂存message的通道。message=日志+附加信息 实线箭头表示数据直接写入，虚线箭头仅表示数据流向，两个逻辑channel并无实际数据转移。 为了与代码表述一致，虚线箭头两次仍然画成两个channel。两个channel实际为同一个物理channel，数据写入其中一个channel中，另一个channel可以直接使用数据。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/istio/":{"url":"blog/istio/","title":"Istio","keywords":"","body":"Istio Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/istio/readme.html":{"url":"blog/istio/readme.html","title":"Readme","keywords":"","body":"Istio Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/istio/virtualservice.html":{"url":"blog/istio/virtualservice.html","title":"Virtualservice","keywords":"","body":"virtualservice 虚拟服务（Virtual Service）以及目标规则（Destination Rule）是 Istio 流量路由的两大基石。虚拟服务可以将流量路由到 Istio 服务网格中的服务。每个虚拟服务由一组路由规则组成，这些路由规则按顺序进行评估。 如果没有 Istio virtual service，仅仅使用 k8s service 的话，那么只能实现最基本的流量负载均衡转发，但是就不能实现类似按百分比来分配流量等更加复杂、丰富、细粒度的流量控制了。 备注：虚拟服务相当于 K8s 服务的 sidecar，在原本 K8s 服务的功能之上，提供了更加丰富的路由控制。 virtualService 的路由规则 创建 两个deployment， nginx 和 tomcat, kubectl create ns test apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy namespace: test spec: replicas: 1 selector: matchLabels: type: web app: nginx template: metadata: labels: type: web app: nginx spec: containers: - image: nginx:1.14-alpine imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: port protocol: TCP --- apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deploy namespace: test spec: replicas: 1 selector: matchLabels: type: web app: tomcat template: metadata: labels: type: web app: tomcat spec: containers: - image: docker.io/kubeguide/tomcat-app:v1 imagePullPolicy: IfNotPresent name: tomcat ports: - containerPort: 8080 name: port protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web-svc namespace: test spec: ports: - name: port port: 8080 protocol: TCP targetPort: 8080 selector: type: web sessionAffinity: None type: NodePort 创建完nginx ，需要进入容器修改监听端口 80->8080, nginx -s reload 创建istio 路由规则 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: test-dr namespace: test spec: host: web-svc subsets: - name: tomcat labels: app: tomcat - name: nginx labels: app: nginx --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: test-virtual-svc namespace: test spec: gateways: - test-web - mesh # 对所有网格内服务有效 hosts: - web-svc - www.cai.com # 因为有了网格内的服务主机的定义，这里就不能用 * 了 http: - route: - destination: host: web-svc subset: nginx weight: 90 - destination: host: web-svc subset: tomcat weight: 10 match: - gateways: - test-web # 限制只对 Ingress 网关的流量有效 uri: exact: / - route: - destination: host: web-svc subset: nginx weight: 10 - destination: host: web-svc subset: tomcat weight: 90 match: - gateways: - mesh # 对所有网格内服务有效 uri: exact: / --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: test-web namespace: test spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" 访问测试： nginx ingress controller 访问 for i in {1..1000};do curl -H \"host: www.cai.com\" http://8.210.125.72:31425;done 测试结果： virtualService 规则没有生效 未注入istio sidecar 容器内访问 kubectl run busybox --rm -ti --image busybox /bin/sh wget -q -O - http://web-svc:8080 测试结果： virtualService 规则没有生效 注入istio sidecar 容器访问web-svc测试 测试结果： virtualService 规则有效 ingressGateway Controller 访问测试 测试结果： virtualService 规则有效 Question Q1. 进入 busy 容器对 已经创建virtualservice 分流的服务进行curl 访问测试， virtualservice 比例是1:4， 但是 实际确实 1：1， busy 容器没有注入istio 答：VirtualService.Spec.Hosts 域名可以是服务的svc 名称或者是 外部的域名。 如果填写的是外部域名，那么路由策略只针对 ingressGateway有效 如果填写的是内部服务名称，路由策略则对网格内的所有容器有效，非网格内的无效。 Q2. ingressController 代理service web-svc 流量没有按virtualService 的比例分流 答：virtaulService 的网关路由策略只对ingressGateway 有效。 Q3. [2020-12-25T12:36:09.713Z] \"GET /tomcat.png HTTP/1.1\" 404 NR \"-\" 0 0 0 - \"172.31.221.208\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36\" \"ed3f2774-d8cb-9f89-a1a0-44cbcdb6a747\" \"www.cai.com:31425\" \"-\" - - 10.244.101.116:8080 172.31.221.208:1988 - - 答： 路由匹配规则， exact -> prefix match: - gateways: - test-web # 限制只对 Ingress 网关的流量有效 uri: exact: / link 入门到放弃 网络故障排查 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/java/jvm.html":{"url":"blog/java/jvm.html","title":"Jvm","keywords":"","body":"https://mp.weixin.qq.com/s/tmMpFmmCx2sWqpB_KibdQw 这是面试专题系列第五篇JVM篇。这一篇可能稍微比较长，没有耐心的同学建议直接拖到最后。 说说JVM的内存布局？ Java虚拟机主要包含几个区域： 堆：堆Java虚拟机中最大的一块内存，是线程共享的内存区域，基本上所有的对象实例数组都是在堆上分配空间。堆区细分为Yound区年轻代和Old区老年代，其中年轻代又分为Eden、S0、S1 3个部分，他们默认的比例是8:1:1的大小。 栈：栈是线程私有的内存区域，每个方法执行的时候都会在栈创建一个栈帧，方法的调用过程就对应着栈的入栈和出栈的过程。每个栈帧的结构又包含局部变量表、操作数栈、动态连接、方法返回地址。 局部变量表用于存储方法参数和局部变量。当第一个方法被调用的时候，他的参数会被传递至从0开始的连续的局部变量表中。 操作数栈用于一些字节码指令从局部变量表中传递至操作数栈，也用来准备方法调用的参数以及接收方法返回结果。 动态连接用于将符号引用表示的方法转换为实际方法的直接引用。 元数据：在Java1.7之前，包含方法区的概念，常量池就存在于方法区（永久代）中，而方法区本身是一个逻辑上的概念，在1.7之后则是把常量池移到了堆内，1.8之后移出了永久代的概念(方法区的概念仍然保留)，实现方式则是现在的元数据。它包含类的元信息和运行时常量池。 Class文件就是类和接口的定义信息。 运行时常量池就是类和接口的常量池运行时的表现形式。 本地方法栈：主要用于执行本地native方法的区域 程序计数器：也是线程私有的区域，用于记录当前线程下虚拟机正在执行的字节码的指令地址 知道new一个对象的过程吗？ 当虚拟机遇见new关键字时候，实现判断当前类是否已经加载，如果类没有加载，首先执行类的加载机制，加载完成后再为对象分配空间、初始化等。 首先校验当前类是否被加载，如果没有加载，执行类加载机制 加载：就是从字节码加载成二进制流的过程 验证：当然加载完成之后，当然需要校验Class文件是否符合虚拟机规范，跟我们接口请求一样，第一件事情当然是先做个参数校验了 准备：为静态变量、常量赋默认值 解析：把常量池中符号引用(以符号描述引用的目标)替换为直接引用(指向目标的指针或者句柄等)的过程 初始化：执行static代码块(cinit)进行初始化，如果存在父类，先对父类进行初始化 Ps：静态代码块是绝对线程安全的，只能隐式被java虚拟机在类加载过程中初始化调用！(此处该有问题static代码块线程安全吗？) 当类加载完成之后，紧接着就是对象分配内存空间和初始化的过程 首先为对象分配合适大小的内存空间 接着为实例变量赋默认值 设置对象的头信息，对象hash码、GC分代年龄、元数据信息等 执行构造函数(init)初始化 知道双亲委派模型吗？ 类加载器自顶向下分为： Bootstrap ClassLoader启动类加载器：默认会去加载JAVA_HOME/lib目录下的jar Extention ClassLoader扩展类加载器：默认去加载JAVA_HOME/lib/ext目录下的jar Application ClassLoader应用程序类加载器：比如我们的web应用，会加载web程序中ClassPath下的类 User ClassLoader用户自定义类加载器：由用户自己定义 当我们在加载类的时候，首先都会向上询问自己的父加载器是否已经加载，如果没有则依次向上询问，如果没有加载，则从上到下依次尝试是否能加载当前类，直到加载成功。 说说有哪些垃圾回收算法？ 标记-清除 统一标记出需要回收的对象，标记完成之后统一回收所有被标记的对象，而由于标记的过程需要遍历所有的GC ROOT，清除的过程也要遍历堆中所有的对象，所以标记-清除算法的效率低下，同时也带来了内存碎片的问题。 复制算法 为了解决性能的问题，复制算法应运而生，它将内存分为大小相等的两块区域，每次使用其中的一块，当一块内存使用完之后，将还存活的对象拷贝到另外一块内存区域中，然后把当前内存清空，这样性能和内存碎片的问题得以解决。但是同时带来了另外一个问题，可使用的内存空间缩小了一半！ 因此，诞生了我们现在的常见的年轻代+老年代的内存结构：Eden+S0+S1组成，因为根据IBM的研究显示，98%的对象都是朝生夕死，所以实际上存活的对象并不是很多，完全不需要用到一半内存浪费，所以默认的比例是8:1:1。 这样，在使用的时候只使用Eden区和S0S1中的一个，每次都把存活的对象拷贝另外一个未使用的Survivor区，同时清空Eden和使用的Survivor，这样下来内存的浪费就只有10%了。 如果最后未使用的Survivor放不下存活的对象，这些对象就进入Old老年代了。 PS：所以有一些初级点的问题会问你为什么要分为Eden区和2个Survior区？有什么作用？就是为了节省内存和解决内存碎片的问题，这些算法都是为了解决问题而产生的，如果理解原因你就不需要死记硬背了 标记-整理 针对老年代再用复制算法显然不合适，因为进入老年代的对象都存活率比较高了，这时候再频繁的复制对性能影响就比较大，而且也不会再有另外的空间进行兜底。所以针对老年代的特点，通过标记-整理算法，标记出所有的存活对象，让所有存活的对象都向一端移动，然后清理掉边界以外的内存空间。 那么什么是GC ROOT？有哪些GC ROOT？ 上面提到的标记的算法，怎么标记一个对象是否存活？简单的通过引用计数法，给对象设置一个引用计数器，每当有一个地方引用他，就给计数器+1，反之则计数器-1，但是这个简单的算法无法解决循环引用的问题。 Java通过可达性分析算法来达到标记存活对象的目的，定义一系列的GC ROOT为起点，从起点开始向下开始搜索，搜索走过的路径称为引用链，当一个对象到GC ROOT没有任何引用链相连的话，则对象可以判定是可以被回收的。 而可以作为GC ROOT的对象包括： 栈中引用的对象 静态变量、常量引用的对象 本地方法栈native方法引用的对象 垃圾回收器了解吗？年轻代和老年代都有哪些垃圾回收器？ 年轻代的垃圾收集器包含有Serial、ParNew、Parallell，老年代则包括Serial Old老年代版本、CMS、Parallel Old老年代版本和JDK11中的船新的G1收集器。 Serial：单线程版本收集器，进行垃圾回收的时候会STW（Stop The World），也就是进行垃圾回收的时候其他的工作线程都必须暂停 ParNew：Serial的多线程版本，用于和CMS配合使用 Parallel Scavenge：可以并行收集的多线程垃圾收集器 Serial Old：Serial的老年代版本，也是单线程 Parallel Old：Parallel Scavenge的老年代版本 CMS（Concurrent Mark Sweep）：CMS收集器是以获取最短停顿时间为目标的收集器，相对于其他的收集器STW的时间更短暂，可以并行收集是他的特点，同时他基于标记-清除算法，整个GC的过程分为4步。 初始标记：标记GC ROOT能关联到的对象，需要STW 并发标记：从GCRoots的直接关联对象开始遍历整个对象图的过程，不需要STW 重新标记：为了修正并发标记期间，因用户程序继续运作而导致标记产生改变的标记，需要STW 并发清除：清理删除掉标记阶段判断的已经死亡的对象，不需要STW 从整个过程来看，并发标记和并发清除的耗时最长，但是不需要停止用户线程，而初始标记和重新标记的耗时较短，但是需要停止用户线程，总体而言，整个过程造成的停顿时间较短，大部分时候是可以和用户线程一起工作的。 G1（Garbage First）：G1收集器是JDK9的默认垃圾收集器，而且不再区分年轻代和老年代进行回收。 G1的原理了解吗？ G1作为JDK9之后的服务端默认收集器，且不再区分年轻代和老年代进行垃圾回收，他把内存划分为多个Region，每个Region的大小可以通过-XX：G1HeapRegionSize设置，大小为1~32M，对于大对象的存储则衍生出Humongous的概念，超过Region大小一半的对象会被认为是大对象，而超过整个Region大小的对象被认为是超级大对象，将会被存储在连续的N个Humongous Region中，G1在进行回收的时候会在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间优先回收收益最大的Region。 G1的回收过程分为以下四个步骤： 初始标记：标记GC ROOT能关联到的对象，需要STW 并发标记：从GCRoots的直接关联对象开始遍历整个对象图的过程，扫描完成后还会重新处理并发标记过程中产生变动的对象 最终标记：短暂暂停用户线程，再处理一次，需要STW 筛选回收：更新Region的统计数据，对每个Region的回收价值和成本排序，根据用户设置的停顿时间制定回收计划。再把需要回收的Region中存活对象复制到空的Region，同时清理旧的Region。需要STW 总的来说除了并发标记之外，其他几个过程也还是需要短暂的STW，G1的目标是在停顿和延迟可控的情况下尽可能提高吞吐量。 什么时候会触发YGC和FGC？对象什么时候会进入老年代？ 当一个新的对象来申请内存空间的时候，如果Eden区无法满足内存分配需求，则触发YGC，使用中的Survivor区和Eden区存活对象送到未使用的Survivor区，如果YGC之后还是没有足够空间，则直接进入老年代分配，如果老年代也无法分配空间，触发FGC，FGC之后还是放不下则报出OOM异常。 YGC之后，存活的对象将会被复制到未使用的Survivor区，如果S区放不下，则直接晋升至老年代。而对于那些一直在Survivor区来回复制的对象，通过-XX：MaxTenuringThreshold配置交换阈值，默认15次，如果超过次数同样进入老年代。 此外，还有一种动态年龄的判断机制，不需要等到MaxTenuringThreshold就能晋升老年代。如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。 频繁FullGC怎么排查？ 这种问题最好的办法就是结合有具体的例子举例分析，如果没有就说一般的分析步骤。发生FGC有可能是内存分配不合理，比如Eden区太小，导致对象频繁进入老年代，这时候通过启动参数配置就能看出来，另外有可能就是存在内存泄露，可以通过以下的步骤进行排查： jstat -gcutil或者查看gc.log日志，查看内存回收情况 S0 S1 分别代表两个Survivor区占比 E代表Eden区占比，图中可以看到使用78% O代表老年代，M代表元空间，YGC发生54次，YGCT代表YGC累计耗时，GCT代表GC累计耗时。 [GC [FGC 开头代表垃圾回收的类型 PSYoungGen: 6130K->6130K(9216K)] 12274K->14330K(19456K), 0.0034895 secs代表YGC前后内存使用情况 Times: user=0.02 sys=0.00, real=0.00 secs，user表示用户态消耗的CPU时间，sys表示内核态消耗的CPU时间，real表示各种墙时钟的等待时间 这两张图只是举例并没有关联关系，比如你从图里面看能到是否进行FGC，FGC的时间花费多长，GC后老年代，年轻代内存是否有减少，得到一些初步的情况来做出判断。 dump出内存文件在具体分析，比如通过jmap命令jmap -dump:format=b,file=dumpfile pid，导出之后再通过Eclipse Memory Analyzer等工具进行分析，定位到代码，修复 这里还会可能存在一个提问的点，比如CPU飙高，同时FGC怎么办？办法比较类似 找到当前进程的pid，top -p pid -H 查看资源占用，找到线程 printf “%x\\n” pid，把线程pid转为16进制，比如0x32d jstack pid|grep -A 10 0x32d查看线程的堆栈日志，还找不到问题继续 dump出内存文件用MAT等工具进行分析，定位到代码，修复 JVM调优有什么经验吗？ 要明白一点，所有的调优的目的都是为了用更小的硬件成本达到更高的吞吐，JVM的调优也是一样，通过对垃圾收集器和内存分配的调优达到性能的最佳。 简单的参数含义 首先，需要知道几个主要的参数含义。 -Xms设置初始堆的大小，-Xmx设置最大堆的大小 -XX:NewSize年轻代大小，-XX:MaxNewSize年轻代最大值，-Xmn则是相当于同时配置-XX:NewSize和-XX:MaxNewSize为一样的值 -XX:NewRatio设置年轻代和年老代的比值，如果为3，表示年轻代与老年代比值为1:3，默认值为2 -XX:SurvivorRatio年轻代和两个Survivor的比值，默认8，代表比值为8:1:1 -XX:PretenureSizeThreshold 当创建的对象超过指定大小时，直接把对象分配在老年代。 -XX:MaxTenuringThreshold设定对象在Survivor复制的最大年龄阈值，超过阈值转移到老年代 -XX:MaxDirectMemorySize当Direct ByteBuffer分配的堆外内存到达指定大小后，即触发Full GC 调优 为了打印日志方便排查问题最好开启GC日志，开启GC日志对性能影响微乎其微，但是能帮助我们快速排查定位问题。-XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:gc.log 一般设置-Xms=-Xmx，这样可以获得固定大小的堆内存，减少GC的次数和耗时，可以使得堆相对稳定 -XX:+HeapDumpOnOutOfMemoryError让JVM在发生内存溢出的时候自动生成内存快照，方便排查问题 -Xmn设置新生代的大小，太小会增加YGC，太大会减小老年代大小，一般设置为整个堆的1/4到1/3 设置-XX:+DisableExplicitGC禁止系统System.gc()，防止手动误触发FGC造成问题 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/java/typescript.html":{"url":"blog/java/typescript.html","title":"Typescript","keywords":"","body":"TypeScript is what 一、什么是 TypeScript TypeScript 是近几年被火爆的应用了，这让大家产生了一个错觉：这么多的拥护者，难道TypeScript是一个新的语言？ TypeScript是微软公司开发和维护的一种面向对象的编程语言。它是JavaScript的超集，包含其所有元素。 TypeScript完全遵循OOPS的概念，在TSC（TypeScript编译器）的帮助下，我们可以将TypeScript代码（.ts文件）转换成JavaScript（.js文件） TypeScript是JavaScript的超集 二、TypeScript 简史 2010年，Anders Hejlsberg（TypeScript的创建者）开始在微软开发TypeScript，并于2012年向公众发布了TypeScript的第一个版本（TypeScript 0.8）。尽管TypeScript的发布受到了全世界许多人的称赞，但是由于缺少主要ide的支持，它并没有被JavaScript社区主要采用。 TypeScript的第一个版本（TypeScript 0.8）于2012年10月发布。 最新版本的Typescript（Typescript 3.0）于2018年7月发布，您可以在这里下载最新版本！ 三、为什么我们要使用TypeScript？ TypeScript简化了JavaScript代码，使其更易于阅读和调试。 TypeScript是开源的。 TypeScript为JavaScript ide和实践（如静态检查）提供了高效的开发工具。 TypeScript使代码更易于阅读和理解。 使用TypeScript，我们可以大大改进普通的JavaScript。 TypeScript为我们提供了ES6（ECMAScript 6）的所有优点，以及更高的生产率。 TypeScript通过对代码进行类型检查，可以帮助我们避免在编写JavaScript时经常遇到的令人痛苦的错误。 强大的类型系统，包括泛型。 TypeScript只不过是带有一些附加功能的JavaScript。 TypeScript代码可以按照ES5和ES6标准编译，以支持最新的浏览器。 与ECMAScript对齐以实现兼容性。 以JavaScript开始和结束。 支持静态类型。 TypeScript将节省开发人员的时间。 TypeScript是ES3、ES5和ES6的超集。 TypeScript的附加功能 具有可选参数的函数。 使用REST参数的函数。 泛型支持。 模块支持。 四、大牛现身说法: “我们喜欢TypeScript有很多方面……有了TypeScript，我们的几个团队成员说了类似的话，我现在实际上已经理解了我们自己的大部分代码！因为他们可以轻松地遍历它并更好地理解关系。我们已经通过TypeScript的检查发现了几个漏洞。“-Brad Green，Angular工程总监“ Ionic的主要目标之一是使应用程序开发尽可能快速和简单，工具支持TypeScript为我们 提供了自动完成、类型检查和源文档与之真正一致。”-Tim Lancina，工具开发人员–Ionic“ 在编写基于web或JavaScript的现代应用程序时，TypeScript是一个明智的选择。TypeScript经过仔细考虑的语言特性和功能，以及它不断改进的工具，带来了非常有成效的开发体验。”-Epic研究员Aaron Cornelius“ TypeScript帮助我们重用团队的知识并通过提供与C#相同的优秀开发经验来保持相同的团队速度……比普通JavaScript有了巨大的改进。”-Valio Stoychev，PM Lead–NativeScript 五、你可能不知道的TypeScript顶级功能 1、面向对象程序设计 TypeScript包含一组非常好的面向对象编程（OOP）特性，这些特性有助于维护健壮和干净的代码；这提高了代码质量和可维护性。这些OOP特性使TypeScript代码非常整洁和有组织性。 例如: class CustomerModel { customerId: number; companyName: string; contactName: string; country: string; } class CustomerOperation{ addCustomer(customerData: CustomerModel) : number { // 添加用户 let customerId = 5;// 保存后返回的ID return customerId; } } 2、接口、泛型、继承和方法访问修饰符 TypeScript支持接口、泛型、继承和方法访问修饰符。接口是指定契约的好方法。泛型有助于提供编译时检查，继承使新对象具有现有对象的属性，访问修饰符控制类成员的可访问性。TypeScript有两个访问修饰符-public和private。默认情况下，成员是公共的，但您可以显式地向其添加公共或私有修饰符。 （1）接口 interface ITax { taxpayerId: string; calculateTax(): number; } class IncomeTax implements ITax { taxpayerId: string; calculateTax(): number { return 10000; } } class ServiceTax implements ITax { taxpayerId: string; calculateTax(): number { return 2000; } } （2）访问修饰符 class Customers{ public companyname:string; private country:string; } 显示一个公共变量和一个私有变量 （3）继承 class Employee{ Firstname:string; } class Company extends Employee { Department:string; Role:string private AddEmployee(){ this.Department=\"myDept\"; this.Role=\"Manager\"; this.FirstName=\"Test\"; } } （4）泛型 function identity (arg: T): T { return arg; } // 显示泛型实现的示例 let output = identity (\"myString\"); let outputl = identity (23); （5）强/静态类型 TypeScript不允许将值与不同的数据类型混合。如果违反了这些限制，就会抛出错误。因此，在声明变量时必须定义类型，并且除了在JavaScript中非常可能定义的类型之外，不能分配其他值。 例如: let testnumber: number = 6; testnumber = \"myNumber\"; // 这将引发错误 testnumber = 5; // 这样就可以了 3、编译时/静态类型检查 如果我们不遵循任何编程语言的正确语法和语义，那么编译器就会抛出编译时错误。在删除所有语法错误或调试编译时错误之前，它们不会让程序执行一行代码。TypeScript也是如此。 例如: let isDone: boolean = false; isDone = \"345\"; // 这将引发错误 isDone = true; // 这样就可以了 4、比JavaScript代码更少 TypeScript是JavaScript的包装器，因此可以使用帮助类来减少代码。Typescript中的代码更容易理解。 5、可读性 接口、类等为代码提供可读性。由于代码是用类和接口编写的，因此更有意义，也更易于阅读和理解。 举例: class Greeter { private greeting: string; constructor (private message: string) { this.greeting = message; } greet() { return \"Hello, \" + this.greeting; } } JavaScript 代码: var Greeter = (function () { function Greeter(message) { this.greeting = message; } Greeter.prototype.greet = function () { return \"Hello, \" + this.greeting; }; return Greeter; })(); 6、兼容性 Typescript与JavaScript库兼容，比如 underscore.js，Lodash等。它们有许多内置且易于使用的功能，使开发更快。 7、提供可以将代码转换为JavaScript等效代码的“编译器” TypeScript代码由纯JavaScript代码以及特定于TypeScript的某些关键字和构造组成。但是，编译TypeScript代码时，它会转换为普通的JavaScript。这意味着生成的JavaScript可以与任何支持JavaScript的浏览器一起使用。 8、支持模块 随着TypeScript代码基的增长，组织类和接口以获得更好的可维护性变得非常重要。TypeScript模块允许您这样做。模块是代码的容器，可以帮助您以整洁的方式组织代码。从概念上讲，您可能会发现它们类似于.NET命名空间。 例如: module Company { class Employee { } class EmployeeHelper { targetEmployee: Employee; } export class Customer { } } var obj = new Company.Customer(); 9、ES6 功能支持 Typescript是ES6的一个超集，所以ES6的所有特性它都有。另外还有一些特性，比如它支持通常称为lambda函数的箭头函数。ES6引入了一种稍微不同的语法来定义匿名函数，称为胖箭头(fat arrow)语法。 举例: setTimeout(() => { console.log(\"setTimeout called!\") }, 1000); 10、在流行的框架中使用 TypeScript在过去几年里越来越流行。也许TypeScript流行的决定性时刻是Angular2正式转换到TS的时候，这是一个双赢的局面。 11、减少错误 它减少了诸如空处理、未定义等错误。强类型特性，通过适当的类型检查限制开发人员，来编写特定类型的代码。 12、函数重载 TypeScript允许您定义重载函数。这样，您可以根据参数调用函数的不同实现。但是，请记住，TypeScript函数重载有点奇怪，需要在实现期间进行类型检查。这种限制是由于TypeScript代码最终被编译成纯JavaScript，而JavaScript不支持真正意义上的函数重载概念。 例如: class functionOverloading{ addCustomer(custId: number); addCustomer(company: string); addCustomer(value: any) { if (value && typeof value == \"number\") { alert(\"First overload - \" + value); } if (value && typeof value == \"string\") { alert(\"Second overload - \" + value); } } } 13、构造器 在TypeScript中定义的类可以有构造函数。构造函数通常通过将默认值设置为其属性来完成初始化对象的工作。构造函数也可以像函数一样重载。 例如: export class SampleClass{ private title: string; constructor(public constructorexample: string){ this.title = constructorexample; } } 14、调试 用TypeScript编写的代码很容易调试。 15、TypeScript只是JavaScript TypeScript始于JavaScript，止于JavaScript。Typescript采用JavaScript中程序的基本构建块。为了执行的目的，所有类型脚本代码都转换为其JavaScript等效代码。 例如: class Greeter { greeting: string; constructor (message: string) { this.greeting = message; } greet() { return \"Hello, \" + this.greeting; } } JavaScript 代码: var Greeter = (function () { function Greeter(message) { this.greeting = message; } Greeter.prototype.greet = function () { return \"Hello, \" + this.greeting; }; return Greeter; })(); 16、可移植性 TypeScript可以跨浏览器、设备和操作系统移植。它可以在JavaScript运行的任何环境中运行。与对应的脚本不同，TypeScript不需要专用的VM或特定的运行时环境来执行。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/":{"url":"blog/kubernetes/","title":"Kubernetes","keywords":"","body":"Kubernetes Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/APIService.html":{"url":"blog/kubernetes/APIService.html","title":"APIService","keywords":"","body":"APIService APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。 下面是一个 APIService 的示例配置： apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1alpha1.custom-metrics.metrics.k8s.io spec: insecureSkipTLSVerify: true group: custom-metrics.metrics.k8s.io groupPriorityMinimum: 1000 versionPriority: 5 service: name: api namespace: custom-metrics version: v1alpha1 APIService 详解 使用 apiregistration.k8s.io/v1beta1 版本的 APIService，在 metadata.name 中定义该 API 的名字。 使用上面的 yaml 的创建 v1alpha1.custom-metrics.metrics.k8s.io APIService。 insecureSkipTLSVerify：当与该服务通信时，禁用 TLS 证书认证。强加建议不要设置这个参数，默认为 false。应该使用 CABundle 代替。 service：与该 APIService 通信时引用的 service，其中要注明 service 的名字和所属的 namespace，如果为空的话，则所有的服务都会该 API groupversion 将在本地 443 端口处理所有通信。 groupPriorityMinimum：该组 API 的处理优先级，主要排序是基于 groupPriorityMinimum，该数字越大表明优先级越高，客户端就会与其通信处理请求。次要排序是基于字母表顺序，例如 v1.bar 比 v1.foo 的优先级更高。 versionPriority：VersionPriority 控制其组内的 API 版本的顺序。必须大于零。主要排序基于 VersionPriority，从最高到最低（20 大于 10）排序。次要排序是基于对象名称的字母比较。 （v1.foo 在 v1.bar 之前）由于它们都是在一个组内，因此数字可能很小，一般都小于 10。 查看我们使用上面的 yaml 文件创建的 APIService。 kubectl get apiservice v1alpha1.custom-metrics.metrics.k8s.io -o yaml ``````yaml apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: creationTimestamp: 2017-12-14T08:27:35Z name: v1alpha1.custom-metrics.metrics.k8s.io resourceVersion: \"35194598\" selfLink: /apis/apiregistration.k8s.io/v1beta1/apiservices/v1alpha1.custom-metrics.metrics.k8s.io uid: a31a3412-e0a8-11e7-9fa4-f4e9d49f8ed0 spec: caBundle: null group: custom-metrics.metrics.k8s.io groupPriorityMinimum: 1000 insecureSkipTLSVerify: true service: name: api namespace: custom-metrics version: v1alpha1 versionPriority: 5 status: conditions: - lastTransitionTime: 2017-12-14T08:27:38Z message: all checks passed reason: Passed status: \"True\" type: Available 查看集群支持的 APISerivce 作为 Kubernetes 中的一种资源对象，可以使用 kubectl get apiservice 来查看。 例如查看集群中所有的 APIService： $ kubectl get apiservice NAME AGE v1. 2d v1.authentication.k8s.io 2d v1.authorization.k8s.io 2d v1.autoscaling 2d v1.batch 2d v1.monitoring.coreos.com 1d v1.networking.k8s.io 2d v1.rbac.authorization.k8s.io 2d v1.storage.k8s.io 2d v1alpha1.custom-metrics.metrics.k8s.io 2h v1beta1.apiextensions.k8s.io 2d v1beta1.apps 2d v1beta1.authentication.k8s.io 2d v1beta1.authorization.k8s.io 2d v1beta1.batch 2d v1beta1.certificates.k8s.io 2d v1beta1.extensions 2d v1beta1.policy 2d v1beta1.rbac.authorization.k8s.io 2d v1beta1.storage.k8s.io 2d v1beta2.apps 2d v2beta1.autoscaling 2d 另外查看当前 kubernetes 集群支持的 API 版本还可以使用kubectl api-versions： $ kubectl api-versions apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1beta1 apps/v1beta1 apps/v1beta2 authentication.k8s.io/v1 authentication.k8s.io/v1beta1 authorization.k8s.io/v1 authorization.k8s.io/v1beta1 autoscaling/v1 autoscaling/v2beta1 batch/v1 batch/v1beta1 certificates.k8s.io/v1beta1 custom-metrics.metrics.k8s.io/v1alpha1 extensions/v1beta1 monitoring.coreos.com/v1 networking.k8s.io/v1 policy/v1beta1 rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 storage.k8s.io/v1 storage.k8s.io/v1beta1 v1 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/Aggregated.html":{"url":"blog/kubernetes/Aggregated.html","title":"Aggregated","keywords":"","body":"工作中需要以kubernetes原生的方式构建API接口服务，并将构建出的API接口直接聚合到kubernetes的apiserver服务上。本周花了不少时间研究这个，这里记录一下。 好处 尽管可以使用gin, go-restful等go语言web框架轻易地构建出一个稳定的API接口服务，但以kubernetes原生的方式构建API接口服务还是有很多吸引人的好处的。官方文档中已经将这些好处列出了： User-Defined Types Consider adding a Custom Resource to Kubernetes if you want to define new controllers, application configuration objects or other declarative APIs, and to manage them using Kubernetes tools, such as kubectl. Do not use a Custom Resource as data storage for application, user, or monitoring data. For more about Custom Resources, see the Custom Resources concept guide. Combining New APIs with Automation The combination of a custom resource API and a control loop is called the Operator pattern. The Operator pattern is used to manage specific, usually stateful, applications. These custom APIs and control loops can also be used to control other resources, such as storage or policies. Changing Built-in Resources When you extend the Kubernetes API by adding custom resources, the added resources always fall into a new API Groups. You cannot replace or change existing API groups. Adding an API does not directly let you affect the behavior of existing APIs (e.g. Pods), but API Access Extensions do. API Access Extensions When a request reaches the Kubernetes API Server, it is first Authenticated, then Authorized, then subject to various types of Admission Control. See Controlling Access to the Kubernetes API for more on this flow. Each of these steps offers extension points. Kubernetes has several built-in authentication methods that it supports. It can also sit behind an authenticating proxy, and it can send a token from an Authorization header to a remote service for verification (a webhook). All of these methods are covered in the Authentication documentation. Authentication Authentication maps headers or certificates in all requests to a username for the client making the request. Kubernetes provides several built-in authentication methods, and an Authentication webhook method if those don’t meet your needs. Authorization Authorization determines whether specific users can read, write, and do other operations on API resources. It just works at the level of whole resources – it doesn’t discriminate based on arbitrary object fields. If the built-in authorization options don’t meet your needs, and Authorization webhook allows calling out to user-provided code to make an authorization decision. Dynamic Admission Control After a request is authorized, if it is a write operation, it also goes through Admission Control steps. In addition to the built-in steps, there are several extensions: The Image Policy webhook restricts what images can be run in containers. To make arbitrary admission control decisions, a general Admission webhook can be used. Admission Webhooks can reject creations or updates. 简单一句话就是构建出的API接口更加规范整齐，能利用kubernetes原生的认证、授权、准入机制。当然对个人来说，也能更了解kubernetes里那些API接口到底是如何实现的。 实现方案 官方提供了两种方式以实现对标准kubernetes API接口的扩展：1）Aggregated APIServer 2）Custom Resource 两种方式的区别是定义api-resource的方式不同。在Aggregated APIServer方式中，api-resource是通过代码向kubernetes注册资源类型的方式实现的，而Custom Resource是直接通过yaml文件创建自定义资源的方式实现的。 最终达到的效果倒是比较类似，最终都可以通过访问/apis/myextension.mycompany.io/v1/…之类的API接口来存取api-resource。除此之外，在很多方面也存在一些区别，见这里。 API Access Control Authentication CR: All strategies supported. Configured by root apiserver. AA: Supporting all root apiserver’s authenticating strategies but it has to be done via authentication token review apiexcept for authentication proxy which will cause an extra cost of network RTT. Authorization CR: All strategies supported. Configured by root apiserver. AA: Delegating authorization requests to root apiserver via SubjectAccessReview api. Note that this approach will also cost a network RTT. Admission Control CR: You could extend via dynamic admission control webhook (which is costing network RTT). AA: While You can develop and customize your own admission controller which is dedicated to your AA. While You can’t reuse root-apiserver’s built-in admission controllers nomore. API Schema Note: CR’s integration with OpenAPI schema is being enhanced in the future releases and it will have a stronger integration with OpenAPI mechanism. Validating CR: (landed in 1.12) Defined via OpenAPIv3 Schema grammar. more AA: You can customize any validating flow you want. Conversion CR: (landed in 1.13) The CR conversioning (basically from storage version to requested version) could be done via conversioning webhook. AA: Develop any conversion you want. SubResource CR: Currently only status and scale sub-resource supported. AA: You can customize any sub-resouce you want. OpenAPI Schema CR: (landed in 1.13) The corresponding CRD’s OpenAPI schema will be automatically synced to root-apiserver’s openapi doc api. AA: OpenAPI doc has to be manually generated by code-generating tools. Other Functionalities AA (Aggregated APIServer) CR (Custom Resource) SMP(Strategic Merge Patch) Supported Not yet. Will be replaced via server-side apply instead Informative Kubectl Printing Not supported, unless you develop your own with server-side printing. By AdditionalPrinterColumns Websocket/(Other non-HTTP transport) Supported No metadata.GenerationAuto Increment Supported Nope, and this is designed Use Another Backend/Secondary Storage Supported For now, ETCD3 only More Comparision here 总的来看，AA这个方式相对复杂一点，但灵活度很高，基本后续业务上的所有需求都可以满足。最终我们选择使用AA方案来构建API接口服务。 实现API接口服务 快速实现 虽然官方给了一个sample-apiserver，我们可以照着实现自己的Aggregated APIServer。但完全手工编写还是太费劲了，这里使用官方推荐的工具apiserver-builder帮助快速创建项目骨架。 apiserver-builder构建AA方案的API接口服务的原理还是比较清晰的，总之就是kubernetes里最常见的控制器模式，这里就不具体介绍了，官方文档既有文字又有图片讲得还是挺细致的，强烈推荐大家多看看，学习一下。 apiserver-builder的安装就不细说了，照着官方文档做就可以了。 以下参考apiserver-builder的官方文档，得出的一些关键步骤： # 创建项目目录 mkdir $GOPATH/src/github.com/jeremyxu2010/demo-apiserver # 在项目目录下新建一个名为boilerplate.go.txt，里面是代码的头部版权声明 cd $GOPATH/src/github.com/jeremyxu2010/demo-apiserver curl -o boilerplate.go.txt https://github.com/kubernetes/kubernetes/blob/master/hack/boilerplate/boilerplate.go.txt # 初始化项目 apiserver-boot init repo --domain jeremyxu2010.me # 创建一个非命名空间范围的api-resource apiserver-boot create group version resource --group demo --version v1beta1 --non-namespaced=true --kind Foo # 创建Foo这个api-resource的子资源 apiserver-boot create subresource --subresource bar --group demo --version v1beta1 --kind Foo # 生成上述创建的api-resource类型的相关代码，包括deepcopy接口实现代码、versioned/unversioned类型转换代码、api-resource类型注册代码、api-resource类型的Controller代码、api-resource类型的AdmissionController代码 apiserver-boot build generated # 直接在本地将etcd, apiserver, controller运行起来 apiserver-boot run local 上述这样操作之后，就可以访问我们的APIServer了，如下面的命令： curl -k https://127.0.0.1:9443/apis/demo.jeremyxu2010.me/v1beta1/foos 当然可以新建一个yaml文件，然后用kubectl命令直接对api-resource进行操作： # 创建Foo资源的yaml echo 'apiVersion: demo.jeremyxu2010.me/v1beta1 kind: Foo metadata: name: foo-example namespace: test spec: {}' > sample/foo.yaml # 查看已经注册的api-resource类型 kubectl --kubeconfig api-resources # 列所有foos kubectl --kubeconfig kubeconfig get foos # 创建foo kubectl --kubeconfig kubeconfig create -f sample/foo.yaml # 再列所有foos kubectl --kubeconfig kubeconfig get foos # Get新创建的foo kubectl --kubeconfig kubeconfig get foos foo-example kubectl --kubeconfig kubeconfig get foos foo-example -o yaml # Delete新创建的foo kubectl --kubeconfig kubeconfig delete foos foo-example 如果在apiserver的main方法里补上一些代码，以开启swagger-ui，还能更方便地看到这些API接口： func main() { version := \"v0\" server.StartApiServer(\"/registry/jeremyxu2010.me\", apis.GetAllApiBuilders(), openapi.GetOpenAPIDefinitions, \"Api\", version, func(apiServerConfig *apiserver.Config) error { ... apiServerConfig.RecommendedConfig.EnableSwaggerUI = true apiServerConfig.RecommendedConfig.SwaggerConfig = genericapiserver.DefaultSwaggerConfig() return nil }) } 然后浏览器访问https://127.0.0.1:9443/swagger-ui/就可以在swagger的Web页面上看到创建出来的所有API接口。 定制API接口 像上面这样创建的API接口，接口是都有了，但接口没有啥意义，一般要根据实际情况定义api-resource的spec、status等结构体。 type Foo struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec FooSpec `json:\"spec,omitempty\"` Status FooStatus `json:\"status,omitempty\"` } // FooSpec defines the desired state of Foo type FooSpec struct { } // FooStatus defines the observed state of Foo type FooStatus struct { } 可参考这里。 有时默认的增删改查操作并不满足业务需求，这时可以自定义api-resource或subresource的REST实现，默认实现是存取到etcd的，通过这种方式甚至可以将自定义资源存入后端数据库。自定义REST实现的方法参考adding_custom_rest，foo_rest.go，bar_foo_rest.go。另外kubernetes的代码里也有大量自定义REST实现可参考，见这里。 为api-resource类型的默认值设置可参考这里，添加校验规则可参考这里。 定制Controller 默认生成的api-resource的Reconcile逻辑如下： // Reconcile reads that state of the cluster for a Foo object and makes changes based on the state read // and what is in the Foo.Spec // TODO(user): Modify this Reconcile function to implement your Controller logic. The scaffolding writes // a Deployment as an example // +kubebuilder:rbac:groups=demo.jeremyxu2010.me,resources=foos,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=demo.jeremyxu2010.me,resources=foos/status,verbs=get;update;patch func (r *ReconcileFoo) Reconcile(request reconcile.Request) (reconcile.Result, error) { // Fetch the Foo instance instance := &demov1beta1.Foo{} err := r.Get(context.TODO(), request.NamespacedName, instance) if err != nil { if errors.IsNotFound(err) { // Object not found, return. Created objects are automatically garbage collected. // For additional cleanup logic use finalizers. return reconcile.Result{}, nil } // Error reading the object - requeue the request. return reconcile.Result{}, err } return reconcile.Result{}, nil } 一般来说要按自己的业务逻辑进行定制，可参考这里。 api-resource的admission controller编写可参考这里。 打包部署 程序写好后，通过以下命令即可生成容器镜像及kubernetes的部署manifest文件： # 生成二进制文件 apiserver-boot build executables # 生成容器镜像 apiserver-boot build container --image demo/foo-apiserver:latest # 生成kubernetes的部署manifest文件，可直接在kubernetes里apply即完成部署 apiserver-boot build config --name fool-apiserver --namespace default --image demo/foo-apiserver:latest 观察生成的kubernetes部署manifest文件config/apiserver.yaml，可以发现最终会创建一个Deployment，一个Service和一个APIService类型的kubernetes资源，同时APIService的caBundle及apiserver的TLS证书也配置妥当了。这个跟官方文档中所说的第4、5、6、7、8、14点相符。 生成文档 最终交付除了部署好的程序，还可以生成相应的API文档，操作如下： curl -o docs/openapi-spec/swagger.json https://127.0.0.1:9443/openapi/v2 apiserver-build build docs --build-openapi=false --operations=true 使用浏览器打开docs/build/index.html即可访问生成的API文档，这文档的风格可kubernetes的reference文档风格是一致，相当专业！！！ 其它 在实现过程中还顺带改了apiserver-builder的一个小bug，也算为社区做了点贡献。 apiserver-builder在生成代码时使用了一些kubernetes项目本身使用的code generator，这些code generator也挺有趣的，有时间可以仔细研究下。 总结 编写Aggregated APIServer风格的API接口服务这一工作，终于接触到了kubernetes里的一些内部设计，不得不说这套设计还是相当简洁稳定的，难怪kubernetes项目最终能成功。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/CAK.html":{"url":"blog/kubernetes/CAK.html","title":"CAK","keywords":"","body":"CKA 考试认证 付钱，注册，预约 购买 认证考试 https://training.linuxfoundation.cn/ 用上面买到的劵注册考试， 学员请到 https://identity.linuxfoundation.org/ 创建Linux Foundation ID（LFID）。 到PSI考试中心网站 www.examslocal.com/linuxfoundation 预约考试 顺便吐个槽：还是中国的网站友好。 考试范围 Application Lifecycle Management 8% Installation, Configuration & Validation 12% Core Concepts 19% Networking 11% Scheduling 5% Security 12% Cluster Maintenance 11% Logging / Monitoring 5% Storage 7% Troubleshooting 10% 可以忽略直接看真题 自动命令补全 yum install bash-completion source > ~/.bashrc # 在您的 bash shell 中永久的添加自动补全 show lable [root@cn-hongkong ~]# kubectl get nodes -L beta.kubernetes.io/arch NAME STATUS ROLES AGE VERSION ARCH cn-hongkong.i-j6c16vsekxqadodo9mhx Ready 141m v1.18.0 amd64 cn-hongkong.i-j6cfgd2b4spu7o30og89 Ready master 144m v1.18.0 amd64 select label [root@cn-hongkong ~]# kubectl get nodes -l beta.kubernetes.io/arch=amd64 NAME STATUS ROLES AGE VERSION cn-hongkong.i-j6c16vsekxqadodo9mhx Ready 143m v1.18.0 cn-hongkong.i-j6cfgd2b4spu7o30og89 Ready master 146m v1.18.0 etcd 备份 ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save /var/lib/etcd/snapshot-20200312.db find / -name etcdctl cd /etc/kubernetes/pki/ ETCDCTL_API=3 /data/docker/overlay2/93bc807c1818bcc408e7beabea91e3db2080a593051fa89a43ff5b3256d99fad/merged/usr/local/bin/etcdctl --cacert=ca.crt --cert=server.crt --key=server.key --endpoints=[127.0.0.1:2379] snapshot save snapshotdb 升级镜像 kubectl set image deployment'/nginx nginx=nginx:19.1 升级版本不记录 kubectl rollout paus deployment/nginx 恢复版本升级记录 kubectl rollout resume deployment/nginx 特定版本回滚 # 在 --to-revision 中指定您从步骤 1 中获取的版本序号 kubectl rollout undo deployment --to-revision= dns 查询 kubectl run busyboxy --image=busyboxy nslookup svc-name nslookup podip schedule # 驱逐pod from node kubectl cordon node1 kubectl drain node1 --ignore-daemonsets=true ## 恢复可调度 kubectl uncordon node1 Endpoint kubectl -n ${ns} get endpoints ${service-name} kubectl -n ${ns} get pods --selector=${service-selector} static pod # 1.进入 wk8s-node-1 节点 ssh wk8s-node-1 # 2. 在/etc/kubernetes/manifests 定义pod的yaml文件 #使用下面的参考命令生成pod文件 kubectl run myservice --image=nginx --generator=run-pod/v1 --dry-run -o yaml >21.yml # 3. 在 wk8s-node-1 节点上配置kubelet # 3.1 方式一：编辑kubelet配置（ /usr/lib/systemd/system/kubelet.service.d） # 添加参数 --pod-manifest-path=/etc/kubernetes/manifests KUBELET_ARGS=\"--cluster-dns=10.254.0.10 --cluster-domain=kube.local --pod-manifest-path=/etc/kubernetes/manifests\" # 3.2 方式二： 在kubelet配置（--config=/var/lib/kubelet/config.yaml）文件中 # 添加 staticPodPath: /etc/kubernetes/manifests #4. 重启服务 systemctl daemon-reload systemctl restart kubelet systemctl enable kubelet 考试技巧 根据真题范围，将kubernetes.io/doc 上的考点页面添加到收藏夹，方便复制粘贴yaml 如果你的macos 系统是10.15以上 请下载浏览器 vivaldi 准备好护照，看完护照后的考官发的都是考试规则信息， 都是考试手册上的东西，如果英文不好就不要再浪费时间看了 答题过程只需要保持头在摄像头内，考场安静，然后就可以安心答题了。 考试真题 https://blog.csdn.net/fly910905/article/details/103652034?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-8&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-8 reference https://www.jianshu.com/p/629525af31c4 https://github.com/walidshaari/Kubernetes-Certified-Administrator helm upgrade drone \\ --reuse-values \\ --set 'sourceControl.provider=github' \\ --set 'sourceControl.github.clientID=5f2f26363e1a11e43f11' \\ --set 'sourceControl.secret=71e854a3b433470a76ebf25d16da6fcee6ede26d' \\ stable/drone Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/CICD/argo.html":{"url":"blog/kubernetes/CICD/argo.html","title":"Argo","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/CICD/circle.html":{"url":"blog/kubernetes/CICD/circle.html","title":"Circle","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/CICD/drone.html":{"url":"blog/kubernetes/CICD/drone.html","title":"Drone","keywords":"","body":"drone what Drone is a continuous delivery system built on container technology. Drone uses a simple YAML build file, to define and execute build pipelines inside Docker containers. DRONE_RPC_HOST provides the hostname (and optional port) of your Drone server. The runner connects to the server at the host address to receive pipelines for execution. DRONE_RPC_PROTO provides the protocol used to connect to your Drone server. The value must be either http or https. DRONE_RPC_SECRET provides the shared secret used to authenticate with your Drone server. This must match the secret defined in your Drone server configuration. 部署 helm install drone ./stable/drone kubectl create secret generic drone-server-secrets \\ --namespace=default \\ --from-literal=clientSecret=\"github-oauth2-client-secret\" helm upgrade drone \\ --reuse-values \\ --set 'sourceControl.provider=github' \\ --set 'sourceControl.github.clientID=5f2f26363e1a11e43f11' \\ --set 'sourceControl.secret=drone-server-secrets' \\ --set 'service.type=LoadBalancer' \\ --set 'server.host=drone.xisheng.vip ' \\ stable/drone 代码结构 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/CICD/github action.html":{"url":"blog/kubernetes/CICD/github action.html","title":"Github Action","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/CSI/hostPath.html":{"url":"blog/kubernetes/CSI/hostPath.html","title":"HostPath","keywords":"","body":"Hostpath 取值 行为 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。 DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 Directory 在给定路径上必须存在的目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 File 在给定路径上必须存在的文件。 Socket 在给定路径上必须存在的 UNIX 套接字。 CharDevice 在给定路径上必须存在的字符设备。 BlockDevice 在给定路径上必须存在的块设备。 example apiVersion: v1 kind: Pod metadata: name: test-webserver spec: containers: - name: test-webserver image: k8s.gcr.io/test-webserver:latest volumeMounts: - mountPath: /var/local/aaa name: mydir - mountPath: /var/local/aaa/1.txt name: myfile volumes: - name: mydir hostPath: # 确保文件所在目录成功创建。 path: /var/local/aaa type: DirectoryOrCreate - name: myfile hostPath: path: /var/local/aaa/1.txt type: FileOrCreate Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/Deployment 的实现原理.html":{"url":"blog/kubernetes/Deployment 的实现原理.html","title":"Deployment 的实现原理","keywords":"","body":"详解 Kubernetes Deployment 的实现原理 2019-02-24 Kubernetes 容器编排 Deployment 滚动更新 如果你在生产环境中使用过 Kubernetes，那么相信你对 Deployment 一定不会陌生，Deployment 提供了一种对 Pod 和 ReplicaSet 的管理方式，每一个 Deployment 都对应集群中的一次部署，是非常常见的 Kubernetes 对象。 我们在这篇文章中就会介绍 Deployment 的实现原理，包括它是如何处理 Pod 的滚动更新、回滚以及支持副本的水平扩容。 概述 作为最常用的 Kubernetes 对象，Deployment 经常会用来创建 ReplicaSet 和 Pod，我们往往不会直接在集群中使用 ReplicaSet 部署一个新的微服务，一方面是因为 ReplicaSet 的功能其实不够强大，一些常见的更新、扩容和缩容运维操作都不支持，Deployment 的引入就是为了就是为了支持这些复杂的操作。 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 YAML 当我们在 Kubernetes 集群中创建上述 Deployment 对象时，它不只会创建 Deployment 资源，还会创建另外的 ReplicaSet 以及三个 Pod 对象： $ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 6m55s $ kubectl get replicasets.apps NAME DESIRED CURRENT READY AGE nginx-deployment-76bf4969df 3 3 3 7m27s $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-76bf4969df-58gxj 1/1 Running 0 7m42s nginx-deployment-76bf4969df-9jgk9 1/1 Running 0 7m42s nginx-deployment-76bf4969df-m4pkg 1/1 Running 0 7m43s Bash 每一个 Deployment 都会和它的依赖组成以下的拓扑结构，在这个拓扑结构中的子节点都是『稳定』的，任意节点的删除都会被 Kubernetes 的控制器重启： graph TD Deployment-.->ReplicaSet ReplicaSet-.->Pod1 ReplicaSet-.->Pod2 ReplicaSet-.->Pod3 Mermaid 所有的 Deployment 对象都是由 Kubernetes 集群中的 DeploymentController 进行管理，家下来我们将开始介绍该控制器的实现原理。 实现原理 DeploymentController 作为管理 Deployment 资源的控制器，会在启动时通过 Informer 监听三种不同资源的通知，Pod、ReplicaSet 和 Deployment，这三种资源的变动都会触发 DeploymentController 中的回调。 graph TD DI[DeploymentInformer]-. Add/Update/Delete .->DC[DeploymentController] ReplicaSetInformer-. Add/Update/Delete .->DC PodInformer-. Delete .->DC Mermaid 不同的事件最终都会在被过滤后进入控制器持有的队列，等待工作进程的消费，下面的这些事件都会触发 Deployment 的同步： Deployment 的变动； Deployment 相关的 ReplicaSet 变动； Deployment 相关的 Pod 数量为 0 时，Pod 的删除事件； DeploymentController 会在调用 Run 方法时启动多个工作进程，这些工作进程会运行 worker 方法从队列中读取最新的 Deployment 对象进行同步。 同步 Deployment 对象的同步都是通过以下的 syncDeployment 方法进行的，该方法包含了同步、回滚以及更新的逻辑，是同步 Deployment 资源的唯一入口： func (dc *DeploymentController) syncDeployment(key string) error { namespace, name, _ := cache.SplitMetaNamespaceKey(key) deployment, _ := dc.dLister.Deployments(namespace).Get(name) d := deployment.DeepCopy() rsList, _ := dc.getReplicaSetsForDeployment(d) podMap, _ := dc.getPodMapForDeployment(d, rsList) dc.checkPausedConditions(d) if d.Spec.Paused { return dc.sync(d, rsList) } scalingEvent, _ := dc.isScalingEvent(d, rsList) if scalingEvent { return dc.sync(d, rsList) } switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) } return fmt.Errorf(\"unexpected deployment strategy type: %s\", d.Spec.Strategy.Type) } Go 根据传入的键获取 Deployment 资源； 调用 getReplicaSetsForDeployment 获取集群中与 Deployment 相关的全部 ReplicaSet； 查找集群中的全部 ReplicaSet； 根据 Deployment 的选择器对 ReplicaSet 建立或者释放从属关系； 调用 getPodMapForDeployment 获取当前 Deployment 对象相关的从 ReplicaSet 到 Pod 的映射； 根据选择器查找全部的 Pod； 根据 Pod 的控制器 ReplicaSet 对上述 Pod 进行分类； 如果当前的 Deployment 处于暂停状态或者需要进行扩容，就会调用 sync 方法同步 Deployment; 在正常情况下会根据规格中的策略对 Deployment 进行更新； Recreate 策略会调用 rolloutRecreate 方法，它会先杀掉所有存在的 Pod 后启动新的 Pod 副本； RollingUpdate 策略会调用 rolloutRolling 方法，根据 maxSurge 和 maxUnavailable 配置对 Pod 进行滚动更新； 这就是 Deployment 资源同步的主要流程，我们在这里可以关注一下 getReplicaSetsForDeployment 方法： func (dc *DeploymentController) getReplicaSetsForDeployment(d *apps.Deployment) ([]*apps.ReplicaSet, error) { rsList, _ := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything()) deploymentSelector, _ := metav1.LabelSelectorAsSelector(d.Spec.Selector) canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) { return dc.client.AppsV1().Deployments(d.Namespace).Get(d.Name, metav1.GetOptions{}) }) cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc) return cm.ClaimReplicaSets(rsList) } Go 该方法获取 Deployment 持有的 ReplicaSet 时会重新与集群中符合条件的 ReplicaSet 通过 ownerReferences 建立关系，执行的逻辑与 ReplicaSet 调用 AdoptPod/ReleasePod 几乎完全相同。 扩容 如果当前需要更新的 Deployment 经过 isScalingEvent 的检查发现更新事件实际上是一次扩容或者缩容，也就是 ReplicaSet 持有的 Pod 数量和规格中的 Replicas 字段并不一致，那么就会调用 sync 方法对 Deployment 进行同步： func (dc *DeploymentController) sync(d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) dc.scale(d, newRS, oldRSs) allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(allRSs, newRS, d) } Go 同步的过程其实比较简单，该方法会从 apiserver 中拿到当前 Deployment 对应的最新 ReplicaSet 和历史的 ReplicaSet 并调用 scale 方法开始扩容，scale 就是扩容需要执行的主要方法，我们将下面的方法分成几部分依次进行介绍： func (dc *DeploymentController) scale(deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error { if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil { if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) { return nil } dc.scaleReplicaSetAndRecordEvent(activeOrLatest, *(deployment.Spec.Replicas), deployment) return nil } if deploymentutil.IsSaturated(deployment, newRS) { for _, old := range controller.FilterActiveReplicaSets(oldRSs) { dc.scaleReplicaSetAndRecordEvent(old, 0, deployment) } return nil } Go 如果集群中只有一个活跃的 ReplicaSet，那么就会对该 ReplicaSet 进行扩缩容，但是如果不存在活跃的 ReplicaSet 对象，就会选择最新的 ReplicaSet 进行操作，这部分选择 ReplicaSet 的工作都是由 FindActiveOrLatest 和 scaleReplicaSetAndRecordEvent 共同完成的。 当调用 IsSaturated 方法发现当前的 Deployment 对应的副本数量已经饱和时就会删除所有历史版本 ReplicaSet 持有的 Pod 副本。 但是在 Deployment 使用滚动更新策略时，如果发现当前的 ReplicaSet 并没有饱和并且存在多个活跃的 ReplicaSet 对象就会按照比例分别对各个活跃的 ReplicaSet 进行扩容或者缩容： if deploymentutil.IsRollingUpdate(deployment) { allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS)) allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) allowedSize := int32(0) if *(deployment.Spec.Replicas) > 0 { allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment) } deploymentReplicasToAdd := allowedSize - allRSsReplicas var scalingOperation string switch { case deploymentReplicasToAdd > 0: sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs)) scalingOperation = \"up\" case deploymentReplicasToAdd Go 通过 FilterActiveReplicaSets 获取所有活跃的 ReplicaSet 对象； 调用 GetReplicaCountForReplicaSets 计算当前 Deployment 对应 ReplicaSet 持有的全部 Pod 副本个数； 根据 Deployment 对象配置的 Replicas 和最大额外可以存在的副本数 maxSurge 以计算 Deployment 允许创建的 Pod 数量； 通过 allowedSize 和 allRSsReplicas 计算出需要增加或者删除的副本数； 根据 deploymentReplicasToAdd 变量的符号对 ReplicaSet 数组进行排序并确定当前的操作时扩容还是缩容； 如果 deploymentReplicasToAdd > 0，ReplicaSet 将按照从新到旧的顺序依次进行扩容； 如果 deploymentReplicasToAdd ，ReplicaSet 将按照从旧到新的顺序依次进行缩容； maxSurge、maxUnavailable 是两个处理滚动更新时需要关注的参数，我们会在滚动更新一节中具体介绍。 deploymentReplicasAdded := int32(0) nameToSize := make(map[string]int32) for i := range allRSs { rs := allRSs[i] if deploymentReplicasToAdd != 0 { proportion := deploymentutil.GetProportion(rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded) nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion deploymentReplicasAdded += proportion } else { nameToSize[rs.Name] = *(rs.Spec.Replicas) } } Go 因为当前的 Deployment 持有了多个活跃的 ReplicaSet，所以在计算了需要增加或者删除的副本个数 deploymentReplicasToAdd 之后，就会为多个活跃的 ReplicaSet 分配每个 ReplicaSet 需要改变的副本数，GetProportion 会根据以下几个参数决定最后的结果: Deployment 期望的 Pod 副本数量； 需要新增或者减少的副本数量； Deployment 当前通过 ReplicaSet 持有 Pod 的总数量； Kubernetes 会在 getReplicaSetFraction 使用下面的公式计算每一个 ReplicaSet 在 Deployment 资源中的占比，最后会返回该 ReplicaSet 需要改变的副本数： 该结果又会与目前期望的剩余变化量进行对比，保证变化的副本数量不会超过期望值。 for i := range allRSs { rs := allRSs[i] // ... dc.scaleReplicaSet(rs, nameToSize[rs.Name], deployment, scalingOperation) } } return nil } Go 在 scale 方法的最后会直接调用 scaleReplicaSet 将每一个 ReplicaSet 都扩容或者缩容到我们期望的副本数： func (dc *DeploymentController) scaleReplicaSet(rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) { sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) if sizeNeedsUpdate || annotationsNeedUpdate { rsCopy := rs.DeepCopy() *(rsCopy.Spec.Replicas) = newScale deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) rs, _ = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(rsCopy) } return true, rs, err } Go 这里会直接修改目标 ReplicaSet 规格中的 Replicas 参数和注解 deployment.kubernetes.io/desired-replicas 的值并通过 API 请求更新当前的 ReplicaSet 对象： $ kubectl describe rs nginx-deployment-76bf4969df Name: nginx-deployment-76bf4969df Namespace: default Selector: app=nginx,pod-template-hash=76bf4969df Labels: app=nginx pod-template-hash=76bf4969df Annotations: deployment.kubernetes.io/desired-replicas=3 deployment.kubernetes.io/max-replicas=4 ... Bash 我们可以通过 describe 命令查看 ReplicaSet 的注解，其实能够发现当前 ReplicaSet 的期待副本数和最大副本数，deployment.kubernetes.io/desired-replicas 注解就是在上述方法中被 Kubernetes 的 DeploymentController 更新的。 重新创建 当 Deployment 使用的更新策略类型是 Recreate 时，DeploymentController 就会使用如下的 rolloutRecreate 方法对 Deployment 进行更新： func (dc *DeploymentController) rolloutRecreate(d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID]*v1.PodList) error { newRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) allRSs := append(oldRSs, newRS) activeOldRSs := controller.FilterActiveReplicaSets(oldRSs) scaledDown, _ := dc.scaleDownOldReplicaSetsForRecreate(activeOldRSs, d) if scaledDown { return dc.syncRolloutStatus(allRSs, newRS, d) } if oldPodsRunning(newRS, oldRSs, podMap) { return dc.syncRolloutStatus(allRSs, newRS, d) } if newRS == nil { newRS, oldRSs, _ = dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) allRSs = append(oldRSs, newRS) } dc.scaleUpNewReplicaSetForRecreate(newRS, d) if util.DeploymentComplete(d, &d.Status) { dc.cleanupDeployment(oldRSs, d) } return dc.syncRolloutStatus(allRSs, newRS, d) } Go 利用 getAllReplicaSetsAndSyncRevision 和 FilterActiveReplicaSets 两个方法获取 Deployment 中所有的 ReplicaSet 以及其中活跃的 ReplicaSet 对象； 调用 scaleDownOldReplicaSetsForRecreate 方法将所有活跃的历史 ReplicaSet 持有的副本 Pod 数目降至 0； 同步 Deployment 的最新状态并等待 Pod 的终止； 在需要时通过 getAllReplicaSetsAndSyncRevision 方法创建新的 ReplicaSet 并调用 scaleUpNewReplicaSetForRecreate 函数对 ReplicaSet 进行扩容； 更新完成之后会调用 cleanupDeployment 方法删除历史全部的 ReplicaSet 对象并更新 Deployment 的状态； 也就是说在更新的过程中，之前创建的 ReplicaSet 和 Pod 资源全部都会被删除，只是 Pod 会先被删除而 ReplicaSet 会后被删除；上述方法也会创建新的 ReplicaSet 和 Pod 对象，需要注意的是在这个过程中旧的 Pod 副本一定会先被删除，所以会有一段时间不存在可用的 Pod。 滚动更新 Deployment 的另一个更新策略 RollingUpdate 其实更加常见，在具体介绍滚动更新的流程之前，我们首先需要了解滚动更新策略使用的两个参数 maxUnavailable 和 maxSurge： maxUnavailable 表示在更新过程中能够进入不可用状态的 Pod 的最大值； maxSurge 表示能够额外创建的 Pod 个数； maxUnavailable 和 maxSurge 这两个滚动更新的配置都可以使用绝对值或者百分比表示，使用百分比时需要用 Replicas * Strategy.RollingUpdate.MaxSurge 公式计算相应的数值。 rolloutRolling 方法就是 DeploymentController 用于处理滚动更新的方法： func (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) allRSs := append(oldRSs, newRS) scaledUp, _ := dc.reconcileNewReplicaSet(allRSs, newRS, d) if scaledUp { return dc.syncRolloutStatus(allRSs, newRS, d) } scaledDown, _ := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if scaledDown { return dc.syncRolloutStatus(allRSs, newRS, d) } if deploymentutil.DeploymentComplete(d, &d.Status) { dc.cleanupDeployment(oldRSs, d) } return dc.syncRolloutStatus(allRSs, newRS, d) } Go 首先获取 Deployment 对应的全部 ReplicaSet 资源； 通过 reconcileNewReplicaSet 调解新 ReplicaSet 的副本数，创建新的 Pod 并保证额外的副本数量不超过 maxSurge； 通过 reconcileOldReplicaSets 调解历史 ReplicaSet 的副本数，删除旧的 Pod 并保证不可用的部分数不会超过 maxUnavailable； 最后删除无用的 ReplicaSet 并更新 Deployment 的状态； 需要注意的是，在滚动更新的过程中，Kubernetes 并不是一次性就切换到期望的状态，即『新 ReplicaSet 运行指定数量的副本』，而是会先启动新的 ReplicaSet 以及一定数量的 Pod 副本，然后删除历史 ReplicaSet 中的副本，再启动一些新 ReplicaSet 的副本，不断对新 ReplicaSet 进行扩容并对旧 ReplicaSet 进行缩容最终达到了集群期望的状态。 当我们使用如下的 reconcileNewReplicaSet 方法对新 ReplicaSet 进行调节时，我们会发现在新 ReplicaSet 中副本数量满足期望时会直接返回，在超过期望时会进行缩容： func (dc *DeploymentController) reconcileNewReplicaSet(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) { return false, nil } if *(newRS.Spec.Replicas) > *(deployment.Spec.Replicas) { scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment) return scaled, err } newReplicasCount, _ := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS) scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment) return scaled, err } Go 如果 ReplicaSet 的数量不够就会调用 NewRSNewReplicas 函数计算新的副本个数，计算的过程使用了如下所示的公式： maxTotalPods = deployment.Spec.Replicas + currentPodCount = sum(deployement.ReplicaSets.Replicas) scaleUpCount = maxTotalPods - currentPodCount scaleUpCount = min(scaleUpCount, deployment.Spec.Replicas - newRS.Spec.Replicas)) newRSNewReplicas = newRS.Spec.Replicas + scaleUpCount Go 该过程总共需要考虑 Deployment 期望的副本数量、当前可用的副本数量以及新 ReplicaSet 持有的副本，还有一些最大值和最小值的限制，例如额外 Pod 数量不能超过 maxSurge、新 ReplicaSet 的 Pod 数量不能超过 Deployment 的期望数量，遵循这些规则我们就能计算出 newRSNewReplicas。 另一个滚动更新中使用的方法 reconcileOldReplicaSets 主要作用就是对历史 ReplicaSet 对象持有的副本数量进行缩容： func (dc *DeploymentController) reconcileOldReplicaSets(allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs) if oldPodsCount == 0 { return false, nil } allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) minAvailable := *(deployment.Spec.Replicas) - maxUnavailable newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount if maxScaledDown 0, nil } Go 计算历史 ReplicaSet 持有的副本总数量； 计算全部 ReplicaSet 持有的副本总数量； 根据 Deployment 期望的副本数、最大不可用副本数以及新 ReplicaSet 中不可用的 Pod 数量计算最大缩容的副本个数； 通过 cleanupUnhealthyReplicas 方法清理 ReplicaSet 中处于不健康状态的副本； 调用 scaleDownOldReplicaSetsForRollingUpdate 方法对历史 ReplicaSet 中的副本进行缩容； minAvailable = deployment.Spec.Replicas - maxUnavailable(deployment) maxScaledDown = allPodsCount - minAvailable - newReplicaSetPodsUnavailable Go 该方法会使用上述简化后的公式计算这次总共能够在历史 ReplicaSet 中删除的最大 Pod 数量，并调用 cleanupUnhealthyReplicas 和 scaleDownOldReplicaSetsForRollingUpdate 两个方法进行缩容，这两个方法的实现都相对简单，它们都对历史 ReplicaSet 按照创建时间进行排序依次对这些资源进行缩容，两者的区别在于前者主要用于删除不健康的副本。 回滚 Kubernetes 中的每一个 Deployment 资源都包含有 revision 这个概念，版本的引入可以让我们在更新发生问题时及时通过 Deployment 的版本对其进行回滚，当我们在更新 Deployment 时，之前 Deployment 持有的 ReplicaSet 其实会被 cleanupDeployment 方法清理： func (dc *DeploymentController) cleanupDeployment(oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error { aliveFilter := func(rs *apps.ReplicaSet) bool { return rs != nil && rs.ObjectMeta.DeletionTimestamp == nil } cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter) diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit if diff rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil { continue } dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(rs.Name, nil) } return nil } Go Deployment 资源在规格中由一个 spec.revisionHistoryLimit 的配置，这个配置决定了 Kubernetes 会保存多少个 ReplicaSet 的历史版本，这些历史上的 ReplicaSet 并不会被删除，它们只是不再持有任何的 Pod 副本了，假设我们有一个 spec.revisionHistoryLimit=2 的 Deployment 对象，那么当前资源最多持有两个历史的 ReplicaSet 版本： 这些资源的保留能够方便 Deployment 的回滚，而回滚其实是通过 kubectl 在客户端实现的，我们可以使用如下的命令将 Deployment 回滚到上一个版本： $ kubectl rollout undo deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment Bash 上述 kubectl 命令没有指定回滚到的版本号，所以在默认情况下会回滚到上一个版本，在回滚时会直接根据传入的版本查找历史的 ReplicaSet 资源，拿到这个 ReplicaSet 对应的 Pod 模板后会触发一个资源更新的请求： func (r *DeploymentRollbacker) Rollback(obj runtime.Object, updatedAnnotations map[string]string, toRevision int64, dryRun bool) (string, error) { accessor, _ := meta.Accessor(obj) name := accessor.GetName() namespace := accessor.GetNamespace() deployment, _ := r.c.AppsV1().Deployments(namespace).Get(name, metav1.GetOptions{}) rsForRevision, _ := deploymentRevision(deployment, r.c, toRevision) annotations := ... patchType, patch, _ := getDeploymentPatch(&rsForRevision.Spec.Template, annotations) r.c.AppsV1().Deployments(namespace).Patch(name, patchType, patch) return rollbackSuccess, nil } Go 回滚对于 Kubernetes 服务端来说其实与其他的更新操作没有太多的区别，在每次更新时都会在 FindNewReplicaSet 函数中根据 Deployment 的 Pod 模板在历史 ReplicaSet 中查询是否有相同的 ReplicaSet 存在： func FindNewReplicaSet(deployment *apps.Deployment, rsList []*apps.ReplicaSet) *apps.ReplicaSet { sort.Sort(controller.ReplicaSetsByCreationTimestamp(rsList)) for i := range rsList { if EqualIgnoreHash(&rsList[i].Spec.Template, &deployment.Spec.Template) { return rsList[i] } } return nil } Go 如果存在规格完全相同的 ReplicaSet，就会保留这个 ReplicaSet 历史上使用的版本号并对该 ReplicaSet 重新进行扩容并对正在工作的 ReplicaSet 进行缩容以实现集群的期望状态。 $ k describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 21 Feb 2019 10:14:29 +0800 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 11 kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"nginx\"},\"name\":\"nginx-deployment\",\"namespace\":\"d... Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 20s deployment-controller Scaled up replica set nginx-deployment-5cc74f885d to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-7c6cf994f6 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-5cc74f885d to 2 Normal ScalingReplicaSet 17s deployment-controller Scaled down replica set nginx-deployment-7c6cf994f6 to 1 Normal ScalingReplicaSet 17s deployment-controller Scaled up replica set nginx-deployment-5cc74f885d to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-7c6cf994f6 to 0 Bash 在之前的 Kubernetes 版本中，客户端还会使用注解来实现 Deployment 的回滚，但是在最新的 kubectl 版本中这种使用注解的方式已经被废弃了。 暂停和恢复 Deployment 中有一个不是特别常用的功能，也就是 Deployment 进行暂停，暂停之后的 Deployment 哪怕发生了改动也不会被 Kubernetes 更新，这时我们可以对 Deployment 资源进行更新或者修复，随后当重新恢复 Deployment 时，DeploymentController 才会重新对其进行滚动更新向期望状态迁移： func defaultObjectPauser(obj runtime.Object) ([]byte, error) { switch obj := obj.(type) { case *appsv1.Deployment: if obj.Spec.Paused { return nil, errors.New(\"is already paused\") } obj.Spec.Paused = true return runtime.Encode(scheme.Codecs.LegacyCodec(appsv1.SchemeGroupVersion), obj) // ... default: return nil, fmt.Errorf(\"pausing is not supported\") } } Go 暂停和恢复也都是由 kubectl 在客户端实现的，其实就是通过更改 spec.paused 属性，这里的更改会变成一个更新操作修改 Deployment 资源。 $ kubectl rollout pause deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment paused $ kubectl get deployments.apps nginx-deployment -o yaml apiVersion: apps/v1 kind: Deployment metadata: # ... name: nginx-deployment namespace: default selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment uid: 6b44965f-357e-11e9-af24-0800275e8310 spec: paused: true # ... Bash 如果我们使用 YAML 文件和 kubectl apply 命令来更新整个 Deployment 资源，那么其实用不到暂停这一功能，我们只需要在文件里对资源进行修改并进行一次更新就可以了，但是我们可以在出现问题时，暂停一次正在进行的滚动更新以防止错误的扩散。 删除 如果我们在 Kubernetes 集群中删除了一个 Deployment 资源，那么 Deployment 持有的 ReplicaSet 以及 ReplicaSet 持有的副本都会被 Kubernetes 中的 垃圾收集器 删除： $ kubectl delete deployments.apps nginx-deployment deployment.apps \"nginx-deployment\" deleted $ kubectl get replicasets --watch nginx-deployment-7c6cf994f6 0 0 0 2d1h nginx-deployment-5cc74f885d 0 0 0 2d1h nginx-deployment-c5d875444 3 3 3 30h $ kubectl get pods --watch nginx-deployment-c5d875444-6r4q6 1/1 Terminating 2 30h nginx-deployment-c5d875444-7ssgj 1/1 Terminating 2 30h nginx-deployment-c5d875444-4xvvz 1/1 Terminating 2 30h Go 由于与当前 Deployment 有关的 ReplicaSet 历史和最新版本都会被删除，所以对应的 Pod 副本也都会随之被删除，这些对象之间的关系都是通过 metadata.ownerReference 这一字段关联的，垃圾收集器 一节详细介绍了它的实现原理。 总结 Deployment 是 Kubernetes 中常用的对象类型，它解决了 ReplicaSet 更新的诸多问题，通过对 ReplicaSet 和 Pod 进行组装支持了滚动更新、回滚以及扩容等高级功能，通过对 Deployment 的学习既能让我们了解整个常见资源的实现也能帮助我们理解如何将 Kubernetes 内置的对象组合成更复杂的自定义资源。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/":{"url":"blog/kubernetes/K8s源码/","title":"K8s源码","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/IPVS 模式的工作原理.html":{"url":"blog/kubernetes/K8s源码/IPVS 模式的工作原理.html","title":"IPVS 模式的工作原理","keywords":"","body":"这一次，彻底搞懂 kube-proxy IPVS 模式的工作原理！ > > 作者：Dustin Specker 译者：米开朗基杨 译者简介：KubeSphere 布道师，目前就职于青云科技 KubeSphere 团队，是一名云原生技术爱好者，专注于 Kubernetes 与 KubeSphere 等云原生技术干货分享，你可以通过微信与我联系，我的微信号是 cloud-native-yang，欢迎前来围观我的朋友圈 Kubernetes 中的 Service 就是一组同 label 类型 Pod 的服务抽象，为服务提供了负载均衡和反向代理能力，在集群中表示一个微服务的概念。kube-proxy 组件则是 Service 的具体实现，了解了 kube-proxy 的工作原理，才能洞悉服务之间的通信流程，再遇到网络不通时也不会一脸懵逼。 kube-proxy 有三种模式：userspace、iptables 和 IPVS，其中 userspace 模式不太常用。iptables 模式最主要的问题是在服务多的时候产生太多的 iptables 规则，非增量式更新会引入一定的时延，大规模情况下有明显的性能问题。为解决 iptables 模式的性能问题，v1.11 新增了 IPVS 模式（v1.8 开始支持测试版，并在 v1.11 GA），采用增量式更新，并可以保证 service 更新期间连接保持不断开。 目前网络上关于 kube-proxy 工作原理的文档几乎都是以 iptables 模式为例，很少提及 IPVS，本文就来破例解读 kube-proxy IPVS 模式的工作原理。为了理解地更加彻底，本文不会使用 Docker 和 Kubernetes，而是使用更加底层的工具来演示。 我们都知道，Kubernetes 会为每个 Pod 创建一个单独的网络命名空间 (Network Namespace) ，本文将会通过手动创建网络命名空间并启动 HTTP 服务来模拟 Kubernetes 中的 Pod。 本文的目标是通过模拟以下的 Service 来探究 kube-proxy 的 IPVS 和 ipset 的工作原理： apiVersion: v1 kind: Service metadata: name: app-service spec: clusterIP: 10.100.100.100 selector: component: app ports: - protocol: TCP port: 8080 targetPort: 8080 跟着我的步骤，最后你就可以通过命令 curl 10.100.100.100:8080 来访问某个网络命名空间的 HTTP 服务。为了更好地理解本文的内容，推荐提前阅读以下的文章： How do Kubernetes and Docker create IP Addresses? iptables: How Docker Publishes Ports iptables: How Kubernetes Services Direct Traffic to Pods 注意：本文所有步骤皆是在 Ubuntu 20.04 中测试的，其他 Linux 发行版请自行测试。 准备实验环境 首先需要开启 Linux 的路由转发功能： $ sysctl --write net.ipv4.ip_forward=1 接下来的命令主要做了这么几件事： 创建一个虚拟网桥 bridge_home 创建两个网络命名空间 netns_dustin 和 netns_leah 为每个网络命名空间配置 DNS 创建两个 veth pair 并连接到 bridge_home 给 netns_dustin 网络命名空间中的 veth 设备分配一个 IP 地址为 10.0.0.11 给 netns_leah 网络命名空间中的 veth 设备分配一个 IP 地址为 10.0.021 为每个网络命名空间设定默认路由 添加 iptables 规则，允许流量进出 bridge_home 接口 添加 iptables 规则，针对 10.0.0.0/24 网段进行流量伪装 $ ip link add dev bridge_home type bridge $ ip address add 10.0.0.1/24 dev bridge_home $ ip netns add netns_dustin $ mkdir -p /etc/netns/netns_dustin echo \"nameserver 114.114.114.114\" | tee -a /etc/netns/netns_dustin/resolv.conf $ ip netns exec netns_dustin ip link set dev lo up $ ip link add dev veth_dustin type veth peer name veth_ns_dustin $ ip link set dev veth_dustin master bridge_home $ ip link set dev veth_dustin up $ ip link set dev veth_ns_dustin netns netns_dustin $ ip netns exec netns_dustin ip link set dev veth_ns_dustin up $ ip netns exec netns_dustin ip address add 10.0.0.11/24 dev veth_ns_dustin $ ip netns add netns_leah $ mkdir -p /etc/netns/netns_leah echo \"nameserver 114.114.114.114\" | tee -a /etc/netns/netns_leah/resolv.conf $ ip netns exec netns_leah ip link set dev lo up $ ip link add dev veth_leah type veth peer name veth_ns_leah $ ip link set dev veth_leah master bridge_home $ ip link set dev veth_leah up $ ip link set dev veth_ns_leah netns netns_leah $ ip netns exec netns_leah ip link set dev veth_ns_leah up $ ip netns exec netns_leah ip address add 10.0.0.21/24 dev veth_ns_leah $ ip link set bridge_home up $ ip netns exec netns_dustin ip route add default via 10.0.0.1 $ ip netns exec netns_leah ip route add default via 10.0.0.1 $ iptables --table filter --append FORWARD --in-interface bridge_home --jump ACCEPT $ iptables --table filter --append FORWARD --out-interface bridge_home --jump ACCEPT $ iptables --table nat --append POSTROUTING --source 10.0.0.0/24 --jump MASQUERADE 在网络命名空间 netns_dustin 中启动 HTTP 服务： $ ip netns exec netns_dustin python3 -m http.server 8080 打开另一个终端窗口，在网络命名空间 netns_leah 中启动 HTTP 服务： $ ip netns exec netns_leah python3 -m http.server 8080 测试各个网络命名空间之间是否能正常通信： $ curl 10.0.0.11:8080 $ curl 10.0.0.21:8080 $ ip netns exec netns_dustin curl 10.0.0.21:8080 $ ip netns exec netns_leah curl 10.0.0.11:8080 整个实验环境的网络拓扑结构如图： 安装必要工具 为了便于调试 IPVS 和 ipset，需要安装两个 CLI 工具： $ apt install ipset ipvsadm --yes 本文使用的 ipset 和 ipvsadm 版本分别为 7.5-1~exp1 和 1:1.31-1。 通过 IPVS 来模拟 Service 下面我们使用 IPVS 创建一个虚拟服务 (Virtual Service) 来模拟 Kubernetes 中的 Service : $ ipvsadm \\ --add-service \\ --tcp-service 10.100.100.100:8080 \\ --scheduler rr 这里使用参数 --tcp-service 来指定 TCP 协议，因为我们需要模拟的 Service 就是 TCP 协议。 IPVS 相比 iptables 的优势之一就是可以轻松选择调度算法，这里选择使用轮询调度算法。 目前 kube-proxy 只允许为所有 Service 指定同一个调度算法，未来将会支持为每一个 Service 选择不同的调度算法，详情可参考文章 IPVS-Based In-Cluster Load Balancing Deep Dive[4]。 创建了虚拟服务之后，还得给它指定一个后端的 Real Server，也就是后端的真实服务，即网络命名空间 netns_dustin 中的 HTTP 服务： $ ipvsadm \\ --add-server \\ --tcp-service 10.100.100.100:8080 \\ --real-server 10.0.0.11:8080 \\ --masquerading 该命令会将访问 10.100.100.100:8080 的 TCP 请求转发到 10.0.0.11:8080。这里的 --masquerading 参数和 iptables 中的 MASQUERADE 类似，如果不指定，IPVS 就会尝试使用路由表来转发流量，这样肯定是无法正常工作的。 测试是否正常工作： $ curl 10.100.100.100:8080 实验成功，请求被成功转发到了后端的 HTTP 服务！ 在网络命名空间中访问虚拟服务 上面只是在 Host 的网络命名空间中进行测试，现在我们进入网络命名空间 netns_leah 中进行测试： $ ip netns exec netns_leah curl 10.100.100.100:8080 哦豁，访问失败！ 要想顺利通过测试，只需将 10.100.100.100 这个 IP 分配给一个虚拟网络接口。至于为什么要这么做，目前我还不清楚，我猜测可能是因为网桥 bridge_home 不会调用 IPVS，而将虚拟服务的 IP 地址分配给一个网络接口则可以绕过这个问题。 dummy 接口 当然，我们不需要将 IP 地址分配给任何已经被使用的网络接口，我们的目标是模拟 Kubernetes 的行为。Kubernetes 在这里创建了一个 dummy 接口，它和 loopback 接口类似，但是你可以创建任意多的 dummy 接口。它提供路由数据包的功能，但实际上又不进行转发。dummy 接口主要有两个用途： 用于主机内的程序通信 由于 dummy 接口总是 up（除非显式将管理状态设置为 down），在拥有多个物理接口的网络上，可以将 service 地址设置为 loopback 接口或 dummy 接口的地址，这样 service 地址不会因为物理接口的状态而受影响。 看来 dummy 接口完美符合实验需求，那就创建一个 dummy 接口吧： $ ip link add dev dustin-ipvs0 type dummy 将虚拟 IP 分配给 dummy 接口 dustin-ipvs0 : $ ip addr add 10.100.100.100/32 dev dustin-ipvs0 到了这一步，仍然访问不了 HTTP 服务，还需要另外一个黑科技：bridge-nf-call-iptables。在解释 bridge-nf-call-iptables 之前，我们先来回顾下容器网络通信的基础知识。 基于网桥的容器网络 Kubernetes 集群网络有很多种实现，有很大一部分都用到了 Linux 网桥: 每个 Pod 的网卡都是 veth 设备，veth pair 的另一端连上宿主机上的网桥。 由于网桥是虚拟的二层设备，同节点的 Pod 之间通信直接走二层转发，跨节点通信才会经过宿主机 eth0。 Service 同节点通信问题 不管是 iptables 还是 ipvs 转发模式，Kubernetes 中访问 Service 都会进行 DNAT，将原本访问 ClusterIP:Port 的数据包 DNAT 成 Service 的某个 Endpoint (PodIP:Port)，然后内核将连接信息插入 conntrack 表以记录连接，目的端回包的时候内核从 conntrack 表匹配连接并反向 NAT，这样原路返回形成一个完整的连接链路: 但是 Linux 网桥是一个虚拟的二层转发设备，而 iptables conntrack 是在三层上，所以如果直接访问同一网桥内的地址，就会直接走二层转发，不经过 conntrack: Pod 访问 Service，目的 IP 是 Cluster IP，不是网桥内的地址，走三层转发，会被 DNAT 成 PodIP:Port。 如果 DNAT 后是转发到了同节点上的 Pod，目的 Pod 回包时发现目的 IP 在同一网桥上，就直接走二层转发了，没有调用 conntrack，导致回包时没有原路返回 (见下图)。 由于没有原路返回，客户端与服务端的通信就不在一个 “频道” 上，不认为处在同一个连接，也就无法正常通信。 开启 bridge-nf-call-iptables 启用 bridge-nf-call-iptables 这个内核参数 (置为 1)，表示 bridge 设备在二层转发时也去调用 iptables 配置的三层规则 (包含 conntrack)，所以开启这个参数就能够解决上述 Service 同节点通信问题。 所以这里需要启用 bridge-nf-call-iptables : $ modprobe br_netfilter $ sysctl --write net.bridge.bridge-nf-call-iptables=1 现在再来测试一下连通性： $ ip netns exec netns_leah curl 10.100.100.100:8080 终于成功了！ 开启 Hairpin（发夹弯）模式 虽然我们可以从网络命名空间 netns_leah 中通过虚拟服务成功访问另一个网络命名空间 netns_dustin 中的 HTTP 服务，但还没有测试过从 HTTP 服务所在的网络命名空间 netns_dustin 中直接通过虚拟服务访问自己，话不多说，直接测一把： $ ip netns exec netns_dustin curl 10.100.100.100:8080 啊哈？竟然失败了，这又是哪里的问题呢？不要慌，开启 hairpin 模式就好了。那么什么是 hairpin 模式呢？这是一个网络虚拟化技术中常提到的概念，也即交换机端口的 VEPA 模式。这种技术借助物理交换机解决了虚拟机间流量转发问题。很显然，这种情况下，源和目标都在一个方向，所以就是从哪里进从哪里出的模式。 怎么配置呢？非常简单，只需一条命令： $ brctl hairpin bridge_home veth_dustin on 再次进行测试： $ ip netns exec netns_dustin curl 10.100.100.100:8080 还是失败了。。。 然后我花了一个下午的时间，终于搞清楚了启用混杂模式后为什么还是不能解决这个问题，因为混杂模式和下面的选项要一起启用才能对 IPVS 生效： $ sysctl --write net.ipv4.vs.conntrack=1 最后再测试一次： $ ip netns exec netns_dustin curl 10.100.100.100:8080 这次终于成功了，但我还是不太明白为什么启用 conntrack 能解决这个问题，有知道的大神欢迎留言告诉我！ 开启混杂模式 如果想让所有的网络命名空间都能通过虚拟服务访问自己，就需要在连接到网桥的所有 veth 接口上开启 hairpin 模式，这也太麻烦了吧。有一个办法可以不用配置每个 veth 接口，那就是开启网桥的混杂模式。 什么是混杂模式呢？普通模式下网卡只接收发给本机的包（包括广播包）传递给上层程序，其它的包一律丢弃。混杂模式就是接收所有经过网卡的数据包，包括不是发给本机的包，即不验证 MAC 地址。 如果一个网桥开启了混杂模式，就等同于将所有连接到网桥上的端口（本文指的是 veth 接口）都启用了 hairpin 模式。可以通过以下命令来启用 bridge_home 的混杂模式： $ ip link set bridge_home promisc on 现在即使你把 veth 接口的 hairpin 模式关闭： $ brctl hairpin bridge_home veth_dustin off 仍然可以通过连通性测试： $ ip netns exec netns_dustin curl 10.100.100.100:8080 优化 MASQUERADE 在文章开头准备实验环境的章节，执行了这么一条命令： $ iptables \\ --table nat \\ --append POSTROUTING \\ --source 10.0.0.0/24 \\ --jump MASQUERADE 这条 iptables 规则会对所有来自 10.0.0.0/24 的流量进行伪装。然而 Kubernetes 并不是这么做的，它为了提高性能，只对来自某些具体的 IP 的流量进行伪装。 为了更加完美地模拟 Kubernetes，我们继续改造规则，先把之前的规则删除： $ iptables \\ --table nat \\ --delete POSTROUTING \\ --source 10.0.0.0/24 \\ --jump MASQUERADE 然后添加针对具体 IP 的规则： $ iptables \\ --table nat \\ --append POSTROUTING \\ --source 10.0.0.11/32 \\ --jump MASQUERADE 果然，上面的所有测试都能通过。先别急着高兴，又有新问题了，现在只有两个网络命名空间，如果有很多个怎么办，每个网络命名空间都创建这样一条 iptables 规则？我用 IPVS 是为了啥？就是为了防止有大量的 iptables 规则拖垮性能啊，现在岂不是又绕回去了。 不慌，继续从 Kubernetes 身上学习，使用 ipset 来解决这个问题。先把之前的 iptables 规则删除： $ iptables \\ --table nat \\ --delete POSTROUTING \\ --source 10.0.0.11/32 \\ --jump MASQUERADE 然后使用 ipset 创建一个集合 (set) ： $ ipset create DUSTIN-LOOP-BACK hash:ip,port,ip 这条命令创建了一个名为 DUSTIN-LOOP-BACK 的集合，它是一个 hashmap，里面存储了目标 IP、目标端口和源 IP。 接着向集合中添加条目： $ ipset add DUSTIN-LOOP-BACK 10.0.0.11,tcp:8080,10.0.0.11 现在不管有多少网络命名空间，都只需要添加一条 iptables 规则： $ iptables \\ --table nat \\ --append POSTROUTING \\ --match set \\ --match-set DUSTIN-LOOP-BACK dst,dst,src \\ --jump MASQUERADE 网络连通性测试也没有问题： $ curl 10.100.100.100:8080 $ ip netns exec netns_leah curl 10.100.100.100:8080 $ ip netns exec netns_dustin curl 10.100.100.100:8080 新增虚拟服务的后端 最后，我们把网络命名空间 netns_leah 中的 HTTP 服务也添加到虚拟服务的后端： $ ipvsadm \\ --add-server \\ --tcp-service 10.100.100.100:8080 \\ --real-server 10.0.0.21:8080 \\ --masquerading 再向 ipset 的集合 DUSTIN-LOOP-BACK 中添加一个条目： $ ipset add DUSTIN-LOOP-BACK 10.0.0.21,tcp:8080,10.0.0.21 终极测试来了，试着多运行几次以下的测试命令： $ curl 10.100.100.100:8080 你会发现轮询算法起作用了： 总结 相信通过本文的实验和讲解，大家应该理解了 kube-proxy IPVS 模式的工作原理。在实验过程中，我们还用到了 ipset，它有助于解决在大规模集群中出现的 kube-proxy 性能问题。如果你对这篇文章有任何疑问，欢迎和我进行交流。 参考文章 为什么 kubernetes 环境要求开启 bridge-nf-call-iptables ? [3]iptables: How Kubernetes Services Direct Traffic to Pods: https://dustinspecker.com/posts/iptables-how-kubernetes-services-direct-traffic-to-pods/ 4]IPVS-Based In-Cluster Load Balancing Deep Dive: https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/#ipvs-based-kube-proxy 原文链接：https://dustinspecker.com/posts/ipvs-how-kubernetes-s Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/calico 源码分析.html":{"url":"blog/kubernetes/K8s源码/calico 源码分析.html","title":"Calico 源码分析","keywords":"","body":"calico源码分析 2020年07月12日， 搬运中。。。。 简介 简介 calico node felix calico node 每个节点启动了一个名为calico-node 的daemonset（namespace=kube-system），calico-node 容器以runit 作为进程管理工具，运行多个进程 /etc/service/enabled /bird /bird6 /run /supervise /confd /run /supervise /felix /run /supervise bird, calico 中的 Bird是一个BGP client，它会读取host上的路由信息（由felix写入），然后通过BGP协议广播出去 confd, confd根据etcd上状态信息，与本地模板，生成并更新BIRD配置 felix, Felix Interface management, Felix programs some information about interfaces into the kernel in order to get the kernel to correctly handle the traffic emitted by that endpoint. In particular, it will ensure that the host responds to ARP requests from each workload with the MAC of the host, and will enable IP forwarding for interfaces that it manages.It also monitors for interfaces to appear and disappear so that it can ensure that the programming for those interfaces is applied at the appropriate time. Route programming, Felix is responsible for programming routes to the endpoints on its host into the Linux kernel FIB (Forwarding Information Base) . This ensures that packets destined for those endpoints that arrive on at the host are forwarded accordingly. ACL programming, Felix is also responsible for programming ACLs into the Linux kernel. These ACLs are used to ensure that only valid traffic can be sent between endpoints, and ensure that endpoints are not capable of circumventing Calico’s security measures. State reporting, Felix is responsible for providing data about the health of the network. In particular, it reports errors and problems with configuring its host. This data is written into etcd, to make it visible to other components and operators of the network. /etc/service/enabled/felix/run #!/bin/sh exec 2>&1 # Felix doesn't understand NODENAME, but the container exports it as a common # interface. This ensures Felix gets the right name for the node. if [ ! -z $NODENAME ]; then export FELIX_FELIXHOSTNAME=$NODENAME fi export FELIX_ETCDADDR=$ETCD_AUTHORITY export FELIX_ETCDENDPOINTS=$ETCD_ENDPOINTS export FELIX_ETCDSCHEME=$ETCD_SCHEME export FELIX_ETCDCAFILE=$ETCD_CA_CERT_FILE export FELIX_ETCDKEYFILE=$ETCD_KEY_FILE export FELIX_ETCDCERTFILE=$ETCD_CERT_FILE # Felix hangs if DATASTORETYPE is empty: see projectcalico/felix issue #1156. if [ ! -z $DATASTORE_TYPE ]; then export FELIX_DATASTORETYPE=$DATASTORE_TYPE fi exec calico-node -felix /etc/service/enabled/confd/run #!/bin/sh exec 2>&1 exec calico-node -confd /etc/service/enabled/bird6/run #!/bin/sh exec 2>&1 exec bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg // github.com/projectcalico/node/cmd/calico-node/main.go func main() { ... if *version { fmt.Println(startup.VERSION) os.Exit(0) } else if *runFelix { logrus.SetFormatter(&logutils.Formatter{Component: \"felix\"}) felix.Run(\"/etc/calico/felix.cfg\", buildinfo.GitVersion, buildinfo.GitRevision, buildinfo.BuildDate) } else if *runConfd { logrus.SetFormatter(&logutils.Formatter{Component: \"confd\"}) cfg, err := confdConfig.InitConfig(true) if err != nil { panic(err) } cfg.ConfDir = \"/etc/calico/confd\" cfg.KeepStageFile = *confdKeep cfg.Onetime = *confdRunOnce confd.Run(cfg) } ... } felix felix 需要与k8s apiserver 通信获取pod 数据，若是集群节点太多，会给apiserver 带来较大负担，所以calico 提供typha 机制，缓存apiserver 数据，减少felix 对apiserver的压力。 calico node felix源码分析之二Syncer 协程负责监听 datastore 中的更新，并将更新的内容通过 channel 发送给 Validator 协程。Validator 完成校验后，将其发送给 Calc graph 协程。Calc graph 完成计算后，发送给dataplane协程。最后dataplane完成数据平面处理。 felix 主要的工作组件就是syncer, CalcGraph, dataplane syncer, calicoctl 可以直接向datastore crud 一系列Resource，参见Resource definitions，代码定义在github.com/projectcalico/libcalico-go/lib/backend/model。syncer 同步且监听这些Resource，当资源变动时，通过回调onUpdate 通知下游组件（比如CalcGraph）。 CalcGraph, syncer 弄过来的 datastore的数据 通常不能直接使用，需要CalcGraph 做一些计算和归并 再交给dataplane dataplane, 负责真正对 node 做出处理， 分为 本地和远程（使用grpc 通信）两种形态，为此定义了一个 proto，github.com/projectcalico/felix/proto/felixbackend.pb.go ，如果是本地运行，则通过channel 直接传输 proto model，如果是dataplane 远程独立运行，则执行grpc 调用。InternalDataplane implements an in-process Felix dataplane driver based on iptables and ipsets. It communicates with the datastore-facing part of Felix via the Send/RecvMessage methods, which operate on the protobuf-defined API objects. dataplane 一方面通过ifaceMonitor 监听网卡，发出 ifaceUpdates, ifaceAddrUpdates 事件；另一方面接收 来自syncer/CalcGraph 的proto model。 dataplane 聚合一系列 Manager(felix 剩下的大部分代码 都是按manager 分包的)， 监听上述两类事件变化并执行。 github.com/projectcalico/felix/daemon/daemon.go // Run is the entry point to run a Felix instance. // // Its main role is to sequence Felix's startup by: // // Initialising early logging config (log format and early debug settings). // // Parsing command line parameters. // // Loading datastore configuration from the environment or config file. // // Loading more configuration from the datastore (this is retried until success). // // Starting the configured internal (golang) or external dataplane driver. // // Starting the background processing goroutines, which load and keep in sync with the // state from the datastore, the \"calculation graph\". // // Starting the usage reporting and prometheus metrics endpoint threads (if configured). // // Then, it defers to monitorAndManageShutdown(), which blocks until one of the components // fails, then attempts a graceful shutdown. At that point, all the processing is in // background goroutines. // // To avoid having to maintain rarely-used code paths, Felix handles updates to its // main config parameters by exiting and allowing itself to be restarted by the init // daemon. func Run(configFile string, gitVersion string, buildDate string, gitRevision string) { ... } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/controller-runtime.html":{"url":"blog/kubernetes/K8s源码/controller-runtime.html","title":"Controller Runtime","keywords":"","body":"简介 简介 kubebuilder 和controller-runtime 的关系 示例demo controller-runtime 整体设计 源码分析 启动流程 Manager 启动 Controller 逻辑 Reconciler 开发模式 其它 依赖注入 入队器 其它 Kubernetes 这样的分布式操作系统对外提供服务是通过 API 的形式，分布式操作系统本身提供的 API 相当于单机操作系统的系统调用。Kubernetes 本身提供的API，通过名为 Controller 的组件来支持，由开发者为 Kubernetes 提供的新的 API，则通过 Operator 来支持，Operator 本身和 Controller 基于同一套机制开发。Kubernetes 的工作机制和单机操作系统也较为相似，etcd 提供一个 watch 机制，Controller 和 Operator 需要指定自己 watch 哪些内容，并告诉 etcd，这相当于是微内核架构在 IDT 或 SCV 中注册系统调用的过程。 Kubebuilder：让编写 CRD 变得更简单K8s 作为一个“容器编排”平台，其核心的功能是编排，Pod 作为 K8s 调度的最小单位,具备很多属性和字段，K8s 的编排正是通过一个个控制器根据被控制对象的属性和字段来实现。PS：再具体点就是 crud pod及其属性字段 对于用户来说，实现 CRD 扩展主要做两件事： 编写 CRD 并将其部署到 K8s 集群里；这一步的作用就是让 K8s 知道有这个资源及其结构属性，在用户提交该自定义资源的定义时（通常是 YAML 文件定义），K8s 能够成功校验该资源并创建出对应的 Go struct 进行持久化，同时触发控制器的调谐逻辑。 编写 Controller 并将其部署到 K8s 集群里。这一步的作用就是实现调谐逻辑。 面向 K8s 设计误区 kubebuilder Kubebuilder中文文档 对理解k8s 上下游知识以及使用kubebuiler 编写控制器很有帮助。 和controller-runtime 的关系 对于 CRD Controller 的构建，有几个主流的工具 coreOS 开源的 Operator-SDK（https://github.com/operator-framework/operator-sdk ） K8s 兴趣小组维护的 Kubebuilder（https://github.com/kubernetes-sigs/kubebuilder ） kubebuilder 是一个用来帮助用户快速实现 Kubernetes CRD Operator 的 SDK。当然，kubebuilder 也不是从0 生成所有controller 代码，k8s 提供给一个 Kubernetes controller-runtime Project a set of go libraries for building Controllers. controller-runtime 在Operator SDK中也有被用到。 有点类似于spring/controller-runtime提供核心抽象,springboot/kubebuilder 将一切集成起来，我们只需要实现 Reconcile 方法即可。 type Reconciler interface { // Reconciler performs a full reconciliation for the object referred to by the Request.The Controller will requeue the Request to be processed again if an error is non-nil or Result.Requeue is true, otherwise upon completion it will remove the work from the queue. Reconcile(Request) (Result, error) } 示例demo 在 GOPATH/src/app 创建脚手架工程 kubebuilder init --domain example.io GOPATH/src/app /config // 跟k8s 集群交互所需的一些yaml配置 /certmanager /default /manager /prometheus /rbac /webhook main.go // 创建并启动 Manager，容器的entrypoint Dockerfile // 制作Controller 镜像 go.mod module app go 1.13 require ( k8s.io/apimachinery v0.17.2 k8s.io/client-go v0.17.2 sigs.k8s.io/controller-runtime v0.5.0 ) 创建 API kubebuilder create api --group apps --version v1alpha1 --kind Application 后文件变化 GOPATH/src/app /api/v1alpha1 /application_types.go // 新增 Application/ApplicationSpec/ApplicationStatus struct; 将类型注册到 scheme 辅助接口 /zz_generated.deepcopy.go /config /crd // Application CustomResourceDefinition。提交后apiserver 可crudw该crd /... /controllers /application_controller.go // 定义 ApplicationReconciler ，核心逻辑就在这里实现 main.go // ApplicationReconciler 添加到 Manager，Manager.Start(stopCh) go.mod 执行 make install 实质是执行 kustomize build config/crd | kubectl apply -f - 将cr yaml 提交到apiserver上。之后就可以 提交Application yaml 到 k8s 了。将crd struct 注册到 schema，则client-go 可以支持对crd的 crudw 等操作。 controller-runtime 整体设计 controller-runtime 之控制器实现 controller-runtime 之 manager 实现 当我们谈到 k8s 的控制器模型时，其伪代码如下 for { actualState := GetResourceActualState(rsvc) expectState := GetResourceExpectState(rsvc) // 来自yaml 文件 if actualState == expectState { // do nothing } else { Reconcile(rsvc) // 编排逻辑，调谐的最终结果一般是对被控制对象的某种写操作，比如增/删/改 Pod } } Control Loop通过code-generator生成，开发者编写差异处理逻辑Reconcile即可。controller-runtime 代码结构 /sigs.k8s.io/controller-runtime/pkg /manager /manager.go // 定义了 Manager interface /internal.go // 定义了Manager 实现类 controllerManager /controller /controller.go // 定义了 Controller interface /reconcile /reconcile.go // 定义了 Reconciler interface /handler // 事件处理器/入队器，负责将informer 的cud event 转换为reconcile.Request加入到queue中 controller-runtime 的核心是Manager 驱动 Controller 进而驱动 Reconciler。kubebuiler 用Manager.start 作为驱动入口， Reconciler 作为自定义入口（变的部分），Controller 是不变的部分。 Manager 管理多个Controller 的运行，并提供 数据读（cache）写（client）等crudw基础能力。 Cache, 负责在 Controller 进程里面根据 Scheme 同步 Api Server 中所有该 Controller 关心 的资源对象，其核心是 相关Resource的 Informer,Informer 会负责监听对应 Resource的创建/删除/更新操作，以触发 Controller 的 Reconcile 逻辑。 Clients, Reconciler不可避免地需要对某些资源类型进行crud，就是通过该 Clients 实现的，其中查询功能实际查询是本地的 Cache，写操作直接访问 Api Server。 一个controller中只有一个Reconciler。A Controller manages a work queue fed reconcile.Requests from source.Sources. Work is performed through the reconcile.Reconciler for each enqueued item. Work typically is reads and writes Kubernetes objects to make the system state match the state specified in the object Spec. 源码分析 启动流程 启动逻辑比较简单： 初始化Manager；初始化流程主要是创建cache 与 Clients。 创建 Cache，可以看到 Cache 主要就是创建了 InformersMap，Scheme 里面的每个 GVK 都创建了对应的 Informer，通过 informersByGVK 这个 map 做 GVK 到 Informer 的映射，每个 Informer 会根据 ListWatch 函数对对应的 GVK 进行 List 和 Watch。 创建 Clients，读操作使用上面创建的 Cache，写操作使用 K8s go-client 直连。 将 Manager 的 Client 传给 Controller，并且调用 SetupWithManager 方法传入 Manager 进行 Controller 的初始化； 启动 Manager。即 Manager.Start(stopCh) func main() { ... // 1. init Manager mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: scheme, Port: 9443,}) // 2. init Reconciler（Controller） if err = (&controllers.ApplicationReconciler{ Client: mgr.GetClient(), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err != nil {...} // 3. start Manager if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {...} } ApplicationReconciler 是我们定义的Application object 对应的Reconciler 实现。ApplicationReconciler.SetupWithManager 有点绕，把构造Controller 以及Reconciler/Controller/Manager 一步做掉了。 // kubebuilder 生成 func (r *ApplicationReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). // 生成Controller Builder 对象 For(&appsv1alpha1.Application{}). // 为Controller指定cr Complete(r) // 为Controller指定Reconciler } // sigs.k8s.io/controller-runtime/pkg/controller/controller.go type Controller struct { Name string // Name is used to uniquely identify a Controller in tracing, logging and monitoring. Name is required. Do reconcile.Reconciler Client client.Client // Client is a lazily initialized Client. Scheme *runtime.Scheme Cache cache.Cache Config *rest.Config // Config is the rest.Config used to talk to the apiserver. // Queue is an listeningQueue that listens for events from Informers and adds object keys to the Queue for processing Queue workqueue.RateLimitingInterface ... } // Complete ==> Build func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) { // Set the Config blder.loadRestConfig() // Set the ControllerManagedBy if err := blder.doController(r); err != nil {return nil, err} // Set the Watch if err := blder.doWatch(); err != nil {return nil, err} return blder.ctrl, nil } func New(name string, mgr manager.Manager, options Options) (Controller, error) { ... // Inject dependencies into Reconciler if err := mgr.SetFields(options.Reconciler); err != nil {...} c := &controller.Controller{ // Create controller with dependencies set Do: options.Reconciler, Cache: mgr.GetCache(), Config: mgr.GetConfig(), Scheme: mgr.GetScheme(), Client: mgr.GetClient(), Name: name, } return c, mgr.Add(c) // Add the controller as a Manager components } Manager 启动 Manager 可以管理 Runnable/Controller 的生命周期（添加/启动），持有Controller共同的依赖：client、cache、scheme 等。提供了object getter(例如GetClient())，还有一个简单的依赖注入机制(runtime/inject)，还支持领导人选举，提供了一个用于优雅关闭的信号处理程序。 func (cm *controllerManager) Start(stop cm.startCache = cm.cache.Start ==> InformersMap.Start ==> // InformersMap.structured/unstructured.Start ==> Informer.Run cm.waitForCache() for _, c := range cm.leaderElectionRunnables { ctrl := c go func() { // 启动Controller if err := ctrl.Start(cm.internalStop); err != nil {...} }() } } Controller 逻辑 Controller 逻辑主要有两个（任何Controller 都是如此） 监听 object 事件并加入到 queue 中。为提高扩展性 Controller 将这个职责独立出来交给了 Source 组件，不只是监听apiserver，任何外界资源变动 都可以通过 Source 接口加入 到Reconcile 逻辑中。 从queue 中取出object event 执行Reconcile 逻辑。 func (c *Controller) Start(stop 0 { c.Queue.Forget(obj) c.Queue.AddAfter(req, result.RequeueAfter) return true } else if result.Requeue { c.Queue.AddRateLimited(req) return true } c.Queue.Forget(obj) return true } Reconciler 开发模式 type ApplicationReconciler struct { client.Client Scheme *runtime.Scheme Log logr.Logger } type Reconciler interface { Reconcile(Request) (Result, error) } type Request struct { types.NamespacedName } type NamespacedName struct { Namespace string Name string } type Result struct { Requeue bool RequeueAfter time.Duration } ApplicationReconciler 持有Client，便有能力对 相关资源进行 crud 从request 中资源name ，进而通过client 获取资源obj 处理完毕后，通过Result 告知是Requeue 下次重新处理，还是处理成功开始下一个 其它 依赖注入 controllerManager 持有了Config/Client/APIReader/Scheme/Cache/Injector/StopChannel/Mapper 实例，将这些数据通过 SetFields 注入到Controller 中。Controller 再转手 将部分实例注入到 Source 中（Source 需要监听apiserver） func (cm *controllerManager) SetFields(i interface{}) error { if _, err := inject.ConfigInto(cm.config, i); err != nil {return err} if _, err := inject.ClientInto(cm.client, i); err != nil {return err} if _, err := inject.APIReaderInto(cm.apiReader, i); err != nil {return err} if _, err := inject.SchemeInto(cm.scheme, i); err != nil {return err} if _, err := inject.CacheInto(cm.cache, i); err != nil {return err} if _, err := inject.InjectorInto(cm.SetFields, i); err != nil {return err} if _, err := inject.StopChannelInto(cm.internalStop, i); err != nil {return err} if _, err := inject.MapperInto(cm.mapper, i); err != nil {return err} return nil } func (c *Controller) Watch(src source.Source, evthdler handler.EventHandler, prct ...predicate.Predicate) error { // Inject Cache into arguments if err := c.SetFields(src); err != nil {...} if err := c.SetFields(evthdler); err != nil {...} for _, pr := range prct { if err := c.SetFields(pr); err != nil {...} } ... } 入队器 reconciler 默认只监听注册的crd 的变更， 控制器可能会监控多种类型的对象（例如Pod + ReplicaSet + Deployment），但是控制器的Reconciler一般仅仅处理单一类型的对象。以Application为例，Application Controller 需要监听 Application及关联的pod的变更，有两类方法 Reconcile 方法可以收到 Application 和 Pod object 的变更。此时实现 Reconciler.Reconcile(Request) (Result, error) 要注意区分 Request 中的object 类型。 构建Application Controller时，Watch Pod 用更hack的手段去构建，可能对源码造成入侵。 添加自定义的入队器。比如 当pod 变更时，则找到与pod 相关的Application 加入队列 。这样pod 和Application 变更均可以触发 Application 的Reconciler.Reconcile 逻辑。 controller-runtime 暴露 handler.EventHandler接口，EventHandlers map an Event for one object to trigger Reconciles for either the same object or different objects。这个接口实现了Create,Update,Delete,Generic方法，用来在资源实例的不同生命阶段，进行判断与入队。 其它 kubebuilder2.0学习笔记——搭建和使用 kubebuilder2.0学习笔记——进阶使用 go build 之后，可执行文件即可 监听k8s（由--kubeconfig 参数指定 ），执行Reconcile 逻辑了 如果我们需要对 用户录入的 Application 进行合法性检查，可以开发一个webhook kubebuilder create webhook --group apps --version v1alpha1 --kind Application --programmatic-validation --defaulting kubebuilder 注释标记，比如：令crd支持kubectl scale，对crd实例进行基础的值校验，允许在kubectl get命令中显示crd的更多字段，等等 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/dashboard-log.html":{"url":"blog/kubernetes/K8s源码/dashboard-log.html","title":"Dashboard Log","keywords":"","body":"[toc] dashboard pod container log Struct Interface route handler Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/garbage collector controller 源码分析.html":{"url":"blog/kubernetes/K8s源码/garbage collector controller 源码分析.html","title":"Garbage Collector Controller 源码分析","keywords":"","body":"garbage collector controller 源码分析 kubernetes 中的删除策略 GarbageCollectorController 源码分析 示例 总结 原文章 在前面几篇关于 controller 源码分析的文章中多次提到了当删除一个对象时，其对应的 controller 并不会执行删除对象的操作，在 kubernetes 中对象的回收操作是由 GarbageCollectorController 负责的，其作用就是当删除一个对象时，会根据指定的删除策略回收该对象及其依赖对象，本文会深入分析垃圾收集背后的实现。 kubernetes 中的删除策略 kubernetes 中有三种删除策略：Orphan、Foreground 和 Background，三种删除策略的意义分别为： Orphan 策略：非级联删除，删除对象时，不会自动删除它的依赖或者是子对象，这些依赖被称作是原对象的孤儿对象，例如当执行以下命令时会使用 Orphan 策略进行删除，此时 ds 的依赖对象 controllerrevision 不会被删除； $ kubectl delete ds/nginx-ds --cascade=false Background 策略：在该模式下，kubernetes 会立即删除该对象，然后垃圾收集器会在后台删除这些该对象的依赖对象； Foreground 策略：在该模式下，对象首先进入“删除中”状态，即会设置对象的 deletionTimestamp 字段并且对象的 metadata.finalizers 字段包含了值 “foregroundDeletion”，此时该对象依然存在，然后垃圾收集器会删除该对象的所有依赖对象，垃圾收集器在删除了所有“Blocking” 状态的依赖对象（指其子对象中 ownerReference.blockOwnerDeletion=true的对象）之后，然后才会删除对象本身； 在 v1.9 以前的版本中，大部分 controller 默认的删除策略为 Orphan，从 v1.9 开始，对于 apps/v1 下的资源默认使用 Background 模式。以上三种删除策略都可以在删除对象时通过设置 deleteOptions.propagationPolicy 字段进行指定，如下所示： $ curl -k -v -XDELETE -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{\"propagationPolicy\":\"Foreground\"}' 'https://192.168.99.108:8443/apis/apps/v1/namespaces/default/daemonsets/nginx-ds' finalizer 机制 finalizer 是在删除对象时设置的一个 hook，其目的是为了让对象在删除前确认其子对象已经被完全删除，k8s 中默认有两种 finalizer：OrphanFinalizer 和 ForegroundFinalizer，finalizer 存在于对象的 ObjectMeta 中，当一个对象的依赖对象被删除后其对应的 finalizers 字段也会被移除，只有 finalizers 字段为空时，apiserver 才会删除该对象。 { ...... \"metadata\":{ ...... \"finalizers\":[ \"foregroundDeletion\" ] } ...... } 此外，finalizer 不仅仅支持以上两种字段，在使用自定义 controller 时也可以在 CR 中设置自定义的 finalizer 标识。 GarbageCollectorController 源码分析 kubernetes 版本：v1.16 GarbageCollectorController 负责回收 kubernetes 中的资源，要回收 kubernetes 中所有资源首先得监控所有资源，GarbageCollectorController 会监听集群中所有可删除资源产生的所有事件，这些事件会被放入到一个队列中，然后 controller 会启动多个 goroutine 处理队列中的事件，若为删除事件会根据对象的删除策略删除关联的对象，对于非删除事件会更新对象之间的依赖关系。 startGarbageCollectorController 首先还是看 GarbageCollectorController 的启动方法 startGarbageCollectorController，其主要逻辑为： 1、初始化 discoveryClient，discoveryClient 主要用来获取集群中的所有资源； 2、调用 garbagecollector.GetDeletableResources 获取集群内所有可删除的资源对象，支持 “delete”, “list”, “watch” 三种操作的 resource 称为 deletableResource； 3、调用 garbagecollector.NewGarbageCollector 初始化 garbageCollector 对象； 4、调用 garbageCollector.Run 启动 garbageCollector； 5、调用 garbageCollector.Sync 监听集群中的 DeletableResources ，当出现新的 DeletableResources 时同步到 monitors 中，确保监控集群中的所有资源； 6、调用 garbagecollector.NewDebugHandler 注册 debug 接口，用来提供集群内所有对象的关联关系； func startGarbageCollectorController(ctx ControllerContext)(http.Handler,bool, error){ if!ctx.ComponentConfig.GarbageCollectorController.EnableGarbageCollector{ returnnil,false,nil } // 1、初始化 discoveryClient gcClientset := ctx.ClientBuilder.ClientOrDie(\"generic-garbage-collector\") discoveryClient := cacheddiscovery.NewMemCacheClient(gcClientset.Discovery()) config := ctx.ClientBuilder.ConfigOrDie(\"generic-garbage-collector\") metadataClient, err := metadata.NewForConfig(config) if err !=nil{ returnnil,true, err } // 2、获取 deletableResource deletableResources := garbagecollector.GetDeletableResources(discoveryClient) ignoredResources := make(map[schema.GroupResource]struct{}) for _, r := range ctx.ComponentConfig.GarbageCollectorController.GCIgnoredResources{ ignoredResources[schema.GroupResource{Group: r.Group,Resource: r.Resource}]=struct{}{} } // 3、初始化 garbageCollector 对象 garbageCollector, err := garbagecollector.NewGarbageCollector( ...... ) if err !=nil{ returnnil,true, fmt.Errorf(\"failed to start the generic garbage collector: %v\", err) } // 4、启动 garbage collector workers :=int(ctx.ComponentConfig.GarbageCollectorController.ConcurrentGCSyncs) go garbageCollector.Run(workers, ctx.Stop) // 5、监听集群中的 DeletableResources go garbageCollector.Sync(gcClientset.Discovery(),30*time.Second, ctx.Stop) // 6、注册 debug 接口 return garbagecollector.NewDebugHandler(garbageCollector),true,nil } 在 startGarbageCollectorController 中主要调用了四种方法garbagecollector.NewGarbageCollector、garbageCollector.Run、garbageCollector.Sync 和 garbagecollector.NewDebugHandler 来完成核心功能，下面主要针对这四种方法进行说明。 garbagecollector.NewGarbageCollector NewGarbageCollector 的主要功能是初始化 GarbageCollector 和 GraphBuilder 对象，并调用 gb.syncMonitors方法初始化 deletableResources 中所有 resource controller 的 informer。GarbageCollector 的主要作用是启动 GraphBuilder 以及启动所有的消费者，GraphBuilder 的主要作用是启动所有的生产者。 func NewGarbageCollector(......)(*GarbageCollector, error){ ...... gc :=&GarbageCollector{ ...... } gb :=&GraphBuilder{ ...... } if err := gb.syncMonitors(deletableResources); err !=nil{ utilruntime.HandleError(fmt.Errorf(\"failed to sync all monitors: %v\", err)) } gc.dependencyGraphBuilder = gb return gc,nil } gb.syncMonitors syncMonitors 的主要作用是初始化各个资源对象的 informer，并调用 gb.controllerFor 为每种资源注册 eventHandler，此处每种资源被称为 monitors，因为为每种资源注册 eventHandler 时，对于 AddFunc、UpdateFunc 和 DeleteFunc 都会将对应的 event push 到 graphChanges 队列中，每种资源对象的 informer 都作为生产者。 func (gb *GraphBuilder) syncMonitors(resources map[schema.GroupVersionResource]struct{}) error { gb.monitorLock.Lock() defer gb.monitorLock.Unlock() ...... for resource := range resources { if _, ok := gb.ignoredResources[resource.GroupResource()]; ok { continue } ...... kind, err := gb.restMapper.KindFor(resource) if err !=nil{ errs = append(errs, fmt.Errorf(\"couldn't look up resource %q: %v\", resource, err)) continue } // 为 resource 的 controller 注册 eventHandler c, s, err := gb.controllerFor(resource, kind) if err !=nil{ errs = append(errs, fmt.Errorf(\"couldn't start monitor for resource %q: %v\", resource, err)) continue } current[resource]=&monitor{store: s, controller: c} added++ } gb.monitors = current for _, monitor := range toRemove { if monitor.stopCh !=nil{ close(monitor.stopCh) } } return utilerrors.NewAggregate(errs) } gb.controllerFor 在 gb.controllerFor中主要是为每个 deletableResources 的 informer 注册 eventHandler，此处就可以看到真正的生产者了。 func (gb *GraphBuilder) controllerFor(resource schema.GroupVersionResource, kind schema.GroupVersionKind)(cache.Controller, cache.Store, error){ handlers := cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}){ event:=&event{ eventType: addEvent, obj: obj, gvk: kind, } // 将对应的 event push 到 graphChanges 队列中 gb.graphChanges.Add(event) }, UpdateFunc: func(oldObj, newObj interface{}){ event:=&event{ eventType: updateEvent, obj: newObj, oldObj: oldObj, gvk: kind, } // 将对应的 event push 到 graphChanges 队列中 gb.graphChanges.Add(event) }, DeleteFunc: func(obj interface{}){ if deletedFinalStateUnknown, ok := obj.(cache.DeletedFinalStateUnknown); ok { obj = deletedFinalStateUnknown.Obj } event:=&event{ eventType: deleteEvent, obj: obj, gvk: kind, } // 将对应的 event push 到 graphChanges 队列中 gb.graphChanges.Add(event) }, } shared, err := gb.sharedInformers.ForResource(resource) if err !=nil{ returnnil,nil, err } shared.Informer().AddEventHandlerWithResyncPeriod(handlers,ResourceResyncTime) return shared.Informer().GetController(), shared.Informer().GetStore(),nil } 至此 NewGarbageCollector 的功能已经分析完了，在 NewGarbageCollector 中初始化了两个对象 GarbageCollector 和 GraphBuilder，然后在 gb.syncMonitors 中初始化了所有 deletableResources 的 informer，为每个 informer 添加 eventHandler 并将监听到的所有 event push 到 graphChanges 队列中，此处每个 informer 都被称为 monitor，所有 informer 都被称为生产者。graphChanges 是 GraphBuilder 中的一个对象，GraphBuilder 的主要功能是作为一个生产者，其会处理 graphChanges 中的所有事件并进行分类，将事件放入到 attemptToDelete 和 attemptToOrphan 两个队列中，具体处理逻辑下文讲述。 NewGarbageCollector 中的调用逻辑如下所示： |--> ctx.ClientBuilder. |ClientOrDie | | |--> cacheddiscovery. |NewMemCacheClient ||--> gb.sharedInformers. ||ForResource || startGarbage ----|--> garbagecollector.--> gb.syncMonitors --> gb.controllerFor --| CollectorController|NewGarbageCollector| || ||--> shared.Informer(). |AddEventHandlerWithResyncPeriod |--> garbageCollector.Run | | |--> garbageCollector.Sync | | |--> garbagecollector.NewDebugHandler garbageCollector.Run 上文已经详述了 NewGarbageCollector 的主要功能，然后继续分析 startGarbageCollectorController 中的第二个核心方法 garbageCollector.Run，garbageCollector.Run 的主要作用是启动所有的生产者和消费者，其首先会调用 gc.dependencyGraphBuilder.Run 启动所有的生产者，即 monitors，然后再启动一个 goroutine 处理 graphChanges 队列中的事件并分别放到 attemptToDelete 和 attemptToOrphan 两个队列中，dependencyGraphBuilder 即上文提到的 GraphBuilder，run 方法会调用 gc.runAttemptToDeleteWorker 和 gc.runAttemptToOrphanWorker 启动多个 goroutine 处理 attemptToDelete 和 attemptToOrphan 两个队列中的事件。 func (gc *GarbageCollector)Run(workers int, stopCh Run 方法中调用了 gc.dependencyGraphBuilder.Run 来完成 GraphBuilder 的启动。 gc.dependencyGraphBuilder.Run GraphBuilder 在 garbageCollector 整个环节中起到承上启下的作用，首先看一下 GraphBuilder 对象的结构： type GraphBuilderstruct{ restMapper meta.RESTMapper // informers monitors monitors monitorLock sync.RWMutex // 当 kube-controller-manager 中所有的 controllers 都启动后，informersStarted 会被 close 掉 // informersStarted 会被 close 掉的调用程序在 kube-controller-manager 的启动流程中 informersStarted uidToNode 此处有必要先说明一下 uidToNode 的功能，uidToNode 数据结构中维护着所有对象的依赖关系，此处的依赖关系是指比如当创建一个 deployment 时会创建对应的 rs 以及 pod，pod 的 owner 就是 rs，rs 的 owner 是 deployment，rs 的 dependents 是其关联的所有 pod，deployment 的 dependents 是其关联的所有 rs。 uidToNode 中的 node 不是指 k8s 中的 node 节点，而是将 graphChanges 中的 event 转换为 node 对象，k8s 中所有 object 之间的级联关系是通过 node 的概念来维护的，garbageCollector 在后续的处理中会直接使用 node 对象，node 对象定义如下： type concurrentUIDToNode struct { uidToNodeLock sync.RWMutex uidToNode map[types.UID]*node}type node struct { identity objectReference dependentsLock sync.RWMutex // 其依赖项指 metadata.ownerReference 中的对象 dependents map[*node]struct{} deletingDependents bool deletingDependentsLock sync.RWMutex beingDeleted bool beingDeletedLock sync.RWMutex // 当 virtual 值为 true 时，此时不确定该对象是否存在于 apiserver 中 virtual bool virtualLock sync.RWMutex // 对象本身的 OwnerReference 列表 owners []metav1.OwnerReference} GraphBuilder 主要有三个功能： 1、监控集群中所有的可删除资源； 2、基于 informers 中的资源在 uidToNode 数据结构中维护着所有对象的依赖关系； 3、处理 graphChanges 中的事件并放到 attemptToDelete 和 attemptToOrphan 两个队列中； 上文已经说了 gc.dependencyGraphBuilder.Run 的功能，启动所有的 informers 然后再启动一个 goroutine 处理 graphChanges 队列中的事件并分别放到 attemptToDelete 和 attemptToOrphan 两个队列中，代码如下所示： type concurrentUIDToNode struct{ uidToNodeLock sync.RWMutex uidToNode map[types.UID]*node } type node struct{ identity objectReference dependentsLock sync.RWMutex // 其依赖项指 metadata.ownerReference 中的对象 dependents map[*node]struct{} deletingDependents bool deletingDependentsLock sync.RWMutex beingDeleted bool beingDeletedLock sync.RWMutex // 当 virtual 值为 true 时，此时不确定该对象是否存在于 apiserver 中 virtualbool virtualLock sync.RWMutex // 对象本身的 OwnerReference 列表 owners []metav1.OwnerReference } gc.dependencyGraphBuilder.Run的核心是调用了 gb.startMonitors 和 gb.runProcessGraphChanges 两个方法来完成主要功能，继续看这两个方法的主要逻辑。 gb.startMonitors startMonitors 的功能很简单就是启动所有的 informers，代码如下所示： func (gb *GraphBuilder)Run(stopCh gb.runProcessGraphChanges runProcessGraphChanges 方法的主要功能是处理 graphChanges 中的事件将其分别放到 GraphBuilder 的 attemptToDelete 和 attemptToOrphan 两个队列中，代码主要逻辑为： 1、从 graphChanges 队列中取出一个 item 即 event； 2、获取 event 的 accessor，accessor 是一个 object 的 meta.Interface，里面包含访问 object meta 中所有字段的方法； 3、通过 accessor 获取 UID 判断 uidToNode 中是否存在该 object； 4、若 uidToNode 中不存在该 node 且该事件是 addEvent 或 updateEvent，则为该 object 创建对应的 node，并调用 gb.insertNode 将该 node 加到 uidToNode 中，然后将该 node 添加到其 owner 的 dependents 中，执行完 gb.insertNode 中的操作后再调用 gb.processTransitions 方法判断该对象是否处于删除状态，若处于删除状态会判断该对象是以 orphan 模式删除还是以 foreground 模式删除，若以 orphan 模式删除，则将该 node 加入到 attemptToOrphan 队列中，若以 foreground 模式删除则将该对象以及其所有 dependents 都加入到 attemptToDelete 队列中； 5、若 uidToNode 中存在该 node 且该事件是 addEvent 或 updateEvent 时，此时可能是一个 update 操作，调用 referencesDiffs 方法检查该对象的 OwnerReferences 字段是否有变化，若有变化(1)调用 gb.addUnblockedOwnersToDeleteQueue 将被删除以及更新的 owner 对应的 node 加入到 attemptToDelete 中，因为此时该 node 中已被删除或更新的 owner 可能处于删除状态且阻塞在该 node 处，此时有三种方式避免该 node 的 owner 处于删除阻塞状态，一是等待该 node 被删除，二是将该 node 自身对应 owner 的 OwnerReferences 字段删除，三是将该 node OwnerReferences 字段中对应 owner 的 BlockOwnerDeletion 设置为 false；(2)更新该 node 的 owners 列表；(3)若有新增的 owner，将该 node 加入到新 owner 的 dependents 中；(4) 若有被删除的 owner，将该 node 从已删除 owner 的 dependents 中删除；以上操作完成后，检查该 node 是否处于删除状态并进行标记，最后调用 gb.processTransitions 方法检查该 node 是否要被删除； 举个例子，若以 foreground 模式删除 deployment 时，deployment 的 dependents 列表中有对应的 rs，那么 deployment 的删除会阻塞住等待其依赖 rs 的删除，此时 rs 有三种方法不阻塞 deployment 的删除操作，一是 rs 对象被删除，二是删除 rs 对象 OwnerReferences 字段中对应的 deployment，三是将 rs 对象OwnerReferences 字段中对应的 deployment 配置 BlockOwnerDeletion 设置为 false，文末会有示例演示该操作。 6、若该事件为 deleteEvent，首先从 uidToNode 中删除该对象，然后从该 node 所有 owners 的 dependents 中删除该对象，将该 node 所有的 dependents 加入到 attemptToDelete 队列中，最后检查该 node 的所有 owners，若有处于删除状态的 owner，此时该 owner 可能处于删除阻塞状态正在等待该 node 的删除，将该 owner 加入到 attemptToDelete 中； 总结一下，当从 graphChanges 中取出 event 时，不管是什么 event，主要完成三件时，首先都会将 event 转化为 uidToNode 中的 node 对象，其次一是更新 uidToNode 中维护的依赖关系，二是更新该 node 的 owners 以及 owners 的 dependents，三是检查该 node 的 owners 是否要被删除以及该 node 的 dependents 是否要被删除，若需要删除则根据 node 的删除策略将其添加到 attemptToOrphan 或者 attemptToDelete 队列中； func (gb *GraphBuilder) runProcessGraphChanges(){ for gb.processGraphChanges(){ } } func (gb *GraphBuilder) processGraphChanges()bool{ // 1、从 graphChanges 取出一个 event item, quit := gb.graphChanges.Get() if quit { returnfalse } defer gb.graphChanges.Done(item) event, ok := item.(*event) if!ok { utilruntime.HandleError(fmt.Errorf(\"expect a *event, got %v\", item)) returntrue } obj :=event.obj accessor, err := meta.Accessor(obj) if err !=nil{ utilruntime.HandleError(fmt.Errorf(\"cannot access obj: %v\", err)) returntrue } // 2、若存在 node 对象，从 uidToNode 中取出该 event 的 node 对象 existingNode, found := gb.uidToNode.Read(accessor.GetUID()) if found { existingNode.markObserved() } switch{ // 3、若 event 为 add 或 update 类型以及对应的 node 对象不存在时 case(event.eventType == addEvent ||event.eventType == updateEvent)&&!found: // 4、为 node 创建 event 对象 newNode :=&node{ ...... } // 5、在 uidToNode 中添加该 node 对象 gb.insertNode(newNode) // 6、检查并处理 node 的删除操作 gb.processTransitions(event.oldObj, accessor, newNode) // 7、若 event 为 add 或 update 类型以及对应的 node 对象存在时 case(event.eventType == addEvent ||event.eventType == updateEvent)&& found: added, removed, changed := referencesDiffs(existingNode.owners, accessor.GetOwnerReferences()) // 8、若 node 的 owners 有变化 if len(added)!=0|| len(removed)!=0|| len(changed)!=0{ gb.addUnblockedOwnersToDeleteQueue(removed, changed) // 9、更新 uidToNode 中的 owners existingNode.owners = accessor.GetOwnerReferences() // 10、添加更新后 Owners 对应的 dependent gb.addDependentToOwners(existingNode, added) // 11、移除旧 owners 对应的 dependents gb.removeDependentFromOwners(existingNode, removed) } // 12、检查是否处于删除状态 if beingDeleted(accessor){ existingNode.markBeingDeleted() } // 13、检查并处理 node 的删除操作 gb.processTransitions(event.oldObj, accessor, existingNode) // 14、若为 delete event caseevent.eventType == deleteEvent: if!found { returntrue } // 15、从 uidToNode 中删除该 node gb.removeNode(existingNode) existingNode.dependentsLock.RLock() defer existingNode.dependentsLock.RUnlock() if len(existingNode.dependents)>0{ gb.absentOwnerCache.Add(accessor.GetUID()) } // 16、删除该 node 的 dependents for dep := range existingNode.dependents { gb.attemptToDelete.Add(dep) } // 17、删除该 node 处于删除阻塞状态的 owner for _, owner := range existingNode.owners { ownerNode, found := gb.uidToNode.Read(owner.UID) if!found ||!ownerNode.isDeletingDependents(){ continue } gb.attemptToDelete.Add(ownerNode) } } returntrue } processTransitions 上述在处理 add 或 update event 时最后都调用了 processTransitions 方法检查 node 是否处于删除状态，若处于删除状态会通过其删除策略将 node 放到 attemptToOrphan 或 attemptToDelete 队列中。 k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:509 func (gb *GraphBuilder) processTransitions(oldObj interface{}, newAccessor metav1.Object, n *node) { if startsWaitingForDependentsOrphaned(oldObj, newAccessor) { gb.attemptToOrphan.Add(n) return } if startsWaitingForDependentsDeleted(oldObj, newAccessor) { n.markDeletingDependents() for dep := range n.dependents { gb.attemptToDelete.Add(dep) } gb.attemptToDelete.Add(n) }} gc.runAttemptToDeleteWorker runAttemptToDeleteWorker 是执行删除 attemptToDelete 中 node 的方法，其主要逻辑为： 1、调用 gc.attemptToDeleteItem 删除 node； 2、若删除失败则重新加入到 attemptToDelete 队列中进行重试； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:280 func (gc *GarbageCollector) runAttemptToDeleteWorker() { for gc.attemptToDeleteWorker() { }}func (gc *GarbageCollector) attemptToDeleteWorker() bool { item, quit := gc.attemptToDelete.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit { return false } defer gc.attemptToDelete.Done(item) n, ok := item.(*node) if !ok { utilruntime.HandleError(fmt.Errorf(\"expect *node, got %#v\", item)) return true } err := gc.attemptToDeleteItem(n) if err != nil { if _, ok := err.(*restMappingError); ok { klog.V(5).Infof(\"error syncing item %s: %v\", n, err) } else { utilruntime.HandleError(fmt.Errorf(\"error syncing item %s: %v\", n, err)) } gc.attemptToDelete.AddRateLimited(item) } else if !n.isObserved() { gc.attemptToDelete.AddRateLimited(item) } return true} gc.runAttemptToDeleteWorker 中调用了 gc.attemptToDeleteItem 执行实际的删除操作。 gc.attemptToDeleteItem gc.attemptToDeleteItem 的主要逻辑为： 1、判断 node 是否处于删除状态； 2、从 apiserver 获取该 node 最新的状态，该 node 可能为 virtual node，若为 virtual node 则从 apiserver 中获取不到该 node 的对象，此时会将该 node 重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 3、判断该 node 最新状态的 uid 是否等于本地缓存中的 uid，若不匹配说明该 node 已更新过此时将其设置为 virtual node 并重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 4、通过 node 的 deletingDependents 字段判断该 node 当前是否处于删除 dependents 的状态，若该 node 处于删除 dependents 的状态则调用 processDeletingDependentsItem 方法检查 node 的 blockingDependents 是否被完全删除，若 blockingDependents 已完全被删除则删除该 node 对应的 finalizer，若 blockingDependents 还未删除完，将未删除的 blockingDependents 加入到 attemptToDelete 中； 上文中在 GraphBuilder 处理 graphChanges 中的事件时，若发现 node 处于删除状态，会将 node 的 dependents 加入到 attemptToDelete 中并标记 node 的 deletingDependents 为 true； 5、调用 gc.classifyReferences 将 node 的 ownerReferences 分类为 solid, dangling, waitingForDependentsDeletion 三类：dangling(owner 不存在)、waitingForDependentsDeletion(owner 存在，owner 处于删除状态且正在等待其 dependents 被删除)、solid(至少有一个 owner 存在且不处于删除状态)； 6、对以上分类进行不同的处理，若 solid不为 0 即当前 node 至少存在一个 owner，该对象还不能被回收，此时需要将 dangling 和 waitingForDependentsDeletion 列表中的 owner 从 node 的 ownerReferences 删除，即已经被删除或等待删除的引用从对象中删掉； 7、第二种情况是该 node 的 owner 处于 waitingForDependentsDeletion 状态并且 node 的 dependents 未被完全删除，该 node 需要等待删除完所有的 dependents 后才能被删除； 8、第三种情况就是该 node 已经没有任何 dependents 了，此时按照 node 中声明的删除策略调用 apiserver 的接口删除即可； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:404 func (gc *GarbageCollector) attemptToDeleteItem(item *node) error { // 1、判断 node 是否处于删除状态 if item.isBeingDeleted() && !item.isDeletingDependents() { return nil } // 2、从 apiserver 获取该 node 最新的状态 latest, err := gc.getObject(item.identity) switch { case errors.IsNotFound(err): gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil case err != nil: return err } // 3、判断该 node 最新状态的 uid 是否等于本地缓存中的 uid if latest.GetUID() != item.identity.UID { gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil } // 4、判断该 node 当前是否处于删除 dependents 状态中 if item.isDeletingDependents() { return gc.processDeletingDependentsItem(item) } // 5、检查 node 是否还存在 ownerReferences ownerReferences := latest.GetOwnerReferences() if len(ownerReferences) == 0 { return nil } // 6、对 ownerReferences 进行分类 solid, dangling, waitingForDependentsDeletion, err := gc.classifyReferences(item, ownerReferences) if err != nil { return err } switch { // 7、存在不处于删除状态的 owner case len(solid) != 0: if len(dangling) == 0 && len(waitingForDependentsDeletion) == 0 { return nil } ownerUIDs := append(ownerRefsToUIDs(dangling), ownerRefsToUIDs(waitingForDependentsDeletion)...) patch := deleteOwnerRefStrategicMergePatch(item.identity.UID, ownerUIDs...) _, err = gc.patch(item, patch, func(n *node) ([]byte, error) { return gc.deleteOwnerRefJSONMergePatch(n, ownerUIDs...) }) return err // 8、node 的 owner 处于 waitingForDependentsDeletion 状态并且 node // 的 dependents 未被完全删除 case len(waitingForDependentsDeletion) != 0 && item.dependentsLength() != 0: deps := item.getDependents() // 9、删除 dependents for _, dep := range deps { if dep.isDeletingDependents() { patch, err := item.unblockOwnerReferencesStrategicMergePatch() if err != nil { return err } if _, err := gc.patch(item, patch, gc.unblockOwnerReferencesJSONMergePatch); err != nil { return err } break } } // 10、以 Foreground 模式删除 node 对象 policy := metav1.DeletePropagationForeground return gc.deleteObject(item.identity, &policy) // 11、该 node 已经没有任何依赖了，按照 node 中声明的删除策略调用 apiserver 的接口删除 default: var policy metav1.DeletionPropagation switch { case hasOrphanFinalizer(latest): policy = metav1.DeletePropagationOrphan case hasDeleteDependentsFinalizer(latest): policy = metav1.DeletePropagationForeground default: policy = metav1.DeletePropagationBackground } return gc.deleteObject(item.identity, &policy) }} gc.runAttemptToOrphanWorker runAttemptToOrphanWorker 是处理以 orphan 模式删除的 node，主要逻辑为： 1、调用 gc.orphanDependents 删除 owner 所有 dependents OwnerReferences 中的 owner 字段； 2、调用 gc.removeFinalizer 删除 owner 的 orphan Finalizer； 3、以上两步中若有失败的会进行重试； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:574 func (gc *GarbageCollector) runAttemptToOrphanWorker() { for gc.attemptToOrphanWorker() { }}func (gc *GarbageCollector) attemptToOrphanWorker() bool { item, quit := gc.attemptToOrphan.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit { return false } defer gc.attemptToOrphan.Done(item) owner, ok := item.(*node) if !ok { return true } owner.dependentsLock.RLock() dependents := make([]*node, 0, len(owner.dependents)) for dependent := range owner.dependents { dependents = append(dependents, dependent) } owner.dependentsLock.RUnlock() err := gc.orphanDependents(owner.identity, dependents) if err != nil { gc.attemptToOrphan.AddRateLimited(item) return true } // 更新 owner, 从 finalizers 列表中移除 \"orphaningFinalizer\" err = gc.removeFinalizer(owner, metav1.FinalizerOrphanDependents) if err != nil { gc.attemptToOrphan.AddRateLimited(item) } return true} garbageCollector.Sync garbageCollector.Sync 是 startGarbageCollectorController 中的第三个核心方法，主要功能是周期性的查询集群中所有的资源，过滤出 deletableResources，然后对比已经监控的 deletableResources 和当前获取到的 deletableResources 是否一致，若不一致则更新 GraphBuilder 的 monitors 并重新启动 monitors 监控所有的 deletableResources，该方法的主要逻辑为： 1、通过调用 GetDeletableResources 获取集群内所有的 deletableResources 作为 newResources，deletableResources 指支持 “delete”, “list”, “watch” 三种操作的 resource，包括 CR； 2、检查 oldResources, newResources 是否一致，不一致则需要同步； 3、调用 gc.resyncMonitors 同步 newResources，在 gc.resyncMonitors 中会重新调用 GraphBuilder 的 syncMonitors 和 startMonitors 两个方法完成 monitors 的刷新； 4、等待 newResources informer 中的 cache 同步完成； 5、将 newResources 作为 oldResources，继续进行下一轮的同步； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:164 func (gc *GarbageCollector) Sync(discoveryClient discovery.ServerResourcesInterface, period time.Duration, stopCh 1 { newResources = GetDeletableResources(discoveryClient) if len(newResources) == 0 { return false, nil } } gc.restMapper.Reset() // 4、调用 gc.resyncMonitors 同步 newResources if err := gc.resyncMonitors(newResources); err != nil { return false, nil } // 5、等待所有 monitors 的 cache 同步完成 if !cache.WaitForNamedCacheSync(\"garbage collector\", waitForStopOrTimeout(stopCh, period), gc.dependencyGraphBuilder.IsSynced) { return false, nil } return true, nil }, stopCh) // 6、更新 oldResources oldResources = newResources }, period, stopCh)} garbageCollector.Sync 中主要调用了两个方法，一是调用 GetDeletableResources 获取集群中所有的可删除资源，二是调用 gc.resyncMonitors 更新 GraphBuilder 中 monitors。 GetDeletableResources 在 GetDeletableResources 中首先通过调用 discoveryClient.ServerPreferredResources 方法获取集群内所有的 resource 信息，然后通过调用 discovery.FilteredBy 过滤出支持 “delete”, “list”, “watch” 三种方法的 resource 作为 deletableResources。 k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:636 func GetDeletableResources(discoveryClient discovery.ServerResourcesInterface) map[schema.GroupVersionResource]struct{} { // 1、调用 discoveryClient.ServerPreferredResources 方法获取集群内所有的 resource 信息 preferredResources, err := discoveryClient.ServerPreferredResources() if err != nil { if discovery.IsGroupDiscoveryFailedError(err) { ...... } else { ...... } } if preferredResources == nil { return map[schema.GroupVersionResource]struct{}{} } // 2、调用 discovery.FilteredBy 过滤出 deletableResources deletableResources := discovery.FilteredBy(discovery.SupportsAllVerbs{Verbs: []string{\"delete\", \"list\", \"watch\"}}, preferredResources) deletableGroupVersionResources := map[schema.GroupVersionResource]struct{}{} for _, rl := range deletableResources { gv, err := schema.ParseGroupVersion(rl.GroupVersion) if err != nil { continue } for i := range rl.APIResources { deletableGroupVersionResources[schema.GroupVersionResource{Group: gv.Group, Version: gv.Version, Resource: rl.APIResources[i].Name}] = struct{}{} } } return deletableGroupVersionResources} ServerPreferredResources ServerPreferredResources 的主要功能是获取集群内所有的 resource 以及其 group、version、verbs 信息，该方法的主要逻辑为： 1、调用 ServerGroups 方法获取集群内所有的 GroupList，ServerGroups 方法首先从 apiserver 通过 /api URL 获取当前版本下所有可用的 APIVersions，再通过 /apis URL 获取 所有可用的 APIVersions 以及其下的所有 APIGroupList； 2、调用 fetchGroupVersionResources 通过 serverGroupList 再获取到对应的 resource； 3、将获取到的 version、group、resource 构建成标准格式添加到 metav1.APIResourceList 中； k8s.io/kubernetes/staging/src/k8s.io/client-go/discovery/discovery_client.go:285 func ServerPreferredResources(d DiscoveryInterface) ([]*metav1.APIResourceList, error) { // 1、获取集群内所有的 GroupList serverGroupList, err := d.ServerGroups() if err != nil { return nil, err } // 2、通过 serverGroupList 获取到对应的 resource groupVersionResources, failedGroups := fetchGroupVersionResources(d, serverGroupList) result := []*metav1.APIResourceList{} grVersions := map[schema.GroupResource]string{} // selected version of a GroupResource grAPIResources := map[schema.GroupResource]*metav1.APIResource{} // selected APIResource for a GroupResource gvAPIResourceLists := map[schema.GroupVersion]*metav1.APIResourceList{} // blueprint for a APIResourceList for later grouping // 3、格式化 resource for _, apiGroup := range serverGroupList.Groups { for _, version := range apiGroup.Versions { groupVersion := schema.GroupVersion{Group: apiGroup.Name, Version: version.Version} apiResourceList, ok := groupVersionResources[groupVersion] if !ok { continue } emptyAPIResourceList := metav1.APIResourceList{ GroupVersion: version.GroupVersion, } gvAPIResourceLists[groupVersion] = &emptyAPIResourceList result = append(result, &emptyAPIResourceList) for i := range apiResourceList.APIResources { apiResource := &apiResourceList.APIResources[i] if strings.Contains(apiResource.Name, \"/\") { continue } gv := schema.GroupResource{Group: apiGroup.Name, Resource: apiResource.Name} if _, ok := grAPIResources[gv]; ok && version.Version != apiGroup.PreferredVersion.Version { continue } grVersions[gv] = version.Version grAPIResources[gv] = apiResource } } } for groupResource, apiResource := range grAPIResources { version := grVersions[groupResource] groupVersion := schema.GroupVersion{Group: groupResource.Group, Version: version} apiResourceList := gvAPIResourceLists[groupVersion] apiResourceList.APIResources = append(apiResourceList.APIResources, *apiResource) } if len(failedGroups) == 0 { return result, nil } return result, &ErrGroupDiscoveryFailed{Groups: failedGroups}} GetDeletableResources 方法中的调用流程为： |--> d.ServerGroups | |--> discoveryClient. --| | ServerPreferredResources | | |--> fetchGroupVersionResourcesGetDeletableResources --| | |--> discovery.FilteredBy gc.resyncMonitors gc.resyncMonitors 的主要功能是更新 GraphBuilder 的 monitors 并重新启动 monitors 监控所有的 deletableResources，GraphBuilder 的 syncMonitors 和 startMonitors 方法在前面的流程中已经分析过，此处不再详细说明。 k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:116 func (gc *GarbageCollector) resyncMonitors(deletableResources map[schema. GroupVersionResource]struct{}) error { if err := gc.dependencyGraphBuilder.syncMonitors(deletableResources); err != nil { return err } gc.dependencyGraphBuilder.startMonitors() return nil} garbagecollector.NewDebugHandler garbagecollector.NewDebugHandler 主要功能是对外提供一个接口供用户查询当前集群中所有资源的依赖关系，依赖关系可以以图表的形式展示。 func startGarbageCollectorController(ctx ControllerContext) (http.Handler, bool, error) { ...... return garbagecollector.NewDebugHandler(garbageCollector), true, nil} 具体使用方法如下所示： $ curl http://192.168.99.108:10252/debug/controllers/garbagecollector/graph > tmp.dot$ curl http://192.168.99.108:10252/debug/controllers/garbagecollector/graph\\?uid=f9555d53-2b5f-4702-9717-54a313ed4fe8 > tmp.dot// 生成 svg 文件$ dot -Tsvg -o graph.svg tmp.dot// 然后在浏览器中打开 svg 文件 依赖关系图如下所示： 示例 在此处会有一个小示例验证一下源码中的删除阻塞逻辑，当以 Foreground 策略删除一个对象时，该对象会处于阻塞状态等待其依依赖被删除，此时有三种方式避免该对象处于删除阻塞状态，一是将依赖对象直接删除，二是将依赖对象自身的 OwnerReferences 中 owner 字段删除，三是将该依赖对象 OwnerReferences 字段中对应 owner 的 BlockOwnerDeletion 设置为 false，下面会验证下这三种方式，首先创建一个 deployment，deployment 创建出的 rs 默认不会有 foregroundDeletion finalizers，此时使用 kubectl edit 手动加上 foregroundDeletion finalizers，当 deployment 正常运行时，如下所示： $ kubectl get deployment nginx-deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 2/2 2 2 43s$ kubectl get rs nginx-deployment-69b6b4c5cdNAME DESIRED CURRENT READY AGEnginx-deployment-69b6b4c5cd 2 2 2 57s$ kubectl get podNAME READY STATUS RESTARTS AGEnginx-deployment-69b6b4c5cd-26dsn 1/1 Running 0 66snginx-deployment-69b6b4c5cd-6rqqc 1/1 Running 0 64s$ kubectl edit rs nginx-deployment-69b6b4c5cd // deployment 关联的 rs 对象 apiVersion: apps/v1kind: ReplicaSetmetadata: name: nginx-deployment-69b6b4c5cd namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: Deployment name: nginx-deployment uid: 40a1044e-03d1-48bc-8806-cb79d781c946 finalizers: - foregroundDeletion // 为 rs 手动添加的 Foreground 策略 ......spec: replicas: 2 ......status: ...... 当 deployment、rs、pod 都处于正常运行状态且 deployment 关联的 rs 使用 Foreground 删除策略时，然后验证源码中提到的三种方法，验证时需要模拟一个依赖对象无法删除的场景，当然这个也很好模拟，三种场景如下所示： 1、当 pod 所在的 node 处于 Ready 状态时，以 Foreground 策略删除 deploment，因为 rs 关联的 pod 会直接被删除，rs 也会被正常删除，此时 deployment 也会直接被删除； 2、当 pod 所在的 node 处于 NotReady 状态时，以 Foreground 策略删除 deploment，此时因 rs 关联的 pod 无法被删除，rs 会一直处于删除阻塞状态，deployment 由于 rs 无法被删除也会处于删除阻塞状态，此时更新 rs 去掉其 ownerReferences 中对应的 deployment 部分，deployment 会因无依赖对象被成功删除； 3、和 2 同样的场景，node 处于 NotReady 状态时，以 Foreground 策略删除 deploment，deployment 和 rs 将处于删除阻塞状态，此时将 rs ownerReferences 中关联 deployment 的 blockOwnerDeletion 字段设置为 false，可以看到 deployment 会因无 block 依赖对象被成功删除； $ systemctl stop kubelet // node 处于 NotReady 状态 $ kubectl get node NAME STATUS ROLES AGE VERSION minikube NotReady master 6d11h v1.16.2 // 以 Foreground 策略删除 deployment $ curl -k -v -XDELETE -H \"Accept: application/json\"-H \"Content-Type: application/json\"-d '{\"propagationPolicy\":\"Foreground\"}''https://192.168.99.108:8443/apis/apps/v1/namespaces/default/deployments/nginx-deployment' 总结 GarbageCollectorController 是一种典型的生产者消费者模型，所有 deletableResources 的 informer 都是生产者，每种资源的 informer 监听到变化后都会将对应的事件 push 到 graphChanges 中，graphChanges 是 GraphBuilder 对象中的一个数据结构，GraphBuilder 会启动另外的 goroutine 对 graphChanges 中的事件进行分类并放在其 attemptToDelete 和 attemptToOrphan 两个队列中，garbageCollector 会启动多个 goroutine 对 attemptToDelete 和 attemptToOrphan 两个队列中的事件进行处理，处理的结果就是回收一些需要被删除的对象。最后，再用一个流程图总结一下 GarbageCollectorController 的主要流程: monitors (producer) | | ∨ graphChanges queue | | ∨ processGraphChanges | | ∨ ------------------------------- || || ∨∨ attemptToDelete queue attemptToOrphan queue || || ∨∨ AttemptToDeleteWorkerAttemptToOrphanWorker (consumer)(consumer) Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/informer.html":{"url":"blog/kubernetes/K8s源码/informer.html","title":"Informer","keywords":"","body":"informer 基础功能 infromer是client-go中的一个核心基础包。在kubernetes源码中，如果kubernetes中的某个组件，需要List/Get Kubernetes中的Object，在绝大多数情况下，会直接使用informer实例中的Lister()方法（该方法包含了Get和List方法），而很少直接请求 Kubernetes API。Informer 最基本 的功能就是 List/Get Kubernetes 中的 Object。 如下图所示，仅需要十行左右的代码就能实现对 Pod 的 List 和 Get。 高级功能 Client-go 的首要目标是满足 Kubernetes 的自身需求。Informer 作为其中的核心工具包，面对 Kubernetes 极为复杂业务逻辑，如果仅实现 List/Get 功能，根本无法满足 Kubernetes 自身需求。因此，Informer 被设计为一个灵活而复杂的工具包，除 List/Get Object 外，Informer 还可以监听事件并触发回调函数等，以实现更加复杂的业务逻辑。 设计思路 为了让 Client-go 更快地返回 List/Get 请求的结果、减少对 Kubenetes API 的直接调用，Informer 被设计实现为一个依赖 Kubernetes List/Watch API 、可监听事件并触发回调函数的二级缓存工具包。 使用 Informer 实例的 Lister() 方法， List/Get Kubernetes 中的 Object 时，Informer 不会去请求 Kubernetes API，而是直接查找缓存在本地内存中的数据(这份数据由 Informer 自己维护)。通过这种方式，Informer 既可以更快地返回结果，又能减少对 Kubernetes API 的直接调用。 依赖 Kubernetes List/Watch API Informer 只会调用 Kubernetes List 和 Watch 两种类型的 API。Informer 在初始化的时，先调用 Kubernetes List API 获得某种 resource 的全部 Object，缓存在内存中; 然后，调用 Watch API 去 watch 这种 resource，去维护这份缓存; 最后，Informer 就不再调用 Kubernetes 的任何 API。 用 List/Watch 去维护缓存、保持一致性是非常典型的做法，但令人费解的是，Informer 只在初始化时调用一次 List API，之后完全依赖 Watch API 去维护缓存，没有任何 resync 机制。 笔者在阅读 Informer 代码时候，对这种做法十分不解。按照多数人思路，通过 resync 机制，重新 List 一遍 resource 下的所有 Object，可以更好的保证 Informer 缓存和 Kubernetes 中数据的一致性。 咨询过 Google 内部 Kubernetes 开发人员之后，得到的回复是: 在 Informer 设计之初，确实存在一个 relist 无法去执 resync 操作， 但后来被取消了。原因是现有的这种 List/Watch 机制，完全能够保证永远不会漏掉任何事件，因此完全没有必要再添加 relist 方法去 resync informer 的缓存。这种做法也说明了 Kubernetes 完全信任 etcd。 可监听事件并触发回调函数 Informer 通过 Kubernetes Watch API 监听某种 resource 下的所有事件。而且，Informer 可以添加自定义的回调函数，这个回调函数实例(即 ResourceEventHandler 实例)只需实现 OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) 和 OnDelete(obj interface{}) 三个方法，这三个方法分别对应 informer 监听到创建、更新和删除这三种事件类型。 在 Controller 的设计实现中，会经常用到 informer 的这个功能。 Controller 相关文章请见此文《如何用 client-go 拓展 Kubernetes 的 API》。 二级缓存 二级缓存属于 Informer 的底层缓存机制，这两级缓存分别是 DeltaFIFO 和 LocalStore。 这两级缓存的用途各不相同。DeltaFIFO 用来存储 Watch API 返回的各种事件 ，LocalStore 只会被 Lister 的 List/Get 方法访问 。 虽然 Informer 和 Kubernetes 之间没有 resync 机制，但 Informer 内部的这两级缓存之间存在 resync 机制。 以上是 Informer 设计中的一些关键点，没有介绍一些太细节的东西，尤其对于 Informer 两级缓存还未做深入介绍。下一章节将对 Informer 详细的工作流程做一个详细介绍。 Informer 详细解析 Informer 内部主要组件 Informer 中主要包含 Controller、Reflector、DeltaFIFO、LocalStore、Lister 和 Processor 六个组件，其中 Controller 并不是 Kubernetes Controller，这两个 Controller 并没有任何联系；Reflector 的主要作用是通过 Kubernetes Watch API 监听某种 resource 下的所有事件；DeltaFIFO 和 LocalStore 是 Informer 的两级缓存；Lister 主要是被调用 List/Get 方法；Processor 中记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。 Informer 关键逻辑解析 我们以 Pod 为例，详细说明一下 Informer 的关键逻辑： Informer 在初始化时，Reflector 会先 List API 获得所有的 Pod Reflect 拿到全部 Pod 后，会将全部 Pod 放到 Store 中 如果有人调用 Lister 的 List/Get 方法获取 Pod， 那么 Lister 会直接从 Store 中拿数据 Informer 初始化完成之后，Reflector 开始 Watch Pod，监听 Pod 相关 的所有事件;如果此时 pod_1 被删除，那么 Reflector 会监听到这个事件 Reflector 将 pod_1 被删除 的这个事件发送到 DeltaFIFO DeltaFIFO 首先会将这个事件存储在自己的数据结构中(实际上是一个 queue)，然后会直接操作 Store 中的数据，删除 Store 中的 pod_1 DeltaFIFO 再 Pop 这个事件到 Controller 中 Controller 收到这个事件，会触发 Processor 的回调函数 LocalStore 会周期性地把所有的 Pod 信息重新放到 DeltaFIFO 中 Informer 总结 Informer 的内部原理比较复杂、不太容易上手，但 Informer 却是一个非常稳定可靠的 package，已被 Kubernetes 广泛使用。但是，目前关于 Informer 的文章不是很多，如果文章中有表述不正确的地方，希望各位读者悉心指正。 Link https://cloudnative.to/blog/client-go-informer/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/informer2.html":{"url":"blog/kubernetes/K8s源码/informer2.html","title":"Informer2","keywords":"","body":"深入了解 Kubernetes Informer Kubernetes Client-go Informer 机制及源码理解 Kubernetes 作者 林许亚伦 发表于 2020年8月28日 本文主要根据书籍 《Kubernetes 源码剖析》的基础上，对 Client-go 部分的 Informer 机制进行了解与学习。 Informer 机制 Kubernetes 中使用 http 进行通信，如何不依赖中间件的情况下保证消息的实时性，可靠性和顺序性等呢？答案就是利用了 Informer 机制。Informer 的机制，降低了了 Kubernetes 各个组件跟 Etcd 与 Kubernetes API Server 的通信压力。 Informer 机制架构设计 图片源自 Client-go under the hood ⚠️ 这张图分为两部分，黄色图标是开发者需要自行开发的部分，而其它的部分是 client-go 已经提供的，直接使用即可。 Reflector：用于 Watch 指定的 Kubernetes 资源，当 watch 的资源发生变化时，触发变更的事件，比如 Added，Updated 和 Deleted 事件，并将资源对象存放到本地缓存 DeltaFIFO； DeltaFIFO：拆开理解，FIFO 就是一个队列，拥有队列基本方法（ADD，UPDATE，DELETE，LIST，POP，CLOSE 等），Delta 是一个资源对象存储，保存存储对象的消费类型，比如 Added，Updated，Deleted，Sync 等； Indexer：Client-go 用来存储资源对象并自带索引功能的本地存储，Reflector 从 DeltaFIFO 中将消费出来的资源对象存储到 Indexer，Indexer 与 Etcd 集群中的数据完全保持一致。从而 client-go 可以本地读取，减少 Kubernetes API 和 Etcd 集群的压力。 看书本上的一个例子，想使用 Informer 的关键流程如下： clientset, err := kubernetes.NewForConfig(config) stopCh := make(chan struct{}) defer close(stopch) sharedInformers := informers.NewSharedInformerFactory(clientset, time.Minute) informer := sharedInformer.Core().V1().Pods().Informer() informer.AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{} { // ... }, UpdateFunc: func(obj interface{} { // ... }, DeleteFunc : func(obj interface{} { // ... }) informer.Run(stopCh) }) Informer 需要通过 ClientSet 与 Kubernetes API Server 交互； 创建 stopCh 是用于在程序进程退出前通知 Informer 提前退出，Informer 是一个持久运行的 goroutine； NewSharedInformerFactory 实例化了一个 SharedInformer 对象，用于进行本地资源存储； sharedInformer.Core().V1().Pods().Informer() 得到了具体 Pod 资源的 informer 对象； AddEventHandler 即图中的第6步，这是一个资源事件回调方法，上例中即为当创建/更新/删除 Pod 时触发事件回调方法； 一般而言，其他组件使用 Informer 机制触发资源回调方法会将资源对象推送到 WorkQueue 或其他队列中，具体推送的位置要去回调方法里自行实现。 上面这个示例，当触发了 Add，Update 或者 Delete 事件，就通知 Client-go，告知 Kubernetes 资源事件发生变更并且需要进行相应的处理。 资源 Informer 每一个 k8s Resource 都实现了 Informer 机制，均有 Informer 和 Lister 方法，以 PodInformer 为例： type PodInformer interface { Informer() cache.SharedIndexInformer Lister() v1.PodLister } Shared Informer 共享机制 Informer 又称为 Shared Informer，表明是可以共享使用的，在使用 client-go 写代码时，若同一资源的 Informer 被实例化太多次，每个 Informer 使用一个 Reflector，会运行过多的相同 ListAndWatch（即图中的第一步），太多重复的序列化和反序列化会导致 k8s API Server 负载过重。 而 Shared Informer 通过对同一类资源 Informer 共享一个 Reflector 可以节约很多资源，这通过 map 数据结构即可实现这样一个共享 Informer 机制。 vendor/k8s.io/client-go/informers/factory.go type sharedInformerFactory struct { // ... // map 数据结构 informers map[reflect.Type]cache.SharedIndexInformer // ... } // ... // InternalInformerFor returns the SharedIndexInformer for obj using an internal client. // 当示例中调用 xxx.Informer() 时，内部调用了该方法 func (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer { f.lock.Lock() defer f.lock.Unlock() informerType := reflect.TypeOf(obj) informer, exists := f.informers[informerType] if exists { return informer } resyncPeriod, exists := f.customResync[informerType] if !exists { resyncPeriod = f.defaultResync } informer = newFunc(f.client, resyncPeriod) f.informers[informerType] = informer return informer } Reflector Reflector 用于 Watch 指定的 Kubernetes 资源，当 watch 的资源发生变化时，触发变更的事件，并将资源对象存放到本地缓存 DeltaFIFO。 通过 NewReflector 实例化 Reflector 对象，实例化过程必须传入 ListerWatcher 数据接口对象，它拥有 List 和 Watch 方法。Reflector 对象通过 Run 行数启动监控并处理监控事件，在实现中，其核心为 ListAndWatch 函数。 以 Example 的代码为例，我们在最后一步执行了 informer.Run(stopCh)，内部会执行一个 ListAndWatch 方法： vendor/k8s.io/client-go/tools/cache/reflector.go // Run 执行一个 watch 并且把握所有的 watch events，watch 关闭后会重启 // stopCh 关闭时 Run 退出 func (r *Reflector) Run(stopCh 获取资源数据列表 以之前提过的 Example 的代码为例，通过 Run 获取了所有 Pod 的资源数据，List 流程如下： r.ListWatcher.List 获取资源数据； listMetaInterface.GetResourceVersion 获取资源版本号； meta.ExtractList 将资源数据转换为资源对象列表； r.SyncWith 将资源对象列表的资源对象和资源版本号存储到 DeltaFIFO 中； r.setLastSyncResourceVersion 设置最新的资源版本号。 具体的， r.ListWatcher.List 根据 ResourceVersion 获取资源下的所有对象数据。以 Example 为例，该方法调用的就是 Pod Informer 下的 ListFunc 函数，通过 ClientSet 客户端与 Kubernetes API Server 交互并获取 Pod 资源列表数据； listMetaInterface.GetResourceVersion 获取 ResourceVersion，即资源版本号，注意这里的资源版本号并不是指前面各个客户端的不同 kind 的不同 Version，所有资源都拥有 ResourceVersion，标识当前资源对象的版本号。每次修改 etcd 集群中存储的对象时，Kubernetes API Server 都会更改 ResourceVersion，使得 client-go 执行 watch 时可以根据 ResourceVersion 判断当前资源对象是否发生变化； meta.ExtractList 将 runtime.Object 对象转换为 []runtime.Object 对象。因为 r.ListWatcher.List 获取的是资源下所有对象的数据，因此应当是一个列表； r.SyncWith将结果同步到 DeltaFIFO 中； r.setLastSyncResourceVersion 设置最新的资源版本号。 监控资源对象 Watch 通过 HTTP 协议与 Kubernetes API Server 建立长连接，接收 Kubernetes API Server 发来的资源变更事件。Watch 操作的实现机制使用 HTTP 协议的分块传输编码——当 client-go 调用 Kubernetes API Server 时，Kubernetes API Server 在 Response 的 HTTP Header 中设置 Transfer-Encoding 的值为 chunked，表示采用分块传输编码，客户端收到消息后，与服务端进行连接，并等待下一个数据块。 在源码中关键为 watch 和 watchHandler 函数： staging/src/k8s.io/client-go/tools/cache/reflector.go func (r *Reflector) ListAndWatch(stopCh 以之前的 Example 为例子，r.listerWatcher.Watch 实际调用了 Pod Informer 下的 Watch 函数，通过 ClientSet 客户端与 Kubernetes API Server 建立长链接，监控指定资源的变更事件，如下： staging/src/k8s.io/client-go/informers/core/v1/pod.go func NewFilteredPodInformer(...) cache.SharedIndexInformer { return cache.NewSharedIndexInformer( // ... WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { if tweakListOptions != nil { tweakListOptions(&options) } return client.CoreV1().Pods(namespace).Watch(options) }, }, // ... ) } r.watchHandler 处理资源的变更事件，将对应资源更新到本地缓存 DeltaFIFO 并更新 ResourceVersion 资源版本号。 // watchHandler watches w and keeps *resourceVersion up to date. func (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh DeltaFIFO DeltaFIFO 拆开理解，FIFO 就是一个队列，拥有队列基本方法（ADD，UPDATE，DELETE，LIST，POP，CLOSE 等），Delta 是一个资源对象存储，保存存储对象的消费类型，比如 Added，Updated，Deleted，Sync 等。 看 DeltaFIFO 的数据结构： vendor/k8s.io/client-go/tools/cache/delta_fifo.go type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. lock sync.RWMutex cond sync.Cond // We depend on the property that items in the set are in // the queue and vice versa, and that all Deltas in this // map have at least one Delta. items map[string]Deltas queue []string // ... } 其中，Deltas 部分的数据结构如下： staging/src/k8s.io/client-go/tools/cache/delta_fifo.go // DeltaType is the type of a change (addition, deletion, etc) type DeltaType string // Delta is the type stored by a DeltaFIFO. It tells you what change // happened, and the object's state after* that change. type Delta struct { Type DeltaType Object interface{} } // Deltas is a list of one or more 'Delta's to an individual object. // The oldest delta is at index 0, the newest delta is the last one. type Deltas []Delta DeltaFIFO 会保留所有关于资源对象（obj）的操作类型，队列中会存在拥有不同操作类型的同一资源对象，使得消费者在处理该资源对象时能够了解资源对象所发生的事情。queue 字段存储资源对象的 key，这个 key 通过 KeyOf 函数计算得到，items 字段通过 map 数据结构的方式存储，value 存储的是对象 Deltas 数组，一个结构示例图如下： ┌───────┐┌───────┐┌───────┐ queue │ObjKey1││ObjKey2││ObjKey3│ └───────┘└───────┘└───────┘ ┌─────────────────────────────────────────────────────────────┐ itmes │ObjKey1: [{\"Added\",Obj1} {\"Updated\",Obj1}] │ ├─────────────────────────────────────────────────────────────┤ │ObjKey2: [{\"Added\",Obj2},{\"Deleted\",Obj2},{\"Sync\",Obj2}] │ ├─────────────────────────────────────────────────────────────┤ │ObjKey3: [{\"Added\",Obj3},{\"Updated\",Obj3},{\"Deleted\",Obj3}] │ └─────────────────────────────────────────────────────────────┘ 作为一个 FIFO 的队列，有数据的生产者和消费者，其中生产者是 Reflector 调用的 Add 方法，消费者是 Controller 调用的 Pop 方法。三个核心方法为生产者方法，消费者方法和 Resync 机制。 生产者方法 DeltaFIFO 队列中的资源对象在调用 Added，Updated，Deleted 等事件时都调用了 queueActionLocked 函数： 它是 DeltaFIFO 实现的关键： vendor/k8s.io/client-go/tools/cache/delta_fifo.go // queueActionLocked 为一个 delta list 添加一个 object // 调用方必须先上锁 func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error { // 通过 KeyOf 函数计算出对象的 key id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } // 将 actionType 以及对应的 id 添加到 items 中，并通过 dedupDeltas 对数组中最新的两次添加进行去重 newDeltas := append(f.items[id], Delta{actionType, obj}) newDeltas = dedupDeltas(newDeltas) // 一般不会出现 0 { if _, exists := f.items[id]; !exists { f.queue = append(f.queue, id) } f.items[id] = newDeltas // 广播所有消费者解除阻塞 f.cond.Broadcast() } else { delete(f.items, id) } return nil } 通过 KeyOf 函数计算出对象的 key； 将 actionType 以及对应的 id 添加到 items 中，并通过 dedupDeltas 对数组中最新的两次添加进行去重； 更新构造后的 Deleta 并通过 cond.Broadcast() 广播所有消费者解除阻塞。 消费者方法 Pop 函数作为消费者使用方法，从 DeltaFIFO 的头部取出最早进入队列中的资源对象数据。Pop 方法必须传入 process 函数，用于接收并处理对象的回调方法，如下： vendor/k8s.io/client-go/tools/cache/delta_fifo.go func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { for len(f.queue) == 0 { // 空队列时阻塞 Pop 方法 if f.IsClosed() { return nil, FIFOClosedError } f.cond.Wait() } id := f.queue[0] f.queue = f.queue[1:] if f.initialPopulationCount > 0 { f.initialPopulationCount-- } item, ok := f.items[id] if !ok { // Item may have been deleted subsequently. continue } // 注意这里在执行之前会把该 obj 旧的 delta 数据清空 delete(f.items, id) err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } return item, err } } 首先，使用 f.lock.Lock() 确保了数据的同步，当队列不为空时，取出 f.queue 的头部数据，将对象传入 process 回调函数，由上层消费者进行处理，如果 process 回调方法处理出错，将该对象重新存入队列。 Controller 的 processLoop 方法负责从 DeltaFIFO 队列中取出数据传递给 process 回调函数，process 函数的类型如下： type PopProcessFunc func(interface{}) error 一个 process 回调函数代码示例如下： vendor/k8s.io/client-go/tools/cache/shared_informer.go func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil && exists { if err := s.indexer.Update(d.Object); err != nil { return err } s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 在这个例子中，HandleDeltas 作为 process 的一个回调函数，当资源对象操作类型为 Added，Updated 和 Delted 时，该资源对象存储至 Indexer（它是并发安全的），并通过 distribute 函数将资源对象分发到 SharedInformer，在之前 Informer 机制架构设计的示例代码中，通过 informer.AddEventHandler 函数添加了对资源事件进行处理的函数，distribute 函数将资源对象分发到该事件处理函数。 Resync 机制 本节内容主要参考自 【提问】Informer 中为什么需要引入 Resync 机制？ Resync 机制会将 Indexer 本地存储中的资源对象同步到 DeltaFIFO 中，并将这些资源对象设置为 Sync 的操作类型， // 重新同步一次 Indexer 缓存数据到 Delta FIFO func (f *DeltaFIFO) Resync() error { f.lock.Lock() defer f.lock.Unlock() if f.knownObjects == nil { return nil } // 遍历 indexer 中的 key，传入 syncKeyLocked 中处理 keys := f.knownObjects.ListKeys() for _, k := range keys { if err := f.syncKeyLocked(k); err != nil { return err } } return nil } func (f *DeltaFIFO) syncKeyLocked(key string) error { obj, exists, err := f.knownObjects.GetByKey(key) if err != nil { klog.Errorf(\"Unexpected error %v during lookup of key %v, unable to queue object for sync\", err, key) return nil } else if !exists { klog.Infof(\"Key %v does not exist in known objects store, unable to queue object for sync\", key) return nil } // 若 FIFO 队列中已经有相同 key 的 event 进来了，说明该资源对象有了新的 event， // Indexer 中旧的缓存失效，直接返回 nil id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } if len(f.items[id]) > 0 { return nil } // 重新放入 FIFO 队列中 if err := f.queueActionLocked(Sync, obj); err != nil { return fmt.Errorf(\"couldn't queue object: %v\", err) } return nil } 为什么需要 Resync 机制呢？因为在处理 SharedInformer 事件回调时，可能存在处理失败的情况，定时的 Resync 让这些处理失败的事件有了重新 onUpdate 处理的机会。 那么经过 Resync 重新放入 Delta FIFO 队列的事件，和直接从 apiserver 中 watch 得到的事件处理起来有什么不一样呢？在消费者方法中有介绍过 HandleDeltas，其中就有关于 Resync 的部分： func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { // 上锁 for _, d := range obj.(Deltas) { // 判断事件类型 switch d.Type { case Sync, Replaced, Added, Updated: s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil && exists { if err := s.indexer.Update(d.Object); err != nil { return err } isSync := false switch { case d.Type == Sync: // 如果是通过 Resync 重新同步得到的事件则做个标记 isSync = true case d.Type == Replaced: ... } // 如果是通过 Resync 重新同步得到的事件，则触发 onUpdate 回调 s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, false) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 从上面对 Delta FIFO 的队列处理源码可看出，当从 Resync 重新同步到 Delta FIFO 队列的事件，会分发到 updateNotification 中触发 onUpdate 的回调。Resync 机制的引入，定时将 Indexer 缓存事件重新同步到 Delta FIFO 队列中，在处理 SharedInformer 事件回调时，让处理失败的事件得到重新处理。并且通过入队前判断 FIFO 队列中是否已经有了更新版本的 event，来决定是否丢弃 Indexer 缓存不进行 Resync 入队。在处理 Delta FIFO 队列中的 Resync 的事件数据时，触发 onUpdate 回调来让事件重新处理。 Indexer Client-go 用来存储资源对象并自带索引功能的本地存储，Reflector 从 DeltaFIFO 中将消费出来的资源对象存储到 Indexer，Indexer 与 Etcd 集群中的数据完全保持一致。从而 client-go 可以本地读取，减少 Kubernetes API 和 Etcd 集群的压力。 了解 Indexer 之前，先了解 ThreadSafeMap，ThreadSafeMap 是实现并发安全存储，就像 Go 1.9 后推出 sync.Map 一样。Kubernetes 开始编写的时候还没有 sync.Map。Indexer 在 ThreadSafeMap 的基础上进行了封装，继承了 ThreadSafeMap 的存储相关的增删改查相关操作方法，实现了 Indexer Func 等功能，例如 Index，IndexKeys，GetIndexers 等方法，这些方法为 ThreadSafeMap 提供了索引功能。如下图： ┌───────────────┐ ┌──────────────┐ │ Indeices │--->│ Index │ └───────────────┘ └──────────────┘ ┌───────────────┐ ┌──────────────┐ │ Indexers │--->│ IndexFun │ └───────────────┘ └──────────────┘ ┌───────────────────────────────────┐ │ ThreadSafeStore │ └───────────────────────────────────┘ ThreadSafeStore ThreadSafeStore 是一个内存中存储，数据不会写入本地磁盘，增删改查都会加锁，保证数据一致性。结构如下： vendor/k8s.io/client-go/tools/cache/store.go // threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} // indexers maps a name to an IndexFunc indexers Indexers // indices maps a name to an Index indices Indices } items 字段存储资源对象数据，其中 items 的 key 通过 keyFunc 函数计算得到，计算默认使用 MetaNamespaceKeyFunc 函数，该函数根据资源对象计算出 / 格式的 key，value 用于存储资源对象。 而后面两个字段的定义类型如下： vendor/k8s.io/client-go/tools/cache/index.go // Index maps the indexed value to a set of keys in the store that match on that value type Index map[string]sets.String // Indexers maps a name to a IndexFunc type Indexers map[string]IndexFunc // Indices maps a name to an Index type Indices map[string]Index Indexer 索引器 每次增删改 ThreadSafeStore 的数据时，都会通过 updateIndices 或 deleteFormIndices 函数变更 Indexer。Indexer 被设计为可以自定义索引函数，他有重要的四个数据结构，Indexers，IndexFunc，Indices 和 Index。 看下面这个例子的关键流程： func UsersIndexFunc(obj interfaces{}) ([]string, error) { pod := obj.(*v1.Pod) usersString := pod.Annotations[\"users\"] return strings.Split(userString, \",\"), nil } func main() { index := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{\"byUser\": UsersIndexFunc}) pod1 := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: \"one\", Annotations: map[string]string{\"users\": \"ernie,bert\"}}} // Initialize pod2 and pod3 index.Add(pod1) // Add pod2 and pod3 erniePods, err := omdex.ByIndex(\"byUser\", \"ernie\") } 首先定义了一个索引器函数（IndexFunc），UsersIndexFunc。该函数定义查询所有 Pod 资源下 Annotations 字段的 key 为 users 的 Pod： func UsersIndexFunc(obj interfaces{}) ([]string, error) { pod := obj.(*v1.Pod) usersString := pod.Annotations[\"users\"] return strings.Split(userString, \",\"), nil } Main 函数中 cache.NewIndexer 实例化了一个 Indexer 对象： index := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{\"byUser\": UsersIndexFunc}) 第一个参数计算资源对象的 key，默认就是 MetaNamespaceKeyFunc，第二个参数是一个 Indexers 对象，如上一节展示的定义那样，key 为索引器（IndexFunc）的名称，value 为索引器函数。 通过 index.Add 添加了三个 Pod，再通过 index.ByIndex 函数查询使用 byUser 索引器下匹配 ernie 字段的 Pod 列表： erniePods, err := index.ByIndex(\"byUser\", \"ernie\") 回看这四个类型： // Indexers maps a name to a IndexFunc type Indexers map[string]IndexFunc // IndexFunc knows how to provide an indexed value for an object. type IndexFunc func(obj interface{}) ([]string, error) // Indices maps a name to an Index type Indices map[string]Index // Index maps the indexed value to a set of keys in the store that match on that value type Index map[string]sets.String Indexers：存储索引器，key 为 索引器名称，value 为索引器实现函数； IndexFunc：索引器函数，定义为接收一个资源对象，返回检索结果列表； Indices：存储缓存器，key 为缓存器名称，value 为缓存数据； Index：存储缓存数据，结构为 K/V。 Indexer 索引器核心实现 vendor/k8s.io/client-go/tools/cache/thread_safe_store.go // ByIndex returns a list of items that match an exact value on the index function func (c *threadSafeMap) ByIndex(indexName, indexKey string) ([]interface{}, error) { c.lock.RLock() defer c.lock.RUnlock() indexFunc := c.indexers[indexName] if indexFunc == nil { return nil, fmt.Errorf(\"Index with name %s does not exist\", indexName) } index := c.indices[indexName] set := index[indexKey] list := make([]interface{}, 0, set.Len()) for _, key := range set.List() { list = append(list, c.items[key]) } return list, nil } ByIndex 接收两个参数：indexName（索引器名字）以及 indexKey（需要检索的 key），首先从 c.indexers 查找制定的索引器函数，然后从 c.indices 查找返回的缓存器函数，最后根据需要索引的 indexKey 从缓存数据中查到并返回。 ⚠️ K8s 将 map 结构类型的 key 作为 Set 数据结构，实现 Set 去重特性。 总结 本文从 Informer 的整体架构开始说起，介绍了各个核心组件的功能和作用，Kubernetes 之所以设计这样一个机制架构，核心是为了减少 Ectd 和 Kubernetes API Server 的压力，增强集群的稳定性。 Tags: Kubernetes Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/job.html":{"url":"blog/kubernetes/K8s源码/job.html","title":"Job","keywords":"","body":"job Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kube-proxy-iptables.html":{"url":"blog/kubernetes/K8s源码/kube-proxy-iptables.html","title":"Kube Proxy Iptables","keywords":"","body":"kube-proxy iptables 文章来源 iptables 的功能 在前面的文章中已经介绍过 iptable 的一些基本信息，本文会深入介绍 kube-proxy iptables 模式下的工作原理，本文中多处会与 iptables 的知识相关联，若没有 iptables 基础，请先自行补充。 iptables 的功能： 流量转发：DNAT 实现 IP 地址和端口的映射； 负载均衡：statistic 模块为每个后端设置权重； 会话保持：recent 模块设置会话保持时间； iptables 有五张表和五条链，五条链分别对应为： PREROUTING 链：数据包进入路由之前，可以在此处进行 DNAT； INPUT 链：一般处理本地进程的数据包，目的地址为本机； FORWARD 链：一般处理转发到其他机器或者 network namespace 的数据包； OUTPUT 链：原地址为本机，向外发送，一般处理本地进程的输出数据包； POSTROUTING 链：发送到网卡之前，可以在此处进行 SNAT 这五张表是对 iptables 所有规则的逻辑集群且是有顺序的，当数据包到达某一条链时会按表的顺序进行处理，表的优先级为：raw、mangle、nat、filter、security。 iptables 的工作流程如下图所示： kube-proxy 的 iptables 模式 Kube-proxy 组件负责维护node节点上的防火墙规则和路由规则，在iptables模式下，会根据service以及endpoints对象的改变来实时刷新规则，kube-proxy使用了iptables的filter表和nat表，并对iptbales的链进行了扩充，自定义了KUBE-SERVICE，KUBE-EXTERNAL-SERVICES，KUBE-POSTROUTING，KUBE-MARK-MASK，KUBE-MARK-DROP、KUBE-FORWARD 七条链，另外还新增了以“KUBE-SVC-xxx”和“KUBE-SEP-xxx”开头的数个链，除了创建自定义的链以外还将自定义链插入到已有链的后面以便劫持数据包。 在 nat 表中自定义的链以及追加的链如下所示： 在 filter 表定义的链以及追加的链如下所示如下所示： 对于 KUBE-MARK-MASQ 链中所有规则设置了 kubernetes 独有的 MARK 标记，在 KUBE-POSTROUTING 链中对 node 节点上匹配 kubernetes 独有 MARK 标记的数据包，进行 SNAT 处理。 -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 Kube-proxy 接着为每个服务创建 KUBE-SVC-xxx 链，并在 nat 表中将 KUBE-SERVICES 链中每个目标地址是service 的数据包导入这个 KUBE-SVC-xxx 链，如果 endpoint 尚未创建，则 KUBE-SVC-xxx 链中没有规则，任何 incomming packets 在规则匹配失败后会被 KUBE-MARK-DROP 进行标记然后再 FORWARD 链中丢弃。 这些自定义链与 iptables 的表结合后如下所示，笔者只画出了h- PREROUTING 和 OUTPUT 链中追加的链以及部分自定义链，因为 PREROUTING 和 OUTPUT 的首条 NAT 规则都先将所有流量导入KUBE-SERVICE 链中，这样就截获了所有的入流量和出流量，进而可以对 k8s 相关流量进行重定向处理。 kubernetes 自定义链中数据包的详细流转可以参考： iptables 规则分析 clusterIP 访问方式 创建一个 clusterIP 访问方式的 service 以及带有两个副本，从 pod 中访问 clusterIP 的 iptables 规则流向为： PREROUTING --> KUBE-SERVICE --> KUBE-SVC-XXX --> KUBE-SEP-XXX 访问流程如下所示： 1、对于进入 PREROUTING 链的都转到 KUBE-SERVICES 链进行处理； 2、在 KUBE-SERVICES 链，对于访问 clusterIP 为 10.110.243.155 的转发到 KUBE-SVC-5SB6FTEHND4GTL2W； 3、访问 KUBE-SVC-5SB6FTEHND4GTL2W 的使用随机数负载均衡，并转发到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 上； 4、KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 对应 endpoint 中的 pod 192.168.137.147 和 192.168.98.213，设置 mark 标记，进行 DNAT 并转发到具体的 pod 上，如果某个 service 的 endpoints 中没有 pod，那么针对此 service 的请求将会被 drop 掉； // 1. -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES // 2. -A KUBE-SERVICES -d 10.110.243.155/32 -p tcp -m comment --comment \"pks-system/tenant-service: cluster IP\" -m tcp --dport 7000 -j KUBE-SVC-5SB6FTEHND4GTL2W // 3. -A KUBE-SVC-5SB6FTEHND4GTL2W -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CI5ZO3FTK7KBNRMG -A KUBE-SVC-5SB6FTEHND4GTL2W -j KUBE-SEP-OVNLTDWFHTHII4SC // 4. -A KUBE-SEP-CI5ZO3FTK7KBNRMG -s 192.168.137.147/32 -j KUBE-MARK-MASQ -A KUBE-SEP-CI5ZO3FTK7KBNRMG -p tcp -m tcp -j DNAT --to-destination 192.168.137.147:7000 -A KUBE-SEP-OVNLTDWFHTHII4SC -s 192.168.98.213/32 -j KUBE-MARK-MASQ -A KUBE-SEP-OVNLTDWFHTHII4SC -p tcp -m tcp -j DNAT --to-destination 192.168.98.213:7000 nodePort 方式 在 nodePort 方式下，会用到 KUBE-NODEPORTS 规则链，通过 iptables -t nat -L -n 可以看到 KUBE-NODEPORTS 位于 KUBE-SERVICE 链的最后一个，iptables 在处理报文时会优先处理目的 IP 为clusterIP 的报文，在前面的 KUBE-SVC-XXX 都匹配失败之后再去使用 nodePort 方式进行匹配。 创建一个 nodePort 访问方式的 service 以及带有两个副本，访问 nodeport 的 iptables 规则流向为： 1、非本机访问 PREROUTING --> KUBE-SERVICE --> KUBE-NODEPORTS --> KUBE-SVC-XXX --> KUBE-SEP-XXX 2、本机访问 OUTPUT --> KUBE-SERVICE --> KUBE-NODEPORTS --> KUBE-SVC-XXX --> KUBE-SEP-XXX 该服务的 nodePort 端口为 30070，其 iptables 访问规则和使用 clusterIP 方式访问有点类似，不过 nodePort 方式会比 clusterIP 的方式多走一条链 KUBE-NODEPORTS，其会在 KUBE-NODEPORTS 链设置 mark 标记并转发到 KUBE-SVC-5SB6FTEHND4GTL2W，nodeport 与 clusterIP 访问方式最后都是转发到了 KUBE-SVC-xxx 链。 1、经过 PREROUTING 转到 KUBE-SERVICES 2、经过 KUBE-SERVICES 转到 KUBE-NODEPORTS 3、经过 KUBE-NODEPORTS 转到 KUBE-SVC-5SB6FTEHND4GTL2W 4、经过 KUBE-SVC-5SB6FTEHND4GTL2W 转到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV 5、经过 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV 分别转到 192.168.137.147:7000 和 192.168.89.11:7000 // 1. -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES // 2. ...... -A KUBE-SERVICES xxx ...... -A KUBE-SERVICES -m comment --comment \"kubernetes service nodeports; NOTE: this must be the last rule in this chain\" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS // 3. -A KUBE-NODEPORTS -p tcp -m comment --comment \"pks-system/tenant-service:\" -m tcp --dport 30070 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment \"pks-system/tenant-service:\" -m tcp --dport 30070 -j KUBE-SVC-5SB6FTEHND4GTL2W // 4、 -A KUBE-SVC-5SB6FTEHND4GTL2W -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CI5ZO3FTK7KBNRMG -A KUBE-SVC-5SB6FTEHND4GTL2W -j KUBE-SEP-VR562QDKF524UNPV // 5、 -A KUBE-SEP-CI5ZO3FTK7KBNRMG -s 192.168.137.147/32 -j KUBE-MARK-MASQ -A KUBE-SEP-CI5ZO3FTK7KBNRMG -p tcp -m tcp -j DNAT --to-destination 192.168.137.147:7000 -A KUBE-SEP-VR562QDKF524UNPV -s 192.168.89.11/32 -j KUBE-MARK-MASQ -A KUBE-SEP-VR562QDKF524UNPV -p tcp -m tcp -j DNAT --to-destination 192.168.89.11:7000 其他访问方式对应的 iptables 规则可自行分析。 iptables 模式源码分析 kubernetes 版本：v1.16 上篇文章已经在源码方面做了许多铺垫，下面就直接看 kube-proxy iptables 模式的核心方法。首先回顾一下 iptables 模式的调用流程，kube-proxy 根据给定的 proxyMode 初始化对应的 proxier 后会调用 Proxier.SyncLoop() 执行 proxier 的主循环，而其最终会调用 proxier.syncProxyRules() 刷新 iptables 规则。 proxier.SyncLoop() --> proxier.syncRunner.Loop()-->bfr.tryRun()-->bfr.fn()-->proxier.syncProxyRules() proxier.syncProxyRules()这个函数比较长，大约 800 行，其中有许多冗余的代码，代码可读性不佳，我们只需理解其基本流程即可，该函数的主要功能为： 更新proxier.endpointsMap，proxier.servieMap 创建自定义链 将当前内核中 filter 表和 nat 表中的全部规则导入到内存中 为每个 service 创建规则 为 clusterIP 设置访问规则 为 externalIP 设置访问规则 为 ingress 设置访问规则 为 nodePort 设置访问规则 为 endpoint 生成规则链 写入 DNAT 规则 删除不再使用的服务自定义链 使用 iptables-restore 同步规则 首先是更新 proxier.endpointsMap，proxier.servieMap 两个对象。 func (proxier *Proxier) syncProxyRules() { ...... serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges) staleServices := serviceUpdateResult.UDPStaleClusterIP for _, svcPortName := range endpointUpdateResult.StaleServiceNames { if svcInfo, ok := proxier.serviceMap[svcPortName]; ok && svcInfo != nil && svcInfo.Protocol() == v1.ProtocolUDP { staleServices.Insert(svcInfo.ClusterIP().String()) for _, extIP := range svcInfo.ExternalIPStrings() { staleServices.Insert(extIP) } } } ...... } 然后创建所需要的 iptable 链： for _, jump := range iptablesJumpChains { // 创建自定义链 if _, err := proxier.iptables.EnsureChain(jump.table, jump.dstChain); err != nil { ..... } args := append(jump.extraArgs, ...... ) //插入到已有的链 if _, err := proxier.iptables.EnsureRule(utiliptables.Prepend, jump.table, jump.srcChain, args...); err != nil { ...... } } 将当前内核中 filter 表和 nat 表中的全部规则临时导出到 buffer 中： err := proxier.iptables.SaveInto(utiliptables.TableFilter, proxier.existingFilterChainsData) if err != nil { } else { existingFilterChains = utiliptables.GetChainLines(utiliptables.TableFilter, proxier.existingFilterChainsData.Bytes()) } ...... err = proxier.iptables.SaveInto(utiliptables.TableNAT, proxier.iptablesData) if err != nil { } else { existingNATChains = utiliptables.GetChainLines(utiliptables.TableNAT, proxier.iptablesData.Bytes()) } writeLine(proxier.filterChains, \"*filter\") writeLine(proxier.natChains, \"*nat\") 检查已经创建出的表是否存在： for _, chainName := range []utiliptables.Chain{kubeServicesChain, kubeExternalServicesChain, kubeForwardChain} { if chain, ok := existingFilterChains[chainName]; ok { writeBytesLine(proxier.filterChains, chain) } else { writeLine(proxier.filterChains, utiliptables.MakeChainLine(chainName)) } } for _, chainName := range []utiliptables.Chain{kubeServicesChain, kubeNodePortsChain, kubePostroutingChain, KubeMarkMasqChain} { if chain, ok := existingNATChains[chainName]; ok { writeBytesLine(proxier.natChains, chain) } else { writeLine(proxier.natChains, utiliptables.MakeChainLine(chainName)) } } 写入 SNAT 地址伪装规则，在 POSTROUTING 阶段对地址进行 MASQUERADE 处理，原始请求源 IP 将被丢失，被请求 pod 的应用看到为 NodeIP 或 CNI 设备 IP(bridge/vxlan设备)： masqRule := []string{ ...... } if proxier.iptables.HasRandomFully() { masqRule = append(masqRule, \"--random-fully\") } else { } writeLine(proxier.natRules, masqRule...) writeLine(proxier.natRules, []string{ ...... }...) 为每个 service 创建规则，创建 KUBE-SVC-xxx 和 KUBE-XLB-xxx 链、创建 service portal 规则、为 clusterIP 创建规则： for svcName, svc := range proxier.serviceMap { svcInfo, ok := svc.(*serviceInfo) ...... if hasEndpoints { ...... } svcXlbChain := svcInfo.serviceLBChainName if svcInfo.OnlyNodeLocalEndpoints() { ...... } if hasEndpoints { ...... } else { ...... } 若服务使用了 externalIP，创建对应的规则： for _, externalIP := range svcInfo.ExternalIPStrings() { if local, err := utilproxy.IsLocalIP(externalIP); err != nil { ...... if proxier.portsMap[lp] != nil { ...... } else { ...... } } if hasEndpoints { ...... } else { ...... } } 若服务使用了 ingress，创建对应的规则： for _, ingress := range svcInfo.LoadBalancerIPStrings() { if ingress != \"\" { if hasEndpoints { ...... if !svcInfo.OnlyNodeLocalEndpoints() { ...... } if len(svcInfo.LoadBalancerSourceRanges()) == 0 { ...... } else { ...... } ...... } else { ...... } } } 若使用了 nodePort，创建对应的规则： if svcInfo.NodePort() != 0 { addresses, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer) lps := make([]utilproxy.LocalPort, 0) for address := range addresses { ...... lps = append(lps, lp) } for _, lp := range lps { if proxier.portsMap[lp] != nil { } else if svcInfo.Protocol() != v1.ProtocolSCTP { socket, err := proxier.portMapper.OpenLocalPort(&lp) ...... if lp.Protocol == \"udp\" { ...... } replacementPortsMap[lp] = socket } } if hasEndpoints { ...... } else { ...... } } 为 endpoint 生成规则链 KUBE-SEP-XXX： endpoints = endpoints[:0] endpointChains = endpointChains[:0] var endpointChain utiliptables.Chain for _, ep := range proxier.endpointsMap[svcName] { epInfo, ok := ep.(*endpointsInfo) ...... if chain, ok := existingNATChains[utiliptables.Chain(endpointChain)]; ok { writeBytesLine(proxier.natChains, chain) } else { writeLine(proxier.natChains, utiliptables.MakeChainLine(endpointChain)) } activeNATChains[endpointChain] = true } 如果创建 service 时指定了 SessionAffinity 为 clientIP 则使用 recent 创建保持会话连接的规则： if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { for _, endpointChain := range endpointChains { ...... } } 写入负载均衡和 DNAT 规则，对于 endpoints 中的 pod 使用随机访问负载均衡策略。 在 iptables 规则中加入该 service 对应的自定义链“KUBE-SVC-xxx”，如果该服务对应的 endpoints 大于等于2，则添加负载均衡规则； 针对非本地 Node 上的 pod，需进行 DNAT，将请求的目标地址设置成候选的 pod 的 IP 后进行路由，KUBE-MARK-MASQ 将重设(伪装)源地址； for i, endpointChain := range endpointChains { ...... if svcInfo.OnlyNodeLocalEndpoints() && endpoints[i].IsLocal { ...... } ...... epIP := endpoints[i].IP() if epIP == \"\" { ...... } ...... args = append(args, \"-j\", string(endpointChain)) writeLine(proxier.natRules, args...) ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { ...... } ...... writeLine(proxier.natRules, args...) } 若启用了 clusterCIDR 则生成对应的规则链： if len(proxier.clusterCIDR) > 0 { ...... writeLine(proxier.natRules, args...) } 为本机的 pod 开启会话保持： args = append(args[:0], \"-A\", string(svcXlbChain)) writeLine(proxier.natRules, ......) numLocalEndpoints := len(localEndpointChains) if numLocalEndpoints == 0 { ...... writeLine(proxier.natRules, args...) } else { if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { for _, endpointChain := range localEndpointChains { ...... } } ...... for i, endpointChain := range localEndpointChains { ...... args = append(args, \"-j\", string(endpointChain)) writeLine(proxier.natRules, args...) } } } 删除不存在服务的自定义链，KUBE-SVC-xxx、KUBE-SEP-xxx、KUBE-FW-xxx、KUBE-XLB-xxx： for chain := range existingNATChains { if !activeNATChains[chain] { ...... if !strings.HasPrefix(chainString, \"KUBE-SVC-\") && !strings.HasPrefix(chainString, \"KUBE-SEP-\") && !strings.HasPrefix(chainString, \"KUBE-FW-\") && ! strings.HasPrefix(chainString, \"KUBE-XLB-\") { ...... continue } writeBytesLine(proxier.natChains, existingNATChains[chain]) writeLine(proxier.natRules, \"-X\", chainString) } } 在 KUBE-SERVICES 链最后添加 nodePort 规则： writeLine(proxier.filterRules, ...... ) writeLine(proxier.filterRules, ...... ) if len(proxier.clusterCIDR) != 0 { writeLine(proxier.filterRules, ...... ) writeLine(proxier.filterRules, ...... ) } 为 INVALID 状态的包添加规则，为 KUBE-FORWARD 链添加对应的规则： addresses, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer) if err != nil { ...... } else { for address := range addresses { if utilproxy.IsZeroCIDR(address) { ...... } if isIPv6 && !utilnet.IsIPv6String(address) || !isIPv6 && utilnet.IsIPv6String(address) { ...... } ..... writeLine(proxier.natRules, args...) } } 在结尾添加标志： writeLine(proxier.filterRules, \"COMMIT\") writeLine(proxier.natRules, \"COMMIT\") 使用 iptables-restore 同步规则： proxier.iptablesData.Reset() proxier.iptablesData.Write(proxier.filterChains.Bytes()) proxier.iptablesData.Write(proxier.filterRules.Bytes()) proxier.iptablesData.Write(proxier.natChains.Bytes()) proxier.iptablesData.Write(proxier.natRules.Bytes()) err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters) if err != nil { ...... } 以上就是对 kube-proxy iptables 代理模式核心源码的一个走读。 总结 本文主要讲了 kube-proxy iptables 模式的实现，可以看到其中的 iptables 规则是相当复杂的，在实际环境中尽量根据已有服务再来梳理整个 iptables 规则链就比较清楚了，笔者对于 iptables 的知识也是现学的，文中如有不当之处望指正。上面分析完了整个 iptables 模式的功能，但是 iptable 存在一些性能问题，比如有规则线性匹配时延、规则更新时延、可扩展性差等，为了解决这些问题于是有了 ipvs 模式，在下篇文章中会继续介绍 ipvs 模式的实现。 Question： iptables 中的表和链的关系 当我们访问一个svc IP地址的时候 内核， iptables， 网卡， 网关， 路由规则 都是如何协同工作的 参考： https://www.jianshu.com/p/a978af8e5dd8 https://blog.csdn.net/ebay/article/details/52798074 https://blog.csdn.net/horsefoot/article/details/51249161 https://rootdeep.github.io/posts/kube-proxy-code-analysis/ https://www.cnblogs.com/charlieroro/p/9588019.html Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kube-proxy-ipvs.html":{"url":"blog/kubernetes/K8s源码/kube-proxy-ipvs.html","title":"Kube Proxy Ipvs","keywords":"","body":"kube-proxy ipvs 文章来源 ipvs ipvs (IP Virtual Server) 是基于 Netfilter 的，作为 linux 内核的一部分实现了传输层负载均衡，ipvs 集成在LVS(Linux Virtual Server)中，它在主机中运行，并在真实服务器集群前充当负载均衡器。ipvs 可以将对 TCP/UDP 服务的请求转发给后端的真实服务器，因此 ipvs 天然支持 Kubernetes Service。ipvs 也包含了多种不同的负载均衡算法，例如轮询、最短期望延迟、最少连接以及各种哈希方法等，ipvs 的设计就是用来为大规模服务进行负载均衡的。 ipvs 的负载均衡方式 ipvs 有三种负载均衡方式，分别为： NAT TUN DR 关于三种模式的原理可以参考：LVS 配置小结。 上面的负载均衡方式中只有 NAT 模式可以进行端口映射，因此 kubernetes 中 ipvs 的实现使用了 NAT 模式，用来将 service IP 和 service port 映射到后端的 container ip 和container port。 NAT 模式下的工作流程如下所示： +--------+ | Client | +--------+ (CIP) 其具体流程为：当用户发起一个请求时，请求从 VIP 接口流入，此时数据源地址是 CIP，目标地址是 VIP，当接收到请求后拆掉 mac 地址封装后看到目标 IP 地址就是自己，按照正常流程会通过 INPUT 转入用户空间，但此时工作在 INPUT 链上的 ipvs 会强行将数据转到 POSTROUTING 链上，并根据相应的负载均衡算法选择后端具体的服务器，再通过 DNAT 转发给 Real server，此时源地址 CIP，目标地址变成了 RIP。 ipvs 与 iptables 的区别与联系 区别： 底层数据结构：iptables 使用链表，ipvs 使用哈希表 负载均衡算法：iptables 只支持随机、轮询两种负载均衡算法而 ipvs 支持的多达 8 种； 操作工具：iptables 需要使用 iptables 命令行工作来定义规则，ipvs 需要使用 ipvsadm 来定义规则。 此外 ipvs 还支持 realserver 运行状况检查、连接重试、端口映射、会话保持等功能。 联系： ipvs 和 iptables 都是基于 netfilter内核模块，两者都是在内核中的五个钩子函数处工作，下图是 ipvs 所工作的几个钩子函数： 关于 kube-proxy iptables 与 ipvs 模式的区别，更多详细信息可以查看官方文档：https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md。 ipset IP sets 是 Linux 内核中的一个框架，可以由 ipset 命令进行管理。根据不同的类型，IP set 可以以某种方式保存 IP地址、网络、(TCP/UDP)端口号、MAC地址、接口名或它们的组合，并且能够快速匹配。 根据官网的介绍，若有以下使用场景： 在保存了多个 IP 地址或端口号的 iptables 规则集合中想使用哈希查找; 根据 IP 地址或端口动态更新 iptables 规则时希望在性能上无损； 在使用 iptables 工具创建一个基于 IP 地址和端口的复杂规则时觉得非常繁琐； 此时，使用 ipset 工具可能是你最好的选择。 ipset 是 iptables 的一种扩展，在 iptables 中可以使用-m set启用 ipset 模块，具体来说，ipvs 使用 ipset 来存储需要 NAT 或 masquared 时的 ip 和端口列表。在数据包过滤过程中，首先遍历 iptables 规则，在定义了使用 ipset 的条件下会跳转到 ipset 列表中进行匹配。 kube-proxy ipvs 模式 kube-proxy 的 ipvs 模式是在 2015 年由 k8s 社区的大佬 thockin 提出的(Try kube-proxy via ipvs instead of iptables or userspace)，在 2017 年由华为云团队实现的(Implement IPVS-based in-cluster service load balancing)。前面的文章已经提到了，在kubernetes v1.8 中已经引入了 ipvs 模式。 kube-proxy 在 ipvs 模式下自定义了八条链，分别为 KUBE-SERVICES、KUBE-FIREWALL、KUBE-POSTROUTING、KUBE-MARK-MASQ、KUBE-NODE-PORT、KUBE-MARK-DROP、KUBE-FORWARD、KUBE-LOAD-BALANCER ，如下所示： NAT 表： Filter 表： 此外，由于 linux 内核原生的 ipvs 模式只支持 DNAT，不支持 SNAT，所以，在以下几种场景中 ipvs 仍需要依赖 iptables 规则： 1、kube-proxy 启动时指定 –-masquerade-all=true 参数，即集群中所有经过 kube-proxy 的包都做一次 SNAT； 2、kube-proxy 启动时指定 --cluster-cidr= 参数； 3、对于 Load Balancer 类型的 service，用于配置白名单； 4、对于 NodePort 类型的 service，用于配置 MASQUERADE； 5、对于 externalIPs 类型的 service； 但对于 ipvs 模式的 kube-proxy，无论有多少 pod/service，iptables 的规则数都是固定的。 ipvs 模式的启用 1、首先要加载 IPVS 所需要的 kernel module modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 cut -f1 -d \" \" /proc/modules | grep -e ip_vs -e nf_conntrack_ipv4 2、在启动 kube-proxy 时，指定 proxy-mode 参数 --proxy-mode=ipvs (如果要使用其他负载均衡算法，可以指定 --ipvs-scheduler= 参数，默认为 rr) 当创建 ClusterIP type 的 service 时，IPVS proxier 会执行以下三个操作： 确保本机已创建 dummy 网卡，默认为 kube-ipvs0。为什么要创建 dummy 网卡？因为 ipvs netfilter 的 DNAT 钩子挂载在 INPUT 链上，当访问 ClusterIP 时，将 ClusterIP 绑定在 dummy 网卡上为了让内核识别该 IP 就是本机 IP，进而进入 INPUT 链，然后通过钩子函数 ip_vs_in 转发到 POSTROUTING 链； 将 ClusterIP 绑定到 dummy 网卡； 为每个 ClusterIP 创建 IPVS virtual servers 和 real server，分别对应 service 和 endpoints； 例如下面的示例： // kube-ipvs0 dummy 网卡 $ ip addr ...... 4: kube-ipvs0: mtu 1500 qdisc noop state DOWN group default link/ether de:be:c0:73:bc:c7 brd ff:ff:ff:ff:ff:ff inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.97.4.140/32 brd 10.97.4.140 scope global kube-ipvs0 valid_lft forever preferred_lft forever ...... // 10.97.4.140 为 CLUSTER-IP 挂载在 kube-ipvs0 上 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE tenant-service ClusterIP 10.97.4.140 7000/TCP 23s // 10.97.4.140 后端的 realserver 分别为 10.244.1.2 和 10.244.1.3 $ ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.97.4.140:7000 rr -> 10.244.1.2:7000 Masq 1 0 0 -> 10.244.1.3:7000 Masq 1 0 0 ipvs 模式下数据包的流向 clusterIP 访问方式 PREROUTING --> KUBE-SERVICES --> KUBE-CLUSTER-IP --> INPUT --> KUBE-FIREWALL --> POSTROUTING 首先进入 PREROUTING 链 从 PREROUTING 链会转到 KUBE-SERVICES 链，10.244.0.0/16 为 ClusterIP 网段 在 KUBE-SERVICES 链打标记 从 KUBE-SERVICES 链再进入到 KUBE-CLUSTER-IP 链 KUBE-CLUSTER-IP 为 ipset 集合，在此处会进行 DNAT 然后会进入 INPUT 链 从 INPUT 链会转到 KUBE-FIREWALL 链，在此处检查标记 在 INPUT 链处，ipvs 的 LOCAL_IN Hook 发现此包在 ipvs 规则中则直接转发到 POSTROUTING 链 -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ // 执行完 PREROUTING 规则,数据打上0x4000/0x4000的标记 -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 -A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT KUBE-CLUSTER-IP 为 ipset 列表： # ipset list | grep -A 20 KUBE-CLUSTER-IP Name: KUBE-CLUSTER-IP Type: hash:ip,port Revision: 5 Header: family inet hashsize 1024 maxelem 65536 Size in memory: 352 References: 2 Members: 10.96.0.10,17:53 10.96.0.10,6:53 10.96.0.1,6:443 10.96.0.10,6:9153 然后会进入 INPUT： -A INPUT -j KUBE-FIREWALL -A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP 如果进来的数据带有 0x8000/0x8000 标记则丢弃，若有 0x4000/0x4000 标记则正常执行： -A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE nodePort 方式 PREROUTING --> KUBE-SERVICES --> KUBE-NODE-PORT --> INPUT --> KUBE-FIREWALL --> POSTROUTING 首先进入 PREROUTING 链 从 PREROUTING 链会转到 KUBE-SERVICES 链 在 KUBE-SERVICES 链打标记 从 KUBE-SERVICES 链再进入到 KUBE-NODE-PORT 链 KUBE-NODE-PORT 为 ipset 集合，在此处会进行 DNAT 然后会进入 INPUT 链 从 INPUT 链会转到 KUBE-FIREWALL 链，在此处检查标记 在 INPUT 链处，ipvs 的 LOCAL_IN Hook 发现此包在 ipvs 规则中则直接转发到 POSTROUTING 链 -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT KUBE-NODE-PORT 对应的 ipset 列表： # ipset list | grep -B 10 KUBE-NODE-PORT Name: KUBE-NODE-PORT-TCP Type: bitmap:port Revision: 3 Header: range 0-65535 Size in memory: 8268 References: 0 Members: 流入 INPUT 后与 ClusterIP 的访问方式相同。 kube-proxy ipvs 源码分析 kubernetes 版本：v1.16 在前面的文章中已经介绍过 ipvs 的初始化了，下面直接看其核心方法：proxier.syncRunner。 func NewProxier(......) { ...... proxier.syncRunner = async.NewBoundedFrequencyRunner(\"sync-runner\", proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) ...... } proxier.syncRunner() 执行流程： 通过 iptables-save 获取现有的 Filter 和 NAT 表存在的链数据 创建自定义链与规则 创建 Dummy 接口和 ipset 默认列表 为每个服务生成 ipvs 规则 对 serviceMap 内的每个服务进行遍历处理，对不同的服务类型(clusterip/nodePort/externalIPs/load-balancer)进行不同的处理(ipset 列表/vip/ipvs 后端服务器) 根据 endpoint 列表，更新 KUBE-LOOP-BACK 的 ipset 列表 若为 clusterIP 类型更新对应的 ipset 列表 KUBE-CLUSTER-IP 若为 externalIPs 类型更新对应的 ipset 列表 KUBE-EXTERNAL-IP 若为 load-balancer 类型更新对应的 ipset 列表 KUBE-LOAD-BALANCER、KUBE-LOAD-BALANCER-LOCAL、KUBE-LOAD-BALANCER-FW、KUBE-LOAD-BALANCER-SOURCE-CIDR、KUBE-LOAD-BALANCER-SOURCE-IP 若为 NodePort 类型更新对应的 ipset 列表 KUBE-NODE-PORT-TCP、KUBE-NODE-PORT-LOCAL-TCP、KUBE-NODE-PORT-LOCAL-SCTP-HASH、KUBE-NODE-PORT-LOCAL-UDP、KUBE-NODE-PORT-SCTP-HASH、KUBE-NODE-PORT-UDP 同步 ipset 记录 刷新 iptables 规则 func (proxier *Proxier) syncProxyRules() { proxier.mu.Lock() defer proxier.mu.Unlock() serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges) staleServices := serviceUpdateResult.UDPStaleClusterIP // 合并 service 列表 for _, svcPortName := range endpointUpdateResult.StaleServiceNames { if svcInfo, ok := proxier.serviceMap[svcPortName]; ok && svcInfo != nil && svcInfo.Protocol() == v1.ProtocolUDP { staleServices.Insert(svcInfo.ClusterIP().String()) for _, extIP := range svcInfo.ExternalIPStrings() { staleServices.Insert(extIP) } } } ...... 读取系统 iptables 到内存，创建自定义链以及 iptables 规则，创建 dummy interface kube-ipvs0，创建默认的 ipset 规则。 proxier.natChains.Reset() proxier.natRules.Reset() proxier.filterChains.Reset() proxier.filterRules.Reset() writeLine(proxier.filterChains, \"*filter\") writeLine(proxier.natChains, \"*nat\") // 创建kubernetes的表连接链数据 proxier.createAndLinkeKubeChain() // 创建 dummy interface kube-ipvs0 _, err := proxier.netlinkHandle.EnsureDummyDevice(DefaultDummyDevice) if err != nil { ...... return } // 创建默认的 ipset 规则 for _, set := range proxier.ipsetList { if err := ensureIPSet(set); err != nil { return } set.resetEntries() } 对每一个服务创建 ipvs 规则。根据 endpoint 列表，更新 KUBE-LOOP-BACK 的 ipset 列表。 for svcName, svc := range proxier.serviceMap { svcInfo, ok := svc.(*serviceInfo) if !ok { ...... } for _, e := range proxier.endpointsMap[svcName] { ep, ok := e.(*proxy.BaseEndpointInfo) if !ok { klog.Errorf(\"Failed to cast BaseEndpointInfo %q\", e.String()) continue } ...... if valid := proxier.ipsetList[kubeLoopBackIPSet].validateEntry(entry); !valid { ...... } proxier.ipsetList[kubeLoopBackIPSet].activeEntries.Insert(entry.String()) } 对于 clusterIP 类型更新对应的 ipset 列表 KUBE-CLUSTER-IP。 if valid := proxier.ipsetList[kubeClusterIPSet].validateEntry(entry); !valid { ...... } proxier.ipsetList[kubeClusterIPSet].activeEntries.Insert(entry.String()) ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { ...... } // 绑定 ClusterIP to dummy interface if err := proxier.syncService(svcNameString, serv, true); err == nil { // 同步 endpoints 信息 if err := proxier.syncEndpoint(svcName, false, serv); err != nil { ...... } } else { ...... } 为 externalIP 创建 ipvs 规则。 for _, externalIP := range svcInfo.ExternalIPStrings() { if local, err := utilproxy.IsLocalIP(externalIP); err != nil { ...... } else if local && (svcInfo.Protocol() != v1.ProtocolSCTP) { ...... if proxier.portsMap[lp] != nil { ...... } else { socket, err := proxier.portMapper.OpenLocalPort(&lp) if err != nil { ...... } replacementPortsMap[lp] = socket } } ...... if valid := proxier.ipsetList[kubeExternalIPSet].validateEntry(entry); !valid { ...... } proxier.ipsetList[kubeExternalIPSet].activeEntries.Insert(entry.String()) ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { ...... } if err := proxier.syncService(svcNameString, serv, true); err == nil { ...... if err := proxier.syncEndpoint(svcName, false, serv); err != nil { ...... } } else { ...... } } 为 load-balancer类型创建 ipvs 规则。 for _, ingress := range svcInfo.LoadBalancerIPStrings() { if ingress != \"\" { ...... if valid := proxier.ipsetList[kubeLoadBalancerSet].validateEntry(entry); !valid { ...... } proxier.ipsetList[kubeLoadBalancerSet].activeEntries.Insert(entry.String()) if svcInfo.OnlyNodeLocalEndpoints() { ...... } if len(svcInfo.LoadBalancerSourceRanges()) != 0 { ...... for _, src := range svcInfo.LoadBalancerSourceRanges() { ...... } ...... } ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { ...... } if err := proxier.syncService(svcNameString, serv, true); err == nil { ...... if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil { ...... } } else { ...... } } } 为 nodePort 类型创建 ipvs 规则。 if svcInfo.NodePort() != 0 { ...... var lps []utilproxy.LocalPort for _, address := range nodeAddresses { ...... lps = append(lps, lp) } for _, lp := range lps { if proxier.portsMap[lp] != nil { ...... } else if svcInfo.Protocol() != v1.ProtocolSCTP { socket, err := proxier.portMapper.OpenLocalPort(&lp) if err != nil { ...... } if lp.Protocol == \"udp\" { ...... } } } switch protocol { case \"tcp\": ...... case \"udp\": ...... case \"sctp\": ...... default: ...... } if nodePortSet != nil { for _, entry := range entries { ...... nodePortSet.activeEntries.Insert(entry.String()) } } if svcInfo.OnlyNodeLocalEndpoints() { var nodePortLocalSet *IPSet switch protocol { case \"tcp\": nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetTCP] case \"udp\": nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetUDP] case \"sctp\": nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetSCTP] default: ...... } if nodePortLocalSet != nil { entryInvalidErr := false for _, entry := range entries { ...... nodePortLocalSet.activeEntries.Insert(entry.String()) } ...... } } for _, nodeIP := range nodeIPs { ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { ...... } if err := proxier.syncService(svcNameString, serv, false); err == nil { if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil { ...... } } else { ...... } } } } 同步 ipset 记录，清理 conntrack。 for _, set := range proxier.ipsetList { set.syncIPSetEntries() } proxier.writeIptablesRules() proxier.iptablesData.Reset() proxier.iptablesData.Write(proxier.natChains.Bytes()) proxier.iptablesData.Write(proxier.natRules.Bytes()) proxier.iptablesData.Write(proxier.filterChains.Bytes()) proxier.iptablesData.Write(proxier.filterRules.Bytes()) err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters) if err != nil { ...... } ...... proxier.deleteEndpointConnections(endpointUpdateResult.StaleEndpoints) } 总结 本文主要讲述了 kube-proxy ipvs 模式的原理与实现，iptables 模式与 ipvs 模式下在源码实现上有许多相似之处，但二者原理不同，理解了原理分析代码则更加容易，笔者对于 ipvs 的知识也是现学的，文中如有不当之处望指正。虽然 ipvs 的性能要比 iptables 更好，但社区中已有相关的文章指出 BPF(Berkeley Packet Filter) 比 ipvs 的性能更好，且 BPF 将要取代 iptables，至于下一步如何发展，让我们拭目以待。 参考： http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.filter_rules.html https://bestsamina.github.io/posts/2018-10-19-ipvs-based-kube-proxy-4-scaled-k8s-lb/ https://www.bookstack.cn/read/k8s-source-code-analysis/core-kube-proxy-ipvs.md https://blog.51cto.com/goome/2369150 https://xigang.github.io/2019/07/21/kubernetes-service/ https://segmentfault.com/a/1190000016333317 https://cilium.io/blog/2018/04/17/why-is-the-kernel-community-replacing-iptables/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kube-proxy.html":{"url":"blog/kubernetes/K8s源码/kube-proxy.html","title":"Kube Proxy","keywords":"","body":"kube-proxy 本文的目的是通过阅读k8s源码学习其技术，应用到自己的项目中 function command metric BindAddress EnableProfiling kube-proxy workflow Chice mod nf_conntrack_max starting endpoints Starting service config sync cache of service config sync cache of endpoint config Application ecs > 100台； 选择自建 机房 kubernetes 相关服务 ---->. 内部的运维解决 容器云 服务商 价值 在哪里 ？ 如果托管给第三方，安全性和灵活性 怎么解决 ？ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kubectl-exec.html":{"url":"blog/kubernetes/K8s源码/kubectl-exec.html","title":"Kubectl Exec","keywords":"","body":"kubectl exec 对于经常和 Kubernetes 打交道的 YAML 工程师来说，最常用的命令就是 kubectl exec 了，通过它可以直接在容器内执行命令来调试应用程序。如果你不满足于只是用用而已，想了解 kubectl exec 的工作原理，那么本文值得你仔细读一读。本文将通过参考 kubectl、API Server、Kubelet 和容器运行时接口（CRI）Docker API 中的相关代码来了解该命令是如何工作的。 kubectl exec 的工作原理用一张图就可以表示： 这里有两个重要的 HTTP 请求： GET 请求用来获取 Pod 信息。 POST 请求调用 Pod 的子资源 exec 在容器内执行命令。 子资源（subresource）隶属于某个 K8S 资源，表示为父资源下方的子路径，例如 /logs、/status、/scale、/exec 等。其中每个子资源支持的操作根据对象的不同而改变。 最后 API Server 返回了 101 Ugrade 响应，向客户端表示已切换到 SPDY 协议。 SPDY 允许在单个 TCP 连接上复用独立的 stdin/stdout/stderr/spdy-error 流。 Question： http 如何复用SPDY协议 kubectl exec 是tcp 协议还是http Link: https://my.oschina.net/u/4131034/blog/3224587 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kubectl源码阅读1.html":{"url":"blog/kubernetes/K8s源码/kubectl源码阅读1.html","title":"Kubectl源码阅读1","keywords":"","body":"kubectl 源码阅读1 资源对象创建过程 Question: 创建流程 构建Cmd， 构建 资源对象， request api， handle result Info 用到了哪些设计模式 工厂模式 建造者模式： The Builder object simplifies converting standard command line arguments and parameters into a Visitor that can iterate over all of the identified resources, whether on the server or on the local filesystem. 访问者模式: 将多个方法用于作用于同一个对象，该设计模式使 操作和对象分离 用到了哪些接口 Factory Visitor：The Visitor interface makes it easy to deal with multiple resources in bulk for retrieval and operation. Helper：The Helper object provides simple CRUD operations on resources. Builder： The Builder object simplifies converting standard command line arguments and parameters into a Visitor that can iterate over all of the identified resources, whether on the server or on the local filesystem. NewCommand ---> add commands ​ add sub Commands create example // NewCmdCreate returns new initialized instance of create sub command func NewCmdCreate(f cmdutil.Factory, ioStreams genericclioptions.IOStreams) *cobra.Command { o := NewCreateOptions(ioStreams) cmd := &cobra.Command{ ... Run: func(cmd *cobra.Command, args []string) { .. cmdutil.CheckErr(o.RunCreate(f, cmd)) }, } ... // create subcommands cmd.AddCommand(NewCmdCreateNamespace(f, ioStreams)) ... return cmd } type Result struct{ Operation Opearation // ERROR, SET, DROP MergedResult interface{} } strategic visitor createReplaceStrategy createMergeStrategy createRetainKeysStrategy openapi.Resource 是干啥的 // Factory creates an Element by combining object values from recorded, local and remote sources with // the metadata from an openapi schema. type Factory struct { // Resources contains the openapi field metadata for the object models Resources openapi.Resources } dryRunVerifier := &DryRunVerifier{ Finder: cmdutil.NewCRDFinder(cmdutil.CRDFromDynamic(o.DynamicClient)), OpenAPIGetter: o.DiscoveryClient, } r := o.Builder. // unstructuredscheme.NewUnstructuredObjectTyper() // unstructuredObjectTyper runtime.ObjectTyper // imple get kind and version // target : set objectType and mapper Unstructured(). // Schema(o.Validator). ContinueOnError(). NamespaceParam(o.Namespace).DefaultNamespace(). FilenameParam(o.EnforceNamespace, &o.DeleteOptions.FilenameOptions). LabelSelectorParam(o.Selector). Flatten(). // Do returns a Result object with a Visitor for the resources identified by the Builder. // The visitor will respect the error behavior specified by ContinueOnError. Note that stream // inputs are consumed by the first execution - use Infos() or Object() on the Result to capture a list // for further iteration. Do() err = r.Visit(func(info *resource.Info, err error) error { if o.ServerSideApply { data, err := runtime.Encode(unstructured.UnstructuredJSONScheme, info.Object) if err != nil { return cmdutil.AddSourceToErr(\"serverside-apply\", info.Source, err) } ... obj, err := resource.NewHelper(info.Client, info.Mapping).Patch( info.Namespace, info.Name, types.ApplyPatchType, data, &options, ) info.Refresh(obj, true) } } NewDecoratedVisitor --> ContinueOnErrorVisitor ---> NewFlattenListVisitor --->EagerVisitorList--> DecoratedVisitor→ContinueOnErrorVisitor → FlattenListVisitor →FlattenListVisitor → StreamVisitor →FileVisitor→EagerVisitorList 构建restClient factoryImpl Complete 方法中可获取 ​ o.DynamicClient, err = f.DynamicClient() discoveryClient, err := f.ToDiscoveryClient() o.OpenAPISchema, _ = f.OpenAPISchema() o.Builder = f.NewBuilder() o.Mapper, err = f.ToRESTMapper() getObjects func (o *ApplyOptions) GetObjects() ([]*resource.Info, error) { var err error = nil r := o.Builder. Unstructured(). Schema(o.Validator). ContinueOnError(). NamespaceParam(o.Namespace).DefaultNamespace(). Flatten(). Do() o.objects, err = r.Infos() o.objectsCached = true return o.objects, err } apply func (o *ApplyOptions) applyOneObject(info *resource.Info) error { // ????? o.MarkNamespaceVisited(info) // ????? if err := o.Recorder.Record(info.Object); err != nil { klog.V(4).Infof(\"error recording current command: %v\", err) } if len(info.Name) == 0 { // ????? metadata, _ := meta.Accessor(info.Object) generatedName := metadata.GetGenerateName() if len(generatedName) > 0 { return fmt.Errorf(\"from %s: cannot use generate name with apply\", generatedName) } } helper := resource.NewHelper(info.Client, info.Mapping). // ????? WithFieldManager(o.FieldManager) if o.ServerSideApply { // Send the full object to be applied on the server side. data, err := runtime.Encode(unstructured.UnstructuredJSONScheme, info.Object) if err != nil { return cmdutil.AddSourceToErr(\"serverside-apply\", info.Source, err) } options := metav1.PatchOptions{ Force: &o.ForceConflicts, } obj, err := helper.Patch( info.Namespace, info.Name, types.ApplyPatchType, data, &options, ) if err != nil { if isIncompatibleServerError(err) { err = fmt.Errorf(\"Server-side apply not available on the server: (%v)\", err) } if errors.IsConflict(err) { err = fmt.Errorf(`%v Please review the fields above--they currently have other managers. Here are the ways you can resolve this warning: * If you intend to manage all of these fields, please re-run the apply command with the `+\"`--force-conflicts`\"+` flag. * If you do not intend to manage all of the fields, please edit your manifest to remove references to the fields that should keep their current managers. * You may co-own fields by updating your manifest to match the existing value; in this case, you'll become the manager if the other manager(s) stop managing the field (remove it from their configuration). See http://k8s.io/docs/reference/using-api/api-concepts/#conflicts`, err) } return err } info.Refresh(obj, true) if err := o.MarkObjectVisited(info); err != nil { return err } if o.shouldPrintObject() { return nil } printer, err := o.ToPrinter(\"serverside-applied\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } // Get the modified configuration of the object. Embed the result // as an annotation in the modified configuration, so that it will appear // in the patch sent to the server. modified, err := util.GetModifiedConfiguration(info.Object, true, unstructured.UnstructuredJSONScheme) if err != nil { return cmdutil.AddSourceToErr(fmt.Sprintf(\"retrieving modified configuration from:\\n%s\\nfor:\", info.String()), info.Source, err) } if err := info.Get(); err != nil { if !errors.IsNotFound(err) { return cmdutil.AddSourceToErr(fmt.Sprintf(\"retrieving current configuration of:\\n%s\\nfrom server for:\", info.String()), info.Source, err) } // Create the resource if it doesn't exist // First, update the annotation used by kubectl apply if err := util.CreateApplyAnnotation(info.Object, unstructured.UnstructuredJSONScheme); err != nil { return cmdutil.AddSourceToErr(\"creating\", info.Source, err) } if o.DryRunStrategy != cmdutil.DryRunClient { // Then create the resource and skip the three-way merge obj, err := helper.Create(info.Namespace, true, info.Object) if err != nil { return cmdutil.AddSourceToErr(\"creating\", info.Source, err) } info.Refresh(obj, true) } if err := o.MarkObjectVisited(info); err != nil { return err } if o.shouldPrintObject() { return nil } printer, err := o.ToPrinter(\"created\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } if err := o.MarkObjectVisited(info); err != nil { return err } if o.DryRunStrategy != cmdutil.DryRunClient { metadata, _ := meta.Accessor(info.Object) annotationMap := metadata.GetAnnotations() if _, ok := annotationMap[corev1.LastAppliedConfigAnnotation]; !ok { fmt.Fprintf(o.ErrOut, warningNoLastAppliedConfigAnnotation, o.cmdBaseName) } patcher, err := newPatcher(o, info, helper) if err != nil { return err } patchBytes, patchedObject, err := patcher.Patch(info.Object, modified, info.Source, info.Namespace, info.Name, o.ErrOut) if err != nil { return cmdutil.AddSourceToErr(fmt.Sprintf(\"applying patch:\\n%s\\nto:\\n%v\\nfor:\", patchBytes, info), info.Source, err) } info.Refresh(patchedObject, true) if string(patchBytes) == \"{}\" && !o.shouldPrintObject() { printer, err := o.ToPrinter(\"unchanged\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } } if o.shouldPrintObject() { return nil } printer, err := o.ToPrinter(\"configured\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kubectl源码阅读2.html":{"url":"blog/kubernetes/K8s源码/kubectl源码阅读2.html","title":"Kubectl源码阅读2","keywords":"","body":"kubectl 源码阅读2 简介 从API Server看，kubectl 其实是高级定制版的 curl 工具 从kubectl 开始 A Tour of the Kubernetes Source Code Part One: From kubectl to API Server 以kubectl create -f xx.yaml 为例，你会在pkg/kubectl/cmd/create 下找到一个create.go。类似的，所有kubectl 的命令 都可以在 pkg/kubectl/cmd 下找到。kubectl 命令行库 用的是 spf13/cobra 可以看到，阅读Kubernetes 源码需要一些go 语言基础、常用库、go上的设计模式等沉淀，否则会将技术细节与kubernetes 思想混在在一起 package结构 Go 常用的一些库 在介绍 spf13/cobra会提到 一个command line application 的推荐结构 appName/ cmd/ add.go your.go commands.go here.go main.go 但我们要注意几个问题 其实大部分golang 应用 都可以以command line application 的结构来组织 k8s 是由 多个command line application 组成的集合，所以其package 结构又有一点不一样。pkg/kubectl 是根据 cobra 推荐样式来的，main.go 的角色由pkg/kubectl/cmd.go 来承担（cmd.go 的方法被cmd/kubectl/kubectl.go 引用），kubectl 具体命令实现在 pkg/kubectl/cmd 下 Builders and Visitors 背景 而kubectl本身并不包含对其核心资源的访问与控制，而是通过http通信与api-server进行交互实现资源的管理，所以kubectl 操作的最后落脚点 是发送 http 请求 kubectl 读取用户输入（包括参数、yaml 文件、yaml http地址）后，肯定要在内部用一个数据结构来表示，然后针对这个数据结构 发送http 请求。 实际实现中，数据结构有倒是有，但是 k8s 笼统的称之为 resource，但没有一个实际的resource 对象存在。 resource 可能有多个，因为 kubectl create -f 可以创建多个文件，便有了多个resource resource 有多个处理步骤，比如对于 kubectl create -f http://xxx 要先下载yaml 文件、校验、再发送http 请求 针对这几个问题 k8s 没有使用一个 类似Resources 的对象来聚合所有 resource 针对一个resource 的多个处理步骤，k8s 也没有为resource 提供download、sendHttp、downloadAndSendHttp 之类的方法，而是通过对象的 聚合来 代替方法的顺序调用。 所以，k8s 采用了访问者 模式，利用其 动态双分派 特性，参见函数式编程的设计模式 Visitor 接口 type VisitorFunc func(*Info, error) error // Visitor lets clients walk a list of resources. type Visitor interface { Visit(VisitorFunc) error } visitor 模式 这部分代码主要在 k8s.io/cli-runtime/pkg/genericclioptions/resource 包下，可以使用goland 单独打开看 以pkg/kubectl/cmd/create/create.go 为demo // 根据 请求参数 构造 resource r := f.NewBuilder(). Unstructured(). Schema(schema). // 简单赋值 ContinueOnError(). // 简单赋值 NamespaceParam(cmdNamespace).DefaultNamespace(). // 简单赋值 FilenameParam(enforceNamespace, &o.FilenameOptions). // 文件可以是http 也可以 本地文件，最终都是为了给 builder.path 赋值，path 是一个Visitor 集合 LabelSelectorParam(o.Selector). // 简单赋值 Flatten(). // 简单赋值 Do() ... // 发送http 请求 err = r.Visit(func(info *resource.Info, err error) error { ... if !o.DryRun { if err := createAndRefresh(info); err != nil { return cmdutil.AddSourceToErr(\"creating\", info.Source, err) } } ... return o.PrintObj(info.Object) }) Builder provides convenience functions for taking arguments and parameters from the command line and converting them to a list of resources to iterate over using the Visitor interface. Result contains helper methods for dealing with the outcome of a Builder. Info contains temporary info to execute a REST call, or show the results of an already completed REST call. Builder build 了什么东西？将输入转换为 resource，还不只一个。每个resource 都实现了 Visitor 接口，可以接受 VisitorFunc。Result 持有 Builder 的结果——多个实现visitor 接口的resource ，并提供对它们的快捷 访问操作。 在Result 层面，一个resource/visitor 一般代表一个yaml 文件（无论local 还是http），一个resource 内部 是多个visitor 的纵向聚合，比如 URLVisitor.visit type URLVisitor struct { URL *url.URL *StreamVisitor HttpAttemptCount int } func (v *URLVisitor) Visit(fn VisitorFunc) error { body, err := readHttpWithRetries(httpgetImpl, time.Second, v.URL.String(), v.HttpAttemptCount) if err != nil { return err } defer body.Close() v.StreamVisitor.Reader = body return v.StreamVisitor.Visit(fn) } URLVisitor.visit 自己实现了 读取http yaml 文件内容，然后通过StreamVisitor.Visit 负责后续的发送http 逻辑。 visitor 函数聚合 k8s.io/cli-runtime/pkg/genericclioptions/resource/interfaces.go 这里面比较有意思的 是 DecoratedVisitor (纵向聚合，扩充一个visitor的 visit 逻辑)和 VisitorList（横向聚合，将多个平级的visitor 聚合为1个） type DecoratedVisitor struct { visitor Visitor decorators []VisitorFunc } func (v DecoratedVisitor) Visit(fn VisitorFunc) error { return v.visitor.Visit(func(info *Info, err error) error { if err != nil { return err } for i := range v.decorators { if err := v.decorators[i](info, nil); err != nil { return err } } return fn(info, nil) }) } DecoratedVisitor 封装了一个visitor， DecoratedVisitor.Visit 执行时 会先让 visitor 执行DecoratedVisitor 自己的私货VisitorFunc ，然后再执行 传入的VisitorFunc，相当于对函数逻辑做了一个聚合。 type VisitorList []Visitor // Visit implements Visitor func (l VisitorList) Visit(fn VisitorFunc) error { for i := range l { if err := l[i].Visit(fn); err != nil { return err } } return nil } VisitorList 则是将多个 Visitor 聚合成一个visitor，VisitorList.visit 执行时会依次执行 其包含的visitor的Visit 逻辑 回头看 整这么复杂 k8s.io/cli-runtime/pkg/genericclioptions/resource 对上层提供两个 抽象Builder 和 Result，上层的调用模式很固定 构造Builder、result result.visit 尽可能在通用层面 实现了kubectl 的所有逻辑，上层通过配置、传入function 即可个性化整体流程 其实最初看完这个实现，笔者在质疑这样做是否有必要，因为kubectl 就是一个发送http 请求的工具（http client）。从常规的实现角度看，create/deploy 等操作各干个的，然后共用一些抽象（比如pod）、工具类（比如HttpUtils） 就可以了。 可以看到这个复用层次是比较浅的 k8s 将一些公共组件单独提取出来，作为一个库，有点layer的感觉，但还不完全是。 借助visitor 模式，完全依靠 函数聚合来聚合逻辑，或许要从函数式编程模式的一些角度来找找感觉。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kubelete start pods.html":{"url":"blog/kubernetes/K8s源码/kubelete start pods.html","title":"Kubelete Start Pods","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/kubernetes terminate pods.html":{"url":"blog/kubernetes/K8s源码/kubernetes terminate pods.html","title":"Kubernetes Terminate Pods","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/readme.html":{"url":"blog/kubernetes/K8s源码/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/schedule.html":{"url":"blog/kubernetes/K8s源码/schedule.html","title":"Schedule","keywords":"","body":" Kubernetes资源调度——scheduler 简介 工作流程 Kubernetes 资源模型与资源管理 request and limit request limit 关系 ==> QoS ==> 驱逐策略 Kubernetes 基于资源的调度 谓词和优先级算法 Predicate Priorities 优先级和抢占 基于Scheduling Framework 代码分析 Kubernetes资源调度——scheduler 简介 几个主要点 Kubernetes 调度本身的演化 从 Kubernetes 资源控制到开放应用模型，控制器的进化之旅 调度器的原理， How Kubernetes Initializers work the scheduler is yet another controller, watching for Pods to show up in the API server and assigns each of them to a Node How does the Kubernetes scheduler work?. while True: pods = queue.getPod() assignNode(pod) scheduler is not responsible for actually running the pod – that’s the kubelet’s job. So it basically just needs to make sure every pod has a node assigned to it. 工作流程 Create a custom Kubernetes scheduler The default scheduler starts up according to the parameters given. It watches on apiserver, and puts pods where its spec.nodeName is empty into its internal scheduling queue. It pops out a pod from the scheduling queue and starts a standard scheduling cycle. It retrieves “hard requirements” (like cpu/memory requests, nodeSelector/nodeAffinity) from the pod’s API spec. Then the predicates phase occurs where it calculates to give a node candidates list which satisfies those requirements. It retrieves “soft requirements” from the pod’s API spec and also applies some default soft “policies” (like the pods prefer to be more packed or scattered across the nodes). It finally gives a score for each candidate node, and picks up the final winner with the highest score. It talks to the apiserver (by issuing a bind call) and sets Kubernetes 资源模型与资源管理 在 Kubernetes 里，Pod 是最小的原子调度单位。这也就意味着，所有跟调度和资源管理相关的属性都应该是属于 Pod 对象的字段。而这其中最重要的部分，就是 Pod 的CPU 和内存配置 在 Kubernetes 中，像 CPU 这样的资源被称作“可压缩资源”（compressible resources）。它的典型特点是，当可压缩资源不足时，Pod 只会“饥饿”，但不会退出。而像内存这样的资源，则被称作“不可压缩资源（incompressible resources）。当不可压缩资源不足时，Pod 就会因为 OOM（Out-Of-Memory）被内核杀掉。 request and limit Kubernetes 里 Pod 的 CPU 和内存资源，实际上还要分为 limits 和 requests 两种情况：在调度的时候，kube-scheduler 只会按照 requests 的值进行计算。而在真正设置 Cgroups 限制的时候，kubelet 则会按照 limits 的值来进行设置。这个理念基于一种假设：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。 request limit 关系 ==> QoS ==> 驱逐策略 QoS 划分的主要应用场景，是当宿主机资源（主要是不可压缩资源）紧张的时候，kubelet 对 Pod 进行 Eviction（即资源回收）时需要用到的。“紧张”程度可以作为kubelet 启动参数配置，默认为 memory.availableKubernetes 计算 Eviction 阈值的数据来源，主要依赖于从 Cgroups 读取到的值，以及使用 cAdvisor 监控到的数据。当宿主机的 Eviction 阈值达到后，就会进入 MemoryPressure 或者 DiskPressure 状态，从而避免新的 Pod 被调度到这台宿主机上。 limit 不设定，默认值由 LimitRange object确定 limits requests Qos模型 有 有 两者相等 Guaranteed 有 无 默认两者相等 Guaranteed x 有 两者不相等 Burstable 无 无 BestEffort 而当 Eviction 发生的时候，kubelet 具体会挑选哪些 Pod 进行删除操作，就需要参考这些 Pod 的 QoS 类别了。PS：怎么有一种 缓存 evit 的感觉。limit 越“模糊”，物理机MemoryPressure/DiskPressure 时，越容易优先被干掉。 DaemonSet 的 Pod 都设置为 Guaranteed 的 QoS 类型。否则，一旦 DaemonSet 的 Pod 被回收，它又会立即在原宿主机上被重建出来，这就使得前面资源回收的动作，完全没有意义了。 Kubernetes 基于资源的调度 在 Kubernetes 项目中，默认调度器的主要职责，就是为一个新创建出来的 Pod，寻找一个最合适的节点（Node）而这里“最合适”的含义，包括两层： 从集群所有的节点中，根据调度算法挑选出所有可以运行该 Pod 的节点； 从第一步的结果中，再根据调度算法挑选一个最符合条件的节点作为最终结果。 所以在具体的调度流程中，默认调度器会首先调用一组叫作 Predicate 的调度算法，来检查每个 Node。然后，再调用一组叫作 Priority 的调度算法，来给上一步得到的结果里的每个 Node 打分。最终的调度结果，就是得分最高的那个Node。 除了Pod，Scheduler 需要调度其它对象么？不需要。因为Kubernetes 对象虽多，但只有Pod 是调度对象，其它对象包括数据对象（比如PVC等）、编排对象（Deployment）、Pod辅助对象（NetworkPolicy等） 都只是影响Pod的调度，本身不直接消耗计算和内存资源。 Scheduler对一个 Pod 调度成功，是通过设置Pod的.Spec.NodeName为节点的名称，将一个Pod绑定到一个节点。然而，Scheduler是间接地设置.Spec.NodeName而不是直接设置。Kubernetes Scheduler不被允许更新Pod的Spec。因此，KubernetesScheduler创建了一个Kubernetes绑定对象, 而不是更新Pod。在创建绑定对象后，Kubernetes API将负责更新Pod的.Spec.NodeName。 调度主要包括两个部分 组件交互，包括如何与api server交互感知pod 变化，如何感知node 节点的cpu、内存等参数。PS：任何调度系统都有这个问题。 调度算法，上文的Predicate和Priority 算法 调度这个事情，在不同的公司和团队里的实际需求一定是大相径庭的。上游社区不可能提供一个大而全的方案出来。所以，将默认调度器插件化是 kube-scheduler 的演进方向。 谓词和优先级算法 调度系统设计精要 我们假设调度器中存在一个谓词算法和一个 Map-Reduce 优先级算法，当我们为一个 Pod 在 6 个节点中选择最合适的一个时，6 个节点会先经过谓词的筛选，图中的谓词算法会过滤掉一半的节点，剩余的 3 个节点经过 Map 和 Reduce 两个过程分别得到了 5、10 和 5 分，最终调度器就会选择分数最高的 4 号节点。 Predicate GeneralPredicates PodFitsResources，检查的只是 Pod 的 requests 字段 PodFitsHost，宿主机的名字是否跟 Pod 的 spec.nodeName 一致。 PodFitsHostPorts，Pod 申请的宿主机端口（spec.nodePort）是不是跟已经被使用的端口有冲突。 PodMatchNodeSelector，Pod 的 nodeSelector 或者 nodeAffinity 指定的节点，是否与待考察节点匹配 与 Volume 相关的过滤规则 是宿主机相关的过滤规则 Pod 相关的过滤规则。比较特殊的，是 PodAffinityPredicate。这个规则的作用，是检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系 在具体执行的时候， 当开始调度一个 Pod 时，Kubernetes 调度器会同时启动 16 个 Goroutine，来并发地为集群里的所有 Node 计算 Predicates，最后返回可以运行这个 Pod 的宿主机列表。 Priorities 在 Predicates 阶段完成了节点的“过滤”之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。 LeastRequestedPriority + BalancedResourceAllocation LeastRequestedPriority计算方法score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2 实际上就是在选择空闲资源（CPU 和 Memory）最多的物理机 BalancedResourceAllocation，计算方法score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10 每种资源的 Fraction 的定义是 ：Pod 请求的资源/ 节点上的可用资源。而 variance 算法的作用，则是资源 Fraction 差距最小的节点。BalancedResourceAllocation 选择的，其实是调度完成后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况。 NodeAffinityPriority TaintTolerationPriority InterPodAffinityPriority ImageLocalityPriority 优先级和抢占 优先级和抢占机制，解决的是 （高优先级的）Pod 调度失败时该怎么办的问题 apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority Pod 通过 priorityClassName 字段，声明了要使用名叫 high-priority 的 PriorityClass。当这个 Pod 被提交给 Kubernetes 之后，Kubernetes 的 PriorityAdmissionController 就会自动将这个 Pod 的 spec.priority 字段设置为 PriorityClass 对应的value 值。 如果确定抢占可以发生，那么调度器就会把自己缓存的所有节点信息复制一份，然后使用这个副本来模拟抢占过程。 找到牺牲者，判断抢占者是否可以部署在牺牲者所在的Node上 真正开始抢占 调度器会检查牺牲者列表，清理这些 Pod 所携带的 nominatedNodeName 字段。 调度器会把抢占者的 nominatedNodeName，设置为被抢占的 Node 的名字。 调度器会开启一个 Goroutine，同步地删除牺牲者。 调度器就会通过正常的调度流程把抢占者调度成功 基于Scheduling Framework 明确了 Kubernetes 中的各个调度阶段，提供了设计良好的基于插件的接口。调度框架认为 Kubernetes 中目前存在调度（Scheduling）和绑定（Binding）两个循环： 调度循环在多个 Node 中为 Pod 选择最合适的 Node； 绑定循环将调度决策应用到集群中，包括绑定 Pod 和 Node、绑定持久存储等工作； 除了两个大循环之外，调度框架中还包含 QueueSort、PreFilter、Filter、PostFilter、Score、Reserve、Permit、PreBind、Bind、PostBind 和 Unreserve 11 个扩展点（Extension Point），这些扩展点会在调度的过程中触发，它们的运行顺序如下： Scheduling Frameworkscheduling cycle QueueSort, These plugins are used to sort pods in the scheduling queue. A queue sort plugin essentially will provide a “less(pod1, pod2)” function. Only one queue sort plugin may be enabled at a time. PreFilter, PreFilter 类似于调度流程启动之前的预处理，可以对 Pod 的信息进行加工。同时 PreFilter 也可以进行一些预置条件的检查，去检查一些集群维度的条件，判断否满足 pod 的要求。 Filter, 是 scheduler v1 版本中的 Predicate 的逻辑，用来过滤掉不满足 Pod 调度要求的节点 PostFilter, 主要是用于处理当 Pod 在 Filter 阶段失败后的操作，例如抢占，Autoscale 触发等行为。 PreScore, 主要用于在 Score 之前进行一些信息生成。此处会获取到通过 Filter 阶段的节点列表，我们也可以在此处进行一些信息预处理或者生成一些日志或者监控信息。 Scoring, 是 scheduler v1 版本中 Priority 的逻辑，目的是为了基于 Filter 过滤后的剩余节点，根据 Scoring 扩展点定义的策略挑选出最优的节点。分为两个阶段： 打分：打分阶段会对 Filter 后的节点进行打分，scheduler 会调用所配置的打分策略 归一化: 对打分之后的结构在 0-100 之间进行归一化处理 Reserve, 是 scheduler v1 版本的 assume 的操作，此处会对调度结果进行缓存，如果在后边的阶段发生了错误或者失败的情况，会直接进入 Unreserve 阶段，进行数据回滚。 Permit, ，当 Pod 在 Reserve 阶段完成资源预留之后，Bind 操作之前，开发者可以定义自己的策略在 Permit 节点进行拦截，根据条件对经过此阶段的 Pod 进行 allow、reject 和 wait 的 3 种操作。 binding cycle, 需要调用 apiserver 的接口，耗时较长，为了提高调度的效率，需要异步执行，所以此阶段线程不安全。 Bind, 是 scheduler v1 版本中的 Bind 操作，会调用 apiserver 提供的接口，将 pod 绑定到对应的节点上。 PreBind 和 PostBind, 在 PreBind 和 PostBind 分别在 Bind 操作前后执行，这两个阶段可以进行一些数据信息的获取和更新。 UnReserve, 用于清理到 Reserve 阶段的的缓存，回滚到初始的状态。当前版本 UnReserve 与 Reserve 是分开定义的，未来会将 UnReserve 与 Reserve 统一到一起，即要求开发者在实现 Reserve 同时需要定义 UnReserve，保证数据能够有效的清理，避免留下脏数据。 插件规范定义在 $GOPATH/src/k8s.io/kubernetes/pkg/scheduler/framework/v1alpha1/interface.go 中，各类插件继承 Plugin type Plugin interface { Name() string } type PreFilterPlugin interface { Plugin // PreFilter is called at the beginning of the scheduling cycle. All PreFilter plugins must return success or the pod will be rejected. PreFilter(ctx context.Context, state *CycleState, p *v1.Pod) *Status // PreFilterExtensions returns a PreFilterExtensions interface if the plugin implements one,or nil if it does not. A Pre-filter plugin can provide extensions to incrementally modify its pre-processed info. The framework guarantees that the extensions // AddPod/RemovePod will only be called after PreFilter, possibly on a cloned CycleState, and may call those functions more than once before calling Filter again on a specific node. PreFilterExtensions() PreFilterExtensions } Kubernetes scheduling frameworkKubernetes 负责 Kube-scheduler 的小组 sig-scheduling 为了更好的管理调度相关的 Plugin，新建了项目 scheduler-plugins 来方便用户管理不同的插件。 github.com/kubernetes-sigs/scheduler-plugins /pkg /qos /queue_sort.go // 插件实现 k8s.io/kubernetes /cmd /kube-scheduler /scheduler.go // 插件注册 以其中的 Qos 的插件来为例，Qos 的插件主要基于 Pod 的 QoS(Quality of Service) class 来实现的，目的是为了实现调度过程中如果 Pod 的优先级相同时，根据 Pod 的 Qos 来决定调度顺序。 注册逻辑如下 // scheduler.go func main() { rand.Seed(time.Now().UnixNano()) command := app.NewSchedulerCommand( app.WithPlugin(qos.Name, qos.New), // 这一行为新增的注册代码 ) pflag.CommandLine.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 代码分析 How does the Kubernetes scheduler work? sched, err := scheduler.New(cc.Client, cc.InformerFactory, cc.PodInformer, recorderFactory, ctx.Done(), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithAlgorithmSource(cc.ComponentConfig.AlgorithmSource), scheduler.WithPreemptionDisabled(cc.ComponentConfig.DisablePreemption), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithBindTimeoutSeconds(cc.ComponentConfig.BindTimeoutSeconds), scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry), scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds), scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds), scheduler.WithExtenders(cc.ComponentConfig.Extenders...), ) 在创建Scheduler的New函数里面，做了以下几件事情： 创建SchedulerCache，这里面有podStates保存Pod的状态，有nodes保存节点的状态，整个调度任务是完成两者的匹配。 创建volumeBinder，因为调度很大程度上和Volume是相关的，有可能因为要求某个Pod需要满足一定大小或者条件的Volume，而这些Volume只能在某些节点上才能被挂载。 创建调度队列，将来尚未调度的Pod都会放在这个队列里面 创建调度算法，将来这个对象的Schedule函数会被调用进行调度 创建调度器，组合上面所有的对象 addAllEventHandlers，添加事件处理器。如果Pod已经调度过，发生变化则更新Cache，如果Node发生变化更新Cache，如果Pod没有调度过，则放入队列中等待调度，PV和PVC发生变化也会做相应的处理。 创建了Scheduler之后，接下来是调用Scheduler的Run函数，运行scheduleOne进行调度。 从队列中获取下一个要调度的Pod 根据调度算法，选择出合适的Node，放在scheduleResult中 在本地缓存中，先绑定Volume，真正的绑定要调用API Server将绑定信息放在ETCD里面，但是因为调度器不能等写入ETCD后再调度下一个，这样太慢了，因而在本地缓存中绑定后，同一个Volume，其他的Pod调度的时候就不会使用了。 在本地缓存中，绑定Node，原因类似 通过API Server的客户端做真正的绑定，是异步操作 接下来我们来看调度算法的Schedule函数，Schedule算法做了以下的事情： findNodesThatFitPod：根据所有预选算法过滤符合的node列表 prioritizeNodes: 对符合的节点进行优选评分，一个排序的列表 selectHost：对优选的 node 列表选择一个最优的节点 Link: service mesh 南京邮电 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/scheme.html":{"url":"blog/kubernetes/K8s源码/scheme.html","title":"Scheme","keywords":"","body":"k8s client and scheme 1. 什么是scheme 1.1 在了解scheme之前， 我们先了解下kubernetes 中的资源分类 1.2 scheme 是什么 1.3 由表及里,scheme结构体定义 2. 如何使用scheme 2.1 定义注册方法 AddToScheme 2.2 调用AddToScheme进行GVK注册 2.3 docode 3. kubectl 是如何将输入字节转换成k8s对象的 3.1 visitor 接口 3.2 StreamVisitor 3.3 方法infoForData的具体实现 4. 代码实践 1. 什么是scheme 1.1 在了解scheme之前， 我们先了解下kubernetes 中的资源分类 运维人员在创建资源的时候，可能只关注kind（如deployment，本能的忽略分组和版本信息）， 但是在k8s的资源定位中只说deployment是不准确的。因为Kubernetes系统支持多个Group，每个Group支持多个Version，每个Version支持多个Resource，其中部分资源同时会拥有自己的子资源（即SubResource）。例如，Deployment资源拥有Status子资源。 资源组、资源版本、资源、子资源的完整表现形式为///。以常用的Deployment资源为例，其完整表现形式为apps/v1/deployments/status。 为了方便资源管理和有序迭代，资源有Group（组）和Version（版本）的概念。 ​ ● Group：被称为资源组，在Kubernetes API Server中也可称其为APIGroup。 ​ ● Version：被称为资源版本，在Kubernetes API Server中也可称其为APIVersions。 ​ ● Resource：被称为资源，在Kubernetes API Server中也可称其为APIResource。 ​ ● Kind：资源种类，描述Resource的种类，与Resource为同一级别。 1.2 scheme 是什么 Kubernetes系统拥有众多资源，每一种资源就是一个资源类型，这些资源类型需要有统一的注册、存储、查询、管理等机制。目前Kubernetes系统中的所有资源类型都已注册到Scheme资源注册表中，其是一个内存型的资源注册表，拥有如下特点。 ​ ● 支持注册多种资源类型，包括内部版本和外部版本。 ​ ● 支持多种版本转换机制。 ​ ● 支持不同资源的序列化/反序列化机制。 Scheme资源注册表支持两种资源类型（Type）的注册，分别是UnversionedType和KnownType资源类型，分别介绍如下。 ​ ● UnversionedType：无版本资源类型，这是一个早期Kubernetes系统中的概念，它主要应用于某些没有版本的资源类型，该类型的资源对象并不需要进行转换。在目前的Kubernetes发行版本中，无版本类型已被弱化，几乎所有的资源对象都拥有版本，但在metav1元数据中还有部分类型，它们既属于meta.k8s.io/v1又属于UnversionedType无版本资源类型，例如metav1.Status、metav1.APIVersions、metav1.APIGroupList、metav1.APIGroup、metav1.APIResourceList。 ​ ● KnownType：是目前Kubernetes最常用的资源类型，也可称其为“拥有版本的资源类型”。在Scheme资源注册表中，UnversionedType资源类型的对象通过scheme.AddUnversionedTypes方法进行注册，KnownType资源类型的对象通过scheme.AddKnownTypes方法进行注册。 1.3 由表及里,scheme结构体定义 staging/src/k8s.io/apimachinery/pkg/runtime/scheme.go type Scheme struct { gvkToType map[schema.GroupVersionKind]reflect.Type typeToGVK map[reflect.Type][]schema.GroupVersionKind unversionedTypes map[reflect.Type]schema.GroupVersionKind unversionedKinds map[string]reflect.Type ... } Scheme资源注册表结构字段说明如下。 ​ ● gvkToType：存储GVK与Type的映射关系。 ​ ● typeToGVK：存储Type与GVK的映射关系，一个Type会对应一个或多个GVK。 ​ ● unversionedTypes：存储UnversionedType与GVK的映射关系。 ​ ● unversionedKinds：存储Kind（资源种类）名称与UnversionedType的映射关系。 Scheme资源注册表通过Go语言的map结构实现映射关系，这些映射关系可以实现高效的正向和反向检索，从Scheme资源注册表中检索某个GVK的Type，它的时间复杂度为O（1）。资源注册表映射关系如下图： 2. 如何使用scheme 2.1 定义注册方法 AddToScheme 通过runtime.NewScheme实例化一个新的Scheme资源注册表。注册资源类型到Scheme资源注册表有两种方式， 第一种通过scheme.AddKnownTypes方法注册KnownType类型的对象 第二种通过scheme.AddUnversionedTypes方法注册UnversionedType类型的对象 在我们创建crd 资源的时候，通过代码生成工具，都会在register.go文件里帮我们自动生成 AddToScheme方法 package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" \"lsh-mcp-lcs-timer/constant\" ) var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) // 外部的scheme 直接调用该方法进行scheme注册 AddToScheme = SchemeBuilder.AddToScheme SchemeGroupVersion = schema.GroupVersion{Group: constant.ResourceOverviewGroupName, Version: \"v1\"} ) func Kind(kind string) schema.GroupKind { return SchemeGroupVersion.WithKind(kind).GroupKind() } func Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource() } func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, &RsOverView{}, &RsOverViewList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } 2.2 调用AddToScheme进行GVK注册 场景： 将其他项目的GVK 注册到自己项目的scheme中。 example，下面代码摘自kubevela： import( kruise \"github.com/openkruise/kruise-api/apps/v1alpha1\" certmanager \"github.com/wonderflow/cert-manager-api/pkg/apis/certmanager/v1\" istioclientv1beta1 \"istio.io/client-go/pkg/apis/networking/v1beta1\" crdv1 \"k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1\" k8sruntime \"k8s.io/apimachinery/pkg/runtime\" clientgoscheme \"k8s.io/client-go/kubernetes/scheme\" oamcore \"github.com/oam-dev/kubevela/apis/core.oam.dev\" oamstandard \"github.com/oam-dev/kubevela/apis/standard.oam.dev/v1alpha1\" ) var ( // Scheme defines the default KubeVela schema Scheme = k8sruntime.NewScheme() ) func init() { _ = clientgoscheme.AddToScheme(Scheme) _ = crdv1.AddToScheme(Scheme) _ = oamcore.AddToScheme(Scheme) _ = oamstandard.AddToScheme(Scheme) _ = istioclientv1beta1.AddToScheme(Scheme) _ = certmanager.AddToScheme(Scheme) _ = kruise.AddToScheme(Scheme) // +kubebuilder:scaffold:scheme } 2.3 docode 这里以 k8s.io/client-go 为例， 代码位置： kubernetes/scheme/register.go 注册scheme var Scheme = runtime.NewScheme() var Codecs = serializer.NewCodecFactory(Scheme) var ParameterCodec = runtime.NewParameterCodec(Scheme) // 生成一个Decode var decode = Codecs.UniversalDeserializer().Decode UniversalDeserializer 返回 CodeFactory 的属性universal // UniversalDeserializer can convert any stored data recognized by this factory into a Go object that satisfies // runtime.Object. It does not perform conversion. It does not perform defaulting. func (f CodecFactory) UniversalDeserializer() runtime.Decoder { return f.universal } 查看CodecFactory生成过程，发现universal 的值是recognizer.NewDecoder(decoders...) func NewCodecFactory(scheme *runtime.Scheme, mutators ...CodecFactoryOptionsMutator) CodecFactory { options := CodecFactoryOptions{Pretty: true} for _, fn := range mutators { fn(&options) } serializers := newSerializersForScheme(scheme, json.DefaultMetaFactory, options) return newCodecFactory(scheme, serializers) } // newCodecFactory is a helper for testing that allows a different metafactory to be specified. func newCodecFactory(scheme *runtime.Scheme, serializers []serializerType) CodecFactory { decoders := make([]runtime.Decoder, 0, len(serializers)) var accepts []runtime.SerializerInfo alreadyAccepted := make(map[string]struct{}) var legacySerializer runtime.Serializer for _, d := range serializers { // 组装decoder decoders = append(decoders, d.Serializer) .... } if legacySerializer == nil { legacySerializer = serializers[0].Serializer } return CodecFactory{ scheme: scheme, serializers: serializers, // set universal universal: recognizer.NewDecoder(decoders...), accepts: accepts, legacySerializer: legacySerializer, } } 结论：universal 是一个数组， 里面的元素都实现了接口 Decoder // NewDecoder creates a decoder that will attempt multiple decoders in an order defined // by: // // 1. The decoder implements RecognizingDecoder and identifies the data // 2. All other decoders, and any decoder that returned true for unknown. // // The order passed to the constructor is preserved within those priorities. func NewDecoder(decoders ...runtime.Decoder) runtime.Decoder { return &decoder{ decoders: decoders, } } 调用Decoder 的 decode 方法 obj, _, err := decode([]byte(objStr),nil, nil) decoders 关于Decode 的具体实现， 代码：k8s.io/apimachinery@v0.20.1/pkg/runtime/serializer/recognizer/recognizer.go func (d *decoder) Decode(data []byte, gvk *schema.GroupVersionKind, into runtime.Object) (runtime.Object, *schema.GroupVersionKind, error) { var ( lastErr error skipped []runtime.Decoder ) // try recognizers, record any decoders we need to give a chance later for _, r := range d.decoders { switch t := r.(type) { case RecognizingDecoder: buf := bytes.NewBuffer(data) ok, unknown, err := t.RecognizesData(buf) if err != nil { lastErr = err continue } if unknown { skipped = append(skipped, t) continue } if !ok { continue } return r.Decode(data, gvk, into) default: skipped = append(skipped, t) } } // try recognizers that returned unknown or didn't recognize their data for _, r := range skipped { out, actual, err := r.Decode(data, gvk, into) if err != nil { lastErr = err continue } return out, actual, nil } if lastErr == nil { lastErr = fmt.Errorf(\"no serialization format matched the provided data\") } return nil, nil, lastErr } 挖掘Decoder的具体实现类： 查看CodeFactory 相关代码，发现这里Decoder的实现类是Serializer // form. If typer is not nil, the object has the group, version, and kind fields set. Options are copied into the Serializer // and are immutable. func NewSerializerWithOptions(meta MetaFactory, creater runtime.ObjectCreater, typer runtime.ObjectTyper, options SerializerOptions) *Serializer { return &Serializer{ meta: meta, creater: creater, typer: typer, options: options, identifier: identifier(options), } } Serializer 类的Docder 接口实现 // Decode attempts to convert the provided data into YAML or JSON, extract the stored schema kind, apply the provided default gvk, and then // load that data into an object matching the desired schema kind or the provided into. // If into is *runtime.Unknown, the raw data will be extracted and no decoding will be performed. // If into is not registered with the typer, then the object will be straight decoded using normal JSON/YAML unmarshalling. // If into is provided and the original data is not fully qualified with kind/version/group, the type of the into will be used to alter the returned gvk. // If into is nil or data's gvk different from into's gvk, it will generate a new Object with ObjectCreater.New(gvk) // On success or most errors, the method will return the calculated schema kind. // The gvk calculate priority will be originalData > default gvk > into func (s *Serializer) Decode(originalData []byte, gvk *schema.GroupVersionKind, into runtime.Object) (runtime.Object, *schema.GroupVersionKind, error) { data := originalData if s.options.Yaml { altered, err := yaml.YAMLToJSON(data) if err != nil { return nil, nil, err } data = altered } actual, err := s.meta.Interpret(data) if err != nil { return nil, nil, err } if gvk != nil { *actual = gvkWithDefaults(*actual, *gvk) } if unk, ok := into.(*runtime.Unknown); ok && unk != nil { unk.Raw = originalData unk.ContentType = runtime.ContentTypeJSON unk.GetObjectKind().SetGroupVersionKind(*actual) return unk, actual, nil } if into != nil { _, isUnstructured := into.(runtime.Unstructured) types, _, err := s.typer.ObjectKinds(into) switch { case runtime.IsNotRegisteredError(err), isUnstructured: if err := caseSensitiveJsonIterator.Unmarshal(data, into); err != nil { return nil, actual, err } return into, actual, nil case err != nil: return nil, actual, err default: *actual = gvkWithDefaults(*actual, types[0]) } } if len(actual.Kind) == 0 { return nil, actual, runtime.NewMissingKindErr(string(originalData)) } if len(actual.Version) == 0 { return nil, actual, runtime.NewMissingVersionErr(string(originalData)) } // use the target if necessary, 重点： 生成一个空数据的对象 obj, err := runtime.UseOrCreateObject(s.typer, s.creater, *actual, into) if err != nil { return nil, actual, err } // 通过 json 解析，给上面生成的空数据对象，赋值 if err := caseSensitiveJsonIterator.Unmarshal(data, obj); err != nil { return nil, actual, err } // If the deserializer is non-strict, return successfully here. if !s.options.Strict { return obj, actual, nil } // In strict mode pass the data trough the YAMLToJSONStrict converter. // This is done to catch duplicate fields regardless of encoding (JSON or YAML). For JSON data, // the output would equal the input, unless there is a parsing error such as duplicate fields. // As we know this was successful in the non-strict case, the only error that may be returned here // is because of the newly-added strictness. hence we know we can return the typed strictDecoderError // the actual error is that the object contains duplicate fields. altered, err := yaml.YAMLToJSONStrict(originalData) if err != nil { return nil, actual, runtime.NewStrictDecodingError(err.Error(), string(originalData)) } // As performance is not an issue for now for the strict deserializer (one has regardless to do // the unmarshal twice), we take the sanitized, altered data that is guaranteed to have no duplicated // fields, and unmarshal this into a copy of the already-populated obj. Any error that occurs here is // due to that a matching field doesn't exist in the object. hence we can return a typed strictDecoderError, // the actual error is that the object contains unknown field. strictObj := obj.DeepCopyObject() if err := strictCaseSensitiveJsonIterator.Unmarshal(altered, strictObj); err != nil { return nil, actual, runtime.NewStrictDecodingError(err.Error(), string(originalData)) } // Always return the same object as the non-strict serializer to avoid any deviations. return obj, actual, nil } 3. kubectl 是如何将输入字节转换成k8s对象的 3.1 visitor 接口 Visitor接口包含Visit方法，实现了Visit（VisitorFunc） error的结构体都可以成为Visitor。其中，VisitorFunc是一个匿名函数，它接收Info与error信息，Info结构用于存储RESTClient请求的返回结果，而VisitorFunc匿名函数则生成或处理Info结构。Visitor的设计较为复杂，并非单纯实现了访问者模式，它相当于一个匿名函数集。在Kubernetes源码中，Visitor被设计为可以多层嵌套（即多层匿名函数嵌套，使用一个Visitor嵌套另一个Visitor） // Visitor lets clients walk a list of resources. type Visitor interface { Visit(VisitorFunc) error } 3.2 StreamVisitor kubectl 将输入的字节码通过如下visitor进行处理 DecoratedVisitor→ContinueOnErrorVisitor → FlattenListVisitor →FlattenListVisitor → StreamVisitor →FileVisitor→EagerVisitorList 这里我们重点关注StreamVisitor的操作 // Visit implements Visitor over a stream. StreamVisitor is able to distinct multiple resources in one stream. func (v *StreamVisitor) Visit(fn VisitorFunc) error { d := yaml.NewYAMLOrJSONDecoder(v.Reader, 4096) for { ext := runtime.RawExtension{} // 使用unstructured.UnstructuredJSONScheme 解析 // d 循环解析，自动通过---分割对象，每次只解析一个对象 if err := d.Decode(&ext); err != nil { if err == io.EOF { return nil } return fmt.Errorf(\"error parsing %s: %v\", v.Source, err) } // TODO: This needs to be able to handle object in other encodings and schemas. ext.Raw = bytes.TrimSpace(ext.Raw) if len(ext.Raw) == 0 || bytes.Equal(ext.Raw, []byte(\"null\")) { continue } if err := ValidateSchema(ext.Raw, v.Schema); err != nil { return fmt.Errorf(\"error validating %q: %v\", v.Source, err) } // StreamVisitor 对NewYAMLOrJSONDecoder 解析出的数据进一步 解码出k8s对象 // info 就是一个k8s runtime.Object info, err := v.infoForData(ext.Raw, v.Source) if err != nil { if fnErr := fn(info, err); fnErr != nil { return fnErr } continue } if err := fn(info, nil); err != nil { return err } } } 3.3 方法infoForData的具体实现 infoForData 属于StreamVisitor 内嵌结构体 mapper的方法， 所以上面可以直接通过StreamVisitor调用infoForData方法 // Mapper is a convenience struct for holding references to the interfaces // needed to create Info for arbitrary objects. type mapper struct { // localFn indicates the call can't make server requests localFn func() bool restMapperFn RESTMapperFunc clientFn func(version schema.GroupVersion) (RESTClient, error) decoder runtime.Decoder } func (m *mapper) infoForData(data []byte, source string) (*Info, error) { // 对字节数组进行解码 obj, gvk, err := m.decoder.Decode(data, nil, nil) if err != nil { return nil, fmt.Errorf(\"unable to decode %q: %v\", source, err) } name, _ := metadataAccessor.Name(obj) namespace, _ := metadataAccessor.Namespace(obj) resourceVersion, _ := metadataAccessor.ResourceVersion(obj) ret := &Info{ Source: source, Namespace: namespace, Name: name, ResourceVersion: resourceVersion, Object: obj, } ... return ret, nil } 4. 代码实践 目标： 实现类似kubectl 一样的client ，可以apply 任意yaml文件 关键代码之：对象反序列化 var ( // 解码器 decode = unstructured.UnstructuredJSONScheme ) func GetKubernetesObjectByBytes(ioBytes []byte) ([]interface{}, error) { objList := make([]interface{}, 0) d := yaml.NewYAMLOrJSONDecoder(bytes.NewReader(ioBytes), 4096) for { ext := runtime.RawExtension{} if err := d.Decode(&ext); err != nil { if err == io.EOF { return objList, nil } } // TODO: This needs to be able to handle object in other encodings and schemas. ext.Raw = bytes.TrimSpace(ext.Raw) if len(ext.Raw) == 0 || bytes.Equal(ext.Raw, []byte(\"null\")) { return objList, nil } // 参数data 必须先yaml to json，否则会报错 obj, _, err := decode.Decode(ext.Raw, nil, nil) if err != nil { return nil, err } objList = append(objList, obj) } } 为什么yaml 必须要转换为 json // YAMLToJSON converts YAML to JSON. Since JSON is a subset of YAML, // passing JSON through this method should be a no-op. // // Things YAML can do that are not supported by JSON: // * In YAML you can have binary and null keys in your maps. These are invalid // in JSON. (int and float keys are converted to strings.) // * Binary data in YAML with the !!binary tag is not supported. If you want to // use binary data with this library, encode the data as base64 as usual but do // not use the !!binary tag in your YAML. This will ensure the original base64 // encoded data makes it all the way through to the JSON. // // For strict decoding of YAML, use YAMLToJSONStrict. func YAMLToJSON(y []byte) ([]byte, error) { return yamlToJSON(y, nil, yaml.Unmarshal) } 关键代码之： 对象apply client 要实现apply 需要实现两个接口： creator 和 patcher。 creator 负责获取集群中以存在的对象，不存在则创建。 patcher 负责对将集群中的对象更新为要修改的对象。 func NewClient() *KubernetesClient { return &KubernetesClient{ // 不存在则创建，存在则获取服务端对象 creator: creatorFn(createOrGetExisting), // apply 的关键方法，后面再具体介绍 patcher: patcherFn(threeWayMergePatch), } } // Apply applies new state to an object or create it if not exist func (k *KubernetesClient) Apply(ctx context.Context, desired client.Object, ao ...ApplyOption) error { existing, err := k.createOrGetExisting(ctx, k.Client, desired, ao...) if err != nil { return err } // existing 为nil，表明这是第一次创建，直接退出 if existing == nil { return nil } // the object already exists, patch new state if err := executeApplyOptions(ctx, existing, desired, ao); err != nil { return err } loggingApply(\"patching object\", desired) // 如果已经存在，这里执行threeWayMergePatch patch, err := k.patcher.patch(existing, desired) if err != nil { return errors.Wrap(err, \"cannot calculate patch by computing a three way diff\") } return errors.Wrapf(k.Client.Patch(ctx, desired, patch), \"cannot patch object\") } threeWayMergePatch： 通过将 集群中的对象（current），apply后的对象（modified）， 存在于注解中的对象（original）三者对比，根据在runtime中是否注册改对象，又分两种方式patch。 资源对象在runtime中没有注册： 使用JSON Merge Patch, RFC 7386，对JSON Patch, RFC 6902 进行了简化。 但是仍然有如下缺陷：1. delete must set null； 2. add new element must report entire array。3. 缺少JSON Schema验证。 资源对象在runtime中有注册：使用StrategicMergePatch。不必提供完整的字段，新字段会添加，出现的已有字段会更新，没有出现的已有字段不变。缺点是不支持 custom resource。 func threeWayMergePatch(currentObj, modifiedObj client.Object) (client.Patch, error) { // 集中的对象 current, err := json.Marshal(currentObj) if err != nil { return nil, err } // 集中的对象的注解值 xxx/last-applied-configuration: {} original, err := getOriginalConfiguration(currentObj) if err != nil { return nil, err } // apply 后的对象 modified, err := getModifiedConfiguration(modifiedObj, true) if err != nil { return nil, err } var patchType types.PatchType var patchData []byte var lookupPatchMeta strategicpatch.LookupPatchMeta versionedObject, err := k8sScheme.New(currentObj.GetObjectKind().GroupVersionKind()) switch { // crd 对象，在默认的runtime中是没有注册的 case runtime.IsNotRegisteredError(err): // use JSONMergePatch for custom resources // because StrategicMergePatch doesn't support custom resources patchType = types.MergePatchType preconditions := []mergepatch.PreconditionFunc{ mergepatch.RequireKeyUnchanged(\"apiVersion\"), mergepatch.RequireKeyUnchanged(\"kind\"), mergepatch.RequireMetadataKeyUnchanged(\"name\")} patchData, err = jsonmergepatch.CreateThreeWayJSONMergePatch(original, modified, current, preconditions...) if err != nil { return nil, err } case err != nil: return nil, err default: // use StrategicMergePatch for K8s built-in resources patchType = types.StrategicMergePatchType lookupPatchMeta, err = strategicpatch.NewPatchMetaFromStruct(versionedObject) if err != nil { return nil, err } patchData, err = strategicpatch.CreateThreeWayMergePatch(original, modified, current, lookupPatchMeta, true) if err != nil { return nil, err } } return client.RawPatch(patchType, patchData), nil } 详细代码见： https://github.com/xishengcai/ganni-tool/blob/master/k8s/kubeapp.go 我们以下面的yaml 为例，执行apply，然后debug，观察threeWayMergePatch中的对象 apiVersion: v1 kind: Namespace metadata: name: launcher-test --- apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # 名字必需与下面的 spec 字段匹配，并且格式为 '.' name: crontabs.stable.example.com spec: # 组名称，用于 REST API: /apis// group: stable.example.com # 列举此 CustomResourceDefinition 所支持的版本 versions: - name: v1 # 每个版本都可以通过 served 标志来独立启用或禁止 served: true # 其中一个且只有一个版本必需被标记为存储版本 storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer # 可以是 Namespaced 或 Cluster scope: Namespaced names: # 名称的复数形式，用于 URL：/apis/// plural: crontabs # 名称的单数形式，作为命令行使用时和显示时的别名 singular: crontab # kind 通常是单数形式的驼峰编码（CamelCased）形式。你的资源清单会使用这一形式。 kind: CronTab # shortNames 允许你在命令行使用较短的字符串来匹配资源 shortNames: - ct --- apiVersion: stable.example.com/v1 kind: CronTab metadata: name: crontab namespace: launcher-test spec: image: \"xx\" replicas: 1 --- apiVersion: stable.example.com/v1 kind: CronTab metadata: name: crontab namespace: launcher-test spec: image: \"xxx\" replicas: 2 结果如下 current： { \"apiVersion\": \"stable.example.com/v1\", \"kind\": \"CronTab\", \"metadata\": { \"annotations\": { \"ganni-tool/last-applied-configuration\": \"{\\\"apiVersion\\\":\\\"stable.example.com/v1\\\",\\\"kind\\\":\\\"CronTab\\\",\\\"metadata\\\":{\\\"name\\\":\\\"crontab\\\",\\\"namespace\\\":\\\"launcher-test\\\"},\\\"spec\\\":{\\\"image\\\":\\\"xx\\\",\\\"replicas\\\":1}}\" }, \"creationTimestamp\": \"2021-06-03T03:08:29Z\", \"generation\": 1, \"name\": \"crontab\", \"namespace\": \"launcher-test\", \"resourceVersion\": \"16154\", \"selfLink\": \"/apis/stable.example.com/v1/namespaces/launcher-test/crontabs/crontab\", \"uid\": \"7705fc1e-1a1c-457d-b9dd-9ff56658343d\" }, \"spec\": { \"image\": \"xx\", \"replicas\": 1 } } Origin: { \"apiVersion\": \"stable.example.com/v1\", \"kind\": \"CronTab\", \"metadata\": { \"name\": \"crontab\", \"namespace\": \"launcher-test\" }, \"spec\": { \"image\": \"xx\", \"replicas\": 1 } } Modify: { \"apiVersion\": \"stable.example.com/v1\", \"kind\": \"CronTab\", \"metadata\": { \"annotations\": { \"ganni-tool/last-applied-configuration\": \"{\\\"apiVersion\\\":\\\"stable.example.com/v1\\\",\\\"kind\\\":\\\"CronTab\\\",\\\"metadata\\\":{\\\"name\\\":\\\"crontab\\\",\\\"namespace\\\":\\\"launcher-test\\\"},\\\"spec\\\":{\\\"image\\\":\\\"xxx\\\",\\\"replicas\\\":2}}\" }, \"name\": \"crontab\", \"namespace\": \"launcher-test\" }, \"spec\": { \"image\": \"xxx\", \"replicas\": 2 } } Question: Q1：为什么kubectl 在没有注册scheme 的情况下可以生成runtime.Object A：因为kubectl 使用的是 UnstructuredJSONScheme， 可以不需要提注册scheme（without a predefined scheme）。 UnstructuredJSONScheme.Decode 方法将 yaml or json 的字节流转换成了 Unstructured.Object（ 而object 的类型是map[string]interface{}）。 下面代码是UnstructuredJSONScheme反序列化流程 代码位置： staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured/helpers.go 版本：v1.19.0 // UnstructuredJSONScheme is capable of converting JSON data into the Unstructured // type, which can be used for generic access to objects without a predefined scheme. // TODO: move into serializer/json. 329 var UnstructuredJSONScheme runtime.Codec = unstructuredJSONScheme{} 335 func (s unstructuredJSONScheme) Decode(data []byte, _ *schema.GroupVersionKind, obj runtime.Object) (runtime.Object, *schema.GroupVersionKind, error) { var err error if obj != nil { err = s.decodeInto(data, obj) } else { //注意 look down obj, err = s.decode(data) } ... gvk := obj.GetObjectKind().GroupVersionKind() if len(gvk.Kind) == 0 { return nil, &gvk, runtime.NewMissingKindErr(string(data)) } return obj, &gvk, nil } 391 func (s unstructuredJSONScheme) decode(data []byte) (runtime.Object, error) { ... // No Items field, so it wasn't a list. unstruct := &Unstructured{} //注意 look down， err := s.decodeToUnstructured(data, unstruct) return unstruct, err } 343 func (unstructuredJSONScheme) decodeToUnstructured(data []byte, unstruct *Unstructured) error { m := make(map[string]interface{}) // 注意 if err := json.Unmarshal(data, &m); err != nil { return err } unstruct.Object = m return nil } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/sig.controller-runtime.html":{"url":"blog/kubernetes/K8s源码/sig.controller-runtime.html","title":"Sig.Controller Runtime","keywords":"","body":"controller-runtime mgr Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/K8s源码/调度队列.html":{"url":"blog/kubernetes/K8s源码/调度队列.html","title":"调度队列","keywords":"","body":"kubernetes 中的调度队列 作者： 王一钧 kube-scheduler 中调度队列是一个重要的组成部分。它可以控制pod按照合适的顺序、合适的时间被scheduler 核心代码进行调度。pod可能需要在某种特定的条件才能进行调度，比如PV, pod的亲和度反亲和度或者容忍node上的污点，这时候就需要延迟调度的机制，只有等待所有条件具备才进入调度循环。kube-scheduler 中有三种不同的调度队列： activeQ: 是一个堆类型的队列，scheduler 能够主动的获取到需要调度的pod, 堆的顶部始终是优先级最高的pod。 podBackoffQ: 也是一个堆类型的队列，根据backoff 的时间进行排序。backoff结束时将会从本队列中弹出，重新回到activeQ。 unschedulableQ ： 一个map,记录了那些经过重试但是最终决定无法调度的pod. kubernetst/pkg/scheduler/internal/queue/scheduling_queue.go type PriorityQueue struct { ... //activeQ是一种堆结构，调度器会主动查看以找到要调度的pods。堆的头部是最高优先级的pod。 activeQ *heap.Heap // podBackoffQ是一个按回退期限排序的堆。在调度器查看activeQ之前，backoff 结束的Pods会从堆中弹出 podBackoffQ *heap.Heap // unschedulableQ 是一个map，保存了那些已经尝试过调度，但是失败了的pod unschedulableQ *UnschedulablePodsMap .... } 在kube-scheduler 启动的时候还会启动两个协程，负责将pod 移动到activeQ。 flushBackoffQCompleted 每秒执行一次， 检查所有在backoffQ 的pod backoff 时候结束，若结束移动到 activeQ func (p *PriorityQueue) flushBackoffQCompleted() { p.lock.Lock() defer p.lock.Unlock() for { // 获取一个pod rawPodInfo := p.podBackoffQ.Peek() if rawPodInfo == nil { return } pod := rawPodInfo.(*framework.QueuedPodInfo).Pod // 获取backoff time boTime := p.getBackoffTime(rawPodInfo.(*framework.QueuedPodInfo)) // 检查backoff是否结束 if boTime.After(p.clock.Now()) { return } // 从backoffQ 删除 _, err := p.podBackoffQ.Pop() if err != nil { klog.Errorf(\"Unable to pop pod %v from backoff queue despite backoff completion.\", nsNameForPod(pod)) return } // 添加到activeQ p.activeQ.Add(rawPodInfo) metrics.SchedulerQueueIncomingPods.WithLabelValues(\"active\", BackoffComplete).Inc() defer p.cond.Broadcast() } } flushUnschedulableQLeftover: 每隔30秒启动一次，查看pod 最近一次调度时间是否已经超过60 若是则移动到activeQ 或者backoffQ. func (p *PriorityQueue) flushUnschedulableQLeftover() { p.lock.Lock() defer p.lock.Unlock() var podsToMove []*framework.QueuedPodInfo currentTime := p.clock.Now() for _, pInfo := range p.unschedulableQ.podInfoMap { lastScheduleTime := pInfo.Timestamp // 距离上次调度时间是否超过60秒 if currentTime.Sub(lastScheduleTime) > unschedulableQTimeInterval { podsToMove = append(podsToMove, pInfo) } } if len(podsToMove) > 0 { // 移动到activeQ 或者backoffQ p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout) } } 如图pod 在kube-scheduler的调度周期 active queue 在这个队列默认按照优先度的大小进行排序，也可以通过QueueSort 插件进行扩展。每当新建一个pod如果spec.nodeName 为空，它将会被加入到队列中。每一个调度循环都会取出该队列的第一个pod进行调度，经过调度算法出现任何失败都会进入Unschedulable queue. 或者在同一时间接收到moveRequest 也会被调度到backOffQ 。如果调度过程没有任何错误，则将该pod从队列中删除。 kube-scheduler启动时也会启动informer，来监听pod的各种事件。例如监听到了pod 添加事件，pod将会将其加入到ActiveQ 中。 func addAllEventHandlers( sched *Scheduler, informerFactory informers.SharedInformerFactory, podInformer coreinformers.PodInformer, ) { //// spc.nodeName 为空的pod podInformer.Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { switch t := obj.(type) { case *v1.Pod: // 判断spc.nodeName 字段是否为空 return assignedPod(t) case cache.DeletedFinalStateUnknown: if pod, ok := t.Obj.(*v1.Pod); ok { return assignedPod(pod) } utilruntime.HandleError(fmt.Errorf(\"unable to convert object %T to *v1.Pod in %T\", obj, sched)) return false default: utilruntime.HandleError(fmt.Errorf(\"unable to handle object in %T: %T\", sched, obj)) return false } }, Handler: cache.ResourceEventHandlerFuncs{ // pod 添加事件 AddFunc: sched.addPodToCache, // pod 更新事件 UpdateFunc: sched.updatePodInCache, // pod 删除事件 DeleteFunc: sched.deletePodFromCache, }, }, ) // spc.nodeName 不为空， 且指定了调度器 podInformer.Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { switch t := obj.(type) { case *v1.Pod: return !assignedPod(t) &amp;&amp; responsibleForPod(t, sched.Profiles) case cache.DeletedFinalStateUnknown: if pod, ok := t.Obj.(*v1.Pod); ok { return !assignedPod(pod) &amp;&amp; responsibleForPod(pod, sched.Profiles) } utilruntime.HandleError(fmt.Errorf(\"unable to convert object %T to *v1.Pod in %T\", obj, sched)) return false default: utilruntime.HandleError(fmt.Errorf(\"unable to handle object in %T: %T\", sched, obj)) return false } }, Handler: cache.ResourceEventHandlerFuncs{ AddFunc: sched.addPodToSchedulingQueue, UpdateFunc: sched.updatePodInSchedulingQueue, DeleteFunc: sched.deletePodFromSchedulingQueue, }, }, ) ... } backoff queue 该队列是保证一个pod调度失败后连续的重试添加了事件间隔得到缓冲。backoff 最短的pod将会在队列的最前面。失败次数的越多backoff的时长越大。该队列中的pod会通过上面所说的 flushBackoffQCompleted 重新回到activeQ 中。 backoff 的时长算法是根据默认时长x 失败次数y， x+2^y 计算出来的。 例如一个pod 重试了3次 默认时长为1s,那么它将需要等待9s 1+2^3 = 9s 当然backoff 不会超过最大值 默认配置是10s. // getBackoffTime returns the time that podInfo completes backoff func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time { duration := p.calculateBackoffDuration(podInfo) backoffTime := podInfo.Timestamp.Add(duration) return backoffTime } // calculateBackoffDuration is a helper function for calculating the backoffDuration // based on the number of attempts the pod has made. func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration { duration := p.podInitialBackoffDuration for i := 1; i p.podMaxBackoffDuration { return p.podMaxBackoffDuration } } return duration } unschedulable queue 这里记录了所有调度失败的pod(未接收到moveRequest 请求)。 move request move request 会触发一个事件负责将pod充unschedulable queue 移动到backoff queue 或者active queue。 集群事件将会异步触发一个事件将所有处于该调度的pod重新可以调度。集群事件包括：pod本身改变，pv,pvc ,storage class, CSI node 的变化。例如一个pod A正在被调度，另外一个和A亲和度匹配的pod B在unscheduled queue 中，那么B将会通过move request 重新回到 ActiveQ 重新尝试调度。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/Pixie.html":{"url":"blog/kubernetes/Pixie.html","title":"Pixie","keywords":"","body":"New Relic 开源 Pixie，其 Kubernetes 原生集群内观察平台 原创 ZDNet Cloud Native Community 5天前 收录于话题 #可观察性 3个 本文译自 ZDNet 的文章 New Relic open sources Pixie, its Kubernetes-native in-cluster observability platform[1]，作者 Steven J. Vaughan-Nichols，译者宋净超[2]。 好消息是，云计算、Kubernetes[3] 和云原生计算结合在一起，使软件开发比以前更快、更强大。坏消息是，保持对所有这些的关注比以往任何时候都更难。这就是为什么 New Relic[4] 将其 Kubernetes 原生集群内观察平台 Pixie[5] 作为一个新的开源项目，在 Apache 2.0 许可[6]下贡献给云原生计算基金会（CNCF）的[7]原因，这是一个好消息。 Pixie 是一个新的云原生应用程序的可观察性平台。有了它，开发人员可以通过一个 shell 命令看到他们应用程序的所有指标、事件、日志和追踪。有了 Pixie，你不需要添加度量（instrumentation ）代码，设置临时仪表板，或将数据移出集群，就能看到正在发生的事情。这将为你节省宝贵的时间，这样你就可以致力于建立更好的软件，而不是用更好的方法来监控它。 该程序作为一组 Kubernetes 服务部署在被监控的集群内。简而言之，Pixie 是一个原生的 Kubernetes 程序。它的 Pixie 边缘模块（PEM）被部署为 DaemonSet。在你的集群内，PEM 利用 Pixie 的 eBPF[8] 程序来收集网络事务和系统指标，而不需要修改代码。 亚马逊网络服务（AWS）[9]可观察性服务总经理 Mark Carter 补充说：“有了 eBPF[10]，即 Pixie 平台支持的 Linux 中的新度量能力，开发和运维可以利用一种新的可观察性的超级力量。” 这是非常方便的。正如 New Relic 总裁 Bill Staples 在博客中所说。“这些云原生环境[11]的动态、分布式性质带来了一系列新的可观察性挑战[12]。我们相信开源的、社区驱动是解决这些挑战的最好方法”。因此，通过使用 OpenTelemetry 作为度量化标准[13]，分析和故障排除都变得更加容易。 Staples 继续说道。“通过一个命令，你可以点亮你的整个云环境并立即获得遥测数据。我们相信所有的开发者都应该获得这种惊人的开发者体验，它可以减少观察的摩擦，节省宝贵的时间以用来开发更好的软件。为了实现这一目标，我们还将 Pixie 的大部分工程资源投入到这个开源项目中。” 展望未来，New Relic 公司 Pixie 和 New Relic 开源部总经理、最近收购的 Pixie 实验室[14]的首席执行官兼联合创始人 Zain Asgar 说，“开源是 New Relic 和 Pixie 的决定性价值，这就是为什么我们正在用 OpenTelemetry 对我们的可观察性产品进行标准化，并正在将 Pixie 作为一个开源项目进行贡献。我们已经亲眼看到了开放治理对开源项目的积极影响，我们期待着通过我们在 CNCF 的 [新] 白金会员资格，在全行业范围内支持这一倡议。” Pixie 开源现在也将在 AWS 上运行[15]，作为 OpenTelemetry 项目的一个安全的、可生产的、由 AWS 支持的发行版。 CNCF 总经理 Priyanka Sharma 欢迎 New Relic 加入该组织。“我们很高兴欢迎 New Relic 成为白金会员和 Zain Asgar 加入我们的董事会。Zain 和 New Relic 对推进我们的使命和支持我们的社区的承诺将有很大的帮助。我们特别期待着他们在可观察性方面细致入微的专业知识和观点”。 引用链接 [1] New Relic open sources Pixie, its Kubernetes-native in-cluster observability platform: https://www.zdnet.com/article/new-relic-open-sources-pixie-its-kubernetes-native-in-cluster-observability-platform/ [2] 宋净超: https://jimmysong.io/ [3] Kubernetes: https://kubernetes.io/ [4] New Relic: https://newrelic.com/ [5] Pixie: https://px.dev/ [6] Apache 2.0 许可: https://www.apache.org/licenses/LICENSE-2.0 [7] 云原生计算基金会（CNCF）的: https://www.cncf.io/ [8] eBPF: https://lwn.net/Articles/740157/ [9] 亚马逊网络服务（AWS）: https://aws.amazon.com/ [10] eBPF: https://newrelic.com/blog/best-practices/what-is-ebpf [11] 云原生环境: https://newrelic.com/blog/nerd-life/open-source-observability-pixie [12] 带来了一系列新的可观察性挑战: https://newrelic.com/blog/nerd-life/open-source-observability-pixie [13] OpenTelemetry 作为度量化标准: https://opensource.newrelic.com/projects/open-telemetry [14] Pixie 实验室: https://pixielabs.ai/ [15] Pixie 开源现在也将在 AWS 上运行: https://aws.amazon.com/blogs/opensource/gathering-insights-on-kubernetes-applications-services-and-network-traffic-with-pixie Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/Untitled.html":{"url":"blog/kubernetes/Untitled.html","title":"Untitled","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/admission access.html":{"url":"blog/kubernetes/admission access.html","title":"Admission Access","keywords":"","body":"K8s: admission access API 请求认证 认证流程 step1: Authentication 请求用户是否为能够访问集群的合法用户 Authentication：即身份验证，这个环节它面对的输入是整个http request，它负责对来自client的请求进行身份校验， 支持的方法包括：client证书验证（https双向验证）、basic auth、普通token以及jwt token(用于serviceaccount)。 APIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐 个使用这些方法对客户端请求进行验证，只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功； 在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，默认支持client证书验证和serviceaccount两种身份验证 方式。在这个环节，apiserver会通过client证书或http header中的字段(比如serviceaccount的jwt token)来识别出请 求的“用户身份”，包括”user”、”group”等，这些信息将在后面的authorization环节用到。 step2: Authorization 用户是否有权限进行请求中的操作 Authorization：授权。这个环节面对的输入是http request context中的各种属性，包括：user、group、request path（比如：/api/v1、/healthz、/version等）、request verb(比如：get、list、create等)。APIServer会将这些属性值 与事先配置好的访问策略(access policy）相比较。APIServer支持多种authorization mode，包括Node、RBAC、Webhook等。 APIServer启动时，可以指定一种authorization mode，也可以指定多种authorization mode，如果是后者，只要Request 通过了其中一种mode的授权，那么该环节的最终结果就是授权成功。在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中 ，authorization-mode的默认配置是”Node,RBAC”。Node授权器主要用于各个node上的kubelet访问apiserver时使用的，其 他一般均由RBAC授权器来授权。 step3: Admission Control 请求是否安全合规 证书位置 X509认证: 公钥:/etc/kubernetes/pki/ca.crt 私钥:/etc/kubernetes/pki/ca.key 集群组件间通讯用证书都是由集群根CA签发 在证书中有两个身份证凭证相关的重要字段: Comman Name(CN)：apiserver在认证过程中将其作为用户user Organization(O)：apiserver在认证过程中将其作为组(group) 客户端证书 每个kubernetes系统组件都在集群创建时签发了自身对应的客户端证书 controller-manager system: kube-controller-manager scheduler system:kube-scheduler kube-proxy system-kube-proxy kubelet system:node:$(node-hostname) system:nodes 通过kubernetes　api　签发证书 证书签发API: Kubernets 提供了证书签发的API: certificates.k8s.io/v1beta1 客户端证书的签发请求发送到API server 签发请求会以csr资源模型的形式持久化 新创建好的csr模型会保持pending的状态,直到有权限管理员对其approve 一旦csr完成approved, 请求对应的证书即被签发 cat 签发用户证书 生成私钥: openssl genrsa -out test.key 2048 生成csr CN: user O: group openssl req -new -key test.key -out test.csr -subj \"/CN=dahu/O=devs\" 通过API创建k8s csr 实例并等待管理员的审批 基于csr文件或实例通过集群 ca keypair　签发证书，下面是openssl签发实例 openssl x509 --req --in admin.scr --CA CA_LOCATION/ca.crt --Cakey CA_LOCATION/ca.key --Cacreateserial --out admin.crt --days 365 ServiceAccount secret apiVersion: v1 data: ca.crt: $(CA) namespace: default token: $(JSON web Token signed by API server) kind: Secret type: kubernetes.io/service-account-token metadata: .... 　kubeconfig generate kubeconfig 在本地进行kubeconfig 的配置 生成证书和秘钥cat > admin-csr.json cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin 查看生成的admin证书 ls admin* admin.csr admin-csr.json admin-key.pem admin.pem add cluster connection info by kubectl kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.39.7.51:6443 #--server=https://paas.enncloud.cn:6443 如果用的lb做负载均衡 就写lb的地址 配置客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem 添加新的context入口到kubectl配置中 ``` kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes #### use kubeconfig - set kubeconfig env export KUBECONFIG_SAVED=$KUBECONFIG export KUBECONFIG=$KUBECONFIG:config-demo:config-demo-2 kubectl config view - 将$HOME/.kube/config　append KUBECONFIG 环境变量设置中 export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config - 多集群config的合并和切换 KUBECONFIG=file1:file2:file3 kubectl config view --merge --flatten > ~/.kubectl/all-config export KUBECONFIG = ~/.kube/all-config kubectl config get-context kubectl config use-context {your-contexts} ### kubernetes RBAC Role-Based Access Control即Role-Based Access Control，它使用”rbac.authorization.k8s.io” 实现授权决策，允许管理员通过Kubernetes API动态配置策略。在RBAC API中，一个角色(Role)包含了 一组权限规则。Role有两种：Role和ClusterRole。一个Role对象只能用于授予对某一单一命名空间 （namespace）中资源的访问权限。ClusterRole对象可以授予与Role对象相同的权限，但由于它 们属于集群范围对象， 也可以使用它们授予对以下几种资源的访问权限 > * 集群范围资源（例如节点，即node） > * 非资源类型endpoint（例如”/healthz”） > * 跨所有命名空间的命名空间范围资源（例如所有命名空间下的pod资源) rolebinding，角色绑定则是定义了将一个角色的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过RoleBinding对象进行用户授权，而集群范围的用户授权则可以通过ClusterRoleBinding对象完成 subjects: developer, kubectl, pods process, components api resources: pods, nodes, services verbs: get, list, create, watch, patch, delete #### ClusterRole apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: admission-webhook-example-cr labels: app: admission-webhook-example rules: apiGroups: qikqiak.com resources: \"*\" verbs: \"*\" ``` ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: lau-controller namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: admission-webhook-example-cr subjects: - kind: ServiceAccount name: lau-controller namespace: kube-system - kind: User name: dev apiGroup: rbac.authorization.k8s.io Security Context的使用 漏洞　CVE-2019-5736 RunTime　安全策略 pod or container　set Security Context Pod Secruity Policy use admission controllers \"imagePolicyWebhook\" \"AlwaysPullImages\" SecurityContext -> runAsNonRoot SecurityContext -> Capabilities SecurityContext -> readOnlyRootFilesystem PodSecurityContext -> MustRunAsNonRoot Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/blog on kubernetes.html":{"url":"blog/kubernetes/blog on kubernetes.html","title":"Blog On Kubernetes","keywords":"","body":"blog 上云 blog 框架 上云步骤 创建kubernetes 登陆lstack，持续交付功能模块，添加agent 集成 代码仓库 创建kuberntes 创建 ECS 创建kuberntes 登陆lstack 平台 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/ccm.html":{"url":"blog/kubernetes/ccm.html","title":"Ccm","keywords":"","body":"cloud controller manage Kubernetes 是一个云原生平台，但为了让 Kubernetes 能够更好地运行在公有云平台上，能够灵活地使用、管理云上其他的基础资源和基础服务， 云厂商需要实现自己的适配器。 1.6版本之前各云服务商的基础资源管理代码都集成在kubernetes源码的CloudProvider中, 后面为了不影响kubernetes 版本的发布进度，将其从中解耦出来。由各云服务商独自实现其抽象出来的接口即可。 于是在 Kubernetes v1.6，引入了cloud-controller-manager(CCM)项目。 开发CCM 云服务商需要实现哪些接口 cloudprovider.Interface Interface type Interface interface { //　可以通过client　和　kube-apiserver通信,　启动自定义控制器，任何从这里启动的goroutine都 // 必须可以安全退出 // k8s.io/kubernetes/cmd/cloud-controller-manager/app/controllermanager.go Line 225 被调用 // openstack: return nil // alibaba: 启动了route 和　node 控制器 Initialize(clientBuilder ControllerClientBuilder, stop type InformerUser interface { SetInformers(informerFactory informers.SharedInformerFactory) } type LoadBalancer interface { // 通过传入service对象，获取云服务商的LB exists and its status GetLoadBalancer(ctx context.Context, clusterName string, service *v1.Service) (status *v1.LoadBalancerStatus, exists bool, err error) // 通过传入的service 对象，获取LoadBalancerName, 创建lb的时候默认会根据service uid 前32位来命名 GetLoadBalancerName(ctx context.Context, clusterName string, service *v1.Service) string // service controller 监听到 type 修改为LoadBalancher　的时候会触发该接口, 调用ccm 去创建　LB and EIP(floatIP) EnsureLoadBalancer(ctx context.Context, clusterName string, service *v1.Service, nodes []*v1.Node) (*v1.LoadBalancerStatus, error) // service controller 监听到 port or node change, 调用ccm modify lb.Listener or Backend UpdateLoadBalancer(ctx context.Context, clusterName string, service *v1.Service, nodes []*v1.Node) error //　service controller 监听到service　被删除或者type由于LoadBalancer改变为其他类型的时候触发,调用ccm delete LB and EIP EnsureLoadBalancerDeleted(ctx context.Context, clusterName string, service *v1.Service) error } type Instances interface { NodeAddresses(ctx context.Context, name types.NodeName) ([]v1.NodeAddress, error) NodeAddressesByProviderID(ctx context.Context, providerID string) ([]v1.NodeAddress, error) InstanceID(ctx context.Context, nodeName types.NodeName) (string, error) InstanceType(ctx context.Context, name types.NodeName) (string, error) InstanceTypeByProviderID(ctx context.Context, providerID string) (string, error) AddSSHKeyToAllInstances(ctx context.Context, user string, keyData []byte) error CurrentNodeName(ctx context.Context, hostname string) (types.NodeName, error) InstanceExistsByProviderID(ctx context.Context, providerID string) (bool, error) InstanceShutdownByProviderID(ctx context.Context, providerID string) (bool, error) } type Clusters interface { ListClusters(ctx context.Context) ([]string, error) Master(ctx context.Context, clusterName string) (string, error) } type Routes interface { //　列出当前集群的路由规则 ListRoutes(ctx context.Context, clusterName string) ([]*Route, error) // 当前集群新建路由规则 CreateRoute(ctx context.Context, clusterName string, nameHint string, route *Route) error // 删除路由规则 DeleteRoute(ctx context.Context, clusterName string, route *Route) error } type Zones interface { GetZone(ctx context.Context) (Zone, error) GetZoneByProviderID(ctx context.Context, providerID string) (Zone, error) GetZoneByNodeName(ctx context.Context, nodeName types.NodeName) (Zone, error) } type PVLabeler interface { GetLabelsForVolume(ctx context.Context, pv *v1.PersistentVolume) (map[string]string, error) } cloudProvider 到　CCM　的重构演进 因为原先的 Cloud Provider 与 Mater 中的组件 kube-controller-manager、kube-apiserver 以及 Node 中的组件 kubelet 耦合很紧密，所以这三个组件也需要进行重构。 kube-controller-manager 的重构策略 Route Controller 移入 CCM Service Controller 移入 CCM PersistentVolumeLabel Controller 移入 CCM Node Controller 移入 CCM, 并且新增功能 CIDR 的管理 监控节点的状态 节点Pod的驱逐策略 kube-apiserver 的重构策略 分发SSH Keys 由CCM实现 PV的Adminssion Controller　由 kubelet实现 kubelet的重构策略 kubelet 需要增加一个新功能：在 CCM 还未初始化 kubelet 所在节点时，需标记此节点类似“ NotReady ”的状态，防止 scheduler 调度 Pod 到此节点时产生一系列错误。此功能通过给节点加上如下 Taints 并在 CCM 初始化后删去此 Taints 来实现 CCM　架构介绍 node controller 使用 Cloud Provider 来检查 Node 是否已经在云上被删除了。如果 Cloud Provider 返回有 Node 被删除，那么 Node Controller 立马就会把此 Node 从 Kubernetes 中删除。 service controller 负责为type: LoadBalancer的service 创建，删除，更新LB and EIP. route controller 配置node路由.kubernetes 网络的基本原则是每个pod都要有一个独立ip地址,而且假定所有的pod都在直接连通的扁平网络中. 而云上node的基础设施是云服务商提供的,所以 Route Controller 需要调用 Cloud Provider 来配置云上的 Node 的底层路由. pvLabel controller 使用 Cloud Provider 来创建、删除、挂载、卸载 Node 上的卷，这是因为卷也是云厂商额外提供的云存储服务。 CCM 源码分析 本文的源码分析是以openstack为基础:https://github.com/kubernetes/cloud-provider-openstack:origin/release-1.17 kuberentes 1.17 启动流程 1.项目启动后，先执行所有init方法，注册 getCloudProvider方法（map[云服务商名称]创建cloudProvider方法） /root/go/src/k8s.io/cloud-provider-openstack/pkg/cloudprovider/providers/openstack/openstack.go line: 251, 完成cloudProvider　获取方法的注册 func init() { RegisterMetrics() cloudprovider.RegisterCloudProvider(ProviderName, func(config io.Reader) (cloudprovider.Interface, error) { cfg, err := ReadConfig(config) logcfg(cfg) if err != nil { return nil, err } // 这里可以new　一个自己的CloudProvider对象(前提是实现接口cloudprovider.Interface), cloud, err := NewOpenStack(cfg) if err != nil { klog.V(1).Infof(\"New openstack client created failed with config\") } return cloud, err }) } /root/go/src/k8s.io/cloud-provider-.../vendor/k8s.io/cloud-provider/plugins.go // RegisterCloudProvider registers a cloudprovider.Factory by name. This // is expected to happen during app startup. func RegisterCloudProvider(name string, cloud Factory) { providersMutex.Lock() defer providersMutex.Unlock() if _, found := providers[name]; found { klog.Fatalf(\"Cloud provider %q was registered twice\", name) } klog.V(1).Infof(\"Registered cloud provider %q\", name) providers[name] = cloud } // provider是一个map, // key: 云服务商名称 // value: 工厂方法，即创建CloudProvider的方法 providers = make(map[string]Factory) // Factory is a function that returns a cloudprovider.Interface. // The config parameter provides an io.Reader handler to the factory in // order to load specific configurations. If no configuration is provided // the parameter is nil. type Factory func(config io.Reader) (Interface, error) 2.main 函数入口启动程序 生成ccm　默认配置文件对象 解析启动命令行参数 verflag.PrintAndExitIfRequested() utilflag pflag.CommandLine.SetNormalizeFunc pflag.CommandLine.AddGoFlagSet logs.InitLogs() 调用 k8s中的cloud-controller-manager.Run() InitCloudProvider, err, nil, and clusterID　校验 configz.New　不懂 create HealthChecker create (安全和不安全)httpServer 定义controller启动函数 选择参数校验　--leader-elect　 false: 按顺序启动controller select{}　程序阻塞 true: append(healthCheck,NewLeaderHealthzAdaptor) create lock(锁资源类型，namespace, name,corev1Client,coorinationv1Client,rlconfig) try become the leader and start cloud controller manager loops package main import ( goflag \"flag\" \"k8s.io/apimachinery/pkg/util/wait\" \"k8s.io/apiserver/pkg/server/healthz\" \"k8s.io/cloud-provider-huawei/huawei\" \"k8s.io/cloud-provider-huawei/pkg/version\" \"k8s.io/component-base/cli/flag\" \"k8s.io/component-base/logs\" _ \"k8s.io/component-base/metrics/prometheus/restclient\" // for client metric registration _ \"k8s.io/component-base/metrics/prometheus/version\" // for version metric registration \"k8s.io/component-base/version/verflag\" \"k8s.io/klog\" \"k8s.io/kubernetes/cmd/cloud-controller-manager/app\" \"k8s.io/kubernetes/cmd/cloud-controller-manager/app/options\" _ \"k8s.io/kubernetes/pkg/features\" // add the kubernetes feature gates utilflag \"k8s.io/kubernetes/pkg/util/flag\" \"net/http\" \"os\" \"github.com/spf13/cobra\" \"github.com/spf13/pflag\" ) func init() { mux := http.NewServeMux() healthz.InstallHandler(mux) version.Version = \"1.17\" } func main() { // 获取ccm默认配置文件 s, err := options.NewCloudControllerManagerOptions() if err != nil { klog.Fatalf(\"unable to initialize command options: %v\", err) } //　使用lease会报错, 需要继续探究 s.Generic.LeaderElection.ResourceLock = \"endpoints\" // CLI命令行的golang库，也是一个生成程序应用和命令行文件的程序 // 在command.Run中完成config生成和ccm启动 command := &cobra.Command{ Use: \"huawei cloud controller manager\", Long: `The Cloud controller manager is a daemon that embeds the cloud specific control loops shipped with Kubernetes.`, Run: func(cmd *cobra.Command, args []string) { // 如果有请求参数 --version 则打印kuberents　版本 verflag.PrintAndExitIfRequested() // 打印所有命令flag and value utilflag.PrintFlags(cmd.Flags()) // 验证KnownControllers key 是否包含了GenericControllerManagerConfiguration.Controllers中的所有控制器 //获取ccm　config object c, err := s.Config(app.KnownControllers(), app.ControllersDisabledByDefault.List()) if err != nil { klog.Error(os.Stderr, err) os.Exit(1) } // 启动CCM, 会执行 node, route, service, pvLabel controller if err := app.Run(c.Complete(), wait.NeverStop); err != nil { klog.Error(os.Stderr, err) os.Exit(1) } }, } //　生成CLI默认启动参数对 fs := command.Flags() //　将ccm config　的参数对　移入　CLI的启动　参数与对中 namedFlagSets := s.Flags(app.KnownControllers(), app.ControllersDisabledByDefault.List()) for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } pflag.CommandLine.SetNormalizeFunc(flag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) logs.InitLogs() defer logs.FlushLogs() klog.Infof(\"huawei cloud provider version: %s\", version.Version) //　省去从启动命令赋值的步骤 s.KubeCloudShared.CloudProvider.Name = huawei.ProviderName // 执行cli启动命令 if err := command.Execute(); err != nil { klog.Error(os.Stderr, err) os.Exit(1) } } 3.启动所有控制器, 开始监听资源变化 /root/go/pkg/mod/k8s.io/kubernetes@v1.17.4/cmd/cloud-controller-manager/app/core.go cloud-node UpdateNodeStatus updates the node status, such as node addresses cloud-node-lifecycle when you shutdown nodes, will delete node from cluster service update loadbalancer route update node cidr pvController 已经从core中移除 // Run runs the ExternalCMServer. This should never exit. func Run(c *cloudcontrollerconfig.CompletedConfig, stopCh /root/go/pkg/mod/k8s.io/kubernetes@v1.17.4/cmd/cloud-controller-manager/app/controllermanager.go ccm 4个控制器 // line 139 // initFunc is used to launch a particular controller. It may run additional \"should I activate checks\". // Any error returned will cause the controller process to `Fatal` // The bool indicates whether the controller was enabled. type initFunc func(ctx *cloudcontrollerconfig.CompletedConfig, cloud cloudprovider.Interface, stop 4.service type　变化后调用 cloudProvider LoadBalancer相关接口对LB 执行CRUD // create or update LB // EnsureLoadBalancer creates a new load balancer 'name', or updates the existing one. func (lbaas *LbaasV2) EnsureLoadBalancer(ctx context.Context, clusterName string, apiService *v1.Service, nodes []*v1.Node) (*v1.LoadBalancerStatus, error) { ... // 获取service port ports := apiService.Spec.Ports if len(ports) == 0 { return nil, fmt.Errorf(\"no ports provided to openstack load balancer\") } affinity := apiService.Spec.SessionAffinity //若指定了elb_id，则不创建elb elbId := getStringFromServiceAnnotation(apiService, ServiceAnnotationLoadBalancerInstanceID, \"\") if elbId != \"\" { // 根据 loadbalancer id 获取 loadbalancer isCreateElb = false loadbalancer, err = getLoadbalancerByID(lbaas.lb, elbId) .... // 根据service uuid 获取 loadbalancer 的标准命名格式 name := lbaas.GetLoadBalancerName(ctx, clusterName, apiService) // 需要更新lb的name，否则无法删除lb err = lbaas.updateLoadBalancerName(apiService, name, elbId) ... } else { //创建elb lbaas.opts.SubnetID = getStringFromServiceAnnotation(apiService, ServiceAnnotationLoadBalancerSubnetID, lbaas.opts.SubnetID) if len(lbaas.opts.SubnetID) == 0 { subnetID, err := getSubnetIDForLB(lbaas.compute, *nodes[0]) lbaas.opts.SubnetID = subnetID } name := lbaas.GetLoadBalancerName(ctx, clusterName, apiService) //　创建lb loadbalancer, err = lbaas.createLoadBalancer(apiService, name, internalAnnotation) } // 获取负载均衡算法 lbMethod := getLBMethod(apiService) //　为lb　创建监听器，有几个端口，就创建几个 for portIndex, port := range ports { } // 创建eip if floatIP == nil && floatingPool != \"\" && !internalAnnotation { .... } status := &v1.LoadBalancerStatus{} return status, nil } 部署 以下部署方式适用于基于openstack改造后的华为ccm 部署前必须先在kubelet配置文件中加入如下参数 --hostname-override=${INSTANCE_ID} --provider-id=${INSTANCE_ID} 集群外部署 export IDENTITY_ENDPOINT=https://iam.cn-east-3.myhuaweicloud.com/v3 export PROJECT_ID=... export DOMAIN_ID=... export ACCESS_KEY_ID= ... export ACCESS_KEY_SECRET= .... export ROUTER_ID= ... export REGION= ... go run ./cmd/cloud-controller-manager.go --kubeconfig=./kube.config -v 4 集群内部署 RBAC, 这里为了方便好看直接用了超级权限 --- apiVersion: v1 kind: ServiceAccount metadata: name: cloud-controller-manager namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cloud-controller-manager namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: cloud-controller-manager namespace: kube-system DaemonSet apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: cloud-controller-manager name: cloud-controller-manager namespace: kube-system spec: selector: matchLabels: k8s-app: cloud-controller-manager template: metadata: labels: k8s-app: cloud-controller-manager spec: serviceAccountName: cloud-controller-manager containers: - name: cloud-controller-manager image: ...-huawei-ccm imagePullPolicy: Always command: - /cloud-controller-manager env: - name: IDENTITY_ENDPOINT value: \"https://iam.cn-east-3.myhuaweicloud.com/v3\" - name: PROJECT_ID value: ... - name: DOMAIN_ID value: ... - name: ACCESS_KEY_ID value: ... - name: ACCESS_KEY_SECRET value: ... - name: ROUTER_ID value: ... - name: REGION value: ... - name: SUBNET_ID value: ... tolerations: - effect: NoSchedule operator: Exists key: node-role.kubernetes.io/master - effect: NoSchedule operator: Exists key: node.cloudprovider.kubernetes.io/uninitialized - key: node-role.kubernetes.io/master effect: NoSchedule nodeSelector: node-role.kubernetes.io/master: \"\" hostNetwork: true 参考文献: https://mp.weixin.qq.com/s/a_540yJ1EGVroJ9TpvYtPw 作者简介: 蔡锡生，　杭州朗澈科技有限公司k8s工程师 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/cert manage.html":{"url":"blog/kubernetes/cert manage.html","title":"Cert Manage","keywords":"","body":"cert manage 概述 随着 HTTPS 不断普及，大多数网站开始由 HTTP 升级到 HTTPS。使用 HTTPS 需要向权威机构申请证书，并且需要付出一定的成本，如果需求数量多，则开支也相对增加。cert-manager 是 Kubernetes 上的全能证书管理工具，支持利用 cert-manager 基于 ACME 协议与 Let's Encrypt 签发免费证书并为证书自动续期，实现永久免费使用证书。 操作原理 cert-manager 工作原理 cert-manager 部署到 Kubernetes 集群后会查阅其所支持的自定义资源 CRD，可通过创建 CRD 资源来指示 cert-manager 签发证书并为证书自动续期。如下图所示： Issuer/ClusterIssuer ：用于指示 cert-manager 签发证书的方式，本文主要讲解签发免费证书的 ACME 方式。 说明： Issuer 与 ClusterIssuer 之间的区别是：Issuer 只能用来签发自身所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书。 Certificate：用于向 cert-manager 传递域名证书的信息、签发证书所需要的配置，以及对 Issuer/ClusterIssuer 的引用。 免费证书签发原理 Let’s Encrypt 利用 ACME 协议校验域名的归属，校验成功后可以自动颁发免费证书。免费证书有效期只有90天，需在到期前再校验一次实现续期。使用 cert-manager 可以自动续期，即实现永久使用免费证书。校验域名归属的两种方式分别是 HTTP-01 和 DNS-01，校验原理详情可参见 Let's Encrypt 的运作方式。 HTTP-01 校验原理 DNS-01 校验原理 HTTP-01 的校验原理是给域名指向的 HTTP 服务增加一个临时 location。此方法仅适用于给使用 Ingress 暴露流量的服务颁发证书，并且不支持泛域名证书。 例如，Let’s Encrypt 会发送 HTTP 请求到 http:///.well-known/acme-challenge/。YOUR_DOMAIN 是被校验的域名。TOKEN 是 ACME 协议客户端负责放置的文件，在此处 ACME 客户端即 cert-manager，通过修改或创建 Ingress 规则来增加临时校验路径并指向提供 TOKEN 的服务。Let’s Encrypt 会对比 TOKEN 是否符合预期，校验成功后就会颁发证书。 校验方式对比 HTTP-01 校验方式的优点是配置简单通用，不同 DNS 提供商均可使用相同的配置方法。缺点是需要依赖 Ingress，若仅适用于服务支持 Ingress 暴露流量，不支持泛域名证书。 DNS-01 校验方式的优点是不依赖 Ingress，并支持泛域名。缺点是不同 DNS 提供商的配置方式不同，DNS 提供商过多而 cert-manager 的 Issuer 不能全部支持。部分可以通过部署实现 cert-manager 的 Webhook 服务来扩展 Issuer 进行支持。例如 DNSPod 和 阿里 DNS，详情请参见 Webhook 列表。 本文向您推荐 DNS-01 方式，其限制较少，功能较全。 操作步骤 安装 cert-manager 通常使用 yaml 方式一键安装 cert-manager 到集群，可参考官网文档 Installing with regular manifests。 cert-manager 官方使用的镜像在 quay.io 进行拉取，在国内拉取镜像时您可以参考 境外镜像拉取加速。 配置 DNS 登录 DNS 提供商后台，配置域名的 DNS A 记录，指向所需要证书的后端服务对外暴露的 IP 地址。以 cloudflare 为例，如下图所示： HTTP-01 校验方式签发证书 若使用 HTTP-01 的校验方式，则需要用到 Ingress 来配合校验。cert-manager 会通过自动修改 Ingress 规则或自动新增 Ingress 来实现对外暴露校验所需的临时 HTTP 路径。为 Issuer 配置 HTTP-01 校验时，如果指定 Ingress 的 name，表示会自动修改指定 Ingress 的规则来暴露校验所需的临时 HTTP 路径，如果指定 class，则表示会自动新增 Ingress，可参考以下 示例。 TKE 自带的 Ingress 中，每个 Ingress 资源都会对应一个负载均衡 CLB，如果使用 TKE 自带的 Ingress 暴露服务，并且使用 HTTP-01 方式校验，那么只能使用自动修改 Ingress 的方式，不能自动新增 Ingress。自动新增的 Ingress 会自动创建其他 CLB，使对外的 IP 地址与后端服务的 Ingress 不一致，Let's Encrypt 校验时将无法从服务的 Ingress 找到校验所需的临时路径，从而导致校验失败，无法签发证书。如果使用自建 Ingress，例如 在 TKE 上部署 Nginx Ingress，同一个 Ingress class 的 Ingress 共享同一个 CLB，则支持使用自动新增 Ingress 的方式。 示例 如果服务使用 TKE 自带的 Ingress 暴露服务，则不适合用 cert-manager 签发管理免费证书，证书从 证书管理 中被引用，不在 Kubernetes 中管理。 假设是 在 TKE 上部署 Nginx Ingress，且后端服务的 Ingress 是 prod/web，可参考以下代码示例创建 Issuer： apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-http01 namespace: prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-http01-account-key solvers: - http01: ingress: name: web # 指定被自动修改的 Ingress 名称 使用 Issuer 签发证书，cert-manager 会自动创建 Ingress 资源，并自动修改 Ingress 的资源 prod/web，以暴露校验所需的临时路径。参考以下代码示例，自动新增 Ingress： apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-http01 namespace: prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-http01-account-key solvers: - http01: ingress: class: nginx # 指定自动创建的 Ingress 的 ingress class 成功创建 Issuer 后，参考以下代码示例，创建 Certificate 并引用 Issuer 进行签发： apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: test-mydomain-com namespace: prod spec: dnsNames: - test.mydomain.com # 要签发证书的域名 issuerRef: kind: Issuer name: letsencrypt-http01 # 引用 Issuer，指示采用 http01 方式进行校验 secretName: test-mydomain-com-tls # 最终签发出来的证书会保存在这个 Secret 里面 DNS-01 校验方式签发证书 若使用 DNS-01 的校验方式，则需要选择 DNS 提供商。cert-manager 内置 DNS 提供商的支持，详细列表和用法请参见 Supported DNS01 providers。若需要使用列表外的 DNS 提供商，可参考以下两种方案： 方案1：设置 Custom Nameserver 方案2：使用 Webhook 在 DNS 提供商后台设置 custom nameserver，指向例如 cloudflare 此类可管理其它 DNS 提供商域名的 nameserver 地址，具体地址可登录 cloudflare 后台查看。如下图所示： namecheap 可以设置 custom nameserver，如下图所示： 最后配置 Issuer 指定 DNS-01 验证时，添加 cloudflare 的信息即可。 获取和使用证书 创建 Certificate 后，即可通过 kubectl 查看证书是否签发成功。 $ kubectl get certificate -n prod NAME READY SECRET AGE test-mydomain-com True test-mydomain-com-tls 1m READY 为 False ：则表示签发失败，可以通过 describe 命令查看 event 来排查失败原因。 $ kubectl describe certificate test-mydomain-com -n prod READY 为 True ：则表示签发成功，证书将保存在所指定的 Secret 中。例如， default/test-mydomain-com-tls 。可以通过 kubectl 查看，其中 tls.crt 是证书， tls.key 是密钥。 $ kubectl get secret test-mydomain-com-tls -n default ... data: tls.crt: tls.key: 您可以将其挂载到需要证书的应用中，或者直接在自建的 Ingress 中引用 secret。可参考以下示例： apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: test-ingress annotations: kubernetes.io/Ingress.class: nginx spec: rules: - host: test.mydomain.com http: paths: - path: /web backend: serviceName: web servicePort: 80 tls: hosts: - test.mydomain.com secretName: test-mydomain-com-tls 相关文档 cert-manager 官网 Let's Encrypt 的运作方式 Issuer API 文档 Certificate API 文档 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/cilium/BGP.html":{"url":"blog/kubernetes/cilium/BGP.html","title":"BGP","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/cilium/install.html":{"url":"blog/kubernetes/cilium/install.html","title":"Install","keywords":"","body":"helm charts https://artifacthub.io/packages/helm/cilium/cilium/1.11.5 https://kubesphere.io/zh/blogs/cilium-1.11-release/#bgp-%E5%AE%A3%E5%91%8A-pod-cidr Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/client-go-request 设计.html":{"url":"blog/kubernetes/client-go-request 设计.html","title":"Client Go Request 设计","keywords":"","body":"rest 工具包设计 简介 RestClient 在一组资源路径上加入了Kubernets API的约定 服务器返回一个可解码的 接口 // Interface captures the set of operations for generically interacting with Kubernetes REST apis. type Interface interface { GetRateLimiter() flowcontrol.RateLimiter Verb(verb string) *Request Post() *Request Put() *Request Patch(pt types.PatchType) *Request Get() *Request Delete() *Request APIVersion() schema.GroupVersion } struct // RESTClient imposes common Kubernetes API conventions on a set of resource paths. // The baseURL is expected to point to an HTTP or HTTPS path that is the parent // of one or more resources. The server should return a decodable API resource // object, or an api.Status object which contains information about the reason for // any failure. // // Most consumers should use client.New() to get a Kubernetes API client. type RESTClient struct { // base is the root URL for all invocations of the client base *url.URL // versionedAPIPath is a path segment connecting the base URL to the resource root versionedAPIPath string // content describes how a RESTClient encodes and decodes responses. content ClientContentConfig // creates BackoffManager that is passed to requests. createBackoffMgr func() BackoffManager // rateLimiter is shared among all requests created by this client unless specifically // overridden. rateLimiter flowcontrol.RateLimiter // warningHandler is shared among all requests created by this client. // If not set, defaultWarningHandler is used. warningHandler WarningHandler // Set specific behavior of the client. If not set http.DefaultClient will be used. Client *http.Client } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/client-go.html":{"url":"blog/kubernetes/client-go.html","title":"Client Go","keywords":"","body":"k8s client 和schema 1. k8s.io/client-go client-go 项目有4种类型的客户端 RestClient rest.RESTClient RESTClient是最基础的客户端RESTClient对HTTP Request进行了封装，实现了RESTful风格的API。 ClientSet，DynamicClient，DiscoveryClient客户端都是基于RESTClient实现的。 package main import ( \"flag\" \"fmt\" \"k8s.io/client-go/pkg/runtime\" \"k8s.io/client-go/pkg/runtime/serializer\" \"k8s.io/client-go/pkg/api\" v1 \"k8s.io/client-go/pkg/api/v1\" \"k8s.io/client-go/pkg/api/unversioned\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/clientcmd\" ) func main() { kubeconfig := flag.String(\"kubeconfig\", \"/root/.kube/config\", \"Path to a kube config. Only required if out-of-cluster.\") flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) if err != nil { fmt.Println(\"BuildConfigFromFlags error\") } groupversion := &unversioned.GroupVersion{\"\", \"v1\"} config.GroupVersion = groupversion config.APIPath = \"/api\" config.ContentType = runtime.ContentTypeJSON config.NegotiatedSerializer = serializer.DirectCodecFactory{CodecFactory: api.Codecs} restClient, err := rest.RESTClientFor(config) if err != nil { fmt.Println(\"RESTClientFor error\") } pod := v1.Pod{} err = restClient.Get().Resource(\"pods\").Namespace(\"default\").Name(\"nginx-1487191267-b4w5j\").Do().Into(&pod) if err != nil { fmt.Println(\"error\") } fmt.Println(pod) } ClientSet *kubernetes.Clientset ClientSet 是在RESTClient基础上封装了对Resource和Version的管理方法。每一个Resource可以理解为一个客户端，而ClientSet则是多个客户端的集合，每一个Resource和Version都以函数的方式暴露给开发者。ClientSet只能够处理Kubernetes内置资源，他是通过Client-go代码生成器生成的。 ``` package main import ( \"context\" \"fmt\" v1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" ctrl \"sigs.k8s.io/controller-runtime\" ) func main() { config := ctrl.GetConfigOrDie() c, err := kubernetes.NewForConfig(config) if err != nil { fmt.Println(\"error\") } deploys, err := c.AppsV1().Deployments(\"default\").List(context.TODO(),v1.ListOptions{}) if err != nil { fmt.Println(\"error\") } for _, d := range deploys.Items { fmt.Println(d.Name) } } - DynamicClient dynamic.Interface DynamicClient与ClientSet最大的不同之处是，ClientSet仅能访问Kubernetes自带的资源（即client集合哪的资源）， 而不能直接访问CRD自带的资源。DynamicClient能过处理Kubernetes中的所有资源对象，包括Kubernetes内置资源与 CRD自定义资源。 package main import ( \"context\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" \"k8s.io/client-go/dynamic\" \"k8s.io/klog\" ctrl \"sigs.k8s.io/controller-runtime\" ) func main(){ config := ctrl.GetConfigOrDie() // create the dynamic client from kubeconfig dynamicClient, err := dynamic.NewForConfig(config) if err != nil { klog.Fatal(err) } ns := &corev1.Namespace{ ObjectMeta: metav1.ObjectMeta{ Name: \"test-1\", }, } // convert the runtime.Object to unstructured.Unstructured mapData, err := runtime.DefaultUnstructuredConverter.ToUnstructured(ns) if err != nil { klog.Fatal(err) } unstructuredObj := unstructured.Unstructured{ Object: mapData, } // create the object using the dynamic client nameSpaceResource := schema.GroupVersionResource{Version: \"v1\", Resource: \"namespaces\"} nsList, err := dynamicClient.Resource(nameSpaceResource).List(context.Background(),metav1.ListOptions{}) if err != nil { klog.Fatal(err) } klog.Infof(\"list :%d\", len(nsList.Items)) respData, err := dynamicClient.Resource(nameSpaceResource).Create(context.Background(),&unstructuredObj,metav1.CreateOptions{}) if err != nil { klog.Fatal(err) } respNs := &corev1.Namespace{} // convert unstructured.Unstructured to a Node if err = runtime.DefaultUnstructuredConverter.FromUnstructured(respData.UnstructuredContent(), respNs); err != nil { klog.Fatal(err) } klog.Infof(\"namespace: %+v\", respNs) } - DiscoveryClient *discovery.DiscoveryClient 发现客户端，用于发现kube-apiserver所支持的资源组、资源版本、资源信息（即Group, Versions,Resources) ## 2. sigs.k8s.io/controller-runtime 该客户端是可以直接从kubernetes server 中读写的，它能够处理普通类型，自定义类型，内建类型以及未知类型。 使用该client的时候，它会使用scheme去寻找Group， version 和类型。 ### 2.1 scheme 资源注册表 kubernetes中有很多资源，只有被注册到scheme资源注册表中，比如oam中的crd资源，我们是无法通过直接创建的。 var scheme = runtime.NewScheme() func init() { = clientgoscheme.AddToScheme(scheme) = oamcore.AddToScheme(scheme) } ### 2.2 example about apply oam component 在实际使用场景中， 我们可能非常需要类似kubectl apply的功能，即没有对象的时候新建，如果有就更新。 使用改client的patch方法即可实现。 package main import ( \"context\" \"fmt\" \"k8s-demo/common\" \"k8s-demo/k8s_client/deployment\" appsv1 \"k8s.io/api/apps/v1\" \"k8s.io/klog\" ctrl \"sigs.k8s.io/controller-runtime\" \"sigs.k8s.io/controller-runtime/pkg/client\" \"time\" ) / patch deployment if not found, new one; else, update. / var ( workloadName = \"apply-test\" namespace = \"default\" imageName = \"nginx\" ) func main() { ctx := context.Background() config := ctrl.GetConfigOrDie() begin := time.Now() c, err := client.New(config, client.Options{}) if err != nil { klog.Fatal(err) } klog.Info(\"build client cost time: \", time.Since(begin)) dep := deployment.GenerateDeployment(workloadName, namespace, imageName) // 1.delete deployment if err := c.Delete(ctx, &dep, &client.DeleteOptions{}); err != nil { klog.Fatal(err) } // patch 选项 applyOpts := []client.PatchOption{ client.ForceOwnership, client.FieldOwner(dep.GetUID()), &client.PatchOptions{FieldManager: \"apply\"}, } for i:=0;i} ``` Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/command.html":{"url":"blog/kubernetes/command.html","title":"Command","keywords":"","body":"K8s: command tips K8s 常用命令汇总 query 获取所有pod kubectl get pods -A 从最近的一条日志查询 kubectl logs -f {pod_name} --tail=1 获取svc 的终端 kubectl get ep 扩容 --field-selector \"metadata.name\", \"metadata.namespace\", \"spec.nodeName\", \"spec.restartPolicy\", \"spec.schedulerName\", \"spec.serviceAccountName\", \"status.phase\", \"status.podIP\", \"status.podIPs\", \"status.nominatedNodeName\" kubectl get nodes -o json | jq -r '.items[] | select(.status.conditions[] | select(.type==\"Ready\" and .status==\"True\")) | .metadata.name ' Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/controller.html":{"url":"blog/kubernetes/controller.html","title":"Controller","keywords":"","body":"k8s: Custom Resource Definition Custom Resource Definition 是kubernetes的资源扩展方式, 简称CRD 1. sample-controller 开源样板项目 sample-controller 是 kubernetes 官方提供的 CRD Controller 样例实现 sample-controller 2. 使用client-go包访问Kubernetes CRD 2.1 create CRD apiVersion: \"apiextensions.k8s.io/v1beta1\" kind: \"CustomResourceDefinition\" metadata: name: \"projects.example.sealyun.com\" spec: group: \"example.sealyun.com\" version: \"v1alpha1\" scope: \"Namespaced\" names: plural: \"projects\" singular: \"project\" kind: \"Project\" validation: openAPIV3Schema: required: [\"spec\"] properties: spec: required: [\"replicas\"] properties: replicas: type: \"integer\" minimum: 1 2.2 create golang client define type define DeepCopy Method Kubernetes API（在本例中为Project和ProjectList）提供的每种类型都需要实现该k8s.io/apimachinery/pkg/runtime.Object接口. 该接口定义了两种方法GetObjectKind()和DeepCopyObject()。第一种方法已经由嵌入式metav1.TypeMeta结构提供; 第二个你必须自己实现。 registry type schema.GroupVersion runtime.NewSchemeBuilder(addKnownTypes) SchemeBuilder.AddToScheme 3. Write your customer controller 注意: 这张图分为两部分,黄色图标是开发者需要自行开发的部分，而其它的部分是client-go已经提供的，直接使用即可。 通过controller中的Reflector来实现监听，它通过kubernetes的List/Watch机制将得到事件(Object)写入到Stroe(Delta FIFO)中， 后续会基于该Delta FIFO实现完全按事件发生的顺序进行分发处理。 由Reflector生产的事件最终由processor消费。processor通过POP队列(Delta FIFO)里的事件，更新本地的informer indexer缓存， 同时将事件distribute给所有的listener。 processer的listener由外部通过AddEventHandler注册，每个listener提供AddFunc, UpdateFunc, DeleteFunc方法。l istener内部的实现加了一层缓存，用于存放pendingNotification。listener最终实现了事件的分发，事件最终被注册的handler处理。 注册的handler可以根据事件的类型ADD,UPDATE,DELETE，将该事件的key(格式: namespace/resource_name)Enqueue到client-go 提供的Workqueue队列中。 开发者需要实现自己的controller syncHandler(就是自己的核心逻辑)，从Workqueue中获取key，并通过这个key解析出namespace和 resource_name去调用Lister从indexer中获取该key对应的相应的元数据进行后续的逻辑处理。 上面就是开发者想要写一个controller(或者有的人也叫operator)的一个整体的流程。 3.1 动手实践 download code generate cd $GOPATH/src mkdir -p k8s.io && cd k8s.io git clone https://github.com/kubernetes/code-generator.git download one template cd $GOPATH/src/k8s.io git clone https://github.com/xishengcai/example-controller.git file struct [root@cn-hongkong example-controller]# tree . ├── hack │ ├── boilerplate.go.txt │ ├── update-codegen.sh │ └── verify-codegen.sh └── pkg └── apis └── examplecontroller ├── register.go └── v1 ├── doc.go ├── register.go └── types.go 修改模板中的 group, version, object type， 然后使用脚本自动生成代码 ./hack/update-codegen.sh 自动生成了 clientset，informers，listers 三个文件夹下的文件和apis下的zz_generated.deepcopy.go文件。 其中zz_generated.deepcopy.go中包含 pkg/apis/samplecontroller/v1alpha1/types.go 中定义的结构体的 DeepCopy() 方法。 另外三个文件夹clientset，informers，listers下都是 Kubernetes 生成的客户端库，在 controller 中会用到。 [root@cn-hongkong example-controller]# ./hack/update-codegen.sh Generating deepcopy funcs Generating clientset for examplecontroller:v1 at k8s.io/example-controller/pkg/generated/clientset Generating listers for examplecontroller:v1 at k8s.io/example-controller/pkg/generated/listers Generating informers for examplecontroller:v1 at k8s.io/example-controller/pkg/generated/informers file struct [root@cn-hongkong example-controller]# tree . ├── hack │ ├── boilerplate.go.txt │ ├── update-codegen.sh │ └── verify-codegen.sh └── pkg ├── apis │ └── examplecontroller │ ├── register.go │ └── v1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go └── generated ├── clientset │ └── versioned │ ├── clientset.go │ ├── doc.go │ ├── fake │ │ ├── clientset_generated.go │ │ ├── doc.go │ │ └── register.go │ ├── scheme │ │ ├── doc.go │ │ └── register.go │ └── typed │ └── examplecontroller │ └── v1 │ ├── clustertesttype.go │ ├── doc.go │ ├── examplecontroller_client.go │ ├── fake │ │ ├── doc.go │ │ ├── fake_clustertesttype.go │ │ ├── fake_examplecontroller_client.go │ │ └── fake_testtype.go │ ├── generated_expansion.go │ └── testtype.go ├── informers │ └── externalversions │ ├── examplecontroller │ │ ├── interface.go │ │ └── v1 │ │ ├── clustertesttype.go │ │ ├── interface.go │ │ └── testtype.go │ ├── factory.go │ ├── generic.go │ └── internalinterfaces │ └── factory_interfaces.go └── listers └── examplecontroller └── v1 ├── clustertesttype.go ├── expansion_generated.go └── testtype.go 具体的controller 编写可以参考 https://github.com/kubernetes/sample-controller/blob/master/controller.go //main.go // 创建k8s原生资源的client kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil { klog.Fatalf(\"Error building kubernetes clientset: %s\", err.Error()) } // 创建自定义资源的client exampleClient, err := clientset.NewForConfig(cfg) if err != nil { klog.Fatalf(\"Error building example clientset: %s\", err.Error()) } // 生成informerFactrory kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30) exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30) controller := NewController(kubeClient, exampleClient, kubeInformerFactory.Apps().V1().Deployments(), exampleInformerFactory.Samplecontroller().V1alpha1().Foos()) // 运行 Informer，Start 方法为非阻塞，会运行在单独的 goroutine 中 kubeInformerFactory.Start(stopCh) exampleInformerFactory.Start(stopCh) // 多线程运行controller if err = controller.Run(2, stopCh); err != nil { klog.Fatalf(\"Error running controller: %s\", err.Error()) } //controller.go // Controller is the controller implementation for Foo resources // NewController returns a new sample controller func NewController( // 将 CRD 资源类型定义加入到 Kubernetes 的 Scheme 中，以便 Events 可以记录 CRD 的事件 utilruntime.Must(samplescheme.AddToScheme(scheme.Scheme)) eventBroadcaster.StartRecordingToSink(&typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events(\"\")}) // 监听 CRD 类型'Foo'并注册 ResourceEventHandler 方法，当'Foo'的实例变化时进行处理 fooInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueFoo, UpdateFunc: func(old, new interface{}) { controller.enqueueFoo(new) }, }) // 监听 Deployment 变化并注册 ResourceEventHandler 方法， // 当它的 ownerReferences 为 Foo 类型实例时，将该 Foo 资源加入 work queue deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.handleObject, UpdateFunc: func(old, new interface{}) { newDepl := new.(*appsv1.Deployment) oldDepl := old.(*appsv1.Deployment) if newDepl.ResourceVersion == oldDepl.ResourceVersion { return } controller.handleObject(new) }, DeleteFunc: controller.handleObject, }) return controller } func (c *Controller) Run(threadiness int, stopCh --- 以下是源码分析原文 Kubernetes Client-Go Informer 实现源码剖析----- 4. informer 4.1 Use SharedInformerFactory Create EventInformer SharedInformerFactory为kubernetes中的所有资源(API group versions)提供了一个shared informer。所以controller中使用的所有Informer都是 从SharedInformerFactory中通过GroupVersionResource得到. kubernetes 一共15个informer SharedInformerFactory的声明结构: type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh Events的声明结构: // Interface provides access to all the informers in this group version. type Interface interface { // Events returns a EventInformer. Events() EventInformer } EventInformer的声明结构: // EventInformer provides access to a shared informer and lister for // Events. type EventInformer interface { Informer() cache.SharedIndexInformer Lister() v1beta1.EventLister } 这样如果我们想使用EventInformer,那么我们就直接在SharedInformerFactory中获取我们需要的Informer即可。 只需要执行下面的两行代码: sharedInformers := informers.NewSharedInformerFactory(clientset, viper.GetDuration(\"resync-interval\")) eventsInformer := sharedInformers.Core().V1().Events() 4.2 Register Informer 已经获取了我们想要使用的EventInformer，接下来就需要将该Informer注册到factory(SharedInformerFactory), 其实在调用eventsInformer.Informer()时，就已经做了Informer注册的工作,之后通过informerFactory.Start() 将所有注册到factory的Informer都启动。 下面是注册EventInformer的一个实现逻辑: func NewFilteredEventInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer { return cache.NewSharedIndexInformer( &cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { if tweakListOptions != nil { tweakListOptions(&options) } return client.CoreV1().Events(namespace).List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { if tweakListOptions != nil { tweakListOptions(&options) } return client.CoreV1().Events(namespace).Watch(options) }, }, &corev1.Event{}, resyncPeriod, indexers, ) } func (f *eventInformer) defaultInformer(client kubernetes.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer { return NewFilteredEventInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions) } func (f *eventInformer) Informer() cache.SharedIndexInformer { return f.factory.InformerFor(&corev1.Event{}, f.defaultInformer) } sharedInformerFactory的声明结构, 在这里我们主要关注informers和startedInformers，其中informers主要的是存储， 向该factory已经注册的Informer,而startedInformers主要记录哪些Informer已经启动了。 type sharedInformerFactory struct { client kubernetes.Interface namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc lock sync.Mutex defaultResync time.Duration customResync map[reflect.Type]time.Duration informers map[reflect.Type]cache.SharedIndexInformer // startedInformers is used for tracking which informers have been started. // This allows Start() to be called multiple times safely. startedInformers map[reflect.Type]bool } 下面这段代码的逻辑是判断informer是否已经向factory注册完成，如果没有则进行注册操作。 func (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer { f.lock.Lock() defer f.lock.Unlock() informerType := reflect.TypeOf(obj) informer, exists := f.informers[informerType] if exists { return informer } resyncPeriod, exists := f.customResync[informerType] if !exists { resyncPeriod = f.defaultResync } informer = newFunc(f.client, resyncPeriod) f.informers[informerType] = informer return informer } 4.3 Informer Run 最终informerFactory将注册到工厂的所有informer都启动,Informer主要的工作就是监听事件，并分发事件。 // Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh 而WaitForCacheSync的作用主要是确认是否所有的Informer的都已经从kubernetes接收过事件，如果已经接收到事件，那么HasSynced会被设置为true // WaitForCacheSync waits for all started informers' cache were synced. func (f *sharedInformerFactory) WaitForCacheSync(stopCh 4.4 sharedIndexInformer informer作为异步事件处理框架，完成了事件监听和分发处理两个过程，sharedIndexInformer的声明结构，该结构就是代表了一个Informer实例。 type sharedIndexInformer struct { indexer Indexer controller Controller processor *sharedProcessor cacheMutationDetector CacheMutationDetector // This block is tracked to handle late initialization of the controller listerWatcher ListerWatcher objectType runtime.Object // resyncCheckPeriod is how often we want the reflector's resync timer to fire so it can call // shouldResync to check if any of our listeners need a resync. resyncCheckPeriod time.Duration // defaultEventHandlerResyncPeriod is the default resync period for any handlers added via // AddEventHandler (i.e. they don't specify one and just want to use the shared informer's default // value). defaultEventHandlerResyncPeriod time.Duration // clock allows for testability clock clock.Clock started, stopped bool startedLock sync.Mutex // blockDeltas gives a way to stop all event distribution so that a late event handler // can safely join the shared informer. blockDeltas sync.Mutex } 首先来看indexer成员，该成员indexer是一个保存全量数据的缓存Store。 Informer对外提供的Lister就是直接从Store获取的数据，而没有直接操作etcd。 4.5 indexer的声明结构: // Indexer is a storage interface that lets you list objects using multiple indexing functions type Indexer interface { Store math Index(indexName string, obj interface{}) ([]interface{}, error) math IndexKeys(indexName, indexKey string) ([]string, error) // ListIndexFuncValues returns the list of generated values of an Index func ListIndexFuncValues(indexName string) []string math ByIndex(indexName, indexKey string) ([]interface{}, error) // GetIndexer return the indexers GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error } 下面这个代码片段是启动一个Informer实例，需要做的处理逻辑。接下来，让我们分析下这部分处理逻辑的各个细节部分。 func (s *sharedIndexInformer) Run(stopCh 4.6 DeltaFIFO 首先创建一个DeltaFIFO实例对象，该实例对象的声明结构: type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. lock sync.RWMutex cond sync.Cond // We depend on the property that items in the set are in // the queue and vice versa, and that all Deltas in this // map have at least one Delta. items map[string]Deltas queue []string // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item // insertion and retrieval, and should be deterministic. keyFunc KeyFunc // knownObjects list keys that are \"known\", for the // purpose of figuring out which items have been deleted // when Replace() or Delete() is called. knownObjects KeyListerGetter // Indication the queue is closed. // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRED operations. closed bool closedLock sync.Mutex } 在Informer中,DeltaFIFO作为Reflector的Store,根据List/Watch的结果对Store进行ADD,UPDATE,DELETE操作。在Delta的声明结构中， 最重要的就是两个成员iterms和queue。iterms成员缓存了所有添加到Store中的事件，而queue则存储这些事件的id作为FIFO处理的先后顺序。 而populated和initialPopulationCount两个成员主要当Store被首次初始化完成之后，会被设置为true。 iterms的声明结构: const ( Added DeltaType = \"Added\" Updated DeltaType = \"Updated\" Deleted DeltaType = \"Deleted\" // The other types are obvious. You'll get Sync deltas when: // * A watch expires/errors out and a new list/watch cycle is started. // * You've turned on periodic syncs. // (Anything that trigger's DeltaFIFO's Replace() method.) Sync DeltaType = \"Sync\" ) // Delta is the type stored by a DeltaFIFO. It tells you what change // happened, and the object's state after* that change. // // [*] Unless the change is a deletion, and then you'll get the final // state of the object before it was deleted. type Delta struct { Type DeltaType Object interface{} } // Deltas is a list of one or more 'Delta's to an individual object. // The oldest delta is at index 0, the newest delta is the last one. type Deltas []Delta 4.7 Controller 当初始化DeltaFIFO实例之后，就对controller的Config进行初始化操作,Config的声明结构: // Config contains all the settings for a Controller. type Config struct { // The queue for your objects - has to be a DeltaFIFO due to // assumptions in the implementation. Your Process() function // should accept the output of this Queue's Pop() method. Queue // Something that can list and watch your objects. ListerWatcher // Something that can process your objects. Process ProcessFunc // The type of your objects. ObjectType runtime.Object // Reprocess everything at least this often. // Note that if it takes longer for you to clear the queue than this // period, you will end up processing items in the order determined // by FIFO.Replace(). Currently, this is random. If this is a // problem, we can change that replacement policy to append new // things to the end of the queue instead of replacing the entire // queue. FullResyncPeriod time.Duration // ShouldResync, if specified, is invoked when the controller's reflector determines the next // periodic sync should occur. If this returns true, it means the reflector should proceed with // the resync. ShouldResync ShouldResyncFunc // If true, when Process() returns an error, re-enqueue the object. // TODO: add interface to let you inject a delay/backoff or drop // the object completely if desired. Pass the object in // question to this interface as a parameter. RetryOnError bool } 对Config中的主要成员进行介绍下，方便之后介绍controller的时候方便了解。 Queue: DeltaFIFO事件处理队列，之后Queue的POP方法会从该队列中不断的POP数据给Process()方法去处理。 ListerWatcher: 用于List/Watch关心的kubernetes资源对象。 Process: 就是处理从DeltaFIFO中POP出来的数据，这个具体的实现后续会介绍到。 最终对Config初始化完成之后，赋值给sharedIndexInformer的controller成员。 Controller的Run主要是一个生产者消费者模式，reflector是生产者，为controller中的Process方法Process: s.HandleDeltas是消费者。而processLoop会循环的从Queue(DeltaFIFO)中POP事件数据给s.HandleDeltas去处理。 Controller Run的代码片段如下: // Run begins processing items, and will continue until a value is sent down stopCh. // It's an error to call Run more than once. // Run blocks; call via go. func (c *controller) Run(stopCh 在上面的这段代码片段中，首先会先启动reflector来List/Watch我们所关心的资源，并将其添加到Store(DeltaFIFO)中。具体的实现逻辑如下: // Reflector watches a specified resource and causes all changes to be reflected in the given store. type Reflector struct { // name identifies this reflector. By default it will be a file:line if possible. name string // metrics tracks basic metric information about the reflector metrics *reflectorMetrics // The type of object we expect to place in the store. expectedType reflect.Type // The destination to sync up with the watch source store Store // listerWatcher is used to perform lists and watches. listerWatcher ListerWatcher // period controls timing between one watch ending and // the beginning of the next one. period time.Duration resyncPeriod time.Duration ShouldResync func() bool // clock allows tests to manipulate time clock clock.Clock // lastSyncResourceVersion is the resource version token last // observed when doing a sync with the underlying store // it is thread safe, but not synchronized with the underlying store lastSyncResourceVersion string // lastSyncResourceVersionMutex guards read/write access to lastSyncResourceVersion lastSyncResourceVersionMutex sync.RWMutex } 对Reflector对象的初始化逻辑如下: func NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(naming.GetNameFromCallsite(internalPackages...), lw, expectedType, store, resyncPeriod) } // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { reflectorSuffix := atomic.AddInt64(&reflectorDisambiguator, 1) r := &Reflector{ name: name, // we need this to be unique per process (some names are still the same) but obvious who it belongs to metrics: newReflectorMetrics(makeValidPrometheusMetricLabel(fmt.Sprintf(\"reflector_\"+name+\"_%d\", reflectorSuffix))), listerWatcher: lw, store: store, expectedType: reflect.TypeOf(expectedType), period: time.Second, resyncPeriod: resyncPeriod, clock: &clock.RealClock{}, } return r } 当Reflector初始化完成之后，则启动Reflector来让它去帮助你去List/Watch工作。具体的启动实现逻辑如下: // Run starts a watch and handles watch events. Will restart the watch if it is closed. // Run will exit when stopCh is closed. func (r *Reflector) Run(stopCh 上面这段代码的核心就是通过r.ListAndWatch方法去List/Watch。由于r.ListAndWatch实现逻辑太长，就简单的说下它的实现逻辑,这个方法主要做两件事儿: list所有关心的资源对象，并将对象存储到Store中。 watch所关心的资源对象, 并判断对象是否已经存在Store,如果存在则UPDATE,否则添加，或者删除。 ok,这样现在通过Reflector这个生产者，我们就把我们所关心的资源对象添加到Store(DeltaFIFO)中了。接下来通过我们的Process方法来从Queue中POP出事件数据，进行消费处理。 processLoop的处理逻辑代码片段如下: func (c *controller) processLoop() { for { obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == FIFOClosedError { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } process会不断的从Queue中POP事件数据给c.config.Process消费。如果在消费的过程中出现错误的情况，则还会重新的把数据重新加回到queue队列中。 queue.POP方法的实现逻辑如下: func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { for len(f.queue) == 0 { // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.IsClosed() { return nil, FIFOClosedError } f.cond.Wait() } id := f.queue[0] f.queue = f.queue[1:] if f.initialPopulationCount > 0 { f.initialPopulationCount-- } item, ok := f.items[id] if !ok { // Item may have been deleted subsequently. continue } delete(f.items, id) err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } } c.config.Process(就是HandleDeltas)方法的处理逻辑如下: func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil && exists { if err := s.indexer.Update(d.Object); err != nil { return err } s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 每当从Queue队列中POP出新的事件数据时，都会被上面的这个方法处理，首先会根据元素的处理类型,来决定具体的处理逻辑: 下面是每个事件元素能够被处理的时间类型: const ( Added DeltaType = \"Added\" Updated DeltaType = \"Updated\" Deleted DeltaType = \"Deleted\" // The other types are obvious. You'll get Sync deltas when: // * A watch expires/errors out and a new list/watch cycle is started. // * You've turned on periodic syncs. // (Anything that trigger's DeltaFIFO's Replace() method.) Sync DeltaType = \"Sync\" ) 如果是Sync,ADD,Updated,DELETE则相应的处理逻辑如下: 1.如果是ADD类型，则直接将新添加的元素ADD到Store中，之后进行事件的分发distribute操作。 2.如果是UPDATED类型，则从Store中获取该元素，并更新Store中的元素，之后进行事件的分发distribute操作。 3.如果是DELETE类型，则直接从Store中删除元素，之后也是进行事件的分发distribute操作。 sharedProcessor 接下来我们在对sharedProcessor的事件分发处理进行详细的介绍。在消费事件时，通过informer的processer进行distrubute。processer进行分发的处理函数由外部的AddEventHandler向processer里addListener。其中addListener只是添加一个processer管理listeners,并在分发时遍历listeners，将事件发送给所有的listener。 sharedProcessor的声明结构如下: type sharedProcessor struct { listenersStarted bool listenersLock sync.RWMutex listeners []*processorListener syncingListeners []*processorListener clock clock.Clock wg wait.Group } func (p *sharedProcessor) addListener(listener *processorListener) { p.listenersLock.Lock() defer p.listenersLock.Unlock() p.addListenerLocked(listener) if p.listenersStarted { p.wg.Start(listener.run) p.wg.Start(listener.pop) } } func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync { for _, listener := range p.syncingListeners { listener.add(obj) } } else { for _, listener := range p.listeners { listener.add(obj) } } } processor的run保证所有listener都开始运行，并保证退出时所有listener的chan都关闭。 func (p *sharedProcessor) run(stopCh processorListener type processorListener struct { nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler // pendingNotifications is an unbounded ring buffer that holds all notifications not yet distributed. // There is one per listener, but a failing/stalled listener will have infinite pendingNotifications // added until we OOM. // TODO: This is no worse than before, since reflectors were backed by unbounded DeltaFIFOs, but // we should try to do something better. pendingNotifications buffer.RingGrowing // requestedResyncPeriod is how frequently the listener wants a full resync from the shared informer requestedResyncPeriod time.Duration // resyncPeriod is how frequently the listener wants a full resync from the shared informer. This // value may differ from requestedResyncPeriod if the shared informer adjusts it to align with the // informer's overall resync check period. resyncPeriod time.Duration // nextResync is the earliest time the listener should get a full resync nextResync time.Time // resyncLock guards access to resyncPeriod and nextResync resyncLock sync.Mutex } pendingNotifications装了所有还没分发的事件。而handler则是开发者向Informer注册的ResourceEventHandler type processorListener struct { nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler // pendingNotifications is an unbounded ring buffer that holds all notifications not yet distributed. // There is one per listener, but a failing/stalled listener will have infinite pendingNotifications // added until we OOM. // TODO: This is no worse than before, since reflectors were backed by unbounded DeltaFIFOs, but // we should try to do something better. pendingNotifications buffer.RingGrowing // requestedResyncPeriod is how frequently the listener wants a full resync from the shared informer requestedResyncPeriod time.Duration // resyncPeriod is how frequently the listener wants a full resync from the shared informer. This // value may differ from requestedResyncPeriod if the shared informer adjusts it to align with the // informer's overall resync check period. resyncPeriod time.Duration // nextResync is the earliest time the listener should get a full resync nextResync time.Time // resyncLock guards access to resyncPeriod and nextResync resyncLock sync.Mutex } pendingNotifications装了所有还没分发的事件。而handler则是开发者向Informer注册的ResourceEventHandler。 type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } 而ResourceEventHandler这个接口被ResourceEventHandlerFuncs结构已经实现了，开发者去实现下面的AddFunc, UpdateFunc, DeleteFunc并注册到Informer即可。 type ResourceEventHandlerFuncs struct { AddFunc func(obj interface{}) UpdateFunc func(oldObj, newObj interface{}) DeleteFunc func(obj interface{}) } 接下来继续，其中处理事件processor开始distribute时，会调用listener的add方法，将事件发到addCh上 func (p *processorListener) add(notification interface{}) { p.addCh listener的pop goroutine不断地从addCh中获取事件，写到本地的pendingNotification或写给nextCh，而nextCh从本地pendingNotification或addCh获取事件。最后由run方法消费事件和分发事件。run方法支持指数重试，退出也会重新开始。 func (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) // Tell .run() to stop var nextCh chan 这样就对Store中的事件数据进行了分发，如果开发者想要实现自己的custom controller的话，可以在分发之后， 将分发后的数据写入到client-go提供的Workqueue队列中，并在自己实现的syncHandler实现逻辑中不断的中Workqueue中去获取key,然后去实现自己的逻辑。 Link https://zhuanlan.zhihu.com/p/59660536 原文 Kubernetes Client-Go Informer 实现源码剖析 自己构建一个 k8s sample-controller. Kubernetes Deep Dive: Code Generation for CustomResources 深入浅出kubernetes之client-go的SharedInformerFactory kubernetes.io/crd Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/copy docker config.html":{"url":"blog/kubernetes/copy docker config.html","title":"Copy Docker Config","keywords":"","body":"K8s：all nodes scp docker config source doc Here are the recommended steps to configuring your nodes to use a private registry. In this example, run these on your desktop/laptop: Run docker login [server] for each set of credentials you want to use. This updates $HOME/.docker/config.json. docker login {your private registry} View $HOME/.docker/config.json in an editor to ensure it contains just the credentials you want to use. Get a list of your nodes, for example: if you want the names: nodes=$(kubectl get nodes -o jsonpath='{range.items[*].metadata}{.name} {end}') if you want to get the IPs: nodes=$(kubectl get nodes -o jsonpath='{range .items[*].status.addresses[?(@.type==\"ExternalIP\")]}{.address} {end}') Copy your local .docker/config.json to one of the search paths list above. for example: for n in $nodes; do scp ~/.docker/config.json root@$n:/var/lib/kubelet/config.json; done Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/cue/":{"url":"blog/kubernetes/cue/","title":"Cue","keywords":"","body":"cue The CUE Data Constraint Language Configure, Unify, Execute CUE is an open source data constraint language which aims to simplify tasks involving defining and using data. It is a superset of JSON, allowing users familiar with JSON to get started quickly. What is it for? You can use CUE to define a detailed validation schema for your data (manually or automatically from data) reduce boilerplate in your data (manually or automatically from schema) extract a schema from code generate type definitions and validation code merge JSON in a principled way define and run declarative scripts How? CUE merges the notion of schema and data. The same CUE definition can simultaneously be used for validating data and act as a template to reduce boilerplate. Schema definition is enriched with fine-grained value definitions and default values. At the same time, data can be simplified by removing values implied by such detailed definitions. The merging of these two concepts enables many tasks to be handled in a principled way. Constraints provide a simple and well-defined, yet powerful, alternative to inheritance, a common source of complexity with configuration languages. CUE Scripting The CUE scripting layer defines declarative scripting, expressed in CUE, on top of data. This solves three problems: working around the closedness of CUE definitions (we say CUE is hermetic), providing an easy way to share common scripts and workflows for using data, and giving CUE the knowledge of how data is used to optimize validation. There are many tools that interpret data or use a specialized language for a specific domain (Kustomize, Ksonnet). This solves dealing with data on one level, but the problem it solves may repeat itself at a higher level when integrating other systems in a workflow. CUE scripting is generic and allows users to define any workflow. Tooling CUE is designed for automation. Some aspects of this are: convert existing YAML and JSON automatically simplify configurations rich APIs designed for automated tooling formatter arbitrary-precision arithmetic generate CUE templates from source code generate source code from CUE definitions (TODO) Download and Install Release builds Download the latest release from GitHub. Install using Homebrew Using Homebrew, you can install using the CUE Homebrew tap: brew install cuelang/tap/cue Install from Source If you already have Go installed, the short version is: GO111MODULE=on go get cuelang.org/go/cmd/cue Or, if you are using Go 1.16: go install cuelang.org/go/cmd/cue@latest This will install the cue command line tool. For more details see Installing CUE. Learning CUE The fastest way to learn the basics is to follow the tutorial on basic language constructs. A more elaborate tutorial demonstrating of how to convert and restructure an existing set of Kubernetes configurations is available in written form. References Language Specification: official CUE Language specification. API: the API on pkg.go.dev Builtin packages: builtins available from CUE programs cue Command line reference: the cue command Contributing Our canonical Git repository is located at https://cue.googlesource.com. To contribute, please read the Contribution Guide. To report issues or make a feature request, use the issue tracker. Changes can be contributed using Gerrit or Github pull requests. Contact You can get in touch with the cuelang community in the following ways: Ask questions via GitHub Discussions Chat with us on our Slack workspace. Unless otherwise noted, the CUE source files are distributed under the Apache 2.0 license found in the LICENSE file. This is not an officially supported Google product. link github Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/cue/readme.html":{"url":"blog/kubernetes/cue/readme.html","title":"Readme","keywords":"","body":"cue The CUE Data Constraint Language Configure, Unify, Execute CUE is an open source data constraint language which aims to simplify tasks involving defining and using data. It is a superset of JSON, allowing users familiar with JSON to get started quickly. What is it for? You can use CUE to define a detailed validation schema for your data (manually or automatically from data) reduce boilerplate in your data (manually or automatically from schema) extract a schema from code generate type definitions and validation code merge JSON in a principled way define and run declarative scripts How? CUE merges the notion of schema and data. The same CUE definition can simultaneously be used for validating data and act as a template to reduce boilerplate. Schema definition is enriched with fine-grained value definitions and default values. At the same time, data can be simplified by removing values implied by such detailed definitions. The merging of these two concepts enables many tasks to be handled in a principled way. Constraints provide a simple and well-defined, yet powerful, alternative to inheritance, a common source of complexity with configuration languages. CUE Scripting The CUE scripting layer defines declarative scripting, expressed in CUE, on top of data. This solves three problems: working around the closedness of CUE definitions (we say CUE is hermetic), providing an easy way to share common scripts and workflows for using data, and giving CUE the knowledge of how data is used to optimize validation. There are many tools that interpret data or use a specialized language for a specific domain (Kustomize, Ksonnet). This solves dealing with data on one level, but the problem it solves may repeat itself at a higher level when integrating other systems in a workflow. CUE scripting is generic and allows users to define any workflow. Tooling CUE is designed for automation. Some aspects of this are: convert existing YAML and JSON automatically simplify configurations rich APIs designed for automated tooling formatter arbitrary-precision arithmetic generate CUE templates from source code generate source code from CUE definitions (TODO) Download and Install Release builds Download the latest release from GitHub. Install using Homebrew Using Homebrew, you can install using the CUE Homebrew tap: brew install cuelang/tap/cue Install from Source If you already have Go installed, the short version is: GO111MODULE=on go get cuelang.org/go/cmd/cue Or, if you are using Go 1.16: go install cuelang.org/go/cmd/cue@latest This will install the cue command line tool. For more details see Installing CUE. Learning CUE The fastest way to learn the basics is to follow the tutorial on basic language constructs. A more elaborate tutorial demonstrating of how to convert and restructure an existing set of Kubernetes configurations is available in written form. References Language Specification: official CUE Language specification. API: the API on pkg.go.dev Builtin packages: builtins available from CUE programs cue Command line reference: the cue command Contributing Our canonical Git repository is located at https://cue.googlesource.com. To contribute, please read the Contribution Guide. To report issues or make a feature request, use the issue tracker. Changes can be contributed using Gerrit or Github pull requests. Contact You can get in touch with the cuelang community in the following ways: Ask questions via GitHub Discussions Chat with us on our Slack workspace. Unless otherwise noted, the CUE source files are distributed under the Apache 2.0 license found in the LICENSE file. This is not an officially supported Google product. link github Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/cue/小白入门.html":{"url":"blog/kubernetes/cue/小白入门.html","title":"小白入门","keywords":"","body":"CUE 小白入门 本片文章主要介绍 CUE 的基本概念，CUE 是什么以及可以做些什么，通过一些 demo 演示了基本的用法，适合小白入门阅读。 本文首发于 https://russellgao.cn/cue-intro/ ，转载请保留出处。 什么是CUE C(Configure：配置) , U(Unify：统一) , E(Execute：执行) 。 CUE是一种开源的数据约束语言，旨在简化涉及定义和使用数据的任务。 它是JSON的超集，允许熟悉JSON的用户快速上手。 换言之，他和JSON、YAML 等类似，但是比他们的功能强大，可以和 JSON、YAML 等工具对比着来理解。 CUE 可以用来做什么 我们可以用CUE ： 定义一个详细的验证模式 减少数据中的模版 从代码中提取模式 产生类型定义和验证代码 以一种有原则的方式合并JSON 定义和运行声明性脚本 How CUE合并了模式和数据的概念。同一个CUE定义可以同时用于验证数据和作为模板来减少模板。模式定义通过细粒度的值定义和缺省值得到了丰富。同时，通过删除这些详细定义所隐含的值，可以简化数据。这两个概念的合并使得许多任务能够以一种原则性的方式被处理。 约束提供了一个简单的、定义明确的、但功能强大的、替代继承的方法，而继承是配置语言中常见的复杂性来源。 CUE脚本 CUE脚本层定义了声明性的脚本，用CUE表达，在数据之上。这解决了三个问题：解决了CUE定义的封闭性（我们说CUE是密封的），提供了一个简单的方法来分享使用数据的通用脚本和工作流程，并让CUE知道如何使用数据来优化验证。 有很多工具可以解释数据或者为特定领域使用专门的语言（Kustomize, Ksonnet）。这在一个层面上解决了处理数据的问题，但它解决的问题在工作流程中整合其他系统时可能会在更高的层面上重复出现。CUE脚本是通用的，允许用户定义任何工作流程。 安装 如果是 Mac 环境 ，执行下面的命令安装: brew install cuelang/tap/cue 其他环境通过 golang 安装（当然 mac 也是可以的） # go 1.16 之前 GO111MODULE=on go get cuelang.org/go/cmd/cue # go 1.16 之后 go install cuelang.org/go/cmd/cue@latest 详细可参考 安装文档 安装完之后可以通过命令行执行 cue --help 查看基本的帮助文档。 cue --help cue evaluates CUE files, an extension of JSON, and sends them to user-defined commands for processing. Commands are defined in CUE as follows: import \"tool/exec\" command: deploy: { exec.Run cmd: \"kubectl\" args: [ \"-f\", \"deploy\" ] in: json.Encode(userValue) // encode the emitted configuration. } cue can also combine the results of http or grpc request with the input configuration for further processing. For more information on defining commands run 'cue help cmd' or go to cuelang.org/pkg/cmd. For more information on writing CUE configuration files see cuelang.org. Usage: cue [command] Available Commands: cmd run a user-defined shell command completion Generate completion script def print consolidated definitions eval evaluate and print a configuration export output data in a standard format fix rewrite packages to latest standards fmt formats CUE configuration files get add dependencies to the current module help Help about any command import convert other formats to CUE files mod module maintenance trim remove superfluous fields version print CUE version vet validate data Flags: -E, --all-errors print all available errors -h, --help help for cue -i, --ignore proceed in the presence of errors -s, --simplify simplify output --strict report errors for lossy mappings --trace trace computation -v, --verbose print information about progress Additional help topics: cue commands user-defined commands cue filetypes supported file types and qualifiers cue flags common flags for composing packages cue injection inject files or values into specific fields for a build cue inputs package list, patterns, and files Use \"cue [command] --help\" for more information about a command. 下面通过一些具体的例子看看如何使用 CUE 。 JSON 超集 CUE是JSON的一个超集。它增加了以下的便利性。 C风格的注释。 引号可以从字段名中省略，没有特殊字符。 字段末尾的逗号是可选的。 列表中最后一个元素后的逗号是允许的。 外层大括号是可选的。 JSON对象在CUE中被称为结构。一个对象的成员被称为一个字段。 假设我们有 json.cue 文件如下 : one: 1 two: 2 // A field using quotes. \"two-and-a-half\": 2.5 list: [ 1, 2, 3, ] m: { key1: \"v1\" key2: \"v2\" } 可以通过 cue export json.cue看看生成之后的 json 数据 : { \"one\": 1, \"two\": 2, \"two-and-a-half\": 2.5, \"list\": [ 1, 2, 3 ], \"m\": { \"key1\": \"v1\", \"key2\": \"v2\" } } cue export 可以把 cue 文件转化成 json 、 yaml 、 text 等类型的文件 转化成 json cue export json.cue --out json 转化成 yaml cue export json.cue --out yaml 转化成 text cue export json.cue --out text 输出到文件 cue export json.cue --out json --outfile json.cue.json 可以通过 cue help export 查看详细的帮助文档 类型是值 CUE合并了值和类型的概念。下面是这个demo的演示，分别展示了一些数据，这个数据的可能模式，以及介于两者之间的东西：一个典型的CUE约束条件。 Data moscow: { name: \"Moscow\" pop: 11.92M capital: true } Schema municipality: { name: string pop: int capital: bool } CUE： largeCapital: { name: string pop: >5M capital: true } 一般来说，在CUE中，人们从一个广义的模式定义开始，描述所有可能的实例，然后针对特定的用例缩小这些定义的范围，直到剩下一个具体的数据实例。 重复的字段 CUE允许重复的字段定义，只要它们不冲突。 对于基本类型的值，这意味着它们必须是相等的。 对于结构，字段被合并，重复的字段被递归处理。 对于列表，所有的元素必须相应地匹配 假设的 dup.cue 内容如下 : a: 4 a: 4 s: { b: 2 } s: { c: 2 } l: [ 1, 2 ] l: [ 1, 2 ] 通过 cue eval dup.cue 可以得到合并后的内容 ： a: 4 s: { b: 2 c: 2 } l: [1, 2] 也可以通过 export 命令生成 json/yaml 等文件看一下 如果key一样，但是value 不一样，会有什么样的情况呢 ，修改 dup.cue 的文件内容如下 : a: 4 a: 5 s: { b: 2 } s: { c: 2 } l: [ 1, 2 ] l: [ 1, 3 ] 这时再次执行 cue eval dup.cue，则会报错 : a: conflicting values 5 and 4: ./dup.cue:1:4 ./dup.cue:2:4 l.1: conflicting values 3 and 2: ./dup.cue:7:9 ./dup.cue:8:9 限制条件 约束规定了哪些值是允许的。对CUE来说，它们只是像其他东西一样的值，但在概念上，它们可以被解释为介于类型和具体值之间的东西。 但是约束也可以减少模板。如果一个约束定义了一个具体的值，那么就没有必要在这个约束所适用的值中指定它。 假设 check.cue 文件如下 : schema: { name: string age: int human: true // always true } viola: schema viola: { name: \"Viola\" age: 38 } 这个文件的含义如下 : schema 定义了约束， human 只能为 true ，后续在赋值时不能修改为其他的值，否则会报错 viola: schema 表示继承了 schema 的定义，引用了 schema 的约束 viola: { ... 是真正的赋值 执行 cue eval check.cue 可以看到渲染之后的数据 schema: { name: string age: int human: true } viola: { name: \"Viola\" age: 38 human: true } 这时把 check.cue 修改成如下这样会有什么样的输入呢 ? schema: { name: string age: int human: true // always true } viola: schema viola: { name: \"Viola\" age: 38 human: false } 执行 cue eval check.cue viola.human: conflicting values false and true: ./check.cue:4:12 ./check.cue:6:8 ./check.cue:10:12 可以看到给 human 赋值为 false 报错了 Definitions 在CUE中，模式通常被写成 定义 。定义 是一个字段，其标识符以#或_#开头。这告诉CUE它们是用来验证的，不应该作为数据输出；它们可以不被指定。 一个 定义 也告诉CUE允许的全部字段。换句话说，定义 定义了 \"封闭 \"的结构。在结构中包括一个...可以使其保持开放。 假设 schema.cue 文件内容如下 : #Conn: { address: string port: int protocol: string // ... // uncomment this to allow any field } lossy: #Conn & { address: \"1.2.3.4\" port: 8888 protocol: \"udp\" // foo: 2 // uncomment this to get an error } #Conn 申明了一个定义 lossy 引用了这个定义并且实例化数据，在实例化时会按照 schema 进行校验，在如下情况都会报错 多了字段 少了字段 字段类型不匹配 执行 cue export schema.cue查看 ： { \"lossy\": { \"address\": \"1.2.3.4\", \"port\": 8888, \"protocol\": \"udp\" } } 验证 约束条件可以用来验证具体实例的值。它们可以应用于CUE数据，或直接应用于YAML或JSON。 假设 schema.cue 内容如下 ： #Language: { tag: string name: =~\"^\\\\p{Lu}\" // Must start with an uppercase letter. } languages: [...#Language] languages: [...#Language] 可变长的一个列表 ，可以对照 languages: [#Language] 的 eval 输出看一下区别 // languages: [...#Language] 的输出 #Language: { tag: string name: =~\"^\\\\p{Lu}\" } languages: [] // languages: [#Language] 的输出 #Language: { tag: string name: =~\"^\\\\p{Lu}\" } languages: [{ tag: string name: =~\"^\\\\p{Lu}\" }] data.yaml 的内容如下 : languages: - tag: en name: English - tag: nl name: dutch - tag: no name: Norwegian 执行 cue vet schema.cue data.yaml 可以看到会报错 languages.1.name: invalid value \"dutch\" (does not match =~\"^\\\\p{Lu}\"): ./schema.cue:3:8 ./data.yaml:5:12 dutch 验证失败，必须以大写字母开头 顺序不重要 CUE的基本操作是这样定义的：你结合两个配置的顺序与结果无关。 这是CUE的关键属性，它使人类和机器很容易对数值进行推理，并使高级工具和自动化成为可能。 假设 order.cue 内容如下 : a: {x: 1, y: int} a: {x: int, y: 2} b: {x: int, y: 2} b: {x: 1, y: int} 执行 cue eval order.cue得到 ： a: { x: 1 y: 2 } b: { x: 1 y: 2 } 可以看到，定义和赋值谁先谁后都没有影响 单项结构的折叠 在JSON中，人们定义了嵌套值，一次一个值。另一种方式是，JSON配置是一组路径-值对。 在CUE中，人们定义了一组路径，并一次性地应用具体的值或约束条件。由于CUE的顺序独立性，数值会被合并。 这个例子显示了一些路径值对，以及一个应用于这些路径值的约束来验证它们。 fold.cue 内容如下 : // path-value pairs outer: middle1: inner: 3 outer: middle2: inner: 7 // collection-constraint pair outer: [string]: inner: int 执行 cue export fold.cue { \"outer\": { \"middle1\": { \"inner\": 3 }, \"middle2\": { \"inner\": 7 } } } 总结 这里简单的介绍了什么是CUE以及基本用法，可以和 JSON、YAML 等工具进行对比理解。后面的文档会介绍 kubernetes 是如何使用 CUE 工具的。 参考 https://cuelang.org/docs/tutorials/tour/intro/ https://github.com/cuelang/cue yamlgo © 著作权归作者所有 link https://my.oschina.net/russellgao/blog/5045545 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:22:34 "},"blog/kubernetes/dashboard/admin-token.html":{"url":"blog/kubernetes/dashboard/admin-token.html","title":"Admin Token","keywords":"","body":"使用 kubeconfig 或 token 进行用户身份认证 在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。 下文分两块以示例的方式来讲解两种登陆认证方式： 为 brand 命名空间下的 brand 用户创建 kubeconfig 文件 为集群的管理员（拥有所有命名空间的 amdin 权限）创建 token 使用 kubeconfig 如何生成kubeconfig文件请参考创建用户认证授权的kubeconfig文件。 注意我们生成的 kubeconfig 文件中没有 token 字段，需要手动添加该字段。 比如我们为 brand namespace 下的 brand 用户生成了名为 brand.kubeconfig 的 kubeconfig 文件，还要再该文件中追加一行 token 的配置（如何生成 token 将在下文介绍），如下所示： 图 5.4.6.1：kubeconfig文件 对于访问 dashboard 时候的使用 kubeconfig 文件如brand.kubeconfig 必须追到 token 字段，否则认证不会通过。而使用 kubectl 命令时的用的 kubeconfig 文件则不需要包含 token 字段。 生成 token 需要创建一个admin用户并授予admin角色绑定，使用下面的yaml文件创建admin用户并赋予他管理员权限，然后可以通过token访问kubernetes，该文件见admin-role.yaml。 生成kubernetes集群最高权限admin用户的token kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile 然后执行下面的命令创建 serviceaccount 和角色绑定， kubectl create -f admin-role.yaml 创建完成后获取secret中token的值。 # 获取admin-token的secret名字 $ kubectl -n kube-system get secret|grep admin-token admin-token-nwphb kubernetes.io/service-account-token 3 6m # 获取token的值 $ kubectl -n kube-system describe secret admin-token-nwphb Name: admin-token-nwphb Namespace: kube-system Labels: Annotations: kubernetes.io/service-account.name=admin kubernetes.io/service-account.uid=f37bd044-bfb3-11e7-87c0-f4e9d49f8ed0 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: 非常长的字符串 ca.crt: 1310 bytes 也可以使用 jsonpath 的方式直接获取 token 的值，如： kubectl -n kube-system get secret admin-token-nwphb -o jsonpath={.data.token}|base64 -d 注意：yaml 输出里的那个 token 值是进行 base64 编码后的结果，一定要将 kubectl 的输出中的 token 值进行 base64 解码，在线解码工具 base64decode，Linux 和 Mac 有自带的 base64 命令也可以直接使用，输入 base64 是进行编码，Linux 中base64 -d 表示解码，Mac 中使用 base64 -D。 我们使用了 base64 对其重新解码，因为 secret 都是经过 base64 编码的，如果直接使用 kubectl 中查看到的 token 值会认证失败，详见 secret 配置。关于 JSONPath 的使用请参考 JSONPath 手册。 更简单的方式是直接使用kubectl describe命令获取token的内容（经过base64解码之后）： kubectl describe secret admin-token-nwphb 为普通用户生成token 为指定namespace分配该namespace的最高权限，这通常是在为某个用户（组织或者个人）划分了namespace之后，需要给该用户创建token登陆kubernetes dashboard或者调用kubernetes API的时候使用。 每次创建了新的namespace下都会生成一个默认的token，名为default-token-xxxx。default就相当于该namespace下的一个用户，可以使用下面的命令给该用户分配该namespace的管理员权限。 kubectl create rolebinding $ROLEBINDING_NAME --clusterrole=admin --serviceaccount=$NAMESPACE:default --namespace=$NAMESPACE $ROLEBINDING_NAME必须是该namespace下的唯一的 admin表示用户该namespace的管理员权限，关于使用clusterrole进行更细粒度的权限控制请参考RBAC——基于角色的访问控制。 我们给默认的serviceaccount default分配admin权限，这样就不要再创建新的serviceaccount，当然你也可以自己创建新的serviceaccount，然后给它admin权限 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/dashboard/terminal.html":{"url":"blog/kubernetes/dashboard/terminal.html","title":"Terminal","keywords":"","body":"dashboard const END_OF_TRANSMISSION = \"\\u0004\" // PtyHandler is what remotecommand expects from a pty type PtyHandler interface { io.Reader io.Writer remotecommand.TerminalSizeQueue } // TerminalSession implements PtyHandler (using a SockJS connection) type TerminalSession struct { id string bound chan error sockJSSession sockjs.Session sizeChan chan remotecommand.TerminalSize doneChan chan struct{} } // TerminalMessage is the messaging protocol between ShellController and TerminalSession. // // OP DIRECTION FIELD(S) USED DESCRIPTION // --------------------------------------------------------------------- // bind fe->be SessionID Id sent back from TerminalResponse // stdin fe->be Data Keystrokes/paste buffer // resize fe->be Rows, Cols New terminal size // stdout be->fe Data Output from the process // toast be->fe Data OOB message to be shown to the user type TerminalMessage struct { Op, Data, SessionID string Rows, Cols uint16 } // SessionMap stores a map of all TerminalSession objects and a lock to avoid concurrent conflict type SessionMap struct { Sessions map[string]TerminalSession Lock sync.RWMutex } // handleTerminalSession is Called by net/http for any new /api/sockjs connections func handleTerminalSession(session sockjs.Session){} // CreateAttachHandler is called from main for /api/sockjs func CreateAttachHandler(path string) http.Handler 2021/10/18 产品例会 CHS 组遗留问题： 思考对中小型开发团队，CNOPS如何简化操作流程？ 预计解决时间 10月25日 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/dashboard/x.html":{"url":"blog/kubernetes/dashboard/x.html","title":"X","keywords":"","body":"学习基于 dashboard 1.8 版本。 webshell 代码逻辑 webshell 调用大体分为两部： 第一步，首先通过 https://*.dashboard.cn/api/pod/{pod name}/shell 接口请求生成 s essionId 第二步，携带第一步返回的 sessionId 建立 wss 连接，执行 shell 命令 下面，进行详细分析各个步骤具体细节。 shell restful api 浏览器请求 shell 接口后，通过 golang restful 框架路由到代码 PodShell:Get 函数入口。路由表是在 endpoints/install.go 中写入的，后面具体学习。 Get 入口函数代码： func (pod *PodShell) Get(ctx context.Context, name string, options metaV1.GetOptions) (runtime.Object, error) { //生成session Id sessionId, err := exec.GenTerminalSeesionId() ...... //拿k8s client 对象 client, dc, CloseClient := utils.GetK8sClient(ctx) //用完归还 defer CloseClient(client, dc) exec.TerminalSessions.Set(sessionId, exec.TerminalSession{ Id: sessionId, Bound: make(chan error), SizeChan: make(chan remotecommand.TerminalSize), }) ....... //新建协程 等待后端 websocket 连接 go exec.WaitForTerminal(client, cfg, ctx, sessionId) ....... } func WaitForTerminal(k8sClient kubernetes.Interface, cfg *rest.Config, ctx context.Context, sessionId string) { ...... // wss 连接建立后，会通过 Bound chan 通知 select { case websocket 接口 先简要说一下websocket 连接建立流程： 1. 首先，需要通过 http1 协议请求，服务端接收到请求，响应 101 状态码给客户端，告知客户端升级协议（通过header头标示升级的协议，Connection: upgrade upgrade: websocket）为websocket; 2. 然后，客户端通过 websocket 协议与服务端进行双向通信； k8s-watchdog 中使用的 web shell 在 websoket 基础上封装了应用层协议： o 是升级协议后，服务端发送该字符告知客户端，服务端已经做好准备接收数据了； [\"Op\":\"命令\"，\"SessionID\":\"xxxx\", Data:\"xxxx\", \"Rows\":\"\", \"Cols\"] Op是操作指令： bind: 客户端告知服务端 sessionId 绑定指令，在客户端收到 o 字符后，立即发送该指令，之后才完全建立双向数据传输； resize: 调整窗口大小的指令； stdout：服务端输出； stdin：客户端输入； h 心跳字符，服务端定时发送给客户端 接口路由注册函数，注册 handleTerminalSession 到 sockjs websocket 回调中： func CreateAttachHandler(path string) http.Handler { return sockjs.NewHandler(path, sockjs.DefaultOptions, handleTerminalSession) } websocket 入口函数代码逻辑： //wss://*.dashboard.cn/ws/api/sockjs/647/y12a2jym/websocket?{sessionId}请求入口函数 func (h *handler) sockjsWebsocket(rw http.ResponseWriter, req *http.Request) { //协议升级响应 conn, err := websocket.Upgrade(rw, req, nil, WebSocketReadBufSize, WebSocketWriteBufSize) ...... //解析 url 中的 sessionId sessID, _ := h.parseSessionID(req.URL) sess := newSession(sessID, h.options.DisconnectDelay, h.options.HeartbeatDelay) if h.handlerFunc != nil { //异步回调 go h.handlerFunc(sess) } receiver := newWsReceiver(conn) //注册接收器，并启动心跳（通过定时器实现），每5秒发送一个 h 字符到客户端 sess.attachReceiver(receiver) readCloseCh := make(chan struct{}) //接收数据，塞到 session 中 go func() { var d []string for { err := conn.ReadJSON(&d) if err != nil { close(readCloseCh) return } sess.accept(d...) } }() select { case websocket 回调函数 func handleTerminalSession(session sockjs.Session) { ...... if buf, err = session.Recv(); err != nil { glog.Warning(\"handleTerminalSession: can't Recv: %v\\n\", err) return } if err = ; err != nil { glog.Warning(\"handleTerminalSession: can't UnMarshal (%v): %s\\n\", err, buf) return } if msg.Op != \"bind\" { glog.Warning(\"handleTerminalSession: expected 'bind' message, got: %s\\n\", buf) return } if terminalSession = TerminalSessions.Get(msg.SessionID); terminalSession.Id == \"\" { glog.Warning(\"handleTerminalSession: can't find session '%s'\", msg.SessionID) return } ...... terminalSession.SockJSSession = session TerminalSessions.Set(msg.SessionID, terminalSession) //通知 shell 接口 WaitForTerminal 协程 terminalSession.Bound WaitForTerminal 函数： func WaitForTerminal(k8sClient kubernetes.Interface, cfg *rest.Config, ctx context.Context, sessionId string) { shell := \"bash\" //check nil chan //because nil channel will block receive, and throw panic when call close function if TerminalSessions.Get(sessionId).Bound == nil { return } select { case Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/etcd-operation.html":{"url":"blog/kubernetes/etcd-operation.html","title":"Etcd Operation","keywords":"","body":"ETCD etcd is a consistent and highly-available key value store used as Kubernetes’ backing store for all cluster data. If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for those data. You can find in-depth information about etcd in the official documentation. env requires etcd version 3.4.3 kuberentes 1.17.0 run etcd in kubernets login you k8s master node: cat /etc/kubernetes/manifests/etcd.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls=https://192.168.1.33:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.1.33:2380 - --initial-cluster=e1192523-1d5d-43fa-b0ed-80cfffed0de0=https://192.168.1.33:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key - --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.33:2379 - --listen-metrics-urls=http://127.0.0.1:2381 - --listen-peer-urls=https://192.168.1.33:2380 - --name=e1192523-1d5d-43fa-b0ed-80cfffed0de0 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt image: registry.aliyuncs.com/google_containers/etcd:3.4.3-0 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /health port: 2381 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15 name: etcd resources: {} volumeMounts: - mountPath: /var/lib/etcd name: etcd-data - mountPath: /etc/kubernetes/pki/etcd name: etcd-certs hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/kubernetes/pki/etcd type: DirectoryOrCreate name: etcd-certs - hostPath: path: /var/lib/etcd type: DirectoryOrCreate name: etcd-data status: {} etcd operation find etcd on kubernetes [root@master-1 ~]# kubectl -n kube-system get pods -l component=etcd NAME READY STATUS RESTARTS AGE etcd-e1192523-1d5d-43fa-b0ed-80cfffed0de0 1/1 Running 0 3h5m etcd-f61b71cb-8a86-4873-8223-eac734ec6964 1/1 Running 0 138m attach etcd container kubectl exec -it etcd-e1192523-1d5d-43fa-b0ed-80cfffed0de0 sh -n kube-system alias export ETCDCTL_API=3 etl='etcdctl --cacert=ca.crt --cert=healthcheck-client.crt --key=healthcheck-client.key' list all keys etl get / --prefix --keys-only get key etl get /registry/services/specs/default/kubernetes OPTIONS: --cacert=\"\" verify certificates of TLS-enabled secure servers using this CA bundle --cert=\"\" identify secure client using this TLS certificate file --command-timeout=5s timeout for short running command (excluding dial timeout) --debug[=false] enable client-side debug logging --dial-timeout=2s dial timeout for client connections -d, --discovery-srv=\"\" domain name to query for SRV records describing cluster endpoints --discovery-srv-name=\"\" service name to query when using DNS discovery --endpoints=[127.0.0.1:2379] gRPC endpoints -h, --help[=false] help for etcdctl --hex[=false] print byte strings as hex encoded strings --insecure-discovery[=true] accept insecure SRV records describing cluster endpoints --insecure-skip-tls-verify[=false] skip server certificate verification --insecure-transport[=true] disable transport security for client connections --keepalive-time=2s keepalive time for client connections --keepalive-timeout=6s keepalive timeout for client connections --key=\"\" identify secure client using this TLS key file --password=\"\" password for authentication (if this option is used, --user option shouldn't include password) --user=\"\" username[:password] for authentication (prompt if password is not supplied) -w, --write-out=\"simple\" set the output format (fields, json, protobuf, simple, table) backing up at etcd cluster build-in snapshot etcd supports built-in snapshot, so backing up an etcd cluster is easy. A snapshot may either be taken from a live member with the etcdctl snapshot save command or by copying the member/snap/db file from an etcd data directory that is not currently used by an etcd process. Taking the snapshot will normally not affect the performance of the member. Below is an example for taking a snapshot of the keyspace served by $ENDPOINT to the file snapshotdb: ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save /var/lib/etcd/snapshot-20200312.db # exit 0 # verify the snapshot etcdctl --write-out=table snapshot status /var/lib/etcd/snapshot-20200312.db +----------+----------+------------+------------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | +----------+----------+------------+------------+ | c5a9f6c7 | 22904 | 1491 | 1.8 MB | +----------+----------+------------+------------+ start etcd Single-node etcd cluster Run the following./etcd --listen-client-urls=http://$PRIVATE_IP:2379 --advertise-client-urls=http://$PRIVATE_IP:2379 Start Kuebrentes API server with the flag --etcd-servers=$PRIVATE_IP:2379 Replace PRIVATE_IP with your etcd client IP Multi-node etcd cluster To run a load balancing etcd cluster: Set up an etcd cluster. Configure a load balancer in front of the etcd cluster. For example, let the address of the load balancer be $LB. Start Kubernetes API Servers with the flag --etcd-servers=$LB:2379. Securing etcd clusters To secure etcd, either set up firewall rules or use the security features provided by etcd. etcd security features depend on x509 Public Key Infrastructure(PKI).　To begin, establish secure communication channels by generating a key and certificate pair. For example, securing communication between etcd members use key pairs peer.key and peer.cert securing communication between etcd and its clients use client.key and client.cert . more example see script provided provided by the etcd project to generate key pairs and CA files for client authentication. Limiting access of etcd clusters After configuring secure communication, restrict the access of etcd cluster to only the Kubernetes API server. Use TLS authentication to do so. For example, consider key pairs k8sclient.key and k8sclient.cert that are trusted by the CA etcd.ca. When etcd is configured with --client-cert-auth along with TLS, it verifies the certificates from clients by using system CAs or the CA passed in by --trusted-ca-file flag. Specifying flags --client-cert-auth=true and --trusted-ca-file=etcd.ca will restrict the access to clients with the certificate k8sclient.cert. Once etcd is configured correctly, only clients with valid certificates can access it. To give Kubernetes API server the access, configure it with the flags --etcd-certfile=k8sclient.cert, --etcd-keyfile=k8sclient.key and --etcd-cafile=ca.cert. 使用三个文件ca.crt, xx.key, xx.crt 连接etcd endpoint reference: https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/gc.html":{"url":"blog/kubernetes/gc.html","title":"Gc","keywords":"","body":"k8s: garbage collector GC is short of Garbage Collector. 清理kubernetes中 符合特定条件的 Resource Object What are dependent mechanisms to clear needless resource objects? Kubernetes 在不同的 Resource Objects 中维护一定的「从属关系」。内置的 Resource Objects 一般会默认在一个 Resource Object 和它的创建者之间建立一个「从属关系」。当然， 你也可以利用ObjectMeta.OwnerReferences自由的去给两个 Resource Object 建立关系， 前提是被建立关系的两个对象必须在一个 Namespace 下 // OwnerReference contains enough information to let you identify an owning // object. Currently, an owning object must be in the same namespace, so there // is no namespace field. type OwnerReference struct { // API version of the referent. APIVersion string `json:\"apiVersion\" protobuf:\"bytes,5,opt,name=apiVersion\"` // Kind of the referent. // More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds Kind string `json:\"kind\" protobuf:\"bytes,1,opt,name=kind\"` // Name of the referent. // More info: http://kubernetes.io/docs/user-guide/identifiers#names Name string `json:\"name\" protobuf:\"bytes,3,opt,name=name\"` // UID of the referent. // More info: http://kubernetes.io/docs/user-guide/identifiers#uids UID types.UID `json:\"uid\" protobuf:\"bytes,4,opt,name=uid,casttype=k8s.io/apimachinery/pkg/types.UID\"` // If true, this reference points to the managing controller. // +optional Controller *bool `json:\"controller,omitempty\" protobuf:\"varint,6,opt,name=controller\"` // If true, AND if the owner has the \"foregroundDeletion\" finalizer, then // the owner cannot be deleted from the key-value store until this // reference is removed. // Defaults to false. // To set this field, a user needs \"delete\" permission of the owner, // otherwise 422 (Unprocessable Entity) will be returned. // +optional BlockOwnerDeletion *bool `json:\"blockOwnerDeletion,omitempty\" protobuf:\"varint,7,opt,name=blockOwnerDeletion\"` } OwnerReference 一般存在于某一个 Resource Object 信息中的metadata 部分。 OwnerReference中的字段可以唯一的确定 k8s 中的一个 Resource Object。两个 Object 可以通过这种方式建立一个 owner-dependent的关系。 K8s 实现了一种「Cascading deletion」（级联删除）的机制，它利用已经建立的「从属关系」进行资源对 象的清理工作。例如，当一个 dependent 资源的 owner 已经被删除或者不存在的时候，从某种角度就可以判定， 这个 dependent 的对象已经是异常（无人管辖）的了，需要进行清理。而 「cascading deletion」则是被 k8s 中的一个 controller 组件实现的：Garbage Collector。所以，k8s 是通过 Garbage Collector 和 ownerReference 一起配合实现了「垃圾回收」的功能。 K8s GC Design Principle Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/helm chart.html":{"url":"blog/kubernetes/helm chart.html","title":"Helm Chart","keywords":"","body":"Helm charts DirectoryStruct apiVersion: The chart API version, always \"v1\" (required) name: The name of the chart (required) version: A SemVer 2 version (required) kubeVersion: A SemVer range of compatible Kubernetes versions (optional) description: A single-sentence description of this project (optional) keywords: - A list of keywords about this project (optional) home: The URL of this project's home page (optional) sources: - A list of URLs to source code for this project (optional) maintainers: # (optional) - name: The maintainer's name (required for each maintainer) email: The maintainer's email (optional for each maintainer) url: A URL for the maintainer (optional for each maintainer) engine: gotpl # The name of the template engine (optional, defaults to gotpl) icon: A URL to an SVG or PNG image to be used as an icon (optional). appVersion: The version of the app that this contains (optional). This needn't be SemVer. deprecated: Whether this chart is deprecated (optional, boolean) tillerVersion: The version of Tiller that this chart requires. This should be expressed as a SemVer range: \">2.0.0\" (optional) VersionController 每个chart都必须有个版本号。命名必须符合semver2,简而言之就是[主].[次].[修] 许多 Helm 工具都使用 Chart.yaml 的 version 字段，其中包括 CLI 和 Tiller 服务。在生成包时，helm package 命令将使用它在 Chart.yaml 中的版本名作为包名。系统假定 chart 包名称中的版本号与 Chart.yaml 中的版本号相匹配。不符合这个情况会导致错误。 请注意，appVersion 字段与 version 字段无关。这是一种指定应用程序版本的方法。例如，drupal chart 可能有一个 appVersion: 8.2.1，表示 chart 中包含的 Drupal 版本（默认情况下）是 8.2.1。该字段是信息标识，对 chart 版本没有影响。 appVersion 字段 请注意，appVersion 字段与 version 字段无关。这是一种指定应用程序版本的方法。例如，drupal chart 可能有一个 appVersion: 8.2.1，表示 chart 中包含的 Drupal 版本（默认情况下）是 8.2.1。该字段是信息标识，对 chart 版本没有影响。 依赖关系 在 Helm 中，一个 chart 可能依赖于任何数量的其他 chart。这些依赖关系可以通过 requirements.yaml 文件动态链接或引入 charts/ 目录并手动管理。 虽然有一些团队需要手动管理依赖关系的优势，但声明依赖关系的首选方法是使用 chart 内部的 requirements.yaml 文件。 注意： 传统 Helm 的 Chart.yaml dependencies: 部分字段已被完全删除弃用。 用 requirements.yaml 来管理依赖关系 requirements.yaml 文件是列出 chart 的依赖关系的简单文件。 dependencies: - name: apache version: 1.2.3 repository: http://example.com/charts - name: mysql version: 3.2.1 repository: http://another.example.com/charts requirements.yaml 中的 alias 字段 可以通过给相同charts不同版本的依赖包起别名 # parentchart/requirements.yaml dependencies: - name: subchart repository: http://localhost:10191 version: 0.1.0 alias: new-subchart-1 - name: subchart repository: http://localhost:10191 version: 0.1.0 alias: new-subchart-2 - name: subchart repository: http://localhost:10191 version: 0.1.0 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/helm.html":{"url":"blog/kubernetes/helm.html","title":"Helm","keywords":"","body":"Helm 使用默认仓库创建应用 helm repo list helm search redis helm install stable/redis helm 部署 dashboard # 当使用Ingress将HTTPS的服务暴露到集群外部时，需要HTTPS证书，这里将*.frognew.com的证书和秘钥配置到Kubernetes中。 # 后边部署在kube-system命名空间中的dashboard要使用这个证书，因此这里先在kube-system中创建证书的secret kubectl create secret tls frognew-com-tls-secret \\ --cert=/etc/kubernetes/pki/ca.crt \\ --key=/etc/kubernetes/pki/ca.key \\ -n kube-system helm del --purge kubernetes-dashboard cat kubernetes-dashboard.yaml ingress: enabled: true hosts: annotations: nginx.ingress.kubernetes.io/ssl-redirect: \"true\" nginx.ingress.kubernetes.io/secure-backends: \"true\" tls: - secretName: frognew-com-tls-secret hosts: rbac: clusterAdminRole: true EOF helm install ./charts/stable/kubernetes-dashboard \\ -n kubernetes-dashboard \\ --namespace kube-system \\ -f kubernetes-dashboard.yaml 创建私有charts 仓库 yum install supervisor -y cat /etc/supervisord.d/helm-server.ini [program:helm] command = helm serve --address 0.0.0.0:8881 --repo-path /data/loca-char-repo autostart = true autorestart = true user = root startretries = 3 stdout_logfile_maxbytes = 20MB stdout_logfile_maxbytes = 10 stdout_logfile = /var/log/helm.log EOF systemctl restart supervisord mkdir -p /data/loca-char-repo 在私有helm 仓库添加应用 cd /data/loca-char-repo helm create myapp # 创建一个应用的char包 helm lint ./myapp # 语法校验 helm package myapp/ # 生成tar包 helm repo index myapp --url=http://xxxxx:8881 ## 官方示例git clone https://github.com/XishengCai/charts.git helm 通过私有仓库创建应用 helm repo add local-repo http://xxxx:8881 helm update helm search redis helm install local-repo/myapp 获取helm渲染后的k8s可执行的yaml文件（只渲染不运行）。 helm install --debug --dry-run ./mychart 模板函数 quote: 最常用的模板函数，它能把ABC转化为“ABC”。它带一个参数 {{ quote .Values.favorite.drink }} \"|\": 管道，类似linux下的管道 {{ quote .Values.favorite.drink }} 与 {{ .Values.favorite.drink | quote }} 效果一样 default: use default value 如果在values中无法找到favorite.drink，则配置为“tea”。* drink: {{ .Values.favorite.drink | default “tea” | quote }} indent: 对左空出空格 data: myvalue: \"Hello World\" {{ include \"mychart_app\" . | indent 2 }} 会使渲染后的取值于左边空出两个空格，以符合yaml语法。 overwrite map --set service.type=NodePort overwrite string --set image=..... overwrite array --set aaa[0].name=.. aaa[0].value=.... helm 模板名词解释 Release.Name Create a default fully qualified app name. We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec). If release name contains chart name it will be used as a full name. link helm blog helm github helm blog2 supervisor install Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/helm/charts.html":{"url":"blog/kubernetes/helm/charts.html","title":"Charts","keywords":"","body":"charts https://artifacthub.io/packages/helm/bitnami/redis Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/helm/helm 钩子函数.html":{"url":"blog/kubernetes/helm/helm 钩子函数.html","title":"Helm 钩子函数","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/helm/helm-operator.html":{"url":"blog/kubernetes/helm/helm-operator.html","title":"Helm Operator","keywords":"","body":"helm-operator quick start Prerequisites Kubernetes cluster >=1.13.0 Up-to-date Helm 2 or 3 helm binary kubectl Install the Helm Operator First, install the HelmRelease Custom Resource Definition. By adding this CRD it will be possible to define HelmRelease resources on the cluster: kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/1.2.0/deploy/crds.yaml Create a new namespace: kubectl create ns flux Using helm, first add the Flux CD Helm repository: helm repo add fluxcd https://charts.fluxcd.io Next, install the Helm Operator using the available Helm chart: helm upgrade -i helm-operator fluxcd/helm-operator \\ --namespace flux \\ --set helm.versions=v3 This installs the Helm Operator with default settings and support for Helm 3 enabled. Hint See the operator reference and chart documentation for detailed configuration options. Create your first HelmRelease To install a Helm chart using the Helm Operator, create a HelmRelease resource on the cluster: cat Example2: apiVersion: helm.fluxcd.io/v1 kind: HelmRelease metadata: name: podinfo namespace: default spec: chart: repository: \"https://charts.bitnami.com/bitnami\" name: \"redis\" version: \"14.2.1\" The applied resource will install the podinfo chart with a tiny Go web application from a Helm repository chart source. Chart sources are references to places where the operator can find Helm charts. The release name the Helm Operator will use is composed out of the namespace and name of the HelmRelease resource (but can be configured): default-podinfo. Hint Read more about different chart sources in the chart sources section of the HelmRelease guide. Confirm the chart has been installed When a Helm chart has been successfully released the Helm Operator will push a condition of type Released with status True. You can check this condition is set using kubectl: $ kubectl wait --for=condition=released helmrelease/podinfo helmrelease.helm.fluxcd.io/podinfo condition met Or, by describing the HelmRelease itself: $ kubectl describe helmrelease podinfo Name: podinfo Namespace: default Labels: Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"helm.fluxcd.io/v1\",\"kind\":\"HelmRelease\",\"metadata\":{\"annotations\":{},\"name\":\"podinfo\",\"namespace\":\"default\"},\"spec\":{\"chart... API Version: helm.fluxcd.io/v1 Kind: HelmRelease Metadata: Creation Timestamp: 2020-01-01T12:00:00Z Generation: 1 Resource Version: 9017 Self Link: /apis/helm.fluxcd.io/v1/namespaces/default/helmreleases/podinfo UID: e9c11dc8-5ba6-4ee7-9226-cb0f9cab04ff Spec: Chart: Name: podinfo Repository: https://stefanprodan.github.io/podinfo Version: 3.2.0 Status: Conditions: Last Transition Time: 2020-01-01T12:00:00Z Last Update Time: 2020-01-01T12:00:00Z Message: chart fetched: podinfo-3.2.0.tgz Reason: RepoChartInCache Status: True Type: ChartFetched Last Transition Time: 2020-01-01T12:00:01Z Last Update Time: 2020-01-01T12:00:01Z Message: Helm release sync succeeded Reason: HelmSuccess Status: True Type: Released Observed Generation: 1 Release Name: default-podinfo Release Status: deployed Revision: 3.2.0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ChartSynced 35s helm-operator Chart managed by HelmRelease processed Confirm the default-podinfo pod has been deployed: $ kubectl get pods NAME READY STATUS RESTARTS AGE default-podinfo-7f9759cc66-bslsl 1/1 Running 0 59s Tip The available shorthand for kubectl operations on helmrelease resources is hr, i.e: console $ kubectl get hr NAME RELEASE STATUS MESSAGE AGE podinfo default-podinfo deployed Helm release sync succeeded 59s Make a modification The Helm Operator ensures that the Helm release in the cluster matches the defined state in the HelmRelease resource. This means that an upgrade will be performed when the resource is modified. To demonstrate this, we are going to increase the number of podinfo replicas: kubectl edit helmrelease/podinfo Helm values can be defined on the HelmRelease resources under the spec.values key: ... spec: chart: name: podinfo repository: https://stefanprodan.github.io/podinfo version: 3.2.0 values: replicaCount: 2 Save the modification and watch the new pod enroll: $ kubectl get pods -w NAME READY STATUS RESTARTS AGE default-podinfo-7f9759cc66-lk45t 1/1 Running 0 59s default-podinfo-7f9759cc66-w7fj7 0/1 ContainerCreating 0 1s default-podinfo-7f9759cc66-w7fj7 0/1 Running 0 1s default-podinfo-7f9759cc66-w7fj7 1/1 Running 0 1s Hint See the values and release configuration sections in the HelmRelease guide for more details. Reconciliation All HelmRelease resources in the cluster watched by a Helm Operator instance are rescheduled to synchronize every 3 minutes (or configured --charts-sync-interval); this is also known as the reconciliation loop. During reconciliation the result of a dry-run upgrade made with the HelmRelease resource is compared to the current deployed Helm release, and if they differ an upgrade is performed to ensure the defined and in-cluster state match again. You can experience this with your own eyes by rolling back the modification we just made using helm, the Helm Operator created the release with a name composed of the namespace and name of the HelmRelease: $ helm rollback podinfo Rollback was a success! Happy Helming! $ kubectl get pods NAME READY STATUS RESTARTS AGE default-podinfo-7f9759cc66-w7fj7 1/1 Terminating 0 1m1s default-podinfo-7f9759cc66-lk45t 1/1 Running 0 2m1s Watch the Helm Operator reverting the unauthorized modification (this can take a while, but no longer than 3 minutes): $ kubectl get pods -w NAME READY STATUS RESTARTS AGE default-podinfo-7f9759cc66-lk45t 1/1 Running 0 2m19s default-podinfo-7f9759cc66-kd5rk 0/1 Pending 0 0s default-podinfo-7f9759cc66-kd5rk 0/1 Pending 0 0s default-podinfo-7f9759cc66-kd5rk 0/1 ContainerCreating 0 0s default-podinfo-7f9759cc66-kd5rk 0/1 Running 0 1s default-podinfo-7f9759cc66-kd5rk 1/1 Running 0 7s Hint Read more about reconciliation and upgrades in the HelmRelease guide. Uninstalling the chart To uninstall the chart and clean up the release, simply run kubectl delete for the resource: kubectl delete helmrelease podinfo $ kubectl get pods -w NAME READY STATUS RESTARTS AGE default-podinfo-7f9759cc66-fr4vb 1/1 Terminating 0 3m30s default-podinfo-7f9759cc66-kd5rk 1/1 Terminating 0 1m6s default-podinfo-7f9759cc66-kd5rk 0/1 Terminating 0 1m8s default-podinfo-7f9759cc66-fr4vb 0/1 Terminating 0 3m32s Delete the Helm Operator by removing the fluxcd namespace: kubectl delete namespace flux Next steps Want to continue testing the Helm Operator or install it in a cluster environment? Take a look at the available get started guides for more sophisticated setup options: Get started using Helm Get started using Kustomize Get started using YAMLs Want to take a deeper dive in the available features and the HelmRelease resource? Continue with the HelmRelease guide. issue can't build docker current helmRelease CRD not support helm password use kubectl is layer low #!/bin/bash go run cmd/helm-operator/main.go \\ --kubeconfig=/Users/xishengcai/.kube/config \\ --enabled-helm-versions=v3 \\ --git-timeout=20s \\ --git-poll-interval=5m \\ --charts-sync-interval=3m \\ --status-update-interval=30s \\ --update-chart-deps=true \\ --log-release-diffs=false \\ --workers=4 \\ --tiller-namespace=kube-system {\"auths\":{\"registry.cn-hangzhou.aliyuncs.com\":{\"username\":\"xlauncher\",\"password\":\"811@Launcher#1806\",\"auth\":\"eGxhdW5jaGVyOjgxMUBMYXVuY2hlciMxODA2\"}}} {\"auths\":{\"registry.cn-beijing.aliyuncs.com\":{\"username\":\"lstack1\",\"password\":\"Launcher@1302\",\"auth\":\"eGxhdW5jaGVyOjgxMUBMYXVuY2hlciMxODA2\"}}} Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/helm/文档查询.html":{"url":"blog/kubernetes/helm/文档查询.html","title":"文档查询","keywords":"","body":" Helm 和 k8s 版本对应关系 https://helm.sh/zh/docs/topics/version_skew/ helm charts field illustrate https://helm.sh/zh/docs/topics/charts/ helm 基本操作 https://blog.csdn.net/QianLiStudent/article/details/111872100 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/helm/语法.html":{"url":"blog/kubernetes/helm/语法.html","title":"语法","keywords":"","body":"helm 语法总结 作者：陈行 花括号 {{- 模版表达式 -}} 去掉表达式输出结果前面和后面的空格 {{- 模版表达式 }} 去掉表达式输出结果前面的空格 {{ 模版表达式 -}} 去掉表达式输出结果后面的空格 {{ 模板表达式 }} 全局变量 “.”代表全局作用域，用于引用全局对象，例如：{{ .Values.key }}，即表示引用全局作用域下的Values对象中的key 常用的全局作用域对象： 1）Values：即values.yaml中定义的参数 2）Release：代表发布的应用，通常包含以下属性： （1）Name release的名字，一般通过Chart.yaml定义，但是通常在helm install的时候，会指定一个name,这时的name会覆盖 Chart.yaml中的值。 （2）Time release安装的时间 （3）Namespace k8s命名空间 （4）Revision release版本号 自定义变量 {{- $var := .Values.key -}} 自定义变量名以$命名，使用 := 赋值 {{ $var }} 引用变量 函数 语法：{{ function arg1 arg2... }} {{ quote .Values.key }} 调用quote函数，将结果用\"\"括起来 常用函数： quote value 将值以\"\"括起来 ​ upper value 将值转为大写字符 ​ default value 赋默认值 ​ nindent n 将结果缩进n个空格 ​ repeat n 将结果重复输出n次 管道 | 作用与Linux中的管道类似 如：{{ .Values.key | quote }} 将值输出至quote函数处理，处理完成后输出值 ​ {{ .Values.key | default \"value\" }} 如果值为空，为其赋默认值 关系运算符和逻辑运算符 eq 作用等于 = ne 作用等于 != lt 作用等于 = and 作用等于&& or 作用等于 || not 作用等于 ! 流程控制 if/else {{ if 条件表达式 }} {{ else if 条件表达式}} {{ else }} {{ end }} range 循环遍历 遍历数组 {{- range 数组 }} {{ . | quote }} ### “.”表示引用数组元素值 {{- end }} 遍历map {{- range $key,$val := map对象}} {{ $key }}: {{ $val }} {{- end }} with 修改\".\"作用域，默认\".\"为全局作用域 {{ with 被引用的对象 }} {{- end}} 如： {{- with .Values.config }} {{ .oss | default \"test-bucket\" }} ### 这里的.oss相当于 .Values.config.oss,也就是说，在当前流程模块下\".\"代表的是.Values.config，而不是普遍意义上的全局变量 {{- end }} Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/istio-inject.html":{"url":"blog/kubernetes/istio-inject.html","title":"Istio Inject","keywords":"","body":"istio-inject webhook自动注入原理 webhook 现象 deployment template no change Pods add two containers [init, proxy] Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kind.html":{"url":"blog/kubernetes/kind.html","title":"Kind","keywords":"","body":"kind CD+delegation: 产品 蔡锡生， 后端开发 吴江法， 前端开发 孙玉杰 1.install kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.0/kind-linux-amd64 chmod +x ./kind mv ./kind /usr/local/bin 2. install cluster kind create cluster --name kind-2 3. Install docker #!/usr/bin/env bash echo \"clean env\" yum remove -y docker docker-common container-selinux docker-selinux docker-engine rm -rf /var/lib/docker echo \"install docker 18.09.8\" yum install -y yum-utils yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum clean packages #查看docker-ce版本并且安装 yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-19.03.14 docker-ce-cli containerd.io echo \"config docker daemon\" mkdir -p /etc/docker cat > /etc/docker/daemon.json 4. install kubectl #!/usr/bin/env bash # made by Caixisheng Fri Nov 9 CST 2018 #chec user [[ $UID -ne 0 ]] && { echo \"Must run in root user !\";exit; } set -e echo \"添加kubernetes国内yum源\" cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # Set SELinux in permissive mode (effectively disabling it) cat /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF sysctl --system swapoff -a yum -y remove kubeadm kubectl kubelet yum -y install kubectl 5. Install nginx kubectl run nginx --image=nginx https://www.cnblogs.com/charlieroro/p/13711589.html#%E5%B0%86%E9%95%9C%E5%83%8F%E5%8A%A0%E8%BD%BD%E5%88%B0kind%E7%9A%84node%E4%B8%AD Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubeEdge/beehive.html":{"url":"blog/kubernetes/kubeEdge/beehive.html","title":"Beehive","keywords":"","body":"https://cloud.tencent.com/developer/article/1661285 概述 Beehive是基于go-channel的消息传递框架，用于kubeedge模块之间的通信。如果已注册其他beehive模块的名称或该模块的名称已知，则在峰箱中注册的模块可以与其他峰箱模块进行通信。 添加模块 将模块添加到组 清理 Beehive 支持如下操作 发送到模块/组 通过模块接收 发送同步到模块/组 发送对同步消息的响应 消息格式 消息分为三部分 1.header： ID：消息ID（字符串） ParentID：如果是对同步消息的响应，则说明parentID存在（字符串） TimeStamp：生成消息的时间（整数） sync：标志，指示消息是否为同步类型（布尔型） 2.Route： Source：消息的来源（字符串） Group：必须将消息广播到的组（字符串） Operation：对资源的操作（字符串） Resource：要操作的资源（字符串） 3.content：消息的内容（interface{}） 实现两种通信机制，一种是unixsocket;另一种是golang 的channel 注册模块 在启动edgecore时，每个模块都会尝试将其自身注册到beehive内核。 Beehive核心维护一个名为modules的映射，该映射以模块名称为键，模块接口的实现为值。 当模块尝试向蜂巢核心注册自己时，beehive 内核会从已加载的modules.yaml配置文件中进行检查， 以检查该模块是否已启用。如果启用，则将其添加到模块映射中，否则将其添加到禁用的模块映射中。 channel上下文结构字段 channels - channels是字符串（键）的映射，它是模块的名称和消息的通道（值），用于将消息发送到相应的模块。 chsLock - channels map的锁 typeChannels - typeChannels是一个字符串（key）的映射，它是组(将字符串(key)映射到message的chan(value)， 是该组中每个模块的名称到对应通道的映射。 typeChsLock - typeChannels map的锁 anonChannels - anonChannels是消息的字符串（父id）到chan（值）的映射，将用于发送同步消息的响应。 anonChsLock - anonChannels map的锁 代码架构 ​ 2.1 目录结构 ├── pkg │ ├── common │ │ ├── config │ │ │ ├── config.go │ │ │ └── config_test.go │ │ └── util │ │ ├── conn.go │ │ ├── conn_test.go │ │ ├── file_util.go │ │ └── parse_resource.go │ └── core │ ├── context │ │ ├── context.go │ │ ├── context_channel.go │ │ ├── context_channel_test.go │ │ ├── context_factory.go │ │ └── context_unixsocket.go │ ├── core.go │ ├── model │ │ └── message.go │ └── module.go 使用方法 beehive并不是单独能运行的模块，而是直接被其他模块引用的。 core.Register(MODULE) // start all modules core.Run() 注意事项 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubeEdge/cloud stream.html":{"url":"blog/kubernetes/kubeEdge/cloud stream.html","title":"Cloud Stream","keywords":"","body":"[toc] Cloud Stream Code Read(v1.7.1) 架构说明 Kubeedge 提供了云边stream模块，用于支 exec， log， metricr 功能。 StreamServer 模块功能 转发 kube-apiserver 的请求 根据请求的内容，找到node 名称 根据node 名称找到 tunnelServer 中和node 建立的wss session 生成此次请求的message ID 创建一个 响应kube-apiserver 结构体，并将此次请求的response 放入其中 将 4作为可以， 5作为value，进行缓存 通过session 发送 请求信息 edge 端回复数据流信息，该信息是分段的，并且可提取出messageID 根据messageID ，获取步骤5中的响应体，回写response。 TunnelServer 模块功能 提供 客户端 websocket 连接接口 客户端发起的websocket 保存在session中，并进行缓存。 session 用于cloud 和 edge 通信 源码概览 1. cloudStream 注册 and 启动 代码：/cloud/cloudcore/app/server.go, line 121 // registerModules register all the modules started in cloudcore func registerModules(c *v1alpha1.CloudCoreConfig) { cloudhub.Register(c.Modules.CloudHub) edgecontroller.Register(c.Modules.EdgeController, c.CommonConfig) devicecontroller.Register(c.Modules.DeviceController) synccontroller.Register(c.Modules.SyncController) // 模块注册 cloudstream.Register(c.Modules.CloudStream) router.Register(c.Modules.Router) dynamiccontroller.Register(c.Modules.DynamicController) } func (s *cloudStream) Start() { // TODO: Will improve in the future ok := 2. NewTunnelServer 等待edgeStream 连接 edgeNode 在与cloud stream 建立连接的时候，会根据主机名创建key， 同时创建session 存入到CloudStream的session字典中。 该session 是 edge 与 cloud 之间建立的websocket 连接，用于将cloud 的http请求发送到edge。 该session 实现了接口 stream.SafeWriteTunneler 2.1 register router func (s *TunnelServer) installDefaultHandler() { ws := new(restful.WebService) ws.Path(\"/v1/kubeedge/connect\") ws.Route(ws.GET(\"/\"). To(s.connect)) s.container.Add(ws) } 2.2 handler 入口方法 func (s *TunnelServer) connect(r *restful.Request, w *restful.Response) { hostNameOverride := r.HeaderParameter(stream.SessionKeyHostNameOverride) if hostNameOverride == \"\" { // TODO: Fix SessionHostNameOverride typo, remove this in v1.7.x hostNameOverride = r.HeaderParameter(stream.SessionKeyHostNameOverrideOld) } internalIP := r.HeaderParameter(stream.SessionKeyInternalIP) if internalIP == \"\" { internalIP = strings.Split(r.Request.RemoteAddr, \":\")[0] } con, err := s.upgrader.Upgrade(w, r.Request, nil) if err != nil { return } klog.Infof(\"get a new tunnel agent hostname %v, internalIP %v\", hostNameOverride, internalIP) session := &Session{ tunnel: stream.NewDefaultTunnel(con), apiServerConn: make(map[uint64]APIServerConnection), apiConnlock: &sync.RWMutex{}, sessionID: hostNameOverride, } s.addSession(hostNameOverride, session) s.addSession(internalIP, session) s.addNodeIP(hostNameOverride, internalIP) session.Serve() } 2.3 获取并转发edge端发送的信息 // Serve read tunnel message ,and write to specific apiserver connection func (s *Session) Serve() { defer s.Close() for { t, r, err := s.tunnel.NextReader() if err != nil { klog.Errorf(\"get %v reader error %v\", s.String(), err) return } if t != websocket.TextMessage { klog.Errorf(\"Websocket message type must be %v type\", websocket.TextMessage) return } message, err := stream.ReadMessageFromTunnel(r) if err != nil { klog.Errorf(\"Read message from tunnel %v error %v\", s.String(), err) return } if err := s.ProxyTunnelMessageToApiserver(message); err != nil { klog.Errorf(\"Proxy tunnel message [%s] to kube-apiserver error %v\", message.String(), err) continue } } } 2.4 根据messageID ，找到对应的api 请求发起者，flush message to requestResponse func (s *Session) ProxyTunnelMessageToApiserver(message *stream.Message) error { s.apiConnlock.RLock() defer s.apiConnlock.RUnlock() kubeCon, ok := s.apiServerConn[message.ConnectID] if !ok { return fmt.Errorf(\"Can not find apiServer connection id %v in %v\", message.ConnectID, s.String()) } switch message.MessageType { case stream.MessageTypeRemoveConnect: kubeCon.SetEdgePeerDone() case stream.MessageTypeData: for i := 0; i 3.一次日志流请求流程 func (s *StreamServer) getContainerLogs(r *restful.Request, w *restful.Response) { .... sessionKey := strings.Split(r.Request.Host, \":\")[0] session, ok := s.tunnel.getSession(sessionKey) w.Header().Set(\"Transfer-Encoding\", \"chunked\") w.WriteHeader(http.StatusOK) if _, ok := w.ResponseWriter.(http.Flusher); !ok { err = fmt.Errorf(\"Unable to convert %v into http.Flusher, cannot show logs\", reflect.TypeOf(w)) return } // Flusher 接口由 ResponseWriters 实现，它允许 HTTP 处理程序将缓冲数据刷新到客户端。 // 默认的 HTTP/1.x 和 HTTP/2 ResponseWriter实现支持 Flusher，但ResponseWriter包装器可能不支持。 处理程序应始终在运行时测试此能力。 fw := flushwriter.Wrap(w.ResponseWriter) // 缓存此次请求的request and response logConnection, err := session.AddAPIServerConnection(s, &ContainerLogsConnection{ r: r, flush: fw, // 后面edge 返回的信息通过 flush 回写给请求者 session: session, ctx: r.Request.Context(), edgePeerStop: make(chan struct{}), }) if err = logConnection.Serve(); err != nil { err = fmt.Errorf(\"apiconnection Serve %s in %s error %v\", logConnection.String(), session.String(), err) return } } func (s *Session) AddAPIServerConnection(ss *StreamServer, connection APIServerConnection) (APIServerConnection, error) { // cloudStream messageID 递增 // 每次log stream 都有一个唯一的mssageID // session 是唯一的，但是session 里面的api 连接是随着每次请求递增的 // 请求结束需要从session 中删除api连接 // api连接是一个流（长链接）接口：实现类有（log， exec， metrics） id := atomic.AddUint64(&(ss.nextMessageID), 1) s.apiConnlock.Lock() defer s.apiConnlock.Unlock() if s.tunnelClosed { return nil, fmt.Errorf(\"The tunnel connection of %v has closed\", s.String()) } connection.SetMessageID(id) s.apiServerConn[id] = connection klog.Infof(\"Add a new apiserver connection %s in to %s\", connection.String(), s.String()) return connection, nil } func (s *StreamServer) getContainerLogs(r *restful.Request, w *restful.Response) { sessionKey := strings.Split(r.Request.Host, \":\")[0] session, _ := s.tunnel.getSession(sessionKey) w.Header().Set(\"Transfer-Encoding\", \"chunked\") w.WriteHeader(http.StatusOK) w.ResponseWriter.(http.Flusher) fw := flushwriter.Wrap(w.ResponseWriter) logConnection, _ := session.AddAPIServerConnection(s, &ContainerLogsConnection{ r: r, flush: fw, session: session, ctx: r.Request.Context(), edgePeerStop: make(chan struct{}), }) logConnection.Serve() } 4. logConnection Server logic func (l *ContainerLogsConnection) Serve() error { defer func() { klog.Infof(\"%s end successful\", l.String()) }() // first send connect message // 只管向ws 写msg // 自动读 if _, err := l.SendConnection(); err != nil { klog.Errorf(\"%s send %s info error %v\", l.String(), stream.MessageTypeLogsConnect, err) return err } for { select { case func (l *ContainerLogsConnection) SendConnection() (stream.EdgedConnection, error) { // 引入边缘节点的接口对象 connector := &stream.EdgedLogsConnection{ MessID: l.MessageID, // 构建 ContainerLogsConnection 的时候，将Reqeust 传入 URL: *l.r.Request.URL, Header: l.r.Request.Header, } connector.URL.Scheme = httpScheme connector.URL.Host = net.JoinHostPort(defaultServerHost, fmt.Sprintf(\"%v\", constants.ServerPort)) //将请求转化成 edgeStream 可识别的 message 结构体 m, _ := connector.CreateConnectMessage() // 在edge connect to cloud 的时候， 会创建session l.WriteToTunnel(m) return connector, nil } func (l *EdgedLogsConnection) CreateConnectMessage() (*Message, error) { data, err := json.Marshal(l) if err != nil { return nil, err } return NewMessage(l.MessID, MessageTypeLogsConnect, data), nil } func (l *ContainerLogsConnection) WriteToTunnel(m *stream.Message) error { return l.session.WriteMessageToTunnel(m) } func (s *Session) WriteMessageToTunnel(m *stream.Message) error { return s.tunnel.WriteMessage(m) } ​ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubeEdge/config.html":{"url":"blog/kubernetes/kubeEdge/config.html","title":"Config","keywords":"","body":" cloudcore.yaml: | apiVersion: cloudcore.config.kubeedge.io/v1alpha2 kind: CloudCore kubeAPIConfig: kubeConfig: \"\" master: \"\" modules: dynamicController: enable: true cloudHub: advertiseAddress: - 121.41.64.133 nodeLimit: 100 unixsocket: address: unix:///var/lib/kubeedge/kubeedge.sock enable: true websocket: address: 0.0.0.0 enable: true port: 10000 cloudStream: enable: true streamPort: 10003 tlsStreamCAFile: /etc/kubeedge/ca/streamCA.crt tlsStreamCertFile: /etc/kubeedge/certs/stream.crt tlsStreamPrivateKeyFile: /etc/kubeedge/certs/stream.key tunnelPort: 10004 curl -k \\ --include \\ --no-buffer \\ --header \"Connection: Upgrade\" \\ --header \"Upgrade: websocket\" \\ --header \"Sec-WebSocket-Version: 13\" \\ https://172.16.2.249:10003/stats/summary Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubeEdge/edge stream.html":{"url":"blog/kubernetes/kubeEdge/edge stream.html","title":"Edge Stream","keywords":"","body":"[toc] edge Stream code read 架构介绍 服务启动流程 beehive 注册 创建tunnel stream connection 2.1 创建session // TunnelSession type TunnelSession struct { Tunnel stream.SafeWriteTunneler closeLock sync.Mutex closed bool // tunnel whether closed localCons map[uint64]stream.EdgedConnection localConsLock sync.RWMutex } 2.2 启动 session 服务 ​ 启动ping gorouting 循环事件 ​ 读取云端 message ​ 解析 message ​ 判断 msg 数据类型，根据数据请求的类型，创建stream 连接，通过msg ID 标记不同的云端请求。 ​ 0: 添加 edged 的日志流连接 到 localCons ​ 1: 添加 edged 的exec流连接 localCons ​ 2: 添加 edged 的 metric 连接 localCons ​ ​ edged 从容器中获取的数据流，会直接交给 session中tunnel， 执行write msg。 该tunnel 是在创建session 的时候，和cloud stream 建立起来的wss 连接。 ​ 问题： 云端 多个request 日志请求，如果共用了一个tunnel 通道发送消息， 客户端返回的信息，云端如何将信息挑拣出来正确的返回给每个请求者的？ 答： 云端的每次请求都会生成一个唯一的messageID，同时创建一个APIServerConnection， 将 此次请求的response 放入APIServerConnection 从tunnel 读取message，根据messageID，获取APIServerConnection， 调用起写方法。 ​ 共用一个通道会不会拥挤？ ​ ​ ​ Beehive 模块注册 type edgestream struct { enable bool hostnameOveride string nodeIP string } // Register register edgestream func Register(s *v1alpha1.EdgeStream, hostnameOverride, nodeIP string) { config.InitConfigure(s) core.Register(newEdgeStream(s.Enable, hostnameOverride, nodeIP)) } 向云端发起websocket 连接 func (e *edgestream) Start() { serverURL := url.URL{ Scheme: \"wss\", Host: config.Config.TunnelServer, Path: \"/v1/kubeedge/connect\", // cloud 侧的ws 接口 } .... for range time.NewTicker(time.Second * 2).C { select { case func (e *edgestream) TLSClientConnect(url url.URL, tlsConfig *tls.Config) error { klog.Info(\"Start a new tunnel stream connection ...\") dial := websocket.Dialer{ TLSClientConfig: tlsConfig, HandshakeTimeout: time.Duration(config.Config.HandshakeTimeout) * time.Second, } // head 操作略 // 创建websocket 连接 con, _, err := dial.Dial(url.String(), header) if err != nil { klog.Errorf(\"dial %v error %v\", url.String(), err) return err } // 将websocket 连接传入， 构建一个隧道会话， 子程1 session := NewTunnelSession(con) // 启动一个websocket 永久连接，处理cloud 端的请求 return session.Serve() } // 子程1 func NewTunnelSession(c *websocket.Conn) *TunnelSession { return &TunnelSession{ closeLock: sync.Mutex{}, localConsLock: sync.RWMutex{}, Tunnel: stream.NewDefaultTunnel(c), localCons: make(map[uint64]stream.EdgedConnection, 128), } } func (s *TunnelSession) Serve() error { ... for { // 读取云端请求 _, r, err := s.Tunnel.NextReader() if err != nil { klog.Errorf(\"Read Message error %v\", err) return err } // 解析成 标准结构信息 mess, err := stream.ReadMessageFromTunnel(r) if err != nil { klog.Errorf(\"Get tunnel Message error %v\", err) return err } // 根据信息类型，调用edged 创建不同的stream， （log，exec，metric） if mess.MessageType func (s *TunnelSession) ServeConnection(m *stream.Message) { switch m.MessageType { case stream.MessageTypeLogsConnect: if err := s.serveLogsConnection(m); err != nil { klog.Errorf(\"Serve Logs connection error %s\", m.String()) } case stream.MessageTypeExecConnect: if err := s.serveContainerExecConnection(m); err != nil { klog.Errorf(\"Serve Container Exec connection error %s\", m.String()) } case stream.MessageTypeMetricConnect: if err := s.serveMetricsConnection(m); err != nil { klog.Errorf(\"Serve Metrics connection error %s\", m.String()) } default: panic(fmt.Sprintf(\"Wrong message type %v\", m.MessageType)) } s.DeleteLocalConnection(m.ConnectID) klog.V(6).Infof(\"Delete local connection MessageID %v Type %s\", m.ConnectID, m.MessageType.String()) } ServeConnection handler message serverConnection 会根据信息类型，分发给不同的edgedConnection 子对象处理， 下面以EdgedLogsConnection为例 read message json marshar message http request to edged read resp form edged write resp to ws func (s *TunnelSession) serveLogsConnection(m *stream.Message) error { logCon := &stream.EdgedLogsConnection{ ReadChan: make(chan *stream.Message, 128), } if err := json.Unmarshal(m.Data, logCon); err != nil { klog.Errorf(\"unmarshal connector data error %v\", err) return err } // message ID 对应 EdgedLogsConnection s.AddLocalConnection(m.ConnectID, logCon) return logCon.Serve(s.Tunnel) } func (l *EdgedLogsConnection) Serve(tunnel SafeWriteTunneler) error { //connect edged client := http.Client{} req, _ := http.NewRequest(\"GET\", l.URL.String(), nil) req.Header = l.Header resp, _ := client.Do(req) ... defer resp.Body.Close() reader := bufio.NewReader(resp.Body) stop := make(chan struct{}) go func() { defer close(stop) go func() { defer close(l.ReadChan) var data [256]byte for { n, _ := reader.Read(data[:]) msg := NewMessage(l.MessID, MessageTypeData, data[:n]) // 将msg 写到 EdgedLogsConnection.ReadChan tunnel.WriteMessage(msg) } }() func (s *TunnelSession) WriteToLocalConnection(m *stream.Message) { if con, ok := s.GetLocalConnection(m.ConnectID); ok { con.CacheTunnelMessage(m) } } func (l *EdgedLogsConnection) CacheTunnelMessage(msg *Message) { l.ReadChan Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubeEdge/edged.html":{"url":"blog/kubernetes/kubeEdge/edged.html","title":"Edged","keywords":"","body":"kubeedge 源码分析之 edged 简述： Edged：运行在边缘节点上用户管理容器声明周期，实际上是kubelet 的精简版 功能列表 Pod manage 工作机制 初始化 设置backoff的值 创建podmanager 创建image gc policy 创建事件记录器 初始化edged 创建pods目录 创建livenessmanager 定义noderef 创建StateProvider接口实现，用于在垃圾回收期间获取镜像信息 创建镜像GC策略 如果remote-image-endpoint为空则设置为remote-runtime-endpoint 根据配置创建一个docker shim grpc客户端 设置dns地址及配置 创建一个containerRefManager 创建image和运行时管理程序 创建容器声明周期管理器 创建容器运行时管理器 创建容器管理器 创建image GC管理器 创建容器GC管理器 创建卷插件管理器 启动 创建元数据管理客户端，用于和beehive交互同步消息 创建Clientset 包含kube client（k8s clientset）和MetaClient 创建pod状态管理器 创建、运行volume管理器 创建运行probemanager 运行pod add worker 运行pod delete worker 运行pod状态管理器 启动edge server,10255/pods 启动imageGCmanager 启动容器gc 创建CSI插件管理器 sync pod syncPod 发送消息给metamanager获取register-node-namespace指定的命名空间下的pod列表 从beehive中读取发给自己的消息 获取资源的类型和id 当资源类型为pod 当动作为response切ID为空且来源为metamanager,将pod加入运行队列 当动作为response且ID为空来源为metamanager edgecontroller，将坡道加入队列 其他情况 insert 加入队列 update 更新pod到指定队列 delete 加入删除队列 configmap 对cachestore进行增删改 secret 对secret进行增删改 volume 对volume进行增删改 consumePodAddition 重点看一下consumePodAddition func (e *edged) consumePodAddition(namespacedName *types.NamespacedName) error { podName := namespacedName.Name klog.Infof(\"start to consume added pod [%s]\", podName) pod, ok := e.podManager.GetPodByName(namespacedName.Namespace, podName) if !ok || pod.DeletionTimestamp != nil { return apis.ErrPodNotFound } if err := e.makePodDataDirs(pod); err != nil { klog.Errorf(\"Unable to make pod data directories for pod %q: %v\", format.Pod(pod), err) return err } if err := e.volumeManager.WaitForAttachAndMount(pod); err != nil { klog.Errorf(\"Unable to mount volumes for pod %q: %v; skipping pod\", format.Pod(pod), err) return err } secrets, err := e.getSecretsFromMetaManager(pod) if err != nil { return err } curPodStatus, err := e.podCache.Get(pod.GetUID()) if err != nil { klog.Errorf(\"Pod status for %s from cache failed: %v\", podName, err) return err } result := e.containerRuntime.SyncPod(pod, curPodStatus, secrets, e.podAdditionBackoff) if err := result.Error(); err != nil { // Do not return error if the only failures were pods in backoff for _, r := range result.SyncResults { if r.Error != kubecontainer.ErrCrashLoopBackOff && r.Error != images.ErrImagePullBackOff { // Do not record an event here, as we keep all event logging for sync pod failures // local to container runtime so we get better errors return err } } return nil } e.workQueue.Enqueue(pod.UID, utilwait.Jitter(time.Minute, workerResyncIntervalJitterFactor)) klog.Infof(\"consume added pod [%s] successfully\\n\", podName) return nil} 在edgecore启动的时间发送一个获取pod列表的消息给metamanager，当启动时间，从metamanager获取configmap和secret从而保证离线时间依旧能够运行 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubeEdge/架构.html":{"url":"blog/kubernetes/kubeEdge/架构.html","title":"架构","keywords":"","body":"本文在commit a9b0862bed7fc2f9350a850496e60640fc43c15c(2020.06.20)之后对KubeEdge进行源码分析，分析KubeEdge的edgecore的整体框架，对细节不做过多展示。 KubeEdge releases 整个KubeEdge的架构图如下，先感受一下EdgeCore的目标定位。 EdgeCore是什么？ EdgeCore是kubeedge运行在边缘节点的组件，与云端组件CloudCore通信并处理边缘节点的一切事宜，相当于地方管理者。 EdgeCore支持amd以及arm，不能运行在有kubelet以及Kube-proxy的节点。 EdgeCore的结构及模块介绍 EdgeCore包括几个模块：Edged、EdgeHub、MetaManager、DeviceTwin、EventBus、ServiceBus、EdgeStream以及EdgeMesh。 与k8s节点上部署的kubelet相比： 对kubelet不必要的部分进行了精简，即edgecore中的edged； edgecore增加了与设备管理相关的模块如devicetwin以及eventbus； edgemesh模块实现了服务发现；edgecore将元数据进行本地存储，保证云边网络不稳定时边缘端也能正常工作，metamanager进行元数据的管理。 下面是官方对各个模块的介绍： EdgeHub: a web socket client responsible for interacting with Cloud Service for the edge computing (like Edge Controller as in the KubeEdge Architecture). This includes syncing cloud-side resource updates to the edge, and reporting edge-side host and device status changes to the cloud. Edged: an agent that runs on edge nodes and manages containerized applications. EventBus: a MQTT client to interact with MQTT servers (mosquitto), offering publish and subscribe capabilities to other components. ServiceBus: a HTTP client to interact with HTTP servers (REST), offering HTTP client capabilities to components of cloud to reach HTTP servers running at edge. DeviceTwin: responsible for storing device status and syncing device status to the cloud. It also provides query interfaces for applications. MetaManager: the message processor between edged and edgehub. It is also responsible for storing/retrieving metadata to/from a lightweight database (SQLite). EdgeStream EdgeMesh: To support service mesh capabilities on edge to support microservice communication cross cloud and edge. The service discovery, communication should be considered in this feature. EdgeCore的各个模块之间的通信通过beehive微服务框架（底层实现为channel），EdgeCore与CloudCore之间的通信通过websocket/quic。 EdgeHub EdgeHub中有两类client，分别是httpclient以及websocket/quic client，前者用于与EdgeCore与CloudCore通信所需证书的申请，后者负责与CloudCore的日常通信（资源下发、状态上传等） 当EdgeHub启动时，其先从CloudCore申请证书（若正确配置本地证书，则直接使用本地证书） 初始化与CloudCore通信的websocket/quic client，成功连接之后将成功连接的信息传给其他组件（MetaGroup、TwinGroup、BusGroup），分别启动三个goroutine不断的进行云到边以及边到云的消息分发(单纯分发，不做任何封装或改变)、健康状态上报。当云边传送消息过程中出现错误时，则边缘端重新init相应的websocket/quic client，与云端重新建立连接。 MetaManager 当metamanager模块启动时，会开启两个goroutine，一个用于定时（默认60s）给自己发送消息通知进行边到云的podstatus数据同步；一个用于edgehub与edged/edgemesh的数据处理。 到达memanager的数据来源于两部分，一是edgehub，此时是云到边的数据，记为①；二是edged，此时是边到云的数据，记为②。 处理的消息类型： Insert① Update①② Delete① Query 根据云边连接状态以及资源类型的不同，查询的具体方式如下图所示： Response NodeConnection（edgehub->其他modules） MetaSync（podstatus边到云的同步） 处理的资源类型： secret configmap service podlist endpoints node podstatus servicelist pod nodestatus Note: service/servicelist/endpoints进一步发送给edgemesh供服务发现，其余发给edged进行应用管理 edged edged内部模块如图所示： 当edged启动时，首先初始化并启动各个模块，最后进行pod的sync。下面以一个pod的创建来看一下edged中各个模块是如何协作完成pod的生命周期管理的。 当edged接收到pod的insert消息时，将pod所有信息加入podmanager、probemanager，podAdditionQueue加入node-namespace/node-name信息。 启动一个goroutine，创建下发到此节点的pod。 此时我们根据pod的定义成功创建pod，之后还要对pod的状态进行管理。 启动一个goroutine执行syncLoopIteration()函数： ①当liveness探针的结果更新，若内容是“failure”，根据container的restart policy执行相应的操作，比如：never->do nothing；onfailed->判断container的status，若completed->do nothing，否则将加podAdditionQueue，等待被再次创建；always->加入podAddtionQueue，等待被再次创建。 ②当收到PLEG的event，更新podmanager中podstatus(containerruntime中获取当前sataus，probemanager更新ready状态)，更新statusmanager中的缓存podstatus。若event是containerdied，则根据restart policy执行相应操作。 另外，statusmanager中会定时(10s)将podstatus上传至metamanager servicebus/eventbus/devicetwin 此部分此处不作过多介绍 edgemesh 和kube-proxy的对比 kube-proxy： 需要list-watch service，从而进行服务发现 容器化部署在每个节点(daemonset) service with cluster IP edgemesh： 从cloudcore接收service信息，从而进行服务发现 嵌入到edgecore headless service edgemesh目前功能尚不完善，存在诸多限制，将在v1.4进行完善： edgemesh only works for Docker CRI Only support HTTP protocol proxy when apps use hostPort Only support IPV4 DNS resolver Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kuber-apiserver-flag.html":{"url":"blog/kubernetes/kuber-apiserver-flag.html","title":"Kuber Apiserver Flag","keywords":"","body":"重点参数解读 1.advertise-address 向集群成员通知 apiserver 消息的 IP 地址。这个地址必须能够被集群中其他成员访问。如果 IP 地址为空， 将会使用 --bind-address，如果未指定 --bind-address，将会使用主机的默认接口地址。 2.allow-privileged 如果为 true, 将允许特权容器。 3.uthorization-mode=Node,RBAC 在安全端口上进行权限验证的插件的顺序列表。以逗号分隔的列表，包括：AlwaysAllow,AlwaysDeny,ABAC,Webhook,RBAC,Node. （默认值 \"AlwaysAllow\"） 4.client-ca-file=/etc/kubernetes/pki/ca.crt 如果设置此标志，对于任何请求，如果存包含 client-ca-file 中的 authorities 签名的客户端证书，将会使用客户端证书中的 CommonName 对应的身份进行认证。 5.enable-admission-plugins=NodeRestriction 激活准入控制插件 AlwaysAdmit, AlwaysDeny, AlwaysPullImages, DefaultStorageClass, DefaultTolerationSeconds, DenyEscalatingExec, DenyExecOnPrivileged, EventRateLimit, ExtendedResourceToleration, ImagePolicyWebhook, LimitPodHardAntiAffinityTopology, LimitRanger, MutatingAdmissionWebhook, NamespaceAutoProvision, NamespaceExists, NamespaceLifecycle, NodeRestriction, OwnerReferencesPermissionEnforcement, PersistentVolumeClaimResize, PersistentVolumeLabel, PodNodeSelector, PodPreset, PodSecurityPolicy, PodTolerationRestriction, Priority, ResourceQuota, SecurityContextDeny, ServiceAccount, StorageObjectInUseProtection, TaintNodesByCondition, ValidatingAdmissionWebhook. 6.enable-bootstrap-token-auth=true 启用此选项以允许 'kube-system' 命名空间中的 'bootstrap.kubernetes.io/token' 类型密钥可以被用于 TLS 的启动认证。 7.etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt 用于保护 etcd 通信的 SSL CA 文件 8.etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt 用于保护 etcd 通信的的 SSL 证书文件 9.etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key 用于保护 etcd 通信的 SSL 密钥文件 10.etcd-servers=https://127.0.0.1:2379 连接的 etcd 服务器列表 , 形式为（scheme://ip:port)，使用逗号分隔。 11.insecure-port=0 用于监听不安全和为认证访问的端口。这个配置假设你已经设置了防火墙规则，使得这个端口不能从集群外访问。对集群的公共地址的 443 端口的访问将被代理到这个端口。默认设置中使用 nginx 实现。（默认值 8080） 13.kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname 用于 kubelet 连接的首选 NodeAddressTypes 列表。 ( 默认值[Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]) 17.requestheader-allowed-names=front-proxy-client 使用 --requestheader-username-headers 指定的，允许在头部提供用户名的客户端证书通用名称列表。如果为空，任何通过 --requestheader-client-ca-file 中 authorities 验证的客户端证书都是被允许的。 19.requestheader-extra-headers-prefix=X-Remote-Extra- 用于检查的请求头的前缀列表。建议使用 X-Remote-Extra-。 20.requestheader-group-headers=X-Remote-Group 用于检查群组的请求头列表。建议使用 X-Remote-Group. 21.requestheader-username-headers=X-Remote-User 用于检查用户名的请求头列表。建议使用 X-Remote-User。 23.service-account-key-file=/etc/kubernetes/pki/sa.pub 包含 PEM 加密的 x509 RSA 或 ECDSA 私钥或公钥的文件，用于验证 ServiceAccount 令牌。如果设置该值，--tls-private-key-file 将会被使用。 指定的文件可以包含多个密钥，并且这个标志可以和不同的文件一起多次使用。 24.service-cluster-ip-range=20.96.0.0/12 CIDR 表示的 IP 范围，服务的 cluster ip 将从中分配。 一定不要和分配给 nodes 和 pods 的 IP 范围产生重叠。 Generic flags: --advertise-address ip The IP address on which to advertise the apiserver to members of the cluster. This address must be reachable by the rest of the cluster. If blank, the --bind-address will be used. If --bind-address is unspecified, the host's default interface will be used. # 向集群成员发布apiserver的IP地址，该地址必须能够被集群的成员访问。如果为空，则使用 --bind-address，如果 --bind-address未指定，那么使用主机的默认接口(IP)。 --cloud-provider-gce-lb-src-cidrs cidrs CIDRs opened in GCE firewall for LB traffic proxy & health checks (default 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16) # GCE防火墙中开放的L7 LB traffic proxy&health检查的CIDRs设置 # 默认值为130.211.0.0/22, 35.191.0.0/16 --cors-allowed-origins strings List of allowed origins for CORS, comma separated. An allowed origin can be a regular expression to support subdomain matching. If this list is empty CORS will not be enabled. # 跨域数组，使用逗号间隔。支持正则表达进行子域名匹配，如果参数为空，则不启用。 --default-not-ready-toleration-seconds int Indicates the tolerationSeconds of the toleration for notReady:NoExecute that is added by default to every pod that does not already have such a toleration. (default 300) # 指定notReady:NoExecute toleration的值，默认值为300 # 若pod中未为notReady:NoExecute toleration设定值，则使用该值 --default-unreachable-toleration-seconds int Indicates the tolerationSeconds of the toleration for unreachable:NoExecute that is added by default to every pod that does not already have such a toleration. (default 300) # 指定unreachable:NoExecute的tolerationSeconds值，默认值为300 # 若pod中未为unreachable:NoExecute tolerationSeconds设定值，则使用该值 --enable-inflight-quota-handler If true, replace the max-in-flight handler with an enhanced one that queues and dispatches with priority and fairness # 若为true，则以增强版的优先/公平列队及分发功能来替换max-in-flight handler --external-hostname string The hostname to use when generating externalized URLs for this master (e.g. Swagger API Docs). # 为此主机生成外部化URL时要使用的主机名（例如Swagger API文档） --feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental features. Options are: APIListChunking=true|false (BETA - default=true) APIResponseCompression=true|false (ALPHA - default=false) AllAlpha=true|false (ALPHA - default=false) AppArmor=true|false (BETA - default=true) AttachVolumeLimit=true|false (BETA - default=true) BalanceAttachedNodeVolumes=true|false (ALPHA - default=false) BlockVolume=true|false (BETA - default=true) BoundServiceAccountTokenVolume=true|false (ALPHA - default=false) CPUManager=true|false (BETA - default=true) CRIContainerLogRotation=true|false (BETA - default=true) CSIBlockVolume=true|false (BETA - default=true) CSIDriverRegistry=true|false (BETA - default=true) CSIInlineVolume=true|false (ALPHA - default=false) CSIMigration=true|false (ALPHA - default=false) CSIMigrationAWS=true|false (ALPHA - default=false) CSIMigrationAzureDisk=true|false (ALPHA - default=false) CSIMigrationAzureFile=true|false (ALPHA - default=false) CSIMigrationGCE=true|false (ALPHA - default=false) CSIMigrationOpenStack=true|false (ALPHA - default=false) CSINodeInfo=true|false (BETA - default=true) CustomCPUCFSQuotaPeriod=true|false (ALPHA - default=false) CustomResourceDefaulting=true|false (ALPHA - default=false) CustomResourcePublishOpenAPI=true|false (BETA - default=true) CustomResourceSubresources=true|false (BETA - default=true) CustomResourceValidation=true|false (BETA - default=true) CustomResourceWebhookConversion=true|false (BETA - default=true) DebugContainers=true|false (ALPHA - default=false) DevicePlugins=true|false (BETA - default=true) DryRun=true|false (BETA - default=true) DynamicAuditing=true|false (ALPHA - default=false) DynamicKubeletConfig=true|false (BETA - default=true) ExpandCSIVolumes=true|false (ALPHA - default=false) ExpandInUsePersistentVolumes=true|false (BETA - default=true) ExpandPersistentVolumes=true|false (BETA - default=true) ExperimentalCriticalPodAnnotation=true|false (ALPHA - default=false) ExperimentalHostUserNamespaceDefaulting=true|false (BETA - default=false) HyperVContainer=true|false (ALPHA - default=false) KubeletPodResources=true|false (BETA - default=true) LocalStorageCapacityIsolation=true|false (BETA - default=true) LocalStorageCapacityIsolationFSQuotaMonitoring=true|false (ALPHA - default=false) MountContainers=true|false (ALPHA - default=false) NodeLease=true|false (BETA - default=true) NonPreemptingPriority=true|false (ALPHA - default=false) PodShareProcessNamespace=true|false (BETA - default=true) ProcMountType=true|false (ALPHA - default=false) QOSReserved=true|false (ALPHA - default=false) RemainingItemCount=true|false (ALPHA - default=false) RequestManagement=true|false (ALPHA - default=false) ResourceLimitsPriorityFunction=true|false (ALPHA - default=false) ResourceQuotaScopeSelectors=true|false (BETA - default=true) RotateKubeletClientCertificate=true|false (BETA - default=true) RotateKubeletServerCertificate=true|false (BETA - default=true) RunAsGroup=true|false (BETA - default=true) RuntimeClass=true|false (BETA - default=true) SCTPSupport=true|false (ALPHA - default=false) ScheduleDaemonSetPods=true|false (BETA - default=true) ServerSideApply=true|false (ALPHA - default=false) ServiceLoadBalancerFinalizer=true|false (ALPHA - default=false) ServiceNodeExclusion=true|false (ALPHA - default=false) StorageVersionHash=true|false (BETA - default=true) StreamingProxyRedirects=true|false (BETA - default=true) SupportNodePidsLimit=true|false (BETA - default=true) SupportPodPidsLimit=true|false (BETA - default=true) Sysctls=true|false (BETA - default=true) TTLAfterFinished=true|false (ALPHA - default=false) TaintBasedEvictions=true|false (BETA - default=true) TaintNodesByCondition=true|false (BETA - default=true) TokenRequest=true|false (BETA - default=true) TokenRequestProjection=true|false (BETA - default=true) ValidateProxyRedirects=true|false (BETA - default=true) VolumePVCDataSource=true|false (ALPHA - default=false) VolumeSnapshotDataSource=true|false (ALPHA - default=false) VolumeSubpathEnvExpansion=true|false (BETA - default=true) WatchBookmark=true|false (ALPHA - default=false) WinDSR=true|false (ALPHA - default=false) WinOverlay=true|false (ALPHA - default=false) WindowsGMSA=true|false (ALPHA - default=false) # 特性门控， [具体参考连接](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/) # Alpha,Beta 特性代表：默认禁用。 # General Availability (GA) 特性也称为稳定特性,此特性会一直启用；你不能禁用它。 --master-service-namespace string DEPRECATED: the namespace from which the kubernetes master services should be injected into pods. (default \"default\") # --namespace 默认命名空间设置， 实验为生效 --max-mutating-requests-inflight int The maximum number of mutating requests in flight at a given time. When the server exceeds this, it rejects requests. Zero for no limit. (default 200) # 给定时间内变更请求队列（mutating requests in flight）的最大值，默认值为200 # 当服务器超过该值时，将拒绝请求 # 0为没有限制 --max-requests-inflight int The maximum number of non-mutating requests in flight at a given time. When the server exceeds this, it rejects requests. Zero for no limit. (default 400) # 给定时间内的非变更请求队列（non-mutating requests inflight）的最大值，默认值为400 # 当服务器超过该值，将拒绝请求 # 0为没有限制 --min-request-timeout int An optional field indicating the minimum number of seconds a handler must keep a request open before timing it out. Currently only honored by the watch request handler, which picks a randomized value above this number as the connection timeout, to spread out load. (default 1800) # 可选，默认值为1800 # 表示在一个请求超时之前，handler必须保持该请求打开状态的最小秒数 # 当前仅被watch request handler支持，handler将随机数设为链接的超时值，以保持负荷 --request-timeout duration An optional field indicating the duration a handler must keep a request open before timing it out. This is the default request timeout for requests but may be overridden by flags such as --min-request-timeout for specific types of requests. (default 1m0s) # 可选参数 # 一个请求超时前handler保持该请求打开状态的持续时间，默认值为10s # 该参数为默认的请求超时时间，但可能会被其他参数覆盖，比如--min-request-timeout --target-ram-mb int Memory limit for apiserver in MB (used to configure sizes of caches, etc.) # 限制apiserver使用的内存大小，单位MB，（用来配置缓存大小等等） Etcd flags: --default-watch-cache-size int Default watch cache size. If zero, watch cache will be disabled for resources that do not have a default watch size set. (default 100) # 默认的watch缓存大小，默认值为100 # 若为0，未设置默认watch size的资源的watch缓存将关闭 --delete-collection-workers int Number of workers spawned for DeleteCollection call. These are used to speed up namespace cleanup. (default 1) # 设定调用DeleteCollection的worker数量，workers被用来加速清理namespace # 默认值为1 --enable-garbage-collector Enables the generic garbage collector. MUST be synced with the corresponding flag of the kube-controller-manager. (default true) # 是否启用通用garbage collector，默认值为true # 必须与kube-controller-manager对应参数一致 --encryption-provider-config string The file containing configuration for encryption providers to be used for storing secrets in etcd # 存储secrets到etcd内的encryption provider的配置文件路径 --etcd-cafile string SSL Certificate Authority file used to secure etcd communication. # etcd 授权文件 --etcd-certfile string SSL certification file used to secure etcd communication. # etcd 证书文件 --etcd-compaction-interval duration The interval of compaction requests. If 0, the compaction request from apiserver is disabled. (default 5m0s) # 压缩请求间隔，默认值为5m0s # 若为0，则关闭API Server的压缩请求 --etcd-count-metric-poll-period duration Frequency of polling etcd for number of resources per type. 0 disables the metric collection. (default 1m0s) # 调查etcd中每种资源的数量的频率，默认值为1m0s # 若为0，则关闭metric的收集 --etcd-keyfile string SSL key file used to secure etcd communication. # etch 证书key文件 --etcd-prefix string The prefix to prepend to all resource paths in etcd. (default \"/registry\") # 设定etcd中所有资源路径的前缀 # 默认值为/registry --etcd-servers strings List of etcd servers to connect with (scheme://ip:port), comma separated. # 设定etcd server列表（scheme://ip:port），以逗号分隔 --etcd-servers-overrides strings Per-resource etcd servers overrides, comma separated. The individual override format: group/resource#servers, where servers are URLs, semicolon separated. # 以每种资源来重写（分隔？）etcd server，以逗号分隔 # 单个重写格式：group/resorce#servers，servers需为URLs，以分号分隔 --storage-backend string The storage backend for persistence. Options: 'etcd3' (default). # 持久化存储后端的名称，可选项：etcd3（默认） --storage-media-type string The media type to use to store objects in storage. Some resources or storage backends may only support a specific media type and will ignore this setting. (default \"application/vnd.kubernetes.protobuf\") # 仓库中存储对象的媒介类型，默认值为application/vnd.kubernetes.protobuf # 某些资源或存储后端可能只支持特定的媒介类型，将忽略此参数设定 --watch-cache Enable watch caching in the apiserver (default true) --watch-cache-sizes strings Watch cache size settings for some resources (pods, nodes, etc.), comma separated. The individual setting format: resource[.group]#size, where resource is lowercase plural (no version), group is omitted for resources of apiVersion v1 (the legacy core API) and included for others, and size is a number. It takes effect when watch-cache is enabled. Some resources (replicationcontrollers, endpoints, nodes, pods, services, apiservices.apiregistration.k8s.io) have system defaults set by heuristics, others default to default-watch-cache-size # 根据资源类型设定watch缓存，以逗号分隔 # 设置格式resource[.group]#size，资源类型需小写（无版本号），apiVersionV1（legacy core API）可省略group，其他apiVersion不可省略group，size为数字。 # 当--watch-cache开启时生效 # 某些资源（replicationcontrollers, endpoints, nodes, pods, services, apiservices.apiregistration.k8s.io）具有默认的系统设置，其他资源以--default-watch-cache-size为默认值 Secure serving flags: --bind-address ip The IP address on which to listen for the --secure-port port. The associated interface(s) must be reachable by the rest of the cluster, and by CLI/web clients. If blank, all interfaces will be used (0.0.0.0 for all IPv4 interfaces and :: for all IPv6 interfaces). (default 0.0.0.0) # 监听--secure-port端口的IP地址 # 该接口必须被集群中的其他成员、CLI/web客户端可达 # 如果为空，则所有接口将默认使用0.0.0.0(IPV4), ::(IPV6) --cert-dir string The directory where the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. (default \"/var/run/kubernetes\") # TLS certs文件路径 # 默认值为/var/run/kubernetes # 若已设置--tls-cert-file和--tls-private-key-file，该参数将被忽略 --http2-max-streams-per-connection int The limit that the server gives to clients for the maximum number of streams in an HTTP/2 connection. Zero means to use golang's default. # 服务器提供给客户端的最大HTTP/2连接流数量限制 # 0表示使用golang默认值 --secure-port int The port on which to serve HTTPS with authentication and authorization.It cannot be switched off with 0. (default 6443) # https安全端口号，默认值为6443 # 无法通过设为0来关闭此端口 --tls-cert-file string File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory specified by --cert-dir. # https 证书 --tls-cipher-suites strings Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be use. Possible values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_RC4_128_SHA # 服务器的密码套件的逗号分隔列表。 如果省略，将使用默认的Go密码套件。 可能的值： --tls-min-version string Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13 # 支持的最低TLS版本。 可能的值：VersionTLS10，VersionTLS11，VersionTLS12，VersionTLS13 --tls-private-key-file string File containing the default x509 private key matching --tls-cert-file. # 与--tls-cert-file匹配的x509私钥文件路径 --tls-sni-cert-key namedCertKey A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: \"example.crt,example.key\" or \"foo.crt,foo.key:*.foo.com,foo.com\". (default []) # 含有x509证书和私钥对的文件路径，可以以FQDN格式的域名列表作为后缀（支持以通配符为前缀），若未提供域名，将提取证书的文件名 # 优先无通配符的配对而非有通配符的配对，优先有明确的域名格式而非提取的文件名 # 若有多个key/cerficate对，可设置多个--tls-sni-cert-key # 例如：\"example.crt,example.key\" or \"foo.crt,foo.key:*.foo.com,foo.com\" Insecure serving flags: --address ip The IP address on which to serve the insecure --port (set to 0.0.0.0 for all IPv4 interfaces and :: for all IPv6 interfaces). (default 127.0.0.1) (DEPRECATED: see --bind-address instead.) # 安全的ip地址服务 --insecure-bind-address ip The IP address on which to serve the --insecure-port (set to 0.0.0.0 for all IPv4 interfaces and :: for all IPv6 interfaces). (default 127.0.0.1) (DEPRECATED: This flag will be removed in a future version.) # 绑定不安全的ip地址服务 --insecure-port int The port on which to serve unsecured, unauthenticated access. (default 8080) (DEPRECATED: This flag will be removed in a future version.) # 不安全的端口暴露 --port int The port on which to serve unsecured, unauthenticated access. Set to 0 to disable. (default 8080) (DEPRECATED: see --secure-port instead.) # 安全的端口暴露 Auditing flags: --audit-dynamic-configuration Enables dynamic audit configuration. This feature also requires the DynamicAuditing feature flag # 是否启用dynamic audit configuration # 该参数需要先设置DynamicAuditing参数 --audit-log-batch-buffer-size int The size of the buffer to store events before batching and writing. Only used in batch mode. (default 10000) # 批量写入audit-log前存储的事件的缓存大小，默认值10000 # 仅batch模式可用 --audit-log-batch-max-size int The maximum size of a batch. Only used in batch mode. (default 1) # audit-log-batch的最大值，默认值为1 # 仅batch模式可用 --audit-log-batch-max-wait duration The amount of time to wait before force writing the batch that hadn't reached the max size. Only used in batch mode. # 若一直未达到audit-log-batch的最大值，强制写入的等待时间周期 # 仅在batch模式可用 --audit-log-batch-throttle-burst int Maximum number of requests sent at the same moment if ThrottleQPS was not utilized before. Only used in batch mode. # 在ThrottleQPS未被使用前，同一时间可发送的请求的最大值 # 仅batch模式可用 --audit-log-batch-throttle-enable Whether batching throttling is enabled. Only used in batch mode. # 打开audit-log-batch的throttle模式 # 仅batch模式可用 --audit-log-batch-throttle-qps float32 Maximum average number of batches per second. Only used in batch mode. # 设置audit-log-batch的throttle QPS值 # 仅batch模式可用 --audit-log-format string Format of saved audits. \"legacy\" indicates 1-line text format for each event. \"json\" indicates structured json format. Known formats are legacy,json. (default \"json\") # audit-log的文件保存格式，默认值为json # \"legacy\"为一个事件一行 # \"json\"为json结构格式 # 现有值为legacy和json --audit-log-maxage int The maximum number of days to retain old audit log files based on the timestamp encoded in their filename. # audit-log的最大保留天数，以文件名中的时间戳为基础计算 --audit-log-maxbackup int The maximum number of old audit log files to retain. # audit-log文件的最大保留数量 --audit-log-maxsize int The maximum size in megabytes of the audit log file before it gets rotated. # rotated前单个audit-log文件的最大值，单位megabytes --audit-log-mode string Strategy for sending audit events. Blocking indicates sending events should block server responses. Batch causes the backend to buffer and write events asynchronously. Known modes are batch,blocking,blocking-strict. (default \"blocking\") # 发送audit-log事件的策略 # blocking为发送事件时阻塞服务器响应 # batch为backend异步缓冲和写事件 # 现有模式为batch, blocking, blocking-stric --audit-log-path string If set, all requests coming to the apiserver will be logged to this file. '-' means standard out. # audit-log文件的保存路径 # '-'表示stdout --audit-log-truncate-enabled Whether event and batch truncating is enabled. # 是否打开event和batch的截断功能 --audit-log-truncate-max-batch-size int Maximum size of the batch sent to the underlying backend. Actual serialized size can be several hundreds of bytes greater. If a batch exceeds this limit, it is split into several batches of smaller size. (default 10485760) # 发送到底层后端的单个audit-log batch的最大值。默认值为10485760 # 序列化后的大小可能会多几百字节。如果一个batch超过该限制，将会被分割成几个较小的batch --audit-log-truncate-max-event-size int Maximum size of the audit event sent to the underlying backend. If the size of an event is greater than this number, first request and response are removed, and if this doesn't reduce the size enough, event is discarded. (default 102400) # 发送到底层后端的单个audit event的最大值，默认值为102400 # 如果event大于该值，首个请求和响应将被移除，如果仍超过最大值，该event将被丢弃。 --audit-log-version string API group and version used for serializing audit events written to log. (default \"audit.k8s.io/v1\") # 指定写入log的序列化audit event的API group和version # 默认值为\"audit.k8s.io/v1\" --audit-policy-file string Path to the file that defines the audit policy configuration. # audit policy的配置文件路径 --audit-webhook-batch-buffer-size int The size of the buffer to store events before batching and writing. Only used in batch mode. (default 10000) # 在batching和writing之前保存audit-webhook event的最大缓存值，默认值为10000 # 仅batch模式可用 --audit-webhook-batch-max-size int The maximum size of a batch. Only used in batch mode. (default 400) # 单个audit-webhook batch的最大值，默认值为400 # 仅batch模式可用 --audit-webhook-batch-max-wait duration The amount of time to wait before force writing the batch that hadn't reached the max size. Only used in batch mode. (default 30s) # 强制写入未达到最大size的batch的等待时间周期，默认值为30s # 仅batch模式可用 --audit-webhook-batch-throttle-burst int Maximum number of requests sent at the same moment if ThrottleQPS was not utilized before. Only used in batch mode. (default 15) # ThrottleQPS未被使用时，同时发送请求的最大数量，默认值为15 # 仅batch模式可用 --audit-webhook-batch-throttle-enable Whether batching throttling is enabled. Only used in batch mode. (default true) # 是否开启batching throttling，默认值为true # 仅batch模式可用 --audit-webhook-batch-throttle-qps float32 Maximum average number of batches per second. Only used in batch mode. (default 10) # 每秒batch的最大平均数量，默认值为10 # 仅batch模式可用 --audit-webhook-config-file string Path to a kubeconfig formatted file that defines the audit webhook configuration. # audit webhook的配置文件路径（kubeconfig格式） --audit-webhook-initial-backoff duration The amount of time to wait before retrying the first failed request. (default 10s) # 首次请求失败后，重试的等待时间，默认值为10s --audit-webhook-mode string Strategy for sending audit events. Blocking indicates sending events should block server responses. Batch causes the backend to buffer and write events asynchronously. Known modes are batch,blocking,blocking-strict. (default \"batch\") # audit-webhook的策略模式 # blocking为发送event将阻塞服务器响应 # batch为后端异步缓冲和写event # 已有模式为batch, blocking, blocking-strict --audit-webhook-truncate-enabled Whether event and batch truncating is enabled. # 是否打开audit-webhook event和batch的截断模式 --audit-webhook-truncate-max-batch-size int Maximum size of the batch sent to the underlying backend. Actual serialized size can be several hundreds of bytes greater. If a batch exceeds this limit, it is split into several batches of smaller size. (default 10485760) # 发送到底层后端的audit-webhook batch的最大值，默认值为10485760 # 序列化后的大小可能会超过数百字节，如果一个batch超过该限制，则会被分割成几个较小的batch --audit-webhook-truncate-max-event-size int Maximum size of the audit event sent to the underlying backend. If the size of an event is greater than this number, first request and response are removed, and if this doesn't reduce the size enough, event is discarded. (default 102400) # 发送到底层后端的单个audit-webhook event的最大值，默认值为102400 # 若某个event大于该数值，首个请求和响应将被移除，若仍大于该数值，则该event将被丢弃 --audit-webhook-version string API group and version used for serializing audit events written to webhook. (default \"audit.k8s.io/v1\") --audit-webhook-version-string # 写入webhook的序列化audit event的API group和version # 默认值为\"audit.k8s.io/v1\" Features flags: --contention-profiling Enable lock contention profiling, if profiling is enabled # 若分析已启用，则启用锁定竞争分析 # 不太明白，需研究 --profiling Enable profiling via web interface host:port/debug/pprof/ (default true) # 开启web界面的分析功能，默认值为true # 访问地址host:port/debug/pprof/ Authentication flags: --anonymous-auth Enables anonymous requests to the secure port of the API server. Requests that are not rejected by another authentication method are treated as anonymous requests. Anonymous requests have a username of system:anonymous, and a group name of system:unauthenticated. (default true) # 是否允许对API Server安全端口的匿名请求，未被其他身份验证方法拒绝的请求将被视为匿名请求 # 匿名请求具有system username：anonymous，system group name：unauthenticated # 默认值：true --api-audiences strings Identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. If the --service-account-issuer flag is configured and this flag is not, this field defaults to a single element list containing the issuer URL . # API的标识符列表。service account token验证器将验证token使用的API绑定了至少一个标识符列表中的值。 # 如果启动了service-account-issuer参数，但api-audiences未设置，则该字段默认值为包含service-account-issuer的单元素列表 --authentication-token-webhook-cache-ttl duration The duration to cache responses from the webhook token authenticator. (default 2m0s) # webhook token验证器的缓存响应时间，默认值为2m0s --authentication-token-webhook-config-file string File with webhook configuration for token authentication in kubeconfig format. The API server will query the remote service to determine authentication for bearer tokens. # token认证的webhook配置文件（kubeconfig格式） # API server将查询远程服务以确定不记名（bearer）token的认证结果 --basic-auth-file string If set, the file that will be used to admit requests to the secure port of the API server via http basic authentication. --client-ca-file string If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. --enable-bootstrap-token-auth Enable to allow secrets of type 'bootstrap.kubernetes.io/token' in the 'kube-system' namespace to be used for TLS bootstrapping authentication. # 是否启用TLS bootstrap认证模式 --oidc-ca-file string If set, the OpenID server's certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host's root CA set will be used. # OpenID server的ca文件路径 # 若不设置，将使用本机的root ca设置 --oidc-client-id string The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set. # OpenID Connect客户端ID # 当已设置--oidc-issuer-url时，必须设置该参数 --oidc-groups-claim string If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details. # 指定OpenID的user groups # 该值需为字符串或字符串数组 # 该参数为试验性质，请查阅authentication文档获取更多详情 --oidc-groups-prefix string If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies. # 设定OpenID组的前缀，以便与其他认证策略区分 --oidc-issuer-url string The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT). # OpenID issuer的URL，仅接受https # 若设置，将被用来确认OIDC JWT（Json Web Token） --oidc-required-claim mapStringString A key=value pair that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value. Repeat this flag to specify multiple claims. # 以键值对描述的ID Token所需的断言 # 该断言将被用来确认ID Token是否匹配 # 可多次设置该参数来指定多个断言 --oidc-signing-algs strings Comma-separated list of allowed JOSE asymmetric signing algorithms. JWTs with a 'alg' header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1. (default [RS256]) # 可接受的JOSE非对称签名算法列表，以逗号分隔 # 默认值为RS256 # 若JWTs head中的alg字段值不在该列表中，将拒绝该JWT # 可使用的值请参考RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1 --oidc-username-claim string The OpenID claim to use as the user name. Note that claims other than the default ('sub') is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \"sub\") # OpenID的用户名，默认值为sub # 请注意，不保证除sub以外的断言值的唯一性和不可变性 # 该参数为试验性，请查阅authentication文档获取更多详情 --oidc-username-prefix string If provided, all usernames will be prefixed with this value. If not provided, username claims other than 'email' are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value '-'. # OpenID的默认用户名前缀 # 若不设置，除email外的所有用户名将默认以issuer URL开头以避免冲突 # 若设置为'-'，将取消所有默认前缀 --requestheader-allowed-names strings List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed. # 客户端证书的common names列表，--requestheader-username-headers参数中指定可用的用户名 # 若为空，将允许所有可被--requestheader-client-ca-file中ca验证的客户端证书 --requestheader-client-ca-file string Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers. WARNING: generally do not depend on authorization being already done for incoming requests. # 在信任--requestheader-username-headers参数中指定的username之前，用来验证客户端证书的传入请求的ca文件。 # 一般不依赖于已验证过的传入请求 --requestheader-extra-headers-prefix strings List of request header prefixes to inspect. X-Remote-Extra- is suggested. # 需注入到请求头的前缀 # 建议设为X-Remote-Extra- --requestheader-group-headers strings List of request headers to inspect for groups. X-Remote-Group is suggested. # 需注入到请求头中的Group前缀 # 建议设为X-Remote-Group --requestheader-username-headers strings List of request headers to inspect for usernames. X-Remote-User is common. # 需注入到请求头中的username前缀 # 一般为X-Remote-User --service-account-issuer string Identifier of the service account token issuer. The issuer will assert this identifier in \"iss\" claim of issued tokens. This value is a string or URI. # 指定service account token issuer的标识符 # 该issuer将在iss声明中分发Token以使标识符生效 # 该参数的值为字符串或URL --service-account-key-file stringArray File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If unspecified, --tls-private-key-file is used. Must be specified when --service-account-signing-key is provided # 验证ServiceAccount Token的私钥或公钥文件，以x509 RSA或ECDSA PEM编码 # 指定的文件可以包含多个key # 可以多次设置该参数以指定不同的文件 # 若未设置，将使用--tls-private-key-file # 当设置了--service-account-signing-key参数时，必须设置该参数 --service-account-lookup If true, validate ServiceAccount tokens exist in etcd as part of authentication. (default true) # 若true，将验证etcd中是否存在对应的ServiceAccount Token作为认证的一部分 # 默认值为true --service-account-max-token-expiration duration The maximum validity duration of a token created by the service account token issuer. If an otherwise valid TokenRequest with a validity duration larger than this value is requested, a token will be issued with a validity duration of this value. # service account token issuer创建的token的最大有效逾期时间 # 当有以其他形式创建的TokenRequest，逾期时间超过该值，此token的有效逾期时间将被设为该值 --token-auth-file string If set, the file that will be used to secure the secure port of the API server via token authentication. # 若设置，token authentication将使用指定的文件加密API server的加密端口 Authorization flags: --authorization-mode strings Ordered list of plug-ins to do authorization on secure port. Comma-delimited list of: AlwaysAllow,AlwaysDeny,ABAC,Webhook,RBAC,Node. (default [AlwaysAllow]) # 通过安全端口认证模式的插件列表 # 默认值AlwaysAllow # 现有列表（以逗号分隔）：AlwaysAllow,AlwaysDeny,ABAC,Webhook,RBAC,Node --authorization-policy-file string File with authorization policy in json line by line format, used with --authorization-mode=ABAC, on the secure port. # authorization-policy配置文件（json line by line格式） # --authorization-mode=ABAC且在安全端口上时需设置 --authorization-webhook-cache-authorized-ttl duration The duration to cache 'authorized' responses from the webhook authorizer. (default 5m0s) # 缓存来自webhook认证器'authorized'响应的持续时间，默认值为5m0s --authorization-webhook-cache-unauthorized-ttl duration The duration to cache 'unauthorized' responses from the webhook authorizer. (default 30s) # 缓存来自webhook认证器'unauthorized'响应的持续时间，默认值为30s --authorization-webhook-config-file string File with webhook configuration in kubeconfig format, used with --authorization-mode=Webhook. The API server will query the remote service to determine access on the API server's secure port. # webhook认证的配置文件路径（kubeconfig格式） # --authorization-mode=Webhook时需设置 # API server将查询远程服务以确认API server安全端口的使用权限 Cloud provider flags: --cloud-config string The path to the cloud provider configuration file. Empty string for no configuration file. # 云服务商配置文件 --cloud-provider string The provider for cloud services. Empty string for no provider. # 云服务商 Api enablement flags: --runtime-config mapStringString A set of key=value pairs that describe runtime configuration that may be passed to apiserver. / (or for the core group) key can be used to turn on/off specific api versions. api/all is special key to control all api versions, be careful setting it false, unless you know what you do. api/legacy is deprecated, we will remove it in the future, so stop using it. # 以键值对形式开启或关闭内建的APIs # 支持的选项如下： v1=true|false for the core API group /=true|false for a specific API group and version (e.g. apps/v1=true) api/all=true|false controls all API versions api/ga=true|false controls all API versions of the form v[0-9]+ api/beta=true|false controls all API versions of the form v[0-9]+beta[0-9]+ api/alpha=true|false controls all API versions of the form v[0-9]+alpha[0-9]+ api/legacy 已废弃，将在未来版本中移除 Admission flags: --admission-control strings Admission is divided into two phases. In the first phase, only mutating admission plugins run. In the second phase, only validating admission plugins run. The names in the below list may represent a validating plugin, a mutating plugin, or both. The order of plugins in which they are passed to this flag does not matter. Comma-delimited list of: AlwaysAdmit, AlwaysDeny, AlwaysPullImages, DefaultStorageClass, DefaultTolerationSeconds, DenyEscalatingExec, DenyExecOnPrivileged, EventRateLimit, ExtendedResourceToleration, ImagePolicyWebhook, LimitPodHardAntiAffinityTopology, LimitRanger, MutatingAdmissionWebhook, NamespaceAutoProvision, NamespaceExists, NamespaceLifecycle, NodeRestriction, OwnerReferencesPermissionEnforcement, PersistentVolumeClaimResize, PersistentVolumeLabel, PodNodeSelector, PodPreset, PodSecurityPolicy, PodTolerationRestriction, Priority, ResourceQuota, SecurityContextDeny, ServiceAccount, StorageObjectInUseProtection, TaintNodesByCondition, ValidatingAdmissionWebhook. (DEPRECATED: Use --enable-admission-plugins or --disable-admission-plugins instead. Will be removed in a future version.) # 必须关闭的admission插件列表（即使已默认开启） # 默认开启的插件列表：NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, RuntimeClass, ResourceQuota # 可关闭的插件列表（以逗号隔开，排名不分先后）：AlwaysAdmit, AlwaysDeny, AlwaysPullImages, DefaultStorageClass, DefaultTolerationSeconds,..... # 指定admission control configuration文件的路径 --disable-admission-plugins strings admission plugins that should be disabled although they are in the default enabled plugins list (NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, ResourceQuota). Comma-delimited list of admission plugins: AlwaysAdmit, AlwaysDeny, AlwaysPullImages, DefaultStorageClass, DefaultTolerationSeconds, DenyEscalatingExec, DenyExecOnPrivileged, EventRateLimit, ExtendedResourceToleration, ImagePolicyWebhook, LimitPodHardAntiAffinityTopology, LimitRanger, MutatingAdmissionWebhook, NamespaceAutoProvision, NamespaceExists, NamespaceLifecycle, NodeRestriction, OwnerReferencesPermissionEnforcement, PersistentVolumeClaimResize, PersistentVolumeLabel, PodNodeSelector, PodPreset, PodSecurityPolicy, PodTolerationRestriction, Priority, ResourceQuota, SecurityContextDeny, ServiceAccount, StorageObjectInUseProtection, TaintNodesByCondition, ValidatingAdmissionWebhook. The order of plugins in this flag does not matter. # 去激活插件 --enable-admission-plugins strings admission plugins that should be enabled in addition to default enabled ones (NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, ResourceQuota). Comma-delimited list of admission plugins: AlwaysAdmit, AlwaysDeny, AlwaysPullImages, DefaultStorageClass, DefaultTolerationSeconds, DenyEscalatingExec, DenyExecOnPrivileged, EventRateLimit, ExtendedResourceToleration, ImagePolicyWebhook, LimitPodHardAntiAffinityTopology, LimitRanger, MutatingAdmissionWebhook, NamespaceAutoProvision, NamespaceExists, NamespaceLifecycle, NodeRestriction, OwnerReferencesPermissionEnforcement, PersistentVolumeClaimResize, PersistentVolumeLabel, PodNodeSelector, PodPreset, PodSecurityPolicy, PodTolerationRestriction, Priority, ResourceQuota, SecurityContextDeny, ServiceAccount, StorageObjectInUseProtection, TaintNodesByCondition, ValidatingAdmissionWebhook. The order of plugins in this flag does not matter. # 激活插件 Misc flags: --allow-privileged If true, allow privileged containers. [default=false] # 是否允许特权容器（privileged container），默认false --apiserver-count int The number of apiservers running in the cluster, must be a positive number. (In use when --endpoint-reconciler-type=master-count is enabled.) (default 1) # endpoint reconciler模式，默认值为lease # 可用模式：master-count, lease, none --enable-aggregator-routing Turns on aggregator routing requests to endpoints IP rather than cluster IP. # 是否启用aggregator routing请求到endpoints IP而非集群IP --endpoint-reconciler-type string Use an endpoint reconciler (master-count, lease, none) (default \"lease\") --event-ttl duration Amount of time to retain events. (default 1h0m0s) # events的保留时间，默认值为1h0m0s --kubelet-certificate-authority string Path to a cert file for the certificate authority. # kubelet certificate authority的cert文件路径 --kubelet-client-certificate string Path to a client cert file for TLS. # 客户端（kubelet）TLS通讯的cert文件路径 --kubelet-client-key string Path to a client key file for TLS. # 客户端（kubelet）TLS通讯的key文件路径 --kubelet-https Use https for kubelet connections. (default true) # 是否启用kubelet的https链接，默认值为true --kubelet-preferred-address-types strings List of the preferred NodeAddressTypes to use for kubelet connections. (default [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]) # kubelet连接的NodeAddressType优先级列表（优先访问哪个地址） # 默认值为Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalI --kubelet-read-only-port uint DEPRECATED: kubelet port. (default 10255) --kubelet-timeout duration Timeout for kubelet operations. (default 5s) # kubelet操作的超时时间，默认值为5s --kubernetes-service-node-port int If non-zero, the Kubernetes master service (which apiserver creates/maintains) will be of type NodePort, using this as the value of the port. If zero, the Kubernetes master service will be of type ClusterIP. # 若不为0，kubernetes master service(apiserver所在主机)将以NodePort形式暴露（使用该值为端口号） # 若为0，kubernetes master service将以ClusterIP形式暴露 --max-connection-bytes-per-sec int If non-zero, throttle each user connection to this number of bytes/sec. Currently only applies to long-running requests. --proxy-client-cert-file string Client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. It is expected that this cert includes a signature from the CA in the --requestheader-client-ca-file flag. That CA is published in the 'extension-apiserver-authentication' configmap in the kube-system namespace. Components receiving calls from kube-aggregator should use that CA to perform their half of the mutual TLS verification. # 客户端代理的cert文件路径 # 当需要在请求中调用aggregator或kube-apiserver时使用 # 包括代理请求到api-server用户和调用webhook admission插件 # 该cert文件必须包含--requestheader-client-ca-file参数中指定的CA签名，该CA签名保存在kube-system namespace下的‘extension-apiserver-authentication’ configmap中 # 当kube-aggregator调用其他组件时，此CA签名提供一半的TLS相互身份验证信息（half of the mutual TLS verfication） --proxy-client-key-file string Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. # 客户端代理的私钥文件 # 当需要在请求中调用aggregator或kube-apiserver时使用 # 包括代理请求到api-server用户和调用webhook admission插件 --service-account-signing-key-file string Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the 'TokenRequest' feature gate.) # 含有当前service account token issuer私钥的文件路径 # 该issuer将以此私钥签名发布的ID token（需开启TokenRequest特性） --service-cluster-ip-range ipNet A CIDR notation IP range from which to assign service cluster IPs. This must not overlap with any IP ranges assigned to nodes for pods. (default 10.0.0.0/24) # 以CIDR格式声明可分配的集群服务IP范围，不可与分配Pods的nodes IP地址范围重叠 --service-node-port-range portRange A port range to reserve for services with NodePort visibility. Example: '30000-32767'. Inclusive at both ends of the range. (default 30000-32767) # 保留的以NodePort类型暴露的服务的端口范围，默认值为30000-32767，左闭右闭 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubernetes client-go 源码阅读.html":{"url":"blog/kubernetes/kubernetes client-go 源码阅读.html","title":"Kubernetes Client Go 源码阅读","keywords":"","body":"source code reading 关键结构体用途说明 Informer 一个store 和 controller Store 是一个对象存储和处理的接口，一个store拥有一个字典，还拥有增、删、改的接口。其中Reflector知道如何观察一个服务器并且更新一个Store。 Reflector： 观察一个指定的资源，并且使所有的更改都反应到这个存储中 internalinterfaces.factory_interface.go // NewInformerFunc takes kubernetes.Interface and time.Duration to return a SharedIndexInformer. type NewInformerFunc func(kubernetes.Interface, time.Duration) cache.SharedIndexInformer // SharedInformerFactory a small interface to allow for adding an informer without an import cycle type SharedInformerFactory interface { Start(stopCh // SharedIndexInformer provides add and get Indexers ability based on SharedInformer. type SharedIndexInformer interface { SharedInformer // AddIndexers add indexers to the informer before it starts. AddIndexers(indexers Indexers) error GetIndexer() Indexer } type SharedInformer interface { AddEventHandler(handler ResourceEventHandler) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) GetStore() Store GetController() Controller Run(stopCh Core.Interface package core import ( v1 \"k8s.io/client-go/informers/core/v1\" internalinterfaces \"k8s.io/client-go/informers/internalinterfaces\" ) // Interface provides access to each of this group's versions. type Interface interface { // V1 provides access to shared informers for resources in V1. V1() v1.Interface } type group struct { factory internalinterfaces.SharedInformerFactory namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc } // New returns a new Interface. func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) Interface { return &group{factory: f, namespace: namespace, tweakListOptions: tweakListOptions} } // V1 returns a new v1.Interface. func (g *group) V1() v1.Interface { return v1.New(g.factory, g.namespace, g.tweakListOptions) } v1.Pods package v1 import ( \"context\" time \"time\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" runtime \"k8s.io/apimachinery/pkg/runtime\" watch \"k8s.io/apimachinery/pkg/watch\" internalinterfaces \"k8s.io/client-go/informers/internalinterfaces\" kubernetes \"k8s.io/client-go/kubernetes\" v1 \"k8s.io/client-go/listers/core/v1\" cache \"k8s.io/client-go/tools/cache\" ) // PodInformer provides access to a shared informer and lister for // Pods. type PodInformer interface { Informer() cache.SharedIndexInformer Lister() v1.PodLister } type podInformer struct { factory internalinterfaces.SharedInformerFactory tweakListOptions internalinterfaces.TweakListOptionsFunc namespace string } // NewPodInformer constructs a new informer for Pod type. // Always prefer using an informer factory to get a shared informer instead of getting an independent // one. This reduces memory footprint and number of connections to the server. func NewPodInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer { return NewFilteredPodInformer(client, namespace, resyncPeriod, indexers, nil) } // NewFilteredPodInformer constructs a new informer for Pod type. // Always prefer using an informer factory to get a shared informer instead of getting an independent // one. This reduces memory footprint and number of connections to the server. func NewFilteredPodInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer { return cache.NewSharedIndexInformer( &cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { if tweakListOptions != nil { tweakListOptions(&options) } return client.CoreV1().Pods(namespace).List(context.TODO(), options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { if tweakListOptions != nil { tweakListOptions(&options) } return client.CoreV1().Pods(namespace).Watch(context.TODO(), options) }, }, &corev1.Pod{}, resyncPeriod, indexers, ) } func (f *podInformer) defaultInformer(client kubernetes.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer { return NewFilteredPodInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions) } func (f *podInformer) Informer() cache.SharedIndexInformer { return f.factory.InformerFor(&corev1.Pod{}, f.defaultInformer) } func (f *podInformer) Lister() v1.PodLister { return v1.NewPodLister(f.Informer().GetIndexer()) } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubernetes-client.html":{"url":"blog/kubernetes/kubernetes-client.html","title":"Kubernetes Client","keywords":"","body":"k8s client 使用随笔 1. k8s.io/client-go client-go 项目有4种类型的客户端 RestClient rest.RESTClient RESTClient是最基础的客户端RESTClient对HTTP Request进行了封装，实现了RESTful风格的API。 ClientSet，DynamicClient，DiscoveryClient客户端都是基于RESTClient实现的。 ClientSet *kubernetes.Clientset ClientSet 是在RESTClient基础上封装了对Resource和Version的管理方法。每一个Resource可以理解为一个客户端，而ClientSet则是多个客户端的集合，每一个Resource和Version都以函数的方式暴露给开发者。ClientSet只能够处理Kubernetes内置资源，他是通过Client-go代码生成器生成的。 DynamicClient dynamic.Interface DynamicClient与ClientSet最大的不同之处是，ClientSet仅能访问Kubernetes自带的资源（即client集合哪的资源）， 而不能直接访问CRD自带的资源。DynamicClient能过处理Kubernetes中的所有资源对象，包括Kubernetes内置资源与 CRD自定义资源。 DiscoveryClient *discovery.DiscoveryClient 发现客户端，用于发现kube-apiserver所支持的资源组、资源版本、资源信息（即Group, Versions,Resources) 2. sigs.k8s.io/controller-runtime 该客户端是可以直接从kubernetes server 中读写的，它能够处理普通类型，自定义类型，内建类型以及未知类型。 使用该client的时候，它会使用scheme去寻找Group， version 和类型。 2.1 scheme 资源注册表 kubernetes中有很多资源，只有被注册到scheme资源注册表中，比如oam中的crd资源，我们是无法通过直接创建的。 var scheme = runtime.NewScheme() func init() { _ = clientgoscheme.AddToScheme(scheme) _ = oamcore.AddToScheme(scheme) } 2.2 example about apply oam component 在实际使用场景中， 我们可能非常需要类似kubectl apply的功能，即没有对象的时候新建，如果有就更新。 使用改client的patch方法即可实现。 package main import ( \"context\" \"fmt\" \"k8s-demo/common\" \"k8s-demo/k8s_client/deployment\" appsv1 \"k8s.io/api/apps/v1\" \"k8s.io/klog\" ctrl \"sigs.k8s.io/controller-runtime\" \"sigs.k8s.io/controller-runtime/pkg/client\" \"time\" ) /* patch deployment if not found, new one; else, update. */ var ( workloadName = \"apply-test\" namespace = \"default\" imageName = \"nginx\" ) func main() { ctx := context.Background() config := ctrl.GetConfigOrDie() begin := time.Now() c, err := client.New(config, client.Options{}) if err != nil { klog.Fatal(err) } klog.Info(\"build client cost time: \", time.Since(begin)) dep := deployment.GenerateDeployment(workloadName, namespace, imageName) // 1.delete deployment if err := c.Delete(ctx, &dep, &client.DeleteOptions{}); err != nil { klog.Fatal(err) } // patch 选项 applyOpts := []client.PatchOption{ client.ForceOwnership, client.FieldOwner(dep.GetUID()), &client.PatchOptions{FieldManager: \"apply\"}, } for i:=0;iCopyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubernetes灾备.html":{"url":"blog/kubernetes/kubernetes灾备.html","title":"Kubernetes灾备","keywords":"","body":"背景介绍 正常的生产环境一般都是要做数据备份和还原操作，kubernetes的etcd集群本身具有高可用的特性，我们正常情况下是不需要做数据备份的。 下面主要正对如下场景做灾备恢复(集群是通过kubeadm创建的，并且etcd也是在集群内运行的) 集群被误删除 集群数据回退到历史的某一天 恢复被误删除的集群（非高可用集群） backup etcd yum install etcdctl export ETCDCTL_API=3 export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379 export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/peer.crt export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/peer.key etcdctl snapshot save /backup/backup.db backup kubernetes'pki cp /etc/kuberentes/pki /backup/pki kubeadm rebuild cluster kubeadm reset cp -r /backup/pki/. /etc/kubernetes/pki/ kubeadm init --cert-dir=/etc/kubernetes/pki \\ --image-repository=registry.aliyuncs.com/launcher \\ --pod-network-cidr=10.56.0.0/16 --upload-certs stop kubelet && docker systemctl stop kubelet docker etcd restore data rm -rf /var/lib/etcd etcdctl snapshot restore /backup/backup.db --data-dir=/var/lib/etcd restart kubelet and docker systemctl start kubelet docker kubectl get nodes 集群恢复到历史的某一天 在master节点上stop kubelet and docker rm -rf /var/lib/etcd etcdctl snapshot restore /backup/backup.db --data-dir=/var/lib/etcd restart kubelet and docker systemctl start kubelet docker kubectl get nodes cronjob apiVersion: batch/v1beta1 kind: CronJob metadata: name: etcd-backup-test spec: concurrencyPolicy: Forbid failedJobsHistoryLimit: 3 successfulJobsHistoryLimit: 1 schedule: \"0 */1 * * *\" jobTemplate: spec: backoffLimit: 3 template: spec: containers: - name: etcd image: registry.aliyuncs.com/launcher/etcd:3.3.10 command: - etcdctl - snapshot - save - /snapshots/snapshot.db env: - name: ETCDCTL_API value: \"3\" - name: ETCDCTL_ENDPOINTS value: \"https://127.0.0.1:2379\" - name: ETCDCTL_CACERT value: \"/etc/kubernetes/pki/etcd/ca.crt\" - name: ETCDCTL_CERT value: \"/etc/kubernetes/pki/etcd/peer.crt\" - name: ETCDCTL_KEY value: \"/etc/kubernetes/pki/etcd/peer.key\" volumeMounts: - mountPath: /etc/kubernetes/pki/etcd name: etcd-certs - mountPath: /snapshots name: snapshots volumes: - name: etcd-certs hostPath: path: /etc/kubernetes/pki/etcd type: Directory - name: snapshots hostPath: path: /backup type: Directory hostNetwork: true nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - operator: Exists restartPolicy: Never link https://cloud.google.com/anthos/gke/docs/on-prem/archive/1.2/how-to/backing-up?hl=zh-cn https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubesphere/":{"url":"blog/kubernetes/kubesphere/","title":"Kubesphere","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubesphere/bug.html":{"url":"blog/kubernetes/kubesphere/bug.html","title":"Bug","keywords":"","body":" https://github.com/kubesphere/kubesphere/issues/4017 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubesphere/helm.html":{"url":"blog/kubernetes/kubesphere/helm.html","title":"Helm","keywords":"","body":"kubesphere helm repo 源码分析 背景介绍 kubesphere 中的helm 仓库功能 kubesphere helm 仓库添加 helm repo list kubesphere helm 仓库中的应用模版查询 helm 仓库简介 helm charts是存放k8s 应用模版的仓库，该仓库由index.yaml 文件和 .tgz模版包组成 [root@ningbo stable]# ls -al 总用量 400 drwxr-xr-x. 26 root root 4096 6月 22 17:01 . drwxr-xr-x. 4 root root 86 6月 22 16:37 .. -rw-r--r--. 1 root root 10114 6月 22 17:12 index.yaml -rw-r--r--. 1 root root 3803 6月 8 2020 lsh-cluster-csm-om-agent-0.1.0.tgz -rw-r--r--. 1 root root 4022 6月 8 2020 lsh-mcp-cc-alert-service-0.1.0.tgz -rw-r--r--. 1 root root 4340 6月 8 2020 lsh-mcp-cc-sms-service-0.1.0.tgz -rw-r--r--. 1 root root 4103 6月 8 2020 lsh-mcp-cpm-metrics-exchange-0.1.0.tgz -rw-r--r--. 1 root root 4263 6月 8 2020 lsh-mcp-cpm-om-service-0.1.0.tgz -rw-r--r--. 1 root root 4155 6月 8 2020 lsh-mcp-csm-om-service-0.1.0.tgz -rw-r--r--. 1 root root 3541 6月 8 2020 lsh-mcp-deploy-service-0.1.0.tgz -rw-r--r--. 1 root root 5549 6月 8 2020 lsh-mcp-iam-apigateway-service-0.1.0.tgz index.yaml 文件 apiVersion: v1 entries: aliyun-ccm: - apiVersion: v2 appVersion: addon created: \"2021-06-21T08:59:58Z\" description: A Helm chart for Kubernetes digest: 6bda563c86333475255e5edfedc200ae282544e2c6e22b519a59b3c7bdef9a32 name: aliyun-ccm type: application urls: - charts/aliyun-ccm-0.1.0.tgz version: 0.1.0 aliyun-csi-driver: - apiVersion: v2 appVersion: addon created: \"2021-06-21T08:59:58Z\" description: A Helm chart for Kubernetes digest: b49f128d7a49401d52173e6f58caedd3fabbe8e2827dc00e6a824ee38860fa51 name: aliyun-csi-driver type: application urls: - charts/aliyun-csi-driver-0.1.0.tgz version: 0.1.0 application-controller: - apiVersion: v1 appVersion: addon created: \"2021-06-21T08:59:58Z\" description: A Helm chart for application Controller digest: 546e72ce77f865683ce0ea75f6e0203537a40744f2eb34e36a5bd378f9452bc5 name: application-controller urls: - charts/application-controller-0.1.0.tgz version: 0.1.0 tgz 解压缩后的文件目录 [root@ningbo stable]# cd mysql/ [root@ningbo mysql]# ls -al 总用量 20 drwxr-xr-x. 3 root root 97 5月 25 2020 . drwxr-xr-x. 26 root root 4096 6月 22 17:01 .. -rwxr-xr-x. 1 root root 106 5月 25 2020 Chart.yaml -rwxr-xr-x. 1 root root 364 5月 25 2020 .helmignore -rwxr-xr-x. 1 root root 76 5月 25 2020 index.yaml drwxr-xr-x. 3 root root 146 5月 25 2020 templates -rwxr-xr-x. 1 root root 1735 5月 25 2020 values.yaml Chart.yaml [root@ningbo mysql]# cat Chart.yaml apiVersion: v1 appVersion: \"1.0\" description: A Helm chart for Kubernetes name: mysql version: 0.1.0 添加helm 仓库代码介绍 接口实现分析 路由注册 handler：校验参数， 构建 models models ： 调用createRepo方法 crd client： 调用k8s api, 创建 crd HelmRepo 路由注册 webservice.Route(webservice.POST(\"/repos\"). To(handler.CreateRepo). // 跟进 Doc(\"Create a global repository, which is used to store package of app\"). Metadata(restfulspec.KeyOpenAPITags, []string{constants.OpenpitrixTag}). Param(webservice.QueryParameter(\"validate\", \"Validate repository\")). Returns(http.StatusOK, api.StatusOK, openpitrix.CreateRepoResponse{}). Reads(openpitrix.CreateRepoRequest{})) 校验参数， 构建 models func (h *openpitrixHandler) CreateRepo(req *restful.Request, resp *restful.Response) { createRepoRequest := &openpitrix.CreateRepoRequest{} err := req.ReadEntity(createRepoRequest) if err != nil { klog.V(4).Infoln(err) api.HandleBadRequest(resp, nil, err) return } createRepoRequest.Workspace = new(string) *createRepoRequest.Workspace = req.PathParameter(\"workspace\") user, _ := request.UserFrom(req.Request.Context()) creator := \"\" if user != nil { creator = user.GetName() } parsedUrl, err := url.Parse(createRepoRequest.URL) if err != nil { api.HandleBadRequest(resp, nil, err) return } userInfo := parsedUrl.User // trim credential from url parsedUrl.User = nil repo := v1alpha1.HelmRepo{ ObjectMeta: metav1.ObjectMeta{ Name: idutils.GetUuid36(v1alpha1.HelmRepoIdPrefix), Annotations: map[string]string{ constants.CreatorAnnotationKey: creator, }, Labels: map[string]string{ constants.WorkspaceLabelKey: *createRepoRequest.Workspace, }, }, Spec: v1alpha1.HelmRepoSpec{ Name: createRepoRequest.Name, Url: parsedUrl.String(), SyncPeriod: 0, Description: stringutils.ShortenString(createRepoRequest.Description, 512), }, } if strings.HasPrefix(createRepoRequest.URL, \"https://\") || strings.HasPrefix(createRepoRequest.URL, \"http://\") { if userInfo != nil { repo.Spec.Credential.Username = userInfo.Username() repo.Spec.Credential.Password, _ = userInfo.Password() } } else if strings.HasPrefix(createRepoRequest.URL, \"s3://\") { cfg := v1alpha1.S3Config{} err := json.Unmarshal([]byte(createRepoRequest.Credential), &cfg) if err != nil { api.HandleBadRequest(resp, nil, err) return } repo.Spec.Credential.S3Config = cfg } var result interface{} // 1. validate repo result, err = h.openpitrix.ValidateRepo(createRepoRequest.URL, &repo.Spec.Credential) if err != nil { klog.Errorf(\"validate repo failed, err: %s\", err) api.HandleBadRequest(resp, nil, err) return } // 2. create repo validate, _ := strconv.ParseBool(req.QueryParameter(\"validate\")) if !validate { if repo.GetTrueName() == \"\" { api.HandleBadRequest(resp, nil, fmt.Errorf(\"repo name is empty\")) return } result, err = h.openpitrix.CreateRepo(&repo) // 👇 } if err != nil { klog.Errorln(err) handleOpenpitrixError(resp, err) return } resp.WriteEntity(result) } 调用createRepo方法 func (c *repoOperator) CreateRepo(repo *v1alpha1.HelmRepo) (*CreateRepoResponse, error) { name := repo.GetTrueName() items, err := c.repoLister.List(labels.SelectorFromSet(map[string]string{constants.WorkspaceLabelKey: repo.GetWorkspace()})) if err != nil && !apierrors.IsNotFound(err) { klog.Errorf(\"list helm repo failed: %s\", err) return nil, err } for _, exists := range items { if exists.GetTrueName() == name { klog.Error(repoItemExists, \"name: \", name) return nil, repoItemExists } } repo.Spec.Description = stringutils.ShortenString(repo.Spec.Description, DescriptionLen) _, err = c.repoClient.HelmRepos().Create(context.TODO(), repo, metav1.CreateOptions{}) // 👇 if err != nil { klog.Errorf(\"create helm repo failed, repo_id: %s, error: %s\", repo.GetHelmRepoId(), err) return nil, err } else { klog.V(4).Infof(\"create helm repo success, repo_id: %s\", repo.GetHelmRepoId()) } return &CreateRepoResponse{repo.GetHelmRepoId()}, nil } 调用k8s api, 创建 crd HelmRepo // Create takes the representation of a helmRepo and creates it. Returns the server's representation of the helmRepo, and an error, if there is any. func (c *helmRepos) Create(ctx context.Context, helmRepo *v1alpha1.HelmRepo, opts v1.CreateOptions) (result *v1alpha1.HelmRepo, err error) { result = &v1alpha1.HelmRepo{} err = c.client.Post(). Resource(\"helmrepos\"). VersionedParams(&opts, scheme.ParameterCodec). Body(helmRepo). Do(ctx). Into(result) return } 查询helm 仓库应用模版代码介绍 接口实现 路由注册 handler，参数解析，调用models 方面 models ， 调用models 方法 crd client， 调用k8s api 存储 路由注册 webservice.Route(webservice.GET(\"/apps\").LiHui, 6 months ago: • openpitrix crd Deprecate(). To(handler.ListApps). // 跟进 Doc(\"List app templates\"). Param(webservice.QueryParameter(params.ConditionsParam, \"query conditions,connect multiple conditions with commas, equal symbol for exact query, wave symbol for fuzzy query e.g. name~a\"). Required(false). DataFormat(\"key=%s,key~%s\")). Param(webservice.QueryParameter(params.PagingParam, \"paging query, e.g. limit=100,page=1\"). Required(false). DataFormat(\"limit=%d,page=%d\"). DefaultValue(\"limit=10,page=1\")). Param(webservice.QueryParameter(params.ReverseParam, \"sort parameters, e.g. reverse=true\")). Param(webservice.QueryParameter(params.OrderByParam, \"sort parameters, e.g. orderBy=createTime\")). Metadata(restfulspec.KeyOpenAPITags, []string{constants.OpenpitrixTag}). Returns(http.StatusOK, api.StatusOK, models.PageableResponse{})) 参数解析，调用models 方面 func (h *openpitrixHandler) ListApps(req *restful.Request, resp *restful.Response) limit, offset := params.ParsePaging(req) orderBy := params.GetStringValueWithDefault(req, params.OrderByParam, openpitrix.CreateTime) reverse := params.GetBoolValueWithDefault(req, params.ReverseParam, false) conditions, err := params.ParseConditions(req) if err != nil { klog.V(4).Infoln(err) api.HandleBadRequest(resp, nil, err) return } if req.PathParameter(\"workspace\") != \"\" { conditions.Match[openpitrix.WorkspaceLabel] = req.PathParameter(\"workspace\") } result, err := h.openpitrix.ListApps(conditions, orderBy, reverse, limit, offset) // 👇 if err != nil { klog.Errorln(err) handleOpenpitrixError(resp, err) return } resp.WriteEntity(result) } 从缓存中获取applist func (c *applicationOperator) ListApps(conditions *params.Conditions, orderBy string, reverse bool, limit, offset int) (*models.PageableResponse, error) { apps, err := c.listApps(conditions) // 👇 if err != nil { klog.Error(err) return nil, err } apps = filterApps(apps, conditions) if reverse { sort.Sort(sort.Reverse(HelmApplicationList(apps))) } else { sort.Sort(HelmApplicationList(apps)) } totalCount := len(apps) start, end := (&query.Pagination{Limit: limit, Offset: offset}).GetValidPagination(totalCount) apps = apps[start:end] items := make([]interface{}, 0, len(apps)) for i := range apps { versions, err := c.getAppVersionsByAppId(apps[i].GetHelmApplicationId()) if err != nil && !apierrors.IsNotFound(err) { return nil, err } ctg, _ := c.ctgLister.Get(apps[i].GetHelmCategoryId()) items = append(items, convertApp(apps[i], versions, ctg, 0)) } return &models.PageableResponse{Items: items, TotalCount: totalCount}, nil } // line 601 func (c *applicationOperator) listApps(conditions *params.Conditions) (ret []*v1alpha1.HelmApplication, err error) { repoId := conditions.Match[RepoId] if repoId != \"\" && repoId != v1alpha1.AppStoreRepoId { // get helm application from helm repo if ret, exists := c.cachedRepos.ListApplicationsByRepoId(repoId); !exists { klog.Warningf(\"load repo failed, repo id: %s\", repoId) return nil, loadRepoInfoFailed } else { return ret, nil } } else { if c.backingStoreClient == nil { return []*v1alpha1.HelmApplication{}, nil } ret, err = c.appLister.List(labels.SelectorFromSet(buildLabelSelector(conditions))) } return } 缓存具体获取应用逻辑 func (c *cachedRepos) ListApplicationsByRepoId(repoId string) (ret []*v1alpha1.HelmApplication, exists bool) { c.RLock() defer c.RUnlock() if repo, exists := c.repos[repoId]; !exists { return nil, false } else { ret = make([]*v1alpha1.HelmApplication, 0, 10) for _, app := range c.apps { if app.GetHelmRepoId() == repo.Name { // 应用的仓库ID相同则追加 ret = append(ret, app) } } } return ret, true } 既然app template 是从缓存中获取的，那么缓存中的数据又是什么时候录入的呢？ 创建全局缓存变量 添加新helm仓库，k8s中已安装crd控制器helmRepoController 发现有新的helmRepo 创建，更新.Status.Data内容 informer 发现有更新，同时更新缓存 缓存更新的实现 创建全局变量，通过init函数初始化 通过helmRepo的informer实现缓存同步更新 在每次调用接口的时候，hanlder 类中包换了缓存变量 创建接口类openpitrix.Interface type openpitrixHandler struct { openpitrix openpitrix.Interface } func newOpenpitrixHandler(ksInformers informers.InformerFactory, ksClient versioned.Interface, option *openpitrixoptions.Options) *openpitrixHandler { var s3Client s3.Interface if option != nil && option.S3Options != nil && len(option.S3Options.Endpoint) != 0 { var err error s3Client, err = s3.NewS3Client(option.S3Options) if err != nil { klog.Errorf(\"failed to connect to storage, please check storage service status, error: %v\", err) } } return &openpitrixHandler{ openpitrix.NewOpenpitrixOperator(ksInformers, ksClient, s3Client), } } NewOpenpitrixOperator 通过在informer中添加通知函数，执行缓存更新 once.Do 只执行一次 var cachedReposData reposcache.ReposCache var helmReposInformer cache.SharedIndexInformer var once sync.Once func init() { cachedReposData = reposcache.NewReposCache() // 全局缓存 } func NewOpenpitrixOperator(ksInformers ks_informers.InformerFactory, ksClient versioned.Interface, s3Client s3.Interface) Interface { once.Do(func() { klog.Infof(\"start helm repo informer\") helmReposInformer = ksInformers.KubeSphereSharedInformerFactory().Application().V1alpha1().HelmRepos().Informer() helmReposInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { r := obj.(*v1alpha1.HelmRepo) cachedReposData.AddRepo(r) // 缓存更新， 👇 }, UpdateFunc: func(oldObj, newObj interface{}) { oldR := oldObj.(*v1alpha1.HelmRepo) cachedReposData.DeleteRepo(oldR) r := newObj.(*v1alpha1.HelmRepo) cachedReposData.AddRepo(r) }, DeleteFunc: func(obj interface{}) { r := obj.(*v1alpha1.HelmRepo) cachedReposData.DeleteRepo(r) }, }) go helmReposInformer.Run(wait.NeverStop) }) return &openpitrixOperator{ AttachmentInterface: newAttachmentOperator(s3Client), // cachedReposData used ApplicationInterface: newApplicationOperator(cachedReposData, ksInformers.KubeSphereSharedInformerFactory(), ksClient, s3Client), // cachedReposData used RepoInterface: newRepoOperator(cachedReposData, ksInformers.KubeSphereSharedInformerFactory(), ksClient), // cachedReposData used ReleaseInterface: newReleaseOperator(cachedReposData, ksInformers.KubernetesSharedInformerFactory(), ksInformers.KubeSphereSharedInformerFactory(), ksClient), CategoryInterface: newCategoryOperator(ksInformers.KubeSphereSharedInformerFactory(), ksClient), } } 缓存更新逻辑 // 缓存结构体 type cachedRepos struct { sync.RWMutex chartsInRepo map[workspace]map[string]int repoCtgCounts map[string]map[string]int repos map[string]*v1alpha1.HelmRepo apps map[string]*v1alpha1.HelmApplication versions map[string]*v1alpha1.HelmApplicationVersion } ByteArrayToSavedIndex： 将repo.Status.Data 转换为SavedIndex数组对象 遍历 SavedIndex.Applications 保存（app.ApplicationId：HelmApplication）到 cachedRepos.apps func (c *cachedRepos) AddRepo(repo *v1alpha1.HelmRepo) error { return c.addRepo(repo, false) } //Add new Repo to cachedRepos func (c *cachedRepos) addRepo(repo *v1alpha1.HelmRepo, builtin bool) error { if len(repo.Status.Data) == 0 { return nil } index, err := helmrepoindex.ByteArrayToSavedIndex([]byte(repo.Status.Data)) if err != nil { klog.Errorf(\"json unmarshal repo %s failed, error: %s\", repo.Name, err) return err } ... chartsCount := 0 for key, app := range index.Applications { if builtin { appName = v1alpha1.HelmApplicationIdPrefix + app.Name } else { appName = app.ApplicationId } HelmApp := v1alpha1.HelmApplication{ .... } c.apps[app.ApplicationId] = &HelmApp var ctg, appVerName string var chartData []byte for _, ver := range app.Charts { chartsCount += 1 if ver.Annotations != nil && ver.Annotations[\"category\"] != \"\" { ctg = ver.Annotations[\"category\"] } if builtin { appVerName = base64.StdEncoding.EncodeToString([]byte(ver.Name + ver.Version)) chartData, err = loadBuiltinChartData(ver.Name, ver.Version) if err != nil { return err } } else { appVerName = ver.ApplicationVersionId } version := &v1alpha1.HelmApplicationVersion{ .... } c.versions[ver.ApplicationVersionId] = version } .... } return nil } helmRepo 协调器 HelmRepo.Status.Data 加载流程 LoadRepoIndex: convert index.yaml to IndexFile MergeRepoIndex: merge new and old IndexFile savedIndex.Bytes(): compress data with zlib.NewWriter 将savedIndex 数据存入 CRD（helmRepo.Status.Data) 关键结构体 // helmRepo.Status.Data == SavedIndex 压缩后的数据 type SavedIndex struct { APIVersion string `json:\"apiVersion\"` Generated time.Time `json:\"generated\"` Applications map[string]*Application `json:\"apps\"` PublicKeys []string `json:\"publicKeys,omitempty\"` // Annotations are additional mappings uninterpreted by Helm. They are made available for // other applications to add information to the index file. Annotations map[string]string `json:\"annotations,omitempty\"` } // IndexFile represents the index file in a chart repository type IndexFile struct { APIVersion string `json:\"apiVersion\"` Generated time.Time `json:\"generated\"` Entries map[string]ChartVersions `json:\"entries\"` PublicKeys []string `json:\"publicKeys,omitempty\"` } 代码位置 func (r *ReconcileHelmRepo) syncRepo(instance *v1alpha1.HelmRepo) error { // 1. load index from helm repo index, err := helmrepoindex.LoadRepoIndex(context.TODO(), instance.Spec.Url, &instance.Spec.Credential) if err != nil { klog.Errorf(\"load index failed, repo: %s, url: %s, err: %s\", instance.GetTrueName(), instance.Spec.Url, err) return err } existsSavedIndex := &helmrepoindex.SavedIndex{} if len(instance.Status.Data) != 0 { existsSavedIndex, err = helmrepoindex.ByteArrayToSavedIndex([]byte(instance.Status.Data)) if err != nil { klog.Errorf(\"json unmarshal failed, repo: %s, error: %s\", instance.GetTrueName(), err) return err } } // 2. merge new index with old index which is stored in crd savedIndex := helmrepoindex.MergeRepoIndex(index, existsSavedIndex) // 3. save index in crd data, err := savedIndex.Bytes() if err != nil { klog.Errorf(\"json marshal failed, error: %s\", err) return err } instance.Status.Data = string(data) return nil } Question: Q1：helm 仓库发包时如何进行helm release 版本控制 A：修改Charts.yaml 中的字段 version，然后helm package， 等于新增一个tgz包，老版本的不要删除，这时候执行index 的时候会吧所有的tgz包包含在内。 $ helm repo index stable --url=xxx.xx.xx.xxx:8081/ $ cat index.yaml .... redis: - apiVersion: v1 appVersion: \"1.0\" created: \"2021-06-22T16:34:58.286583012+08:00\" description: A Helm chart for Kubernetes digest: fd7c0d962155330527c0a512a74bea33302fca940b810c43ee5f461b1013dbf5 name: redis urls: - xxx.xx.xx.xxx:8081/redis-0.1.1.tgz version: 0.1.1 - apiVersion: v1 appVersion: \"1.0\" created: \"2021-06-22T16:34:58.286109049+08:00\" description: A Helm chart for Kubernetes digest: 1a23bd6d5e45f9d323500bbe170011fb23bfccf2c1bd25814827eb8dc643d7f0 name: redis urls: - xxx.xx.xx.xxx:8081/redis-0.1.0.tgz version: 0.1.0 Q2：kubersphere版本同步功能有缺失？用户添加完helm 仓库后，如果有新的应用发布，查询不到 A：解决方案：使用3种同步策略 定时同步helm仓库（helmRepo 设置一个定时协调的事件） 企业仓库，用户可以设置hook，发布新版本的时候主动触发更新 用户主动更新charts； Q3：index.yaml 缓存位置 A：某些仓库的index.yaml 比较大，如果1000个用户，1000个charts 会太吃内存。建议常用index.yaml的放在内存中，不常用的index.yaml存储在本地磁盘。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubesphere/readme.html":{"url":"blog/kubernetes/kubesphere/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/":{"url":"blog/kubernetes/kubevela/","title":"Kubevela","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/KubeVela 功能.html":{"url":"blog/kubernetes/kubevela/KubeVela 功能.html","title":"KubeVela 功能","keywords":"","body":"KubeVela 功能 项目结构 CRD定义 core.oam.dev V1alpha2 V1beta1 - appdeployment - application - Approllout - Cluster - convertion - stand.oam.dev groupversion Pod specworkload Rollout_plan Rollouttrait 当前支持的workload type webservice task Helm chart worker 当前支持的Trait init-container ingress expose scaler sidecar kservice Application apiVersion: core.oam.dev/v1beta1 kind: Application metadata: name: testapp spec: components: - name: express-server properties: cmd: - node - server.js image: oamdev/testapp:v1 port: 8080 traits: - type: ingress properties: domain: test.my.domain http: \"/api\": 8080 type: webservice CUE kubevela 为什么要引入CUE 为了让用户可以自己对当前任意已有的对象再抽象，因为vlea设计者认为用户的需求是时刻变化的，会一天一个样。 CUE 可以将已有对象封装成只暴露自己想要暴露的参数。 实际上，大多数社区能力虽然很强大，但对于最终用户来都比较复杂，学习和上手非常困难。所以在 KubeVela 中，它允许平台管理员对能力做进一步封装以便对用户暴露简单易用的使用接口，在绝大多数场景下，这些使用接口往往只有几个参数就足够了。在能力封装这一步，KubeVela 选择了 CUE 模板语言，来连接用户界面和后端能力对象，并且天然就支持完全动态的模板绑定（即变更模板不需要重启或者重新部署系统）。下面就是 KubeWatch Trait 的模板例子： ​ 个人认为Pass 平台的参数应该是最简单，最傻瓜化。就像苹果的使用者，无需调整苹果参数，因为设计者已经将参数最优化，普适各种使用场景。 Rollout https://kubevela.io/zh/docs/rollout/appdeploy#cluster multi-cluster deployment Modern application infrastructure involves multiple clusters to ensure high availability and maximize service throughput. In this section, we will introduce how to use KubeVela to achieve application deployment across multiple clusters with following features supported: Rolling Upgrade: To continuously deploy apps requires to rollout in a safe manner which usually involves step by step rollout batches and analysis. Traffic shifting: When rolling upgrade an app, it needs to split the traffic onto both the old and new revisions to verify the new version while preserving service availability. apiVersion: core.oam.dev/v1beta1 kind: AppDeployment metadata: name: sample-appdeploy spec: traffic: hosts: - example.com http: - match: # match any requests to 'example.com/example-app' - uri: prefix: \"/example-app\" # split traffic 50/50 on v1/v2 versions of the app weightedTargets: - revisionName: example-app-v1 componentName: testsvc port: 80 weight: 50 - revisionName: example-app-v2 componentName: testsvc port: 80 weight: 50 appRevisions: - # Name of the AppRevision. # Each modification to Application would generate a new AppRevision. revisionName: example-app-v1 # Cluster specific workload placement config placement: - clusterSelector: # You can select Clusters by name or labels. # If multiple clusters is selected, one will be picked via a unique hashing algorithm. labels: tier: production name: prod-cluster-1 distribution: replicas: 5 - # If no clusterSelector is given, it will use the host cluster in which this CR exists distribution: replicas: 5 - revisionName: example-app-v2 placement: - clusterSelector: labels: tier: production name: prod-cluster-1 distribution: replicas: 5 - distribution: replicas: 5 The clusters selected in the placement part from above is defined in Cluster CRD. Here's what it looks like: apiVersion: core.oam.dev/v1beta1 kind: Cluster metadata: name: prod-cluster-1 labels: tier: production spec: kubeconfigSecretRef: name: kubeconfig-cluster-1 # the secret name The secret must contain the kubeconfig credentials in config field: apiVersion: v1 kind: Secret metadata: name: kubeconfig-cluster-1 data: config: ... # kubeconfig data helm charts support vela 通过创建 helm-operator crd 实现对helm charts 的支持 FAQ Q1: 多集群部署的时候，为什么会有针对应用版本的副本数？和组件的副本数有冲突吗？ A： link ​ -vela 可以干什么 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/cue.html":{"url":"blog/kubernetes/kubevela/cue.html","title":"Cue","keywords":"","body":"CUE 是一种开源数据约束语言，旨在简化涉及定义和使用数据的任务(The CUE Data Constraint Language) CUE 背景介绍 BCL 全名 Borg Configuration Language，是 Google 内部基于 GCL (Generic Configuration Language) 在 Borg 场景的实践。用户通过 BCL 描述对 Borg 的使用需求，通过基于 BCL 的抽象省去对 Borg 复杂配置细节的感知提高单位效率，通过工程化手段满足可抽象、可复用、可测试的协作方式提高团队效率和稳定性，并在其上建立了相应的生态平台，作为 Borg 生态的重要抽象层在 Google 内部服务了超过 10 年，帮助 Google 内部数万开发者更好的使用 Infra。遗憾的是 BCL 并未开源，无法对 BCL 的实现、使用、生态做更多深入的解析。 CUE 是一种服务于云化配置的强类型配置语言，由 Go team 成员 Marcel van Lohiuzen 结合 BCL 及多种其他语言研发并开源，可以说是 BCL 思路的开源版实现。 CUE 延续了 JSON 超集的思路，额外提供了丰富的类型、表达式、import 语句等常用能力；与 JSONNET 不同，CUE 不支持自定义 function，支持基于 typed feature structure 思路的外置 schema，并通过显式的合一化、分离化操作支持类型和数据的融合，但这样的设定及外置类型推导同样增加了理解难度和编写复杂性。 CUE 项目完全由 Golang 编写，同时背靠 Golang，允许通过 “import” 引入 CUE 提供的必需能力协助用户完成如 encoding, strings, math 等配置编写常用功能。可以说 CUE 既属于 JSON 系的模板语言，同时也带有了很多 Configuration Language 的思考，提供了良好的样本，无论是其语言设计思路，还是基于成熟高级编程语言引入能力的工程方式都值得深入学习，但由于其部分设定过于晦涩也使得其难于理解，上手难度高。目前 CUE 在部分开源项目中使用，如在 ISTIO 中有小规模使用。 BCL 在 Google 内部虽然被广泛推广使用，但由于其语言特性定义不清晰、研发测试支持较差、新语言学习成本等问题在一线受到较多的吐槽。CUE 试图解决其中的语言特性问题，并提供了较为清晰的 spec 帮助使用者理解语言定义，但在研发测试支持、新语言理解难度、上手成本上没有较大的提升，使用者仍然无法较好的编写、测试，不能容易的 debug；语言自创的 schema 模板及大量私货写法对于使用者来说仍然意味着学习一种新的难写的语言，受众需要足够 geek 且有足够的耐心来让自己成为专家。归根到底模板定义区别于编程语言的一个重点就在于完善可编程性的缺失，这使得编写总会遇到这样那样的麻烦。 The CUE Data Constraint Language Configure, Unify, Execute CUE is an open source data constraint language which aims to simplify tasks involving defining and using data. It is a superset of JSON, allowing users familiar with JSON to get started quickly. What is it for? You can use CUE to define a detailed validation schema for your data (manually or automatically from data) reduce boilerplate in your data (manually or automatically from schema) extract a schema from code generate type definitions and validation code merge JSON in a principled way define and run declarative scripts How? CUE merges the notion of schema and data. The same CUE definition can simultaneously be used for validating data and act as a template to reduce boilerplate. Schema definition is enriched with fine-grained value definitions and default values. At the same time, data can be simplified by removing values implied by such detailed definitions. The merging of these two concepts enables many tasks to be handled in a principled way. Constraints provide a simple and well-defined, yet powerful, alternative to inheritance, a common source of complexity with configuration languages. CUE Scripting The CUE scripting layer defines declarative scripting, expressed in CUE, on top of data. This solves three problems: working around the closedness of CUE definitions (we say CUE is hermetic), providing an easy way to share common scripts and workflows for using data, and giving CUE the knowledge of how data is used to optimize validation. There are many tools that interpret data or use a specialized language for a specific domain (Kustomize, Ksonnet). This solves dealing with data on one level, but the problem it solves may repeat itself at a higher level when integrating other systems in a workflow. CUE scripting is generic and allows users to define any workflow. Tooling CUE is designed for automation. Some aspects of this are: convert existing YAML and JSON automatically simplify configurations rich APIs designed for automated tooling formatter arbitrary-precision arithmetic generate CUE templates from source code generate source code from CUE definitions (TODO) Download and Install Install using Homebrew Using Homebrew, you can install using the CUE Homebrew tap: brew install cuelang/tap/cue Install From Source If you already have Go installed, the short version is: go get -u cuelang.org/go/cmd/cue This will install the cue command line tool. For more details see Installing CUE. Learning CUE The fastest way to learn the basics is to follow the tutorial on basic language constructs. A more elaborate tutorial demonstrating of how to convert and restructure an existing set of Kubernetes configurations is available in written form. References Language Specification: official CUE Language specification. API: the API on godoc.org Builtin packages: builtins available from CUE programs cue Command line reference: the cue command Contributing Our canonical Git repository is located at https://cue.googlesource.com. To contribute, please read the Contribution Guide. To report issues or make a feature request, use the issue tracker. Changes can be contributed using Gerrit or Github pull requests. Contact You can get in touch with the cuelang community in the following ways: Chat with us on our Slack workspace. Unless otherwise noted, the CUE source files are distributed under the Apache 2.0 license found in the LICENSE file. This is not an officially supported Google product. 参考资料： https://github.com/cuelang/cue/blob/master/README.md https://python.ctolib.com/cuelang-cue.html https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fcuelang%2Fcue%2Fblob%2Fmaster%2Fdoc%2Fref%2Fspec.md The CUE Language Specification Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/oam.html":{"url":"blog/kubernetes/kubevela/oam.html","title":"Oam","keywords":"","body":"1. 什么是OAM ？ OAM（open application model), 是一种新的服务治理规范。其核心诉求是将开发，运维和基础设施管理人员的职责分离。开发人员负责描述微服务或组件的功能， 以及如何配置它；运维负责配置其中一个或多个微服务的运行时环境；基础设施工程师负责建立和维护应用程序运行的基础设施。 Github 项目地址：https://github.com/oam-dev/spec OAM 模型中包含以下基本对象: Workload 模型参照 Kubernetes 规范定义，理论上，平台商可以定义如容器、Pod、Serverless 函数、虚拟机、数据库、消息队列等任何类型的 Workload。 Component：OAM 中最基础的对象，该配置与基础设施无关，定义负载实例的运维特性。例如一个微服务 workload 的定义。 TraitDefinition：一个组件所需的运维策略与配置，例如环境变量、Ingress、AutoScaler、Volume 等 ScopeDefinition：多个 Component 的共同边界。可以根据组件的特性或者作用域来划分 Scope，一个 Component 可能同时属于多个 Scope。 ApplicationConfiguration：将 Component（必须）、Trait（必须）、Scope（非必须）等组合到一起形成一个完整的应用配置。 2. oam-kubernetes-runtime 该项目是kubernetes官方对OAM支持的插件，git地址：: https://github.com/crossplane/oam-kubernetes-runtime OAM Kubernetes Runtime 实现了OAM规范，为任意kubernetes 暴露以应用程序为中心的API。 其最终目的是在kubernetes 上构建OAM平台，用户可以通过OAM中熟悉的概念去创建应用。 3. oam-kubernetes-runtime 实践 3.1 install crd kubectl apply -f https://github.com/crossplane/oam-kubernetes-runtime/tree/master/charts/oam-kubernetes-runtime/crds 3.2 注册trait, workload apiVersion: core.oam.dev/v1alpha2 kind: TraitDefinition metadata: name: manualscalertraits.core.oam.dev spec: workloadRefPath: spec.workloadRef definitionRef: name: manualscalertraits.core.oam.dev apiVersion: core.oam.dev/v1alpha2 kind: WorkloadDefinition metadata: name: containerizedworkloads.core.oam.dev spec: definitionRef: name: containerizedworkloads.core.oam.dev childResourceKinds: - apiVersion: apps/v1 kind: Deployment - apiVersion: v1 kind: Service - apiVersion: apps/v1 kind: StatefulSet 3.3 创建component apiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: example-component spec: workload: apiVersion: core.oam.dev/v1alpha2 kind: ContainerizedWorkload spec: containers: - name: wordpress image: wordpress:4.6.1-apache ports: - containerPort: 80 name: wordpress env: - name: TEST_ENV value: test parameters: - name: instance-name required: true fieldPaths: - metadata.name - name: image fieldPaths: - spec.containers[0].image 3.4 创建applicationConfigComponent apiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: example-appconfig spec: components: - componentName: example-component parameterValues: - name: instance-name value: example-appconfig-workload1 - name: image value: wordpress:php7.2 traits: - trait: apiVersion: core.oam.dev/v1alpha2 kind: ManualScalerTrait metadata: name: example-appconfig-trait spec: replicaCount: 2 4.oam-kubernetes-runtime 工作流程 4.1 注册 trait 和 workload 通过 TraitDefinition 和 WorkloadDefinition 可以注册运维特征和工作负载，这里的trait可以是oam-kubernetes-runtime提供manualscalertraits， 也可以是你自己写的trait。 workloadDefinition 可以是deployment， statefulSet 或者是 oam-kubernetes-runtime提供的ContainerizedWorkload 4.2component 配置workload 一个component只能配置一个workload 假设你的component是一个deployment， 那么在component.spec里面填写deployment即可 4.3applicationComponentConfig（简称appConfig） 应用配置（appConfig）创建后可以实例化所有组件（component） 这里的应用配置可以看成是多个组件组成的应用。 appConfig 协调器工作原理： 转化并创建 workload对象， 如果 workload 是 ContainerizedWorkload， 则ContainerizedWorkload的控制器会根据其对象定义创建deployment和service。 转化并创建 trait， trait 控制器发现被创建的trait，根据trait定义的属性修改workload（比如manualscalertrait控制器会去修改workload的spec.replicas) get all workload and patch scope （比如healthScope 会去查询workload，如果都查询到了，则修改healthScope对象为health。 这里是有问题的，应该通过检查workload的status来决定是否健康） 子资源回收机制： 1)在创建的控制里面通过Owns里面添加需要回收的子资源。 2)cleanupResources, 比如volumeTrait控制器创建了新的pvc, 然后将新创的pvc uid 和 volumeTrait Status 里面的pvc uid 做对比，如果不相同，这删除status 里面的pvc。return ctrl.NewControllerManagedBy(mgr). Named(name). For(&oamv1alpha2.VolumeTrait{}). Owns(&v1.PersistentVolumeClaim{}, builder.WithPredicates(predicate.GenerationChangedPredicate{})). Watches(&source.Kind{Type: &oamv1alpha2.VolumeTrait{}}, &VolumeHandler{ Client: mgr.GetClient(), Logger: log, AppsClient: clientappv1.NewForConfigOrDie(mgr.GetConfig()), }). Complete(r) 5. 已有的扩展插件 可以理解为非OAM Runtime必须实现的OAM插件和扩展 https://github.com/oam-dev/catalog trait-injector（资源注入器） ServiceExpose（服务导出） IngressTrait（Ingress） HPATrait（自动伸缩） CronHPATrait（基于Cron表达式的定时伸缩） MetricHPATrait（基于指标的自动伸缩） SimpleRolloutTrait（简单的滚动更新） ServiceMonitor（服务监控器，还在PR中） 6. 还需要添加的特性及改善点 volumeTrait （实现WorkloadDefinition的多态） config && secret mount （kubevela在实现中） health scope 缺少对资源对象的状态检查 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/readme.html":{"url":"blog/kubernetes/kubevela/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/rollout design.html":{"url":"blog/kubernetes/kubevela/rollout design.html","title":"Rollout Design","keywords":"","body":"rollout design 原则和目标 所有rollout 共享同一个逻辑 所有的rollout 相关逻辑可扩展，支持不同的工作负载类型 核心rollout 逻辑对那些明确的状态转化具有完善的状态机 支持生成环境 提案 提供rollout CRD 给出实现控制器的高级设计 提供状态机和转化事件 列出常见的使用场景，我们相关的经验和实现细节 用户体验工作流 roullout workflows 2个级别的rollout 控制器，最终都会在内存中发出一个rollout计划对象，该对象包括将要执行的目标和源k8s资源。例如，从一个applicationDeployment中提取出实际的工作负载，发送给rollout 计划对象。 Application inplace upgrade workflow 最自然的升级就是原地升级，用户只需要改变application。 applicationConfiguration创建哈希值， 使用组件的修订名称 AC修改哈希值，预设注解 ApplicationDeployment workflow 添加注解 移除其他运维特征 修改svc的选择器 使用webhook确保注解删除后不会被再次添加 回滚失败，会有不可预知的情况，old and new all exist 引入字段‘revertOnDelete’，以便用户可以删除appDeployment并期望旧应用程序完好无损，而新应用程序不起作用。 Rollout trait workflow 组件控制器发射新的组件版本，当组件发射创建或修改的时候 当source 和 target rollout，会创建新组件并分配新的修订版本 Rollout plan work with different type of workloads 不同的工作负载要使用相同的回滚逻辑 Rollout plan works with deployment Rollout plan works with cloneset 控制器具有一下扩展点设置 workloads. Each workload handler needs to implement the following operations: scale the resources determine the health of the workload report how many replicas are upgraded/ready/available State Transition top-level states of rollout Verifying Initializing Rolling Finalising Succeed Failed sub-status of rollout BatchRolling BatchStopped BatchReady BatchVerifying BatchAvailable Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/vela run.html":{"url":"blog/kubernetes/kubevela/vela run.html","title":"Vela Run","keywords":"","body":"Application 是如何转换为对应 K8s 资源对象 1. Application # 获取集群中的 Application $ kubectl get application NAMESPACE NAME AGE default test 24h 2. ApplicationConfiguration 和 Component 当 application controller 获取到 Application 资源对象之后，会根据其内容创建出对应的 ApplicationConfiguration 和 Component。 # 获取 ApplicationConfiguration 和 Component $ kubectl get ApplicationConfiguration,Component NAME AGE applicationconfiguration.core.oam.dev/test 24h NAME WORKLOAD-KIND AGE component.core.oam.dev/nginx Deployment 24h ApplicationiConfiguration 中以名字的方式引入 Component： 3. application controller 基本逻辑： 获取一个 Application 资源对象。 将 Application 资源对象渲染为 ApplicationConfiguration 和 Component。 创建 ApplicationConfiguration 和 Component 资源对象。 流程 起点：Application 中点：ApplicationConfiguration, Component 终点：Deployment, Service 路径： application_controller applicationconfiguration controller 代码： // pkg/controller/core.oam.dev/v1alpha2/application/application_controller.go // Reconcile process app event func (r *Reconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { ctx := context.Background() applog := r.Log.WithValues(\"application\", req.NamespacedName) // 1. 获取 Application app := new(v1alpha2.Application) if err := r.Get(ctx, client.ObjectKey{ Name: req.Name, Namespace: req.Namespace, }, app); err != nil { ... } ... // 2. 将 Application 转换为 ApplicationConfiguration 和 Component handler := &appHandler{r, app, applog} ... appParser := appfile.NewApplicationParser(r.Client, r.dm) ... appfile, err := appParser.GenerateAppFile(ctx, app.Name, app) ... ac, comps, err := appParser.GenerateApplicationConfiguration(appfile, app.Namespace) ... // 3. 在集群中创建 ApplicationConfiguration 和 Component // apply appConfig & component to the cluster if err := handler.apply(ctx, ac, comps); err != nil { applog.Error(err, \"[Handle apply]\") app.Status.SetConditions(errorCondition(\"Applied\", err)) return handler.handleErr(err) } ... return ctrl.Result{}, r.UpdateStatus(ctx, app) } 4. applicationconfiguration controller 基本逻辑： 获取 ApplicationConfiguration 资源对象。 循环遍历，获取每一个 Component 并将 workload 和 trait 渲染为对应的 K8s 资源对象。 创建对应的 K8s 资源对象。 代码： // pkg/controller/core.oam.dev/v1alpha2/applicationcinfiguratioin/applicationconfiguratioin.go // Reconcile an OAM ApplicationConfigurations by rendering and instantiating its // Components and Traits. func (r *OAMApplicationReconciler) Reconcile(req reconcile.Request) (reconcile.Result, error) { ... ac := &v1alpha2.ApplicationConfiguration{} // 1. 获取 ApplicationConfiguration if err := r.client.Get(ctx, req.NamespacedName, ac); err != nil { ... } return r.ACReconcile(ctx, ac, log) } // ACReconcile contains all the reconcile logic of an AC, it can be used by other controller func (r *OAMApplicationReconciler) ACReconcile(ctx context.Context, ac *v1alpha2.ApplicationConfiguration, log logging.Logger) (result reconcile.Result, returnErr error) { ... // 2. 渲染 // 此处 workloads 包含所有Component对应的的 workload 和 tratis 的 k8s 资源对象 workloads, depStatus, err := r.components.Render(ctx, ac) ... applyOpts := []apply.ApplyOption{apply.MustBeControllableBy(ac.GetUID()), applyOnceOnly(ac, r.applyOnceOnlyMode, log)} // 3. 创建 workload 和 traits 对应的 k8s 资源对象 if err := r.workloads.Apply(ctx, ac.Status.Workloads, workloads, applyOpts...); err != nil { ... } ... // the defer function will do the final status update return reconcile.Result{RequeueAfter: waitTime}, nil } 5. 总结 当 vela up 将一个 AppFile 渲染为一个 Application 后，后续的流程由 application controller 和 applicationconfiguration controller 完成。 作者简介 樊大勇，华胜天成研发工程师，GitHub ID：@just-do1。 加入 OAM OAM 官网： https://oam.dev KubeVela GitHub 项目地址： https://github.com/oam-dev/kubevela 社区交流钉群： link： 源码解读：KubeVela 是如何将 appfile 转换为 K8s 特定资源对象的 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/vela up.html":{"url":"blog/kubernetes/kubevela/vela up.html","title":"Vela Up","keywords":"","body":"如何将一个 appfile 转为 Kubernetes 中的 Application 起点：appfile 终点：applicatioin 路径：appfile -> application (services -> component) comp[workload, traits] 1. 起点：AppFile // references/appfile/api/appfile.go // AppFile defines the spec of KubeVela Appfile type AppFile struct { Name string `json:\"name\"` CreateTime time.Time `json:\"createTime,omitempty\"` UpdateTime time.Time `json:\"updateTime,omitempty\"` Services map[string]Service `json:\"services\"` Secrets map[string]string `json:\"secrets,omitempty\"` configGetter config.Store initialized bool } // NewAppFile init an empty AppFile struct func NewAppFile() *AppFile { return &AppFile{ Services: make(map[string]Service), Secrets: make(map[string]string), configGetter: &config.Local{}, } } // references/appfile/api/service.go // Service defines the service spec for AppFile, it will contain all related information including OAM component, traits, source to image, etc... type Service map[string]interface{} 上面两段代码是 AppFile 在客户端的声明，vela 会将指定路径的 yaml 文件读取后，赋值给一个 AppFile。 // references/appfile/api/appfile.go // LoadFromFile will read the file and load the AppFile struct func LoadFromFile(filename string) (*AppFile, error) { b, err := ioutil.ReadFile(filepath.Clean(filename)) if err != nil { return nil, err } af := NewAppFile() // Add JSON format appfile support ext := filepath.Ext(filename) switch ext { case \".yaml\", \".yml\": err = yaml.Unmarshal(b, af) case \".json\": af, err = JSONToYaml(b, af) default: if json.Valid(b) { af, err = JSONToYaml(b, af) } else { err = yaml.Unmarshal(b, af) } } if err != nil { return nil, err } return af, nil } 下面为读取 vela.yaml 文件后，加载到 AppFile 中的数据： # vela.yaml name: test services: nginx: type: webservice image: nginx env: - name: NAME value: kubevela # svc trait svc: type: NodePort ports: - port: 80 nodePort: 32017 Name: test CreateTime: 0001-01-01 00:00:00 +0000 UTC UpdateTime: 0001-01-01 00:00:00 +0000 UTC Services： map[ nginx: map[ env: [map[name: NAME value: kubevela]] image: nginx svc: map[ports: [map[nodePort: 32017 port: 80]] type: NodePort] type: webservice ] ] Secrets map[] configGetter: 0x447abd0 initialized: false 2. 终点：application // apis/core.oam.dev/application_types.go type Application struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ApplicationSpec `json:\"spec,omitempty\"` Status AppStatus `json:\"status,omitempty\"` } // ApplicationSpec is the spec of Application type ApplicationSpec struct { Components []ApplicationComponent `json:\"components\"` // TODO(wonderflow): we should have application level scopes supported here // RolloutPlan is the details on how to rollout the resources // The controller simply replace the old resources with the new one if there is no rollout plan involved // +optional RolloutPlan *v1alpha1.RolloutPlan `json:\"rolloutPlan,omitempty\"` } 上面代码，为 Application 的声明，结合 .vela/deploy.yaml（见下面代码），可以看出，要将一个 AppFile 渲染为 Application 主要就是将 AppFile 的 Services 转化为 Application 的 Components。 # .vela/deploy.yaml apiVersion: core.oam.dev/v1alpha2 kind: Application metadata: creationTimestamp: null name: test namespace: default spec: components: - name: nginx scopes: healthscopes.core.oam.dev: test-default-health settings: env: - name: NAME value: kubevela image: nginx traits: - name: svc properties: ports: - nodePort: 32017 port: 80 type: NodePort type: webservice status: {} 3. 路径：Services -> Components 结合以上内容可以看出，将 Appfile 转化为 Application 主要是将 Services 渲染为 Components。 // references/appfile/api/appfile.go // BuildOAMApplication renders Appfile into Application, Scopes and other K8s Resources. func (app *AppFile) BuildOAMApplication(env *types.EnvMeta, io cmdutil.IOStreams, tm template.Manager, silence bool) (*v1alpha2.Application, []oam.Object, error) { ... servApp := new(v1alpha2.Application) servApp.SetNamespace(env.Namespace) servApp.SetName(app.Name) servApp.Spec.Components = []v1alpha2.ApplicationComponent{} for serviceName, svc := range app.GetServices() { ... // 完成 Service 到 Component 的转化 comp, err := svc.RenderServiceToApplicationComponent(tm, serviceName) if err != nil { return nil, nil, err } servApp.Spec.Components = append(servApp.Spec.Components, comp) } servApp.SetGroupVersionKind(v1alpha2.SchemeGroupVersion.WithKind(\"Application\")) auxiliaryObjects = append(auxiliaryObjects, addDefaultHealthScopeToApplication(servApp)) return servApp, auxiliaryObjects, nil } 上面的代码是 vela 将 Appfile 转化为 Application 代码实现的位置。其中 comp, err := svc.RenderServiceToApplicationComponent(tm, serviceName) 完成 Service 到 Component 的转化。 // references/appfile/api/service.go // RenderServiceToApplicationComponent render all capabilities of a service to CUE values to KubeVela Application. func (s Service) RenderServiceToApplicationComponent(tm template.Manager, serviceName string) (v1alpha2.ApplicationComponent, error) { // sort out configs by workload/trait workloadKeys := map[string]interface{}{} var traits []v1alpha2.ApplicationTrait wtype := s.GetType() comp := v1alpha2.ApplicationComponent{ Name: serviceName, WorkloadType: wtype, } for k, v := range s.GetApplicationConfig() { // 判断是否为 trait if tm.IsTrait(k) { trait := v1alpha2.ApplicationTrait{ Name: k, } .... // 如果是 triat 加入 traits 中 traits = append(traits, trait) continue } workloadKeys[k] = v } // Handle workloadKeys to settings settings := &runtime.RawExte nsion{} pt, err := json.Marshal(workloadKeys) if err != nil { return comp, err } if err := settings.UnmarshalJSON(pt); err != nil { return comp, err } comp.Settings = *settings if len(traits) > 0 { comp.Traits = traits } return comp, nil } 4. 总结 执行 vela up 命令，渲染 appfile 为 Application，将数据写入到 .vela/deploy.yaml 中，并在 K8s 中创建。 作者简介 樊大勇，华胜天成研发工程师，GitHub ID：@just-do1。 加入 OAM OAM 官网： https://oam.dev KubeVela GitHub 项目地址： https://github.com/oam-dev/kubevela 社区交流钉群： link： 源码解读：KubeVela 是如何将 appfile 转换为 K8s 特定资源对象的 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/vela-cluster.html":{"url":"blog/kubernetes/kubevela/vela-cluster.html","title":"Vela Cluster","keywords":"","body":"vela-cluster Task: 1. Rollout deployment 2. Traffic-shift (with istio) 3. Multi-cluster step by step create application, but not deploy workload cat 发布版本v1 AppDeployment, deploy workload cat Update Application properties: cat This will create a new example-app-v2 AppRevision. Check it: $ kubectl get applicationrevisions.core.oam.dev NAME example-app-v1 example-app-v2 Then use the two AppRevisions to update the AppDeployment: cat create gateay apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: example-app-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" Apply traffic shift apiVersion: core.oam.dev/v1beta1 kind: AppDeployment metadata: name: example-appdeploy spec: traffic: hosts: - example-app.example.com gateways: - example-app-gateway http: - weightedTargets: - revisionName: example-app-v1 componentName: testsvc port: 8000 weight: 50 - revisionName: example-app-v2 componentName: testsvc port: 80 weight: 50 appRevisions: - revisionName: example-app-v1 placement: - distribution: replicas: 1 - revisionName: example-app-v2 placement: - distribution: replicas: 1 check # run this in another terminal $ kubectl -n istio-system port-forward service/istio-ingressgateway 8080:80 Forwarding from 127.0.0.1:8080 -> 8080 Forwarding from [::1]:8080 -> 8080 # The command should return pages of either docker whale or nginx in 50/50 $ curl -H \"Host: example-app.example.com\" http://localhost:8080/ clean kubectl delete appdeployments.core.oam.dev --all kubectl delete applications.core.oam.dev --all\\ Controller 流程 获取 Application with annotation “app.oam.dev/revision-only: \"true\" 获取 appdeployment question Q：vela 创建应用的方式是不是有点多， 方式1: 原oam ApplicationConfiguration， 方式2: create vela Application， 方式3: 创建 vela Application + vela AppDeployment vela 创建应用的方式是不是有点多， 方式1: 原oam ApplicationConfiguration， 方式2: create vela Application without annotation of revision， 方式3: 创建 vela Application with revision + vela AppDeployment Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/vela-helm.html":{"url":"blog/kubernetes/kubevela/vela-helm.html","title":"Vela Helm","keywords":"","body":"vela-helm 项目 helm operator 通过crd：HelmRelease 声明期望的chart 发行版本，完成helm 的CRUD。 KubeVela 集成helm 模版，负载创建helm crd对象 1.New chart-component Workload apiVersion: core.oam.dev/v1beta1 kind: WorkloadDefinition metadata: name: webapp-chart annotations: definition.oam.dev/description: helm chart for webapp spec: definitionRef: name: deployments.apps version: v1 Component apiVersion: core.oam.dev/v1beta1 kind: ComponentDefinition metadata: name: webapp-chart namespace: vela-system annotations: definition.oam.dev/description: helm chart for webapp spec: workload: definition: apiVersion: apps/v1 kind: Deployment schematic: helm: release: chart: spec: chart: \"podinfo\" version: \"5.1.4\" repository: url: \"http://oam.dev/catalog/\" ​ by cue define, ​ template is helmRelease ​ argument is helm value 2.application controller 业务逻辑： 获取application Generate AppFile 将application 渲染为 ApplicationConfiguration 和 Component appHandler apply ac && component ac get component ac render component to wrokload， trait Workloads apply traits apply appHandler apply helm repo && release 关键代码 ​ application Controller ​ convert app Application to Appfile ​ ？ why use appFile ​ ？ why wd in appFile, old oam use component convert to wd ​ call Appfile function： GenerateApplicationConfiguration ​ generate cueModule by wd ​ call function baseGenerateComponent(pCtx, wl, appName, ns) ​ comp, acComp, err = evalWorkloadWithContext(pCtx, wl, ns, appName, wl.Name) ​ 需要修改的关键逻辑： compoent 抽象结构修改 通过组合抽象来拓宽抽象能力 // A ComponentSpec defines the desired state of a Component. type ComponentSpec struct { // A Workload that will be created for each ApplicationConfiguration that // includes this Component. Workload is an instance of a workloadDefinition. // We either use the GVK info or a special \"type\" field in the workload to associate // the content of the workload with its workloadDefinition // +kubebuilder:validation:EmbeddedResource // +kubebuilder:pruning:PreserveUnknownFields Workload runtime.RawExtension `json:\"workload\"` // HelmRelease records a Helm release used by a Helm module workload. // +optional Helm *common.Helm `json:\"helm,omitempty\"` // Parameters exposed by this component. ApplicationConfigurations that // reference this component may specify values for these parameters, which // will in turn be injected into the embedded workload. // +optional Parameters []ComponentParameter `json:\"parameters,omitempty\"` } helm 结构体定义 // A Helm represents resources used by a Helm module type Helm struct { // Release records a Helm release used by a Helm module workload. // +kubebuilder:pruning:PreserveUnknownFields Release runtime.RawExtension `json:\"release\"` // HelmRelease records a Helm repository used by a Helm module workload. // +kubebuilder:pruning:PreserveUnknownFields Repository runtime.RawExtension `json:\"repository\"` } 协调器： application， 主要完成以下任务 get appconfig appConfig -----> appFile appFile ----> applicationConfiguration and components apply applicationConfiguration and components 改协调器在 ```go // Reconcile process app event func (r *Reconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { ... // line 76，获取application app := new(v1beta1.Application) if err := r.Get(ctx, client.ObjectKey{ Name: req.Name, Namespace: req.Namespace, }, app); err != nil { if kerrors.IsNotFound(err) { err = nil } return ctrl.Result{}, err } .... // line 122, 通过Application 生成Appfile ctx = oamutil.SetNamespaceInCtx(ctx, app.Namespace) generatedAppfile, err := appParser.GenerateAppFile(ctx, app) if err != nil { applog.Error(err, \"[Handle Parse]\") app.Status.SetConditions(errorCondition(\"Parsed\", err)) r.Recorder.Event(app, event.Warning(velatypes.ReasonFailedParse, err)) return handler.handleErr(err) } // line 146, 通过Appfile 生成 ac， comps // build template to applicationconfig & component ac, comps, err := generatedAppfile.GenerateApplicationConfiguration() ... // line 168， 创建ac， comps if err := handler.apply(ctx, appRev, ac, comps); err != nil { applog.Error(err, \"[Handle apply]\") app.Status.SetConditions(errorCondition(\"Applied\", err)) r.Recorder.Event(app, event.Warning(velatypes.ReasonFailedApply, err)) return handler.handleErr(err) } } 4. **函数 GenerateApplicationConfiguration 实现** 根据不同的category 转换成不同的组件对象 ```go // GenerateApplicationConfiguration converts an appFile to applicationConfig & Components func (af *Appfile) GenerateApplicationConfiguration() (*v1alpha2.ApplicationConfiguration, .... var components []*v1alpha2.Component for _, wl := range af.Workloads { .... switch wl.CapabilityCategory { case types.HelmCategory: // line 196 comp, acComp, err = generateComponentFromHelmModule(wl, af.Name, af.RevisionName, af.Namespace) ... case types.KubeCategory: ... case types.TerraformCategory: ... default: .... } components = append(components, comp) appconfig.Spec.Components = append(appconfig.Spec.Components, *acComp) } return appconfig, components, nil } 函数 generateComponentFromHelmModule 实现 func generateComponentFromHelmModule(wl *Workload, appName, revision, ns string) (*v1alpha2.Component, *v1alpha2.ApplicationConfigurationComponent, error) { ... // re-use the way CUE module generates comp & acComp comp, acComp, err := generateComponentFromCUEModule(wl, appName, revision, ns) if err != nil { return nil, nil, err } release, repo, err := helm.RenderHelmReleaseAndHelmRepo(wl.FullTemplate.Helm, wl.Name, appName, ns, wl.Params) ... rlsBytes, err := json.Marshal(release.Object) ... repoBytes, err := json.Marshal(repo.Object) ... comp.Spec.Helm = &common.Helm{ Release: runtime.RawExtension{Raw: rlsBytes}, Repository: runtime.RawExtension{Raw: repoBytes}, } return comp, acComp, nil } 函数generateComponentFromCUEModule 实现 func generateComponentFromCUEModule(wl *Workload, appName, revision, ns string) (*v1alpha2.Component, *v1alpha2.ApplicationConfigurationComponent, error) { pCtx, err := PrepareProcessContext(wl, appName, revision, ns) if err != nil { return nil, nil, err } return baseGenerateComponent(pCtx, wl, appName, ns) } baseGenerateComponent func baseGenerateComponent(pCtx process.Context, wl *Workload, appName, ns string) (*v1alpha2.Component, *v1alpha2.ApplicationConfigurationComponent, error) { var ( outputSecretName string err error ) if wl.IsCloudResourceProducer() { outputSecretName, err = GetOutputSecretNames(wl) if err != nil { return nil, nil, err } wl.OutputSecretName = outputSecretName } for _, tr := range wl.Traits { if err := tr.EvalContext(pCtx); err != nil { return nil, nil, errors.Wrapf(err, \"evaluate template trait=%s app=%s\", tr.Name, wl.Name) } } var comp *v1alpha2.Component var acComp *v1alpha2.ApplicationConfigurationComponent comp, acComp, err = evalWorkloadWithContext(pCtx, wl, ns, appName, wl.Name) if err != nil { return nil, nil, err } comp.Name = wl.Name acComp.ComponentName = comp.Name for _, sc := range wl.Scopes { acComp.Scopes = append(acComp.Scopes, v1alpha2.ComponentScope{ScopeReference: v1alpha1.TypedReference{ APIVersion: sc.GVK.GroupVersion().String(), Kind: sc.GVK.Kind, Name: sc.Name, }}) } if len(comp.Namespace) == 0 { comp.Namespace = ns } if comp.Labels == nil { comp.Labels = map[string]string{} } comp.Labels[oam.LabelAppName] = appName comp.SetGroupVersionKind(v1alpha2.ComponentGroupVersionKind) return comp, acComp, nil } App handler: apply // apply will // 1. set ownerReference for ApplicationConfiguration and Components // 2. update AC's components using the component revision name // 3. update or create the AC with new revisionsand remember it in the application status // 4. garbage collect unused components func (h *appHandler) apply(ctx context.Context, appRev *v1beta1.ApplicationRevision, ac *v1alpha2.ApplicationConfiguration, comps []*v1alpha2.Component) error { ... for _, comp := range comps { ... newComp := comp.DeepCopy() // newComp will be updated and return the revision name instead of the component name revisionName, err := h.createOrUpdateComponent(ctx, newComp) if err != nil { return err } ... // find the ACC that contains this component for i := 0; i App handler: createOrUpdateComponent // createOrUpdateComponent creates a component if not exist and update if exists. // it returns the corresponding component revisionName and if a new component revision is created func (h *appHandler) createOrUpdateComponent(ctx context.Context, comp *v1alpha2.Component) (string, error) { .... err := h.r.Get(ctx, compKey, &curComp) if err != nil { if !apierrors.IsNotFound(err) { return \"\", err } if err = h.r.Create(ctx, comp); err != nil { return \"\", err } h.logger.Info(\"Created a new component\", \"component name\", comp.GetName()) } else { // remember the revision if there is a previous component if curComp.Status.LatestRevision != nil { preRevisionName = curComp.Status.LatestRevision.Name } comp.ResourceVersion = curComp.ResourceVersion if err := h.r.Update(ctx, comp); err != nil { return \"\", err } h.logger.Info(\"Updated a component\", \"component name\", comp.GetName()) } return curRevisionName, nil } App handler: applyHelmModuleResources func (h *appHandler) applyHelmModuleResources(ctx context.Context, comp *v1alpha2.Component, owners []metav1.OwnerReference) error { klog.Info(\"Process a Helm module component\") repo, err := oamutil.RawExtension2Unstructured(&comp.Spec.Helm.Repository) if err != nil { return err } release, err := oamutil.RawExtension2Unstructured(&comp.Spec.Helm.Release) if err != nil { return err } release.SetOwnerReferences(owners) repo.SetOwnerReferences(owners) if err := h.r.applicator.Apply(ctx, repo); err != nil { return err } klog.InfoS(\"Apply a HelmRepository\", \"namespace\", repo.GetNamespace(), \"name\", repo.GetName()) if err := h.r.applicator.Apply(ctx, release); err != nil { return err } klog.InfoS(\"Apply a HelmRelease\", \"namespace\", release.GetNamespace(), \"name\", release.GetName()) return nil } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/vela-pk-oam.html":{"url":"blog/kubernetes/kubevela/vela-pk-oam.html","title":"Vela Pk Oam","keywords":"","body":"Vela-pk-oam 用户侧 vela 用户application 替换了 ac 新增基于模版的crd | type | name | apply to | annotation | | -------- | ----------- | ------------------------------------------------------------ | ------------------------------------ | | workload | webservice | workload: definition: apiVersion: apps/v1 kind: Deploym | log-runngin workload with sevice | | workload | task | workload: definition: apiVersion: batch/v1 kind: Job | job | | workload | worker | workload: definition: apiVersion: apps/v1 kind: Deployment | log-runngin workload without service | | trait | annotations | | | | trait | cpuscaler | | | | trait | ingress | | | | trait | labels | | | | trait | scaler | - webservice - worker | | | trait | sidecar | - webservice - worker | | diff Type CRD Controller From Control Plane Object applicationconfigurations.core.oam.dev Yes OAM Runtime Control Plane Object components.core.oam.dev Yes OAM Runtime Workload Type containerizedworklaods.core.oam.dev Yes OAM Runtime Scope healthscope.core.oam.dev Yes OAM Runtime Trait manualscalertraits.core.oam.dev Yes OAM Runtime Control Plane Object scopedefinitions.core.oam.dev No OAM Runtime Control Plane Object traitdefinitions.core.oam.dev No OAM Runtime Control Plane Object workloaddefinitions.core.oam.dev No OAM Runtime Trait autoscalers.standard.oam.dev Yes New in KubeVela Trait metricstraits.standard.oam.dev Yes New in KubeVela Workload Type podspecworkloads.standard.oam.dev Yes New in KubeVela Trait route.standard.oam.dev Yes New in KubeVela Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/kubevela/volumeClaim.html":{"url":"blog/kubernetes/kubevela/volumeClaim.html","title":"VolumeClaim","keywords":"","body":"volumeTrait design [toc] target oam ContainerizedWorkload child resource 支持 动态工作负载，原先只支持deployment，现在 是在第一次初始化的工作负载的时候，发现如果添加了 volume trait，则更改工作负载为statefulSet，否则默认是deployment。 用户填写信息 存储类名称 存储大小 容器内挂载路径 主机目录生成规则： 方式一： 用户自己填写主机全目录 方式二： 用户指定主机目录前缀+自动生成后缀目录 后缀目录自动生成规则： {namespace}{component-name}/{containerName-mountDirectoryIndex} storageClass 用户填写storageClass名称和容器挂载路径即可，oam负责创建pvc， 回收pvc。 hostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。 具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod 会由于节点上文件的不同 而在不同节点上有不同的行为。 下层主机上创建的文件或目录只能由 root 用户写入。你需要在 特权容器 中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 hostPath 卷。 hostPath 有两个参数， 主机路径和文件类型 type 有如下类型 DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 Directory 在给定路径上必须存在的目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 File 在给定路径上必须存在的文件。 Socket 在给定路径上必须存在的 UNIX 套接字。 CharDevice 在给定路径上必须存在的字符设备。 BlockDevice 在给定路径上必须存在的块设备。 Example: apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # 宿主上目录位置 path: /data # 此字段为可选 type: Directory # 注意： FileOrCreate 模式不会负责创建文件的父目录。 如果欲挂载的文件的父目录不存在，Pod 启动会失败。 为了确保这种模式能够工作，可以尝试把文件和它对应的目录分开挂载，如 FileOrCreate 配置 所示。 struct define type VolumeTrait struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec VolumeTraitSpec `json:\"spec,omitempty\"` Status VolumeTraitStatus `json:\"status,omitempty\"` } // A VolumeTraitSpec defines the desired state of a VolumeTrait. type VolumeTraitSpec struct { VolumeList []VolumeMountItem `json:\"volumeList\"` // WorkloadReference to the workload this trait applies to. WorkloadReference runtimev1alpha1.TypedReference `json:\"workloadRef\"` } type VolumeMountItem struct { ContainerIndex int `json:\"containerIndex\"` Paths []PathItem `json:\"paths\"` } type PathItem struct { StorageClassName string `json:\"storageClassName\"` HostPath string `json:\"hostPath\"` Size string `json:\"size\"` Path string `json:\"path\"` } 3. 实现逻辑 appConfig ​ apply workload container workload 根据 type 创建deployment or statefulset volumeTrait fetchWorkloadChildResources by traits create pvc by storageClassName and size patch Volumes and VolumeMount 4.volume 挂载场景 创建场景： 4.1 单容器挂载 4.2 多容器挂载 4.3 Deployment 单容器挂载storageClass 4.3 Statefulset 单容器挂载StorageClass 4.4 Deployment 单容器挂载 hostPath 4.5 Statefulset 单容器挂载hostPath 4.6 Deployment 单容器挂载storageClass and ConfigMap 4.7 Statefulset 单容器挂载storageClass and ConfigMap 无状态的 存储挂载是通过pvc 有状态的 所有的挂载都只能是volumeTemplate Deployment 目录挂载逻辑 volumeMount 1. 遍历目录，生成 Volumes 1. 合并comfigMap volumes 遍历目录，生产volumes 合并ConfigMap StatefulSet 目录挂载逻辑 volumeMount 1. 遍历目录，生成 Volumes 2. 合并comfigMap 3. 合并statefulset 5. 注意事项 nfs,nas 大小无法限制 pvc 创建后不能修改大小 VolumeTrait 只支持重建，不支持修改 pvc create 支持 storage class updat get pvc 比较pvc. storage class no delete old create new pvc new pvc uid list gc compare pvcUid && volumeTrait status resource pvc if pvc.uid not in pvcUid delete Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/mk-cert.html":{"url":"blog/kubernetes/mk-cert.html","title":"Mk Cert","keywords":"","body":"手动生成k8s证书 #!/bin/bash rm -rf /root/ssl mkdir -p /root/ssl cd /root/ssl cfssl print-defaults config > config.json cfssl print-defaults csr > csr.json #2.根据config.json文件的格式创建如下的ca-config.json文件,过期时间设置成了 87600h cat > ca-config.json ca-csr.json kubernetes-csr.json > kubernetes-csr.json done cat >> kubernetes-csr.json admin-csr.json admin-csr.json kube-proxy-csr.json kube-controller-manager.json kube-scheduler.json token.csv Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/open_swagger-ui.html":{"url":"blog/kubernetes/open_swagger-ui.html","title":"Open Swagger Ui","keywords":"","body":"kubernetes swagger 标准k8s配置 open swap ui vim /etc/kubernetes/manifests/kube-apiserver.yaml - --enable-swagger-ui=true - --insecure-bind-address=0.0.0.0 - --insecure-port=30010 deploy swagger ui kubectl run swagger-ui --image=swaggerapi/swagger-ui:latest hostport=8080 install nginx yum install nginx -y modify config location /openapi/v2{ proxy_pass http://localhost:30010/openapi/v2; } location /{ proxy_pass http://localhost:8080; } swagger-ui 不支持跨域（同host， 同端口），所以这里用nginx 代理，将两个服务都映射到了80端口。 https://blog.schwarzeni.com/2019/09/16/Minikube%E4%BD%BF%E7%94%A8Swagger%E6%9F%A5%E7%9C%8BAPI/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/pod.html":{"url":"blog/kubernetes/pod.html","title":"Pod","keywords":"","body":"万字长文：K8s 创建 pod 时，背后到底发生了什么？ KubeSphere云原生 1周前 收录于话题 #Kubernetes45 #云原生44 #CNI3 #开源31 本文基于 2019 年的一篇文章 What happens when ... Kubernetes edition![1] 梳理了 K8s 创建 pod（及其 deployment/replicaset）的整个过程， 整理了每个重要步骤的代码调用栈，以在实现层面加深对整个过程的理解。 原文参考的 K8s 代码已经较老（v1.8/v1.14 以及当时的 master），且部分代码 链接已失效；本文代码基于 v1.21[2]。 由于内容已经不与原文一一对应（有增加和删减），因此标题未加 “[译]” 等字样。感谢原作者（们）的精彩文章。 全文大纲： K8s 组件启动过程 kubectl（命令行客户端） kube-apiserver 写入 etcd Initializers Control loops（控制循环） Kubelet 本文试图回答以下问题：敲下 kubectl run nginx --image=nginx --replicas=3 命令后，K8s 中发生了哪些事情？ 要弄清楚这个问题，我们需要： 了解 K8s 几个核心组件的启动过程，它们分别做了哪些事情，以及 从客户端发起请求到 pod ready 的整个过程。 0 K8s 组件启动过程 首先看几个核心组件的启动过程分别做了哪些事情。 0.1 kube-apiserver 启动 调用栈 创建命令行（kube-apiserver）入口： main // cmd/kube-apiserver/apiserver.go |-cmd := app.NewAPIServerCommand() // cmd/kube-apiserver/app/server.go | |-RunE := func() { | Complete() | |-ApplyAuthorization(s.Authorization) | |-if TLS: | ServiceAccounts.KeyFiles = []string{CertKey.KeyFile} | Validate() | Run(completedOptions, handlers) // 核心逻辑 | } |-cmd.Execute() kube-apiserver 启动后，会执行到其中的 Run() 方法： Run() // cmd/kube-apiserver/app/server.go |-server = CreateServerChain() | |-CreateKubeAPIServerConfig() | | |-buildGenericConfig | | | |-genericapiserver.NewConfig() // staging/src/k8s.io/apiserver/pkg/server/config.go | | | | |-return &Config{ | | | | Serializer: codecs, | | | | BuildHandlerChainFunc: DefaultBuildHandlerChain, // 注册 handler | | | | } | | | | | | | |-OpenAPIConfig = DefaultOpenAPIConfig() // OpenAPI schema | | | |-kubeapiserver.NewStorageFactoryConfig() // etcd 相关配置 | | | |-APIResourceConfig = genericConfig.MergedResourceConfig | | | |-storageFactoryConfig.Complete(s.Etcd) | | | |-storageFactory = completedStorageFactoryConfig.New() | | | |-s.Etcd.ApplyWithStorageFactoryTo(storageFactory, genericConfig) | | | |-BuildAuthorizer(s, genericConfig.EgressSelector, versionedInformers) | | | |-pluginInitializers, admissionPostStartHook = admissionConfig.New() | | | | | |-capabilities.Initialize | | |-controlplane.ServiceIPRange() | | |-config := &controlplane.Config{} | | |-AddPostStartHook(\"start-kube-apiserver-admission-initializer\", admissionPostStartHook) | | |-ServiceAccountIssuerURL = s.Authentication.ServiceAccounts.Issuer | | |-ServiceAccountJWKSURI = s.Authentication.ServiceAccounts.JWKSURI | | |-ServiceAccountPublicKeys = pubKeys | | | |-createAPIExtensionsServer | |-CreateKubeAPIServer | |-createAggregatorServer // cmd/kube-apiserver/app/aggregator.go | | |-aggregatorConfig.Complete().NewWithDelegate(delegateAPIServer) // staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go | | | |-apiGroupInfo := NewRESTStorage() | | | |-GenericAPIServer.InstallAPIGroup(&apiGroupInfo) | | | |-InstallAPIGroups | | | |-openAPIModels := s.getOpenAPIModels(APIGroupPrefix, apiGroupInfos...) | | | |-for apiGroupInfo := range apiGroupInfos { | | | | s.installAPIResources(APIGroupPrefix, apiGroupInfo, openAPIModels) | | | | s.DiscoveryGroupManager.AddGroup(apiGroup) | | | | s.Handler.GoRestfulContainer.Add(discovery.NewAPIGroupHandler(s.Serializer, apiGroup).WebService()) | | | | | | | |-GenericAPIServer.Handler.NonGoRestfulMux.Handle(\"/apis\", apisHandler) | | | |-GenericAPIServer.Handler.NonGoRestfulMux.UnlistedHandle(\"/apis/\", apisHandler) | | | |- | | |- |-prepared = server.PrepareRun() // staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go | |-GenericAPIServer.AddPostStartHookOrDie | |-GenericAPIServer.PrepareRun | | |-routes.OpenAPI{}.Install() | | |-registerResourceHandlers // staging/src/k8s.io/apiserver/pkg/endpoints/installer.go | | |-POST: XX | | |-GET: XX | | | |-openapiaggregator.BuildAndRegisterAggregator() | |-openapiaggregator.NewAggregationController() | |-preparedAPIAggregator{} |-prepared.Run() // staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go |-s.runnable.Run() 一些重要步骤 创建 server chain。Server aggregation（聚合）是一种支持多 apiserver 的方式，其中 包括了一个 generic apiserver[3]，作为默认实现。 生成 OpenAPI schema，保存到 apiserver 的 Config.OpenAPIConfig 字段[4]。 遍历 schema 中的所有 API group，为每个 API group 配置一个 storage provider[5]， 这是一个通用 backend 存储抽象层。 遍历每个 group 版本，为每个 HTTP route 配置 REST mappings[6]。稍后处理请求时，就能将 requests 匹配到合适的 handler。 0.2 controller-manager 启动 调用栈 NewDeploymentController NewReplicaSetController 0.3 kubelet 启动 调用栈 main // cmd/kubelet/kubelet.go |-NewKubeletCommand // cmd/kubelet/app/server.go |-Run // cmd/kubelet/app/server.go |-initForOS // cmd/kubelet/app/server.go |-run // cmd/kubelet/app/server.go |-initConfigz // cmd/kubelet/app/server.go |-InitCloudProvider |-NewContainerManager |-ApplyOOMScoreAdj |-PreInitRuntimeService |-RunKubelet // cmd/kubelet/app/server.go | |-k = createAndInitKubelet // cmd/kubelet/app/server.go | | |-NewMainKubelet | | | |-watch k8s Service | | | |-watch k8s Node | | | |-klet := &Kubelet{} | | | |-init klet fields | | | | | |-k.BirthCry() | | |-k.StartGarbageCollection() | | | |-startKubelet(k) // cmd/kubelet/app/server.go | |-go k.Run() // -> pkg/kubelet/kubelet.go | | |-go cloudResourceSyncManager.Run() | | |-initializeModules | | |-go volumeManager.Run() | | |-go nodeLeaseController.Run() | | |-initNetworkUtil() // setup iptables | | |-go Until(PerformPodKillingWork, 1*time.Second, neverStop) | | |-statusManager.Start() | | |-runtimeClassManager.Start | | |-pleg.Start() | | |-syncLoop(updates, kl) // pkg/kubelet/kubelet.go | | | |-k.ListenAndServe | |-go http.ListenAndServe(healthz) 0.4 小结 以上核心组件启动完成后，就可以从命令行发起请求创建 pod 了。 1 kubectl（命令行客户端） 1.0 调用栈概览 NewKubectlCommand // staging/src/k8s.io/kubectl/pkg/cmd/cmd.go |-matchVersionConfig = NewMatchVersionFlags() |-f = cmdutil.NewFactory(matchVersionConfig) | |-clientGetter = matchVersionConfig |-NewCmdRun(f) // staging/src/k8s.io/kubectl/pkg/cmd/run/run.go | |-Complete // staging/src/k8s.io/kubectl/pkg/cmd/run/run.go | |-Run(f) // staging/src/k8s.io/kubectl/pkg/cmd/run/run.go | |-validate parameters | |-generators = GeneratorFn(\"run\") | |-runObj = createGeneratedObject(generators) // staging/src/k8s.io/kubectl/pkg/cmd/run/run.go | | |-obj = generator.Generate() // -> staging/src/k8s.io/kubectl/pkg/generate/versioned/run.go | | | |-get pod params | | | |-pod = v1.Pod{params} | | | |-return &pod | | |-mapper = f.ToRESTMapper() // -> staging/src/k8s.io/cli-runtime/pkg/genericclioptions/config_flags.go | | | |-f.clientGetter.ToRESTMapper() // -> staging/src/k8s.io/kubectl/pkg/cmd/util/factory_client_access.go | | | |-f.Delegate.ToRESTMapper() // -> staging/src/k8s.io/kubectl/pkg/cmd/util/kubectl_match_version.go | | | |-ToRESTMapper // -> staging/src/k8s.io/cli-runtime/pkg/resource/builder.go | | | |-delegate() // staging/src/k8s.io/cli-runtime/pkg/resource/builder.go | | |--actualObj = resource.NewHelper(mapping).XX.Create(obj) | |-PrintObj(runObj.Object) | |-NewCmdEdit(f) // kubectl edit 命令 |-NewCmdScale(f) // kubectl scale 命令 |-NewCmdCordon(f) // kubectl cordon 命令 |-NewCmdUncordon(f) |-NewCmdDrain(f) |-NewCmdTaint(f) |-NewCmdExecute(f) |-... 1.1 参数验证（validation）和资源对象生成器（generator） 参数验证 敲下 kubectl 命令后，它首先会做一些客户端侧的验证。如果命令行参数有问题，例如，镜像名为空或格式不对[7]， 这里会直接报错，从而避免了将明显错误的请求发给 kube-apiserver，减轻了后者的压力。 此外，kubectl 还会检查其他一些配置，例如 是否需要记录（record）这条命令（用于 rollout 或审计） 是否只是测试执行（--dry-run） 创建 HTTP 请求 所有查询或修改 K8s 资源的操作都需要与 kube-apiserver 交互，后者会进一步和 etcd 通信。 因此，验证通过之后，kubectl 接下来会创建发送给 kube-apiserver 的 HTTP 请求。 Generators 创建 HTTP 请求用到了所谓的 generator[8]（文档[9]） ，它封装了资源的序列化（serialization）操作。例如，创建 pod 时用到的 generator 是 BasicPod[10]： // staging/src/k8s.io/kubectl/pkg/generate/versioned/run.go type BasicPod struct{} func (BasicPod) ParamNames() []generate.GeneratorParam { return []generate.GeneratorParam{ {Name: \"labels\", Required: false}, {Name: \"name\", Required: true}, {Name: \"image\", Required: true}, ... } } 每个 generator 都实现了一个 Generate() 方法，用于生成一个该资源的运行时对象（runtime object）。对于 BasicPod，其实现[11]为： func (BasicPod) Generate(genericParams map[string]interface{}) (runtime.Object, error) { pod := v1.Pod{ ObjectMeta: metav1.ObjectMeta{ // metadata 字段 Name: name, Labels: labels, ... }, Spec: v1.PodSpec{ // spec 字段 ServiceAccountName: params[\"serviceaccount\"], Containers: []v1.Container{ { Name: name, Image: params[\"image\"] }, }, }, } return &pod, nil } 1.2 API group 和版本协商（version negotiation） 有了 runtime object 之后，kubectl 需要用合适的 API 将请求发送给 kube-apiserver。 API Group K8s 用 API group 来管理 resource API。这是一种不同于 monolithic API（所有 API 扁平化）的 API 管理方式。 具体来说，同一资源的不同版本的 API，会放到一个 group 里面。例如 Deployment 资源的 API group 名为 apps，最新的版本是 v1。这也是为什么 我们在创建 Deployment 时，需要在 yaml 中指定 apiVersion: apps/v1 的原因。 版本协商 生成 runtime object 之后，kubectl 就开始搜索合适的 API group 和版本[12]： // staging/src/k8s.io/kubectl/pkg/cmd/run/run.go obj := generator.Generate(params) // 创建运行时对象 mapper := f.ToRESTMapper() // 寻找适合这个资源（对象）的 API group 然后创建一个正确版本的客户端（versioned client）[13]， // staging/src/k8s.io/kubectl/pkg/cmd/run/run.go gvks, _ := scheme.Scheme.ObjectKinds(obj) mapping := mapper.RESTMapping(gvks[0].GroupKind(), gvks[0].Version) 这个客户端能感知资源的 REST 语义。 以上过程称为版本协商。在实现上，kubectl 会扫描 kube-apiserver 的 /apis 路径（OpenAPI 格式的 schema 文档），获取所有的 API groups。 出于性能考虑，kubectl 会缓存这份 OpenAPI schema[14]， 路径是 ~/.kube/cache/discovery。想查看这个 API discovery 过程，可以删除这个文件， 然后随便执行一条 kubectl 命令，并指定足够大的日志级别（例如 kubectl get ds -v 10）。 发送 HTTP 请求 现在有了 runtime object，也找到了正确的 API，因此接下来就是 将请求真正发送出去[15]： // staging/src/k8s.io/kubectl/pkg/cmd/cmd.go actualObj = resource. NewHelper(client, mapping). DryRun(o.DryRunStrategy == cmdutil.DryRunServer). WithFieldManager(o.fieldManager). Create(o.Namespace, false, obj) 发送成功后，会以恰当的格式打印返回的消息。 1.3 客户端认证（client auth） 前面其实有意漏掉了一步：客户端认证。它发生在发送 HTTP 请求之前。 用户凭证（credentials）一般都放在 kubeconfig 文件中，但这个文件可以位于多个位置， 优先级从高到低： 命令行 --kubeconfig 环境变量 $KUBECONFIG 某些预定义的路径[16]，例如 ~/.kube。 这个文件中存储了集群、用户认证等信息，如下面所示： apiVersion: v1 clusters: - cluster: certificate-authority: /etc/kubernetes/pki/ca.crt server: https://192.168.2.100:443 name: k8s-cluster-1 contexts: - context: cluster: k8s-cluster-1 user: default-user name: default-context current-context: default-context kind: Config preferences: {} users: - name: default-user user: client-certificate: /etc/kubernetes/pki/admin.crt client-key: /etc/kubernetes/pki/admin.key 有了这些信息之后，客户端就可以组装 HTTP 请求的认证头了。支持的认证方式有几种： X509 证书：放到 TLS[17] 中发送； Bearer token：放到 HTTP \"Authorization\" 头中发送[18]； 用户名密码：放到 HTTP basic auth 发送[19]； OpenID auth：需要先由用户手动处理，将其转成一个 token，然后和 bearer token 类似发送。 2 kube-apiserver 请求从客户端发出后，便来到服务端，也就是 kube-apiserver。 2.0 调用栈概览 buildGenericConfig |-genericConfig = genericapiserver.NewConfig(legacyscheme.Codecs) // cmd/kube-apiserver/app/server.go NewConfig // staging/src/k8s.io/apiserver/pkg/server/config.go |-return &Config{ Serializer: codecs, BuildHandlerChainFunc: DefaultBuildHandlerChain, } / / / / DefaultBuildHandlerChain // staging/src/k8s.io/apiserver/pkg/server/config.go |-handler := filterlatency.TrackCompleted(apiHandler) |-handler = genericapifilters.WithAuthorization(handler) |-handler = genericapifilters.WithAudit(handler) |-handler = genericapifilters.WithAuthentication(handler) |-return handler WithAuthentication |-withAuthentication |-resp, ok := AuthenticateRequest(req) | |-for h := range authHandler.Handlers { | resp, ok := currAuthRequestHandler.AuthenticateRequest(req) | if ok { | return resp, ok, err | } | } | return nil, false, utilerrors.NewAggregate(errlist) | |-audiencesAreAcceptable(apiAuds, resp.Audiences) |-req.Header.Del(\"Authorization\") |-req = req.WithContext(WithUser(req.Context(), resp.User)) |-return handler.ServeHTTP(w, req) 2.1 认证（Authentication） kube-apiserver 首先会对请求进行认证（authentication），以确保用户身份是合法的（verify that the requester is who they say they are）。 具体过程：启动时，检查所有的命令行参数[20]，组织成一个 authenticator list，例如， 如果指定了 --client-ca-file，就会将 x509 证书加到这个列表； 如果指定了 --token-auth-file，就会将 token 加到这个列表； 不同 anthenticator 做的事情有所不同： x509 handler[21] 验证该 HTTP 请求是用 TLS key 加密的，并且有 CA root 证书的签名。 bearer token handler[22] 验证请求中带的 token（HTTP Authorization 头中），在 apiserver 的 auth file 中是存在的（--token-auth-file）。 basicauth handler[23] 对 basic auth 信息进行校验。 如果认证成功，就会将 Authorization 头从请求中删除，然后在上下文中加上用户信息[24]。这使得后面的步骤（例如鉴权和 admission control）能用到这里已经识别出的用户身份信息。 // staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go // WithAuthentication creates an http handler that tries to authenticate the given request as a user, and then // stores any such user found onto the provided context for the request. // On success, \"Authorization\" header is removed from the request and handler // is invoked to serve the request. func WithAuthentication(handler http.Handler, auth authenticator.Request, failed http.Handler, apiAuds authenticator.Audiences) http.Handler { return withAuthentication(handler, auth, failed, apiAuds, recordAuthMetrics) } func withAuthentication(handler http.Handler, auth authenticator.Request, failed http.Handler, apiAuds authenticator.Audiences, metrics recordMetrics) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { resp, ok := auth.AuthenticateRequest(req) // 遍历所有 authenticator，任何一个成功就返回 OK if !ok { return failed.ServeHTTP(w, req) // 所有认证方式都失败了 } if !audiencesAreAcceptable(apiAuds, resp.Audiences) { fmt.Errorf(\"unable to match the audience: %v , accepted: %v\", resp.Audiences, apiAuds) failed.ServeHTTP(w, req) return } req.Header.Del(\"Authorization\") // 认证成功后，这个 header 就没有用了，可以删掉 // 将用户信息添加到请求上下文中，供后面的步骤使用 req = req.WithContext(WithUser(req.Context(), resp.User)) handler.ServeHTTP(w, req) }) } AuthenticateRequest() 实现：遍历所有 authenticator，任何一个成功就返回 OK， // staging/src/k8s.io/apiserver/pkg/authentication/request/union/union.go func (authHandler *unionAuthRequestHandler) AuthenticateRequest(req) (*Response, bool) { for currAuthRequestHandler := range authHandler.Handlers { resp, ok := currAuthRequestHandler.AuthenticateRequest(req) if ok { return resp, ok, err } } return nil, false, utilerrors.NewAggregate(errlist) } 2.2 鉴权（Authorization） 发送者身份（认证）是一个问题，但他是否有权限执行这个操作（鉴权），是另一个问题。因此确认发送者身份之后，还需要进行鉴权。 鉴权的过程与认证非常相似，也是逐个匹配 authorizer 列表中的 authorizer：如果都失败了， 返回 Forbidden 并停止 进一步处理[25]。如果成功，就继续。 内置的 几种 authorizer 类型： webhook[26]：与其他服务交互，验证是否有权限。 ABAC[27]：根据静态文件中规定的策略（policies）来进行鉴权。 RBAC[28]：根据 role 进行鉴权，其中 role 是 k8s 管理员提前配置的。 Node[29]：确保 node clients，例如 kubelet，只能访问本机内的资源。 要看它们的具体做了哪些事情，可以查看它们各自的 Authorize() 方法。 2.3 Admission control 至此，认证和鉴权都通过了。但这还没结束，K8s 中的其它组件还需要对请求进行检查， 其中就包括 admission controllers[30]。 与鉴权的区别 鉴权（authorization）在前面，关注的是用户是否有操作权限， Admission controllers 在更后面，对请求进行拦截和过滤，确保它们符合一些更广泛的集群规则和限制， 是将请求对象持久化到 etcd 之前的最后堡垒。 工作方式 与认证和鉴权类似，也是遍历一个列表， 但有一点核心区别：任何一个 controller 检查没通过，请求就会失败。 设计：可扩展 每个 controller 作为一个 plugin 存放在 plugin/pkg/admission 目录[31], 设计时已经考虑，只需要实现很少的几个接口 但注意，admission controller 最终会编译到 k8s 的二进制文件（而非独立的 plugin binary） 类型 Admission controllers 通常按不同目的分类，包括：资源管理、安全管理、默认值管 理、引用一致性（referential consistency）等类型。 例如，下面是资源管理类的几个 controller： InitialResources：为容器设置默认的资源限制（基于过去的使用量）； LimitRanger：为容器的 requests and limits 设置默认值，或对特定资源设置上限（例如，内存默认 512MB，最高不超过 2GB）。 ResourceQuota：资源配额。 3 写入 etcd 至此，K8s 已经完成对请求的验证，允许它进行接下来的处理。 kube-apiserver 将对请求进行反序列化，构造 runtime objects（ kubectl generator 的反过程），并将它们持久化到 etcd。下面详细 看这个过程。 3.0 调用栈概览 对于本文创建 pod 的请求，相应的入口是POST handler[32]，它又会进一步将请求委托给一个创建具体资源的 handler。 registerResourceHandlers // staging/src/k8s.io/apiserver/pkg/endpoints/installer.go |-case POST: // staging/src/k8s.io/apiserver/pkg/endpoints/installer.go switch () { case \"POST\": // Create a resource. var handler restful.RouteFunction if isNamedCreater { handler = restfulCreateNamedResource(namedCreater, reqScope, admit) } else { handler = restfulCreateResource(creater, reqScope, admit) } handler = metrics.InstrumentRouteFunc(action.Verb, group, version, resource, subresource, .., handler) article := GetArticleForNoun(kind, \" \") doc := \"create\" + article + kind if isSubresource { doc = \"create \" + subresource + \" of\" + article + kind } route := ws.POST(action.Path).To(handler). Doc(doc). Operation(\"create\"+namespaced+kind+strings.Title(subresource)+operationSuffix). Produces(append(storageMeta.ProducesMIMETypes(action.Verb), mediaTypes...)...). Returns(http.StatusOK, \"OK\", producedObject). Returns(http.StatusCreated, \"Created\", producedObject). Returns(http.StatusAccepted, \"Accepted\", producedObject). Reads(defaultVersionedObject). Writes(producedObject) AddObjectParams(ws, route, versionedCreateOptions) addParams(route, action.Params) routes = append(routes, route) } for route := range routes { route.Metadata(ROUTE_META_GVK, metav1.GroupVersionKind{ Group: reqScope.Kind.Group, Version: reqScope.Kind.Version, Kind: reqScope.Kind.Kind, }) route.Metadata(ROUTE_META_ACTION, strings.ToLower(action.Verb)) ws.Route(route) } 3.1 kube-apiserver 请求处理过程 从 apiserver 的请求处理函数开始： // staging/src/k8s.io/apiserver/pkg/server/handler.go func (d director) ServeHTTP(w http.ResponseWriter, req *http.Request) { path := req.URL.Path // check to see if our webservices want to claim this path for _, ws := range d.goRestfulContainer.RegisteredWebServices() { switch { case ws.RootPath() == \"/apis\": if path == \"/apis\" || path == \"/apis/\" { return d.goRestfulContainer.Dispatch(w, req) } case strings.HasPrefix(path, ws.RootPath()): if len(path) == len(ws.RootPath()) || path[len(ws.RootPath())] == '/' { return d.goRestfulContainer.Dispatch(w, req) } } } // if we didn't find a match, then we just skip gorestful altogether d.nonGoRestfulMux.ServeHTTP(w, req) } 如果能匹配到请求（例如匹配到前面注册的路由），它将分派给相应的 handler[33]；否则，fall back 到path-based handler[34]（GET /apis 到达的就是这里）； 基于 path 的 handlers： // staging/src/k8s.io/apiserver/pkg/server/mux/pathrecorder.go func (h *pathHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { if exactHandler, ok := h.pathToHandler[r.URL.Path]; ok { return exactHandler.ServeHTTP(w, r) } for prefixHandler := range h.prefixHandlers { if strings.HasPrefix(r.URL.Path, prefixHandler.prefix) { return prefixHandler.handler.ServeHTTP(w, r) } } h.notFoundHandler.ServeHTTP(w, r) } 如果还是没有找到路由，就会 fallback 到 non-gorestful handler，最终可能是一个 not found handler。 对于我们的场景，会匹配到一条已经注册的、名为createHandler[35]为的路由。 3.2 Create handler 处理过程 // staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go func createHandler(r rest.NamedCreater, scope *RequestScope, admit Interface, includeName bool) http.HandlerFunc { return func(w http.ResponseWriter, req *http.Request) { namespace, name := scope.Namer.Name(req) // 获取资源的 namespace 和 name（etcd item key） s := negotiation.NegotiateInputSerializer(req, false, scope.Serializer) body := limitedReadBody(req, scope.MaxRequestBodyBytes) obj, gvk := decoder.Decode(body, &defaultGVK, original) admit = admission.WithAudit(admit, ae) requestFunc := func() (runtime.Object, error) { return r.Create( name, obj, rest.AdmissionToValidateObjectFunc(admit, admissionAttributes, scope), ) } result := finishRequest(ctx, func() (runtime.Object, error) { if scope.FieldManager != nil { liveObj := scope.Creater.New(scope.Kind) obj = scope.FieldManager.UpdateNoErrors(liveObj, obj, managerOrUserAgent(options.FieldManager, req.UserAgent())) admit = fieldmanager.NewManagedFieldsValidatingAdmissionController(admit) } admit.(admission.MutationInterface) mutatingAdmission.Handles(admission.Create) mutatingAdmission.Admit(ctx, admissionAttributes, scope) return requestFunc() }) code := http.StatusCreated status, ok := result.(*metav1.Status) transformResponseObject(ctx, scope, trace, req, w, code, outputMediaType, result) } } 首先解析 HTTP request，然后执行基本的验证，例如保证 JSON 与 versioned API resource 期望的是一致的； 执行审计和最终 admission； 将资源最终写到 etcd[36]， 这会进一步调用到 storage provider[37]。 etcd key 的格式一般是 /（例如，default/nginx-0），但这个也是可配置的。 最后，storage provider 执行一次 get 操作，确保对象真的创建成功了。如果有额外的收尾任务（additional finalization），会执行 post-create handlers 和 decorators。 返回 生成的[38]HTTP response。 以上过程可以看出，apiserver 做了大量的事情。 总结：至此我们的 pod 资源已经在 etcd 中了。但是，此时 kubectl get pods -n 还看不见它。 4 Initializers 对象持久化到 etcd 之后，apiserver 并未将其置位对外可见，它也不会立即就被调度， 而是要先等一些 initializers[39] 运行完成。 4.1 Initializer Initializer 是与特定资源类型（resource type）相关的 controller， 负责在该资源对外可见之前对它们执行一些处理， 如果一种资源类型没有注册任何 initializer，这个步骤就会跳过，资源对外立即可见。 这是一种非常强大的特性，使得我们能执行一些通用的启动初始化（bootstrap）操作。例如， 向 Pod 注入 sidecar、暴露 80 端口，或打上特定的 annotation。 向某个 namespace 内的所有 pod 注入一个存放了测试证书（test certificates）的 volume。 禁止创建长度小于 20 个字符的 Secret （例如密码）。 4.2 InitializerConfiguration 可以用 InitializerConfiguration 声明对哪些资源类型（resource type）执行哪些 initializer。 例如，要实现所有 pod 创建时都运行一个自定义的 initializer custom-pod-initializer， 可以用下面的 yaml： apiVersion: admissionregistration.k8s.io/v1alpha1 kind: InitializerConfiguration metadata: name: custom-pod-initializer initializers: - name: podimage.example.com rules: - apiGroups: - \"\" apiVersions: - v1 resources: - pods 创建以上配置（kubectl create -f xx.yaml）之后，K8s 会将custom-pod-initializer 追加到每个 pod 的 metadata.initializers.pending 字段。 在此之前需要启动 initializer controller，它会 定期扫描是否有新 pod 创建； 当检测到它的名字出现在 pod 的 pending 字段时，就会执行它的处理逻辑； 执行完成之后，它会将自己的名字从 pending list 中移除。 pending list 中的 initializers，每次只有第一个 initializer 能执行。当所有 initializer 执行完成，pending 字段为空之后，就认为这个对象已经完成初始化了（considered initialized）。 细心的同学可能会有疑问：前面说这个对象还没有对外可见，那用 户空间的 initializer controller 又是如何能检测并操作这个对象的呢？答案是：kube-apiserver 提供了一个 ?includeUninitialized 查询参数，它会返回所有对象， 包括那些还未完成初始化的（uninitialized ones）。 5 Control loops（控制循环） 至此，对象已经在 etcd 中了，所有的初始化步骤也已经完成了。下一步是设置资源拓扑（resource topology）。例如，一个 Deployment 其实就是一组 ReplicaSet，而一个 ReplicaSet 就是一组 Pod。K8s 是如何根据一个 HTTP 请求创建出这个层级关系的呢？靠的是 K8s 内置的控制器（controllers）。 K8s 中大量使用 \"controllers\"， 一个 controller 就是一个异步脚本（an asynchronous script）， 不断检查资源的当前状态（current state）和期望状态（desired state）是否一致， 如果不一致就尝试将其变成期望状态，这个过程称为 reconcile。 每个 controller 负责的东西都比较少，所有 controller 并行运行， 由 kube-controller-manager 统一管理。 5.1 Deployments controller Deployments controller 启动 当一个 Deployment record 存储到 etcd 并（被 initializers）初始化之后， kube-apiserver 就会将其置为对外可见的。此后， Deployment controller 监听了 Deployment 资源的变动，因此此时就会检测到这个新创建的资源。 // pkg/controller/deployment/deployment_controller.go // NewDeploymentController creates a new DeploymentController. func NewDeploymentController(dInformer DeploymentInformer, rsInformer ReplicaSetInformer, podInformer PodInformer, client clientset.Interface) (*DeploymentController, error) { dc := &DeploymentController{ client: client, queue: workqueue.NewNamedRateLimitingQueue(), } dc.rsControl = controller.RealRSControl{ // ReplicaSet controller KubeClient: client, Recorder: dc.eventRecorder, } // 注册 Deployment 事件回调函数 dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, // 有 Deployment 创建时触发 UpdateFunc: dc.updateDeployment, DeleteFunc: dc.deleteDeployment, }) // 注册 ReplicaSet 事件回调函数 rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) // 注册 Pod 事件回调函数 podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue return dc, nil } 创建 Deployment：回调函数处理 在本文场景中，触发的是 controller 注册的 addDeployment() 回调函数[40]其所做的工作就是将 deployment 对象放到一个内部队列： // pkg/controller/deployment/deployment_controller.go func (dc *DeploymentController) addDeployment(obj interface{}) { d := obj.(*apps.Deployment) dc.enqueueDeployment(d) } 主处理循环 worker 不断遍历这个 queue，从中 dequeue item 并进行处理： // pkg/controller/deployment/deployment_controller.go func (dc *DeploymentController) worker() { for dc.processNextWorkItem() { } } func (dc *DeploymentController) processNextWorkItem() bool { key, quit := dc.queue.Get() dc.syncHandler(key.(string)) // dc.syncHandler = dc.syncDeployment } // syncDeployment will sync the deployment with the given key. func (dc *DeploymentController) syncDeployment(key string) error { namespace, name := cache.SplitMetaNamespaceKey(key) deployment := dc.dLister.Deployments(namespace).Get(name) d := deployment.DeepCopy() // 获取这个 Deployment 的所有 ReplicaSets, while reconciling ControllerRef through adoption/orphaning. rsList := dc.getReplicaSetsForDeployment(d) // 获取这个 Deployment 的所有 pods, grouped by their ReplicaSet podMap := dc.getPodMapForDeployment(d, rsList) if d.DeletionTimestamp != nil { // 这个 Deployment 已经被标记，等待被删除 return dc.syncStatusOnly(d, rsList) } dc.checkPausedConditions(d) if d.Spec.Paused { // pause 状态 return dc.sync(d, rsList) } if getRollbackTo(d) != nil { return dc.rollback(d, rsList) } scalingEvent := dc.isScalingEvent(d, rsList) if scalingEvent { return dc.sync(d, rsList) } switch d.Spec.Strategy.Type { case RecreateDeploymentStrategyType: // re-create return dc.rolloutRecreate(d, rsList, podMap) case RollingUpdateDeploymentStrategyType: // rolling-update return dc.rolloutRolling(d, rsList) } return fmt.Errorf(\"unexpected deployment strategy type: %s\", d.Spec.Strategy.Type) } controller 会通过 label selector 从 kube-apiserver 查询 与这个 deployment 关联的 ReplicaSet 或 Pod records（然后发现没有）。 如果发现当前状态与预期状态不一致，就会触发同步过程（（synchronization process））。这个同步过程是无状态的，也就是说，它并不区分是新记录还是老记录，一视同仁。 执行扩容（scale up） 如上，发现 pod 不存在之后，它会开始扩容过程（scaling process）： // pkg/controller/deployment/sync.go // scale up/down 或新创建（pause）时都会执行到这里 func (dc *DeploymentController) sync(d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) dc.scale(d, newRS, oldRSs) // Clean up the deployment when it's paused and no rollback is in flight. if d.Spec.Paused && getRollbackTo(d) == nil { dc.cleanupDeployment(oldRSs, d) } allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(allRSs, newRS, d) } 大致步骤： Rolling out (例如 creating）一个 ReplicaSet resource 分配一个 label selector 初始版本好（revision number）置为 1 ReplicaSet 的 PodSpec，以及其他一些 metadata 是从 Deployment 的 manifest 拷过来的。 最后会更新 deployment 状态，然后重新进入 reconciliation 循环，直到 deployment 进入预期的状态。 小结 由于 Deployment controller 只负责 ReplicaSet 的创建，因此下一步 （ReplicaSet -> Pod）要由 reconciliation 过程中的另一个 controller —— ReplicaSet controller 来完成。 5.2 ReplicaSets controller 上一步周，Deployments controller 已经创建了 Deployment 的第一个 ReplicaSet，但此时还没有任何 Pod。下面就轮到 ReplicaSet controller 出场了。它的任务是监控 ReplicaSet 及其依赖资源（pods）的生命周期，实现方式也是注册事件回调函数。 ReplicaSets controller 启动 // pkg/controller/replicaset/replica_set.go func NewReplicaSetController(rsInformer ReplicaSetInformer, podInformer PodInformer, kubeClient clientset.Interface, burstReplicas int) *ReplicaSetController { return NewBaseController(rsInformer, podInformer, kubeClient, burstReplicas, apps.SchemeGroupVersion.WithKind(\"ReplicaSet\"), \"replicaset_controller\", \"replicaset\", controller.RealPodControl{ KubeClient: kubeClient, }, ) } // 抽象出 NewBaseController() 是为了代码复用，例如 NewReplicationController() 也会调用这个函数。 func NewBaseController(rsInformer, podInformer, kubeClient clientset.Interface, burstReplicas int, gvk GroupVersionKind, metricOwnerName, queueName, podControl PodControlInterface) *ReplicaSetController { rsc := &ReplicaSetController{ kubeClient: kubeClient, podControl: podControl, burstReplicas: burstReplicas, expectations: controller.NewUIDTrackingControllerExpectations(NewControllerExpectations()), queue: workqueue.NewNamedRateLimitingQueue() } rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: rsc.addRS, UpdateFunc: rsc.updateRS, DeleteFunc: rsc.deleteRS, }) rsc.rsLister = rsInformer.Lister() podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: rsc.addPod, UpdateFunc: rsc.updatePod, DeleteFunc: rsc.deletePod, }) rsc.podLister = podInformer.Lister() rsc.syncHandler = rsc.syncReplicaSet return rsc } 创建 ReplicaSet：回调函数处理 主处理循环 当一个 ReplicaSet 被（Deployment controller）创建之后， // pkg/controller/replicaset/replica_set.go // syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled, // meaning it did not expect to see any more of its pods created or deleted. func (rsc *ReplicaSetController) syncReplicaSet(key string) error { namespace, name := cache.SplitMetaNamespaceKey(key) rs := rsc.rsLister.ReplicaSets(namespace).Get(name) selector := metav1.LabelSelectorAsSelector(rs.Spec.Selector) // 包括那些不匹配 rs selector，但有 stale controller ref 的 pod allPods := rsc.podLister.Pods(rs.Namespace).List(labels.Everything()) filteredPods := controller.FilterActivePods(allPods) // Ignore inactive pods. filteredPods = rsc.claimPods(rs, selector, filteredPods) if rsNeedsSync && rs.DeletionTimestamp == nil { // 需要同步，并且没有被标记待删除 rsc.manageReplicas(filteredPods, rs) // *主处理逻辑* } newStatus := calculateStatus(rs, filteredPods, manageReplicasErr) updatedRS := updateReplicaSetStatus(AppsV1().ReplicaSets(rs.Namespace), rs, newStatus) } RS controller 检查 ReplicaSet 的状态， 发现当前状态和期望状态之间有偏差（skew），因此接下来调用 manageReplicas() 来 reconcile 这个状态，在这里做的事情就是增加这个 ReplicaSet 的 pod 数量。 // pkg/controller/replicaset/replica_set.go func (rsc *ReplicaSetController) manageReplicas(filteredPods []*v1.Pod, rs *apps.ReplicaSet) error { diff := len(filteredPods) - int(*(rs.Spec.Replicas)) rsKey := controller.KeyFunc(rs) if diff rsc.burstReplicas { diff = rsc.burstReplicas } rsc.expectations.ExpectCreations(rsKey, diff) successfulCreations := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() { return rsc.podControl.CreatePodsWithControllerRef( // 扩容 // 调用栈 CreatePodsWithControllerRef -> createPod() -> Client.CoreV1().Pods().Create() rs.Namespace, &rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind)) }) // The skipped pods will be retried later. The next controller resync will retry the slow start process. if skippedPods := diff - successfulCreations; skippedPods > 0 { for i := 0; i 0 { if diff > rsc.burstReplicas { diff = rsc.burstReplicas } relatedPods := rsc.getIndirectlyRelatedPods(rs) podsToDelete := getPodsToDelete(filteredPods, relatedPods, diff) rsc.expectations.ExpectDeletions(rsKey, getPodKeys(podsToDelete)) for _, pod := range podsToDelete { go func(targetPod *v1.Pod) { rsc.podControl.DeletePod(rs.Namespace, targetPod.Name, rs) // 缩容 }(pod) } } return nil } 增加 pod 数量的操作比较小心，每次最多不超过 burst count（这个配置是从 ReplicaSet 的父对象 Deployment 那里继承来的）。 另外，创建 Pods 的过程是 批处理的[41], “慢启动”操，开始时是 SlowStartInitialBatchSize，每执行成功一批，下次的 batch size 就翻倍。这样设计是为了避免给 kube-apiserver 造成不必要的压力，例如，如果由于 quota 不足，这批 pod 大部分都会失败，那 这种方式只会有一小批请求到达 kube-apiserver，而如果一把全上的话，请求全部会打过去。同样是失败，这种失败方式比较优雅。 Owner reference K8s 通过 Owner Reference（子资源中的一个字段，指向的是其父资源的 ID）维护对象层级（hierarchy）。这可以带来两方面好处： 实现了 cascading deletion，即父对象被 GC 时会确保 GC 子对象； 父对象之间不会出现竞争子对象的情况（例如，两个父对象认为某个子对象都是自己的） 另一个隐藏的好处是：Owner Reference 是有状态的：如果 controller 重启，重启期间不会影响 系统的其他部分，因为资源拓扑（resource topology）是独立于 controller 的。这种隔离设计也体现在 controller 自己的设计中：controller 不应该操作 其他 controller 的资源（resources they don't explicitly own）。 有时也可能会出现“孤儿”资源（\"orphaned\" resources）的情况，例如 父资源删除了，子资源还在； GC 策略导致子资源无法被删除。 这种情况发生时，controller 会确保孤儿资源会被某个新的父资源收养。多个父资源都可以竞争成为孤儿资源的父资源，但只有一个会成功（其余的会收到一个 validation 错误）。 5.3 Informers 很多 controller（例如 RBAC authorizer 或 Deployment controller）需要将集群信息拉到本地。 例如 RBAC authorizer 中，authenticator 会将用户信息保存到请求上下文中。随后， RBAC authorizer 会用这个信息获取 etcd 中所有与这个用户相关的 role 和 role bindings。 那么，controller 是如何访问和修改这些资源的？在 K8s 中，这是通过 informer 机制实现的。 informer 是一种 controller 订阅存储（etcd）事件的机制，能方便地获取它们感兴趣的资源。 这种方式除了提供一种很好的抽象之外，还负责处理缓存（caching，非常重要，因为可 以减少 kube-apiserver 连接数，降低 controller 测和 kube-apiserver 侧的序列化 成本）问题。 此外，这种设计还使得 controller 的行为是 threadsafe 的，避免影响其他组件或服务。 关于 informer 和 controller 的联合工作机制，可参考这篇博客[42]。 5.4 Scheduler（调度器） 以上 controllers 执行完各自的处理之后，etcd 中已经有了一个 Deployment、一个 ReplicaSet 和三个 Pods，可以通过 kube-apiserver 查询到。但此时，这三个 pod 还卡在 Pending 状态，因为它们还没有被调度到任何节点。另外一个 controller —— 调度器—— 负责做这件事情。 scheduler 作为控制平面的一个独立服务运行，但工作方式与其他 controller 是一样的：监听事件，然后尝试 reconcile 状态。 调用栈概览 Run // pkg/scheduler/scheduler.go |-SchedulingQueue.Run() | |-scheduleOne() |-bind | |-RunBindPlugins | |-runBindPlugins | |-Bind |-sched.Algorithm.Schedule(pod) |-findNodesThatFitPod |-prioritizeNodes |-selectHost 调度过程 // pkg/scheduler/core/generic_scheduler.go // 将 pod 调度到指定 node list 中的某台 node 上 func (g *genericScheduler) Schedule(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { feasibleNodes, diagnosis := g.findNodesThatFitPod(ctx, fwk, state, pod) // 过滤可用 nodes if len(feasibleNodes) == 0 { return result, &framework.FitError{} } if len(feasibleNodes) == 1 { // 可用 node 只有一个，就选它了 return ScheduleResult{SuggestedHost: feasibleNodes[0].Name}, nil } priorityList := g.prioritizeNodes(ctx, fwk, state, pod, feasibleNodes) host := g.selectHost(priorityList) return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap), FeasibleNodes: len(feasibleNodes), }, err } // Filters nodes that fit the pod based on the framework filter plugins and filter extenders. func (g *genericScheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) { diagnosis := framework.Diagnosis{ NodeToStatusMap: make(framework.NodeToStatusMap), UnschedulablePlugins: sets.NewString(), } // Run \"prefilter\" plugins. s := fwk.RunPreFilterPlugins(ctx, state, pod) allNodes := g.nodeInfoSnapshot.NodeInfos().List() if len(pod.Status.NominatedNodeName) > 0 && featureGate.Enabled(features.PreferNominatedNode) { feasibleNodes := g.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis) if len(feasibleNodes) != 0 { return feasibleNodes, diagnosis, nil } } feasibleNodes := g.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, allNodes) feasibleNodes = g.findNodesThatPassExtenders(pod, feasibleNodes, diagnosis.NodeToStatusMap) return feasibleNodes, diagnosis, nil } 它会过滤 过滤 PodSpect 中 NodeName 字段为空的 pods[43]，尝试为这样的 pods 挑选一个 node 调度上去。 调度算法 下面简单看下内置的默认调度算法。 注册默认 predicates 这些 predicates 其实都是函数，被调用到时，执行相应的过滤[44]。例如，如果 PodSpec 里面显式要求了 CPU 或 RAM 资源，而一个 node 无法满足这些条件， 那就会将这个 node 从备选列表中删除。 // pkg/scheduler/algorithmprovider/registry.go // NewRegistry returns an algorithm provider registry instance. func NewRegistry() Registry { defaultConfig := getDefaultConfig() applyFeatureGates(defaultConfig) caConfig := getClusterAutoscalerConfig() applyFeatureGates(caConfig) return Registry{ schedulerapi.SchedulerDefaultProviderName: defaultConfig, ClusterAutoscalerProvider: caConfig, } } func getDefaultConfig() *schedulerapi.Plugins { plugins := &schedulerapi.Plugins{ PreFilter: schedulerapi.PluginSet{...}, Filter: schedulerapi.PluginSet{ Enabled: []schedulerapi.Plugin{ {Name: nodename.Name}, // 指定 node name 调度 {Name: tainttoleration.Name}, // 指定 toleration 调度 {Name: nodeaffinity.Name}, // 指定 node affinity 调度 ... }, }, PostFilter: schedulerapi.PluginSet{...}, PreScore: schedulerapi.PluginSet{...}, Score: schedulerapi.PluginSet{ Enabled: []schedulerapi.Plugin{ {Name: interpodaffinity.Name, Weight: 1}, {Name: nodeaffinity.Name, Weight: 1}, {Name: tainttoleration.Name, Weight: 1}, ... }, }, Reserve: schedulerapi.PluginSet{...}, PreBind: schedulerapi.PluginSet{...}, Bind: schedulerapi.PluginSet{...}, } return plugins } plugin 的实现见 pkg/scheduler/framework/plugins/，以 nodename filter 为例： // pkg/scheduler/framework/plugins/nodename/node_name.go // Filter invoked at the filter extension point. func (pl *NodeName) Filter(ctx context.Context, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { if !Fits(pod, nodeInfo) { return framework.NewStatus(UnschedulableAndUnresolvable, ErrReason) } return nil } // 如果 pod 没有指定 NodeName，或者指定的 NodeName 等于该 node 的 name，返回 true；其他返回 false func Fits(pod *v1.Pod, nodeInfo *framework.NodeInfo) bool { return len(pod.Spec.NodeName) == 0 || pod.Spec.NodeName == nodeInfo.Node().Name } 对筛选出的 node 排序 选择了合适的 nodes 之后，接下来会执行一系列 priority function 对这些 nodes 进行排序。例如，如果算法是希望将 pods 尽量分散到整个集群，那 priority 会选择资源尽量空闲的节点。 这些函数会给每个 node 打分，得分最高的 node 会被选中，调度到该节点。 // pkg/scheduler/core/generic_scheduler.go // 运行打分插件（score plugins）对 nodes 进行排序。 func (g *genericScheduler) prioritizeNodes(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*v1.Node,) (framework.NodeScoreList, error) { // 如果没有指定 priority 配置，所有 node 将都得 1 分。 if len(g.extenders) == 0 && !fwk.HasScorePlugins() { result := make(framework.NodeScoreList, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodeScore{ Name: nodes[i].Name, Score: 1 }) } return result, nil } preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) // PreScoe 插件 scoresMap, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) // Score 插件 result := make(framework.NodeScoreList, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodeScore{Name: nodes[i].Name, Score: 0}) for j := range scoresMap { result[i].Score += scoresMap[j][i].Score } } if len(g.extenders) != 0 && nodes != nil { combinedScores := make(map[string]int64, len(nodes)) for i := range g.extenders { if !g.extenders[i].IsInterested(pod) { continue } go func(extIndex int) { prioritizedList, weight := g.extenders[extIndex].Prioritize(pod, nodes) for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } }(i) } for i := range result { result[i].Score += combinedScores[result[i].Name] * (MaxNodeScore / MaxExtenderPriority) } } return result, nil } 创建 v1.Binding 对象 算法选出一个 node 之后，调度器会创建一个 Binding 对象[45]， Pod 的 ObjectReference 字段的值就是选中的 node 的名字。 // pkg/scheduler/framework/runtime/framework.go func (f *frameworkImpl) runBindPlugin(ctx context.Context, bp BindPlugin, state *CycleState, pod *v1.Pod, nodeName string) *framework.Status { if !state.ShouldRecordPluginMetrics() { return bp.Bind(ctx, state, pod, nodeName) } status := bp.Bind(ctx, state, pod, nodeName) return status } // pkg/scheduler/framework/plugins/defaultbinder/default_binder.go // Bind binds pods to nodes using the k8s client. func (b DefaultBinder) Bind(ctx, state *CycleState, p *v1.Pod, nodeName string) *framework.Status { binding := &v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: p.Namespace, Name: p.Name, UID: p.UID}, Target: v1.ObjectReference{Kind: \"Node\", Name: nodeName}, // ObjectReference 字段为 nodeName } b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{}) } 如上，最后 ClientSet().CoreV1().Pods(binding.Namespace).Bind() 通过一个 POST 请求发给 apiserver。 kube-apiserver 更新 pod 对象 kube-apiserver 收到这个 Binding object 请求后，registry 反序列化对象，更新 Pod 对象的下列字段： 设置 NodeName 添加 annotations 设置 PodScheduled status 为 True // pkg/registry/core/pod/storage/storage.go func (r *BindingREST) setPodHostAndAnnotations(ctx context.Context, podID, oldMachine, machine string, annotations map[string]string, dryRun bool) (finalPod *api.Pod, err error) { podKey := r.store.KeyFunc(ctx, podID) r.store.Storage.GuaranteedUpdate(ctx, podKey, &api.Pod{}, false, nil, storage.SimpleUpdate(func(obj runtime.Object) (runtime.Object, error) { pod, ok := obj.(*api.Pod) pod.Spec.NodeName = machine if pod.Annotations == nil { pod.Annotations = make(map[string]string) } for k, v := range annotations { pod.Annotations[k] = v } podutil.UpdatePodCondition(&pod.Status, &api.PodCondition{ Type: api.PodScheduled, Status: api.ConditionTrue, }) return pod, nil }), dryRun, nil) } 自定义调度器 predicate 和 priority function 都是可扩展的，可以通过 --policy-config-file指定。 K8s 还可以自定义调度器（自己实现调度逻辑）。如果 PodSpec 中 schedulerName 字段不为空，K8s 就会 将这个 pod 的调度权交给指定的调度器。 5.5 小结 总结一下前面已经完成的步骤： HTTP 请求通过了认证、鉴权、admission control Deployment, ReplicaSet 和 Pod resources 已经持久化到 etcd 一系列 initializers 已经执行完毕， 每个 Pod 也已经调度到了合适的 node 上。 但是，到目前为止，我们看到的所有东西（状态），还只是存在于 etcd 中的元数据。下一步就是将这些状态同步到计算节点上，然后计算节点上的 agent（kubelet）就开始干活了。 6 kubelet 每个 K8s node 上都会运行一个名为 kubelet 的 agent，它负责 pod 生命周期管理。 这意味着，它负责将 “Pod” 的逻辑抽象（etcd 中的元数据）转换成具体的容器（container）。 挂载目录 创建容器日志 垃圾回收等等 6.1 Pod sync（状态同步） kubelet 也可以认为是一个 controller，它 通过 ListWatch 接口，从 kube-apiserver 获取属于本节点的 Pod 列表（根据spec.nodeName过滤[46]）， 然后与自己缓存的 pod 列表对比，如果有 pod 创建、删除、更新等操作，就开始同步状态。 下面具体看一下同步过程。 同步过程 // pkg/kubelet/kubelet.go // syncPod is the transaction script for the sync of a single pod. func (kl *Kubelet) syncPod(o syncPodOptions) error { pod := o.pod if updateType == SyncPodKill { // kill pod 操作 kl.killPod(pod, nil, podStatus, PodTerminationGracePeriodSecondsOverride) return nil } firstSeenTime := pod.Annotations[\"kubernetes.io/config.seen\"] // 测量 latency，从 apiserver 第一次看到 pod 算起 if updateType == SyncPodCreate { // create pod 操作 if !firstSeenTime.IsZero() { // Record pod worker start latency if being created metrics.PodWorkerStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime)) } } // Generate final API pod status with pod and status manager status apiPodStatus := kl.generateAPIPodStatus(pod, podStatus) podStatus.IPs = []string{} if len(podStatus.IPs) == 0 && len(apiPodStatus.PodIP) > 0 { podStatus.IPs = []string{apiPodStatus.PodIP} } runnable := kl.canRunPod(pod) if !runnable.Admit { // Pod is not runnable; update the Pod and Container statuses to why. apiPodStatus.Reason = runnable.Reason ... } kl.statusManager.SetPodStatus(pod, apiPodStatus) // Kill pod if it should not be running if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { return kl.killPod(pod, nil, podStatus, nil) } // 如果 network plugin not ready，并且 pod 网络不是 host network 类型，返回相应错误 if err := kl.runtimeState.networkErrors(); err != nil && !IsHostNetworkPod(pod) { return fmt.Errorf(\"%s: %v\", NetworkNotReadyErrorMsg, err) } // Create Cgroups for the pod and apply resource parameters if cgroups-per-qos flag is enabled. pcm := kl.containerManager.NewPodContainerManager() if kubetypes.IsStaticPod(pod) { // Create Mirror Pod for Static Pod if it doesn't already exist ... } kl.makePodDataDirs(pod) // Make data directories for the pod kl.volumeManager.WaitForAttachAndMount(pod) // Wait for volumes to attach/mount pullSecrets := kl.getPullSecretsForPod(pod) // Fetch the pull secrets for the pod // Call the container runtime's SyncPod callback result := kl.containerRuntime.SyncPod(pod, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) } 如果是 pod 创建事件，会记录一些 pod latency 相关的 metrics； 然后调用 generateAPIPodStatus() 生成一个 v1.PodStatus 对象，代表 pod 当前阶段（Phase）的状态。 Pod 的 Phase 是对其生命周期中不同阶段的高层抽象，非常复杂，后面会介绍。 PodStatus 生成之后，将发送给 Pod status manager，后者的任务是异步地通过 apiserver 更新 etcd 记录。 接下来会运行一系列 admission handlers，确保 pod 有正确的安全权限（security permissions）。 其中包括 enforcing AppArmor profiles and NO_NEW_PRIVS[47]。在这个阶段被 deny 的 Pods 将无限期处于 Pending 状态。 如果指定了 cgroups-per-qos，kubelet 将为这个 pod 创建 cgroups。可以实现更好的 QoS。 为容器创建一些目录。包括 pod 目录 （一般是 /var/run/kubelet/pods/） volume 目录 (/volumes) plugin 目录 (/plugins). volume manager 将 等待[48]Spec.Volumes 中定义的 volumes attach 完成。取决于 volume 类型，pod 可能会等待很长时间（例如 cloud 或 NFS volumes）。 从 apiserver 获取 Spec.ImagePullSecrets 中指定的 secrets，注入容器。 容器运行时（runtime）创建容器（后面详细描述）。 Pod 状态 前面提到，generateAPIPodStatus() 生成一个 v1.PodStatus[49]对象，代表 pod 当前阶段（Phase）的状态。 Pod 的 Phase 是对其生命周期中不同阶段的高层抽象，包括 Pending Running Succeeded Failed Unknown 生成这个状态的过程非常复杂，一些细节如下： 首先，顺序执行一系列 PodSyncHandlers 。每个 handler 判断这个 pod 是否还应该留在这个 node 上。如果其中任何一个判断结果是否，那 pod 的 phase 将变为[50]PodFailed 并最终会被从这个 node 驱逐。 一个例子是 pod 的 activeDeadlineSeconds （Jobs 中会用到）超时之后，就会被驱逐。 接下来决定 Pod Phase 的将是其 init 和 real containers。由于此时容器还未启动，因此 将处于 waiting[51] 状态。有 waiting 状态 container 的 pod，将处于 Pending[52] Phase。 由于此时容器运行时还未创建我们的容器 ，因此它将把 PodReady 字段置为 False[53]. 6.2 CRI 及创建 pause 容器 至此，大部分准备工作都已完成，接下来即将创建容器了。创建容器是通过 Container Runtime （例如 docker 或 rkt）完成的。 为实现可扩展，kubelet 从 v1.5.0 开始，使用 CRI（Container Runtime Interface）与具体的容器运行时交互。简单来说，CRI 提供了 kubelet 和具体 runtime implementation 之间的抽象接口， 用 protocol buffers[54] 和 gRPC 通信。 CRI SyncPod // pkg/kubelet/kuberuntime/kuberuntime_manager.go // SyncPod syncs the running pod into the desired pod by executing following steps: // 1. Compute sandbox and container changes. // 2. Kill pod sandbox if necessary. // 3. Kill any containers that should not be running. // 4. Create sandbox if necessary. // 5. Create ephemeral containers. // 6. Create init containers. // 7. Create normal containers. // func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { // Step 1: Compute sandbox and container changes. podContainerChanges := m.computePodActions(pod, podStatus) if podContainerChanges.CreateSandbox { ref := ref.GetReference(legacyscheme.Scheme, pod) if podContainerChanges.SandboxID != \"\" { m.recorder.Eventf(\"Pod sandbox changed, it will be killed and re-created.\") } else { InfoS(\"SyncPod received new pod, will create a sandbox for it\") } } // Step 2: Kill the pod if the sandbox has changed. if podContainerChanges.KillPod { if podContainerChanges.CreateSandbox { InfoS(\"Stopping PodSandbox for pod, will start new one\") } else { InfoS(\"Stopping PodSandbox for pod, because all other containers are dead\") } killResult := m.killPodWithSyncResult(pod, ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if podContainerChanges.CreateSandbox { m.purgeInitContainers(pod, podStatus) } } else { // Step 3: kill any running containers in this pod which are not to keep. for containerID, containerInfo := range podContainerChanges.ContainersToKill { killContainerResult := NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) m.killContainer(pod, containerID, containerInfo) } } // Keep terminated init containers fairly aggressively controlled // This is an optimization because container removals are typically handled by container GC. m.pruneInitContainersBeforeStart(pod, podStatus) // Step 4: Create a sandbox for the pod if necessary. podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox { createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) podSandboxID, msg = m.createPodSandbox(pod, podContainerChanges.Attempt) podSandboxStatus := m.runtimeService.PodSandboxStatus(podSandboxID) } // the start containers routines depend on pod ip(as in primary pod ip) // instead of trying to figure out if we have 0 CRI create sandbox kubelet 发起 RunPodSandbox[55] RPC 调用。 “sandbox” 是一个 CRI 术语，它表示一组容器，在 K8s 里就是一个 Pod。这个词是有意用作比较宽泛的描述，这样对其他运行时的描述也是适用的（例如，在基于 hypervisor 的运行时中，sandbox 可能是一个虚拟机）。 // pkg/kubelet/kuberuntime/kuberuntime_sandbox.go // createPodSandbox creates a pod sandbox and returns (podSandBoxID, message, error). func (m *kubeGenericRuntimeManager) createPodSandbox(pod *v1.Pod, attempt uint32) (string, string, error) { podSandboxConfig := m.generatePodSandboxConfig(pod, attempt) // 创建 pod log 目录 m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755) runtimeHandler := \"\" if m.runtimeClassManager != nil { runtimeHandler = m.runtimeClassManager.LookupRuntimeHandler(pod.Spec.RuntimeClassName) if runtimeHandler != \"\" { InfoS(\"Running pod with runtime handler\", runtimeHandler) } } podSandBoxID := m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler) return podSandBoxID, \"\", nil } // pkg/kubelet/cri/remote/remote_runtime.go // RunPodSandbox creates and starts a pod-level sandbox. func (r *remoteRuntimeService) RunPodSandbox(config *PodSandboxConfig, runtimeHandler string) (string, error) { InfoS(\"[RemoteRuntimeService] RunPodSandbox\", \"config\", config, \"runtimeHandler\", runtimeHandler) resp := r.runtimeClient.RunPodSandbox(ctx, &runtimeapi.RunPodSandboxRequest{ Config: config, RuntimeHandler: runtimeHandler, }) InfoS(\"[RemoteRuntimeService] RunPodSandbox Response\", \"podSandboxID\", resp.PodSandboxId) return resp.PodSandboxId, nil } Create sandbox：docker 相关代码 前面是 CRI 通用代码，如果我们的容器 runtime 是 docker，那接下来就会调用到 docker 相关代码。 在这种 runtime 中，创建一个 sandbox 会转换成创建一个 “pause” 容器的操作。Pause container 作为一个 pod 内其他所有容器的父角色，hold 了很多 pod-level 的资源， 具体说就是 Linux namespace，例如 IPC NS、Net NS、IPD NS。 \"pause\" container 提供了一种持有这些 ns、让所有子容器共享它们 的方式。例如，共享 netns 的好处之一是，pod 内不同容器之间可以通过 localhost 方式访问彼此。pause 容器的第二个用处是回收（reaping）dead processes。更多信息，可参考 这篇博客[56]。 Pause 容器创建之后，会被 checkpoint 到磁盘，然后启动。 // pkg/kubelet/dockershim/docker_sandbox.go // 对于 docker runtime，PodSandbox 实现为一个 holding 网络命名空间（netns）的容器 func (ds *dockerService) RunPodSandbox(ctx context.Context, r *RunPodSandboxRequest) (*RunPodSandboxResponse) { // Step 1: Pull the image for the sandbox. ensureSandboxImageExists(ds.client, image) // Step 2: Create the sandbox container. createConfig := ds.makeSandboxDockerConfig(config, image) createResp := ds.client.CreateContainer(*createConfig) resp := &runtimeapi.RunPodSandboxResponse{PodSandboxId: createResp.ID} ds.setNetworkReady(createResp.ID, false) // 容器 network 状态初始化为 false // Step 3: Create Sandbox Checkpoint. CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config)) // Step 4: Start the sandbox container。 如果失败，kubelet 会 GC 掉 sandbox ds.client.StartContainer(createResp.ID) rewriteResolvFile() // 如果是 hostNetwork 类型，到这里就可以返回了，无需下面的 CNI 流程 if GetNetwork() == NamespaceMode_NODE { return resp, nil } // Step 5: Setup networking for the sandbox with CNI // 包括分配 IP、设置 sandbox 内的路由、创建虚拟网卡等。 cID := kubecontainer.BuildContainerID(runtimeName, createResp.ID) ds.network.SetUpPod(Namespace, Name, cID, Annotations, networkOptions) return resp, nil } 最后调用的 SetUpPod() 为容器创建网络，它有会调用到 plugin manager 的同名方法： // pkg/kubelet/dockershim/network/plugins.go func (pm *PluginManager) SetUpPod(podNamespace, podName, id ContainerID, annotations, options) error { const operation = \"set_up_pod\" fullPodName := kubecontainer.BuildPodFullName(podName, podNamespace) // 调用 CNI 插件为容器设置网络 pm.plugin.SetUpPod(podNamespace, podName, id, annotations, options) } Cgroup 也很重要，是 Linux 掌管资源分配的方式，docker 利用它实现资源隔离。更多信息，参考 What even is a Container?[57] 6.3 CNI 前半部分：CNI plugin manager 处理 现在我们的 pod 已经有了一个占坑用的 pause 容器，它占住了 pod 需要用到的所有 namespace。接下来需要做的就是：调用底层的具体网络方案（bridge/flannel/calico/cilium 等等） 提供的 CNI 插件，创建并打通容器的网络。 CNI 是 Container Network Interface 的缩写，工作机制与 Container Runtime Interface 类似。简单来说，CNI 是一个抽象接口，不同的网络提供商只要实现了 CNI 中的几个方法，就能接入 K8s，为容器创建网络。kubelet 与 CNI 插件之间通过 JSON 数据交互（配置文件放在 /etc/cni/net.d），通过 stdin 将配置数据传递给 CNI binary (located in /opt/cni/bin)。 CNI 插件有自己的配置，例如，内置的 bridge 插件可能配置如下： { \"cniVersion\": \"0.3.1\", \"name\": \"bridge\", \"type\": \"bridge\", \"bridge\": \"cnio0\", \"isGateway\": true, \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"ranges\": [ [{\"subnet\": \"${POD_CIDR}\"}] ], \"routes\": [{\"dst\": \"0.0.0.0/0\"}] } } 还会通过 CNI_ARGS 环境变量传递 pod metadata，例如 name 和 ns。 调用栈概览 下面的调用栈是 CNI 前半部分：CNI plugin manager 调用到具体的 CNI 插件（可执行文件）， 执行 shell 命令为容器创建网络： SetUpPod // pkg/kubelet/dockershim/network/cni/cni.go |-ns = plugin.host.GetNetNS(id) |-plugin.addToNetwork(name, id, ns) // -> pkg/kubelet/dockershim/network/cni/cni.go |-plugin.buildCNIRuntimeConf |-cniNet.AddNetworkList(netConf) // -> github.com/containernetworking/cni/libcni/api.go |-for net := range list.Plugins | result = c.addNetwork | |-pluginPath = FindInPath(c.Path) | |-ValidateContainerID(ContainerID) | |-ValidateNetworkName(name) | |-ValidateInterfaceName(IfName) | |-invoke.ExecPluginWithResult(pluginPath, c.args(\"ADD\", rt)) | |-shell(\"/opt/cni/bin/xx \") | |-c.cacheAdd(result, list.Bytes, list.Name, rt) 最后一层调用 ExecPlugin()： // vendor/github.com/containernetworking/cni/pkg/invoke/raw_exec.go func (e *RawExec) ExecPlugin(ctx, pluginPath, stdinData []byte, environ []string) ([]byte, error) { c := exec.CommandContext(ctx, pluginPath) c.Env = environ c.Stdin = bytes.NewBuffer(stdinData) c.Stdout = stdout c.Stderr = stderr for i := 0; i 可以看到，经过上面的几层调用，最终是通过 shell 命令执行了宿主机上的 CNI 插件， 例如 /opt/cni/bin/cilium-cni，并通过 stdin 传递了一些 JSON 参数。 6.4 CNI 后半部分：CNI plugin 实现 下面看 CNI 处理的后半部分：CNI 插件为容器创建网络，也就是可执行文件 /opt/cni/bin/xxx 的实现。 CNI 相关的代码维护在一个单独的项目 github.com/containernetworking/cni[58]。每个 CNI 插件只需要实现其中的几个方法，然后编译成独立的可执行文件，放在 /etc/cni/bin下面即可。下面是一些具体的插件， $ ls /opt/cni/bin/ bridge cilium-cni cnitool dhcp host-local ipvlan loopback macvlan noop 调用栈概览 CNI 插件（可执行文件）执行时会调用到 PluginMain()，从这往后的调用栈 （注意源文件都是 github.com/containernetworking/cni 项目中的路径）： PluginMain // pkg/skel/skel.go |-PluginMainWithError // pkg/skel/skel.go |-pluginMain // pkg/skel/skel.go |-switch cmd { case \"ADD\": checkVersionAndCall(cmdArgs, cmdAdd) // pkg/skel/skel.go |-configVersion = Decode(cmdArgs.StdinData) |-Check(configVersion, pluginVersionInfo) |-toCall(cmdArgs) // toCall == cmdAdd |-cmdAdd(cmdArgs) |-specific CNI plugin implementations case \"DEL\": checkVersionAndCall(cmdArgs, cmdDel) case \"VERSION\": versionInfo.Encode(t.Stdout) default: return createTypedError(\"unknown CNI_COMMAND: %v\", cmd) } 可见对于 kubelet 传过来的 \"ADD\" 命令，最终会调用到 CNI 插件的 cmdAdd() 方法 —— 该方法默认是空的，需要由每种 CNI 插件自己实现。同理，删除 pod 时对应的是 \"DEL\"操作，调用到的 cmdDel() 方法也是要由具体 CNI 插件实现的。 CNI 插件实现举例：Bridge github.com/containernetworking/plugins[59]项目中包含了很多种 CNI plugin 的实现，例如 IPVLAN、Bridge、MACVLAN、VLAN 等等。 bridge CNI plugin 的实现见plugins/main/bridge/bridge.go[60] 执行逻辑如下： 在默认 netns 创建一个 Linux bridge，这台宿主机上的所有容器都将连接到这个 bridge。 创建一个 veth pair，将容器和 bridge 连起来。 分配一个 IP 地址，配置到 pause 容器，设置路由。 IP 从配套的网络服务 IPAM（IP Address Management）中分配的。最场景的 IPAM plugin 是host-local，它从预先设置的一个网段里分配一个 IP，并将状态信息写到宿主机的本地文件系统，因此重启不会丢失。host-local IPAM 的实现见 plugins/ipam/host-local[61]。 修改 resolv.conf，为容器配置 DNS。这里的 DNS 信息是从传给 CNI plugin 的参数中解析的。 以上过程完成之后，容器和宿主机（以及同宿主机的其他容器）之间的网络就通了， CNI 插件会将结果以 JSON 返回给 kubelet。 CNI 插件实现举例：Noop 再来看另一种比较有趣的 CNI 插件：noop。这个插件是 CNI 项目自带的， 代码见 plugins/test/noop/main.go[62]。 func cmdAdd(args *skel.CmdArgs) error { return debugBehavior(args, \"ADD\") } func cmdDel(args *skel.CmdArgs) error { return debugBehavior(args, \"DEL\") } 从名字以及以上代码可以看出，这个 CNI 插件（几乎）什么事情都不做。用途： 测试或调试：它可以打印 debug 信息。 给只支持 hostNetwork 的节点使用。 每个 node 上必须有一个配置正确的 CNI 插件，kubelet 自检才能通过，否则 node 会处于 NotReady 状态。 某些情况下，我们不想让一些 node（例如 master node）承担正常的、创建带 IP pod 的工作， 只要它能创建 hostNetwork 类型的 pod 就行了（这样就无需给这些 node 分配 PodCIDR， 也不需要在 node 上启动 IPAM 服务）。 这种情况下，就可以用 noop 插件。参考配置： $ cat /etc/cni/net.d/98-noop.conf { \"cniVersion\": \"0.3.1\", \"type\": \"noop\" } CNI 插件实现举例：Cilium 这个就很复杂了，做的事情非常多，可参考 Cilium Code Walk Through: CNI Create Network。 6.5 为容器配置跨节点通信网络（inter-host networking） 这项工作不在 K8s 及 CNI 插件的职责范围内，是由具体网络方案 在节点上的 agent 完成的，例如 flannel 网络的 flanneld，cilium 网络的 cilium-agent。 简单来说，跨节点通信有两种方式： 隧道（tunnel or overlay） 直接路由 这里赞不展开，可参考迈入 Cilium+BGP 的云原生网络时代。 6.6 创建 init 容器及业务容器 至此，网络部分都配置好了。接下来就开始启动真正的业务容器。 Sandbox 容器初始化完成后，kubelet 就开始创建其他容器。首先会启动 PodSpec 中指定的所有 init 容器，代码[63]然后才启动主容器（main containers）。 调用栈概览 startContainer |-m.runtimeService.CreateContainer // pkg/kubelet/cri/remote/remote_runtime.go | |-r.runtimeClient.CreateContainer // -> pkg/kubelet/dockershim/docker_container.go | |-new(CreateContainerResponse) // staging/src/k8s.io/cri-api/pkg/apis/runtime/v1/api.pb.go | |-Invoke(\"/runtime.v1.RuntimeService/CreateContainer\") | | CreateContainer // pkg/kubelet/dockershim/docker_container.go | |-ds.client.CreateContainer // -> pkg/kubelet/dockershim/libdocker/instrumented_client.go | |-d.client.ContainerCreate // -> vendor/github.com/docker/docker/client/container_create.go | |-cli.post(\"/containers/create\") | |-json.NewDecoder().Decode(&resp) | |-m.runtimeService.StartContainer(containerID) // -> pkg/kubelet/cri/remote/remote_runtime.go |-r.runtimeClient.StartContainer |-new(CreateContainerResponse) // staging/src/k8s.io/cri-api/pkg/apis/runtime/v1/api.pb.go |-Invoke(\"/runtime.v1.RuntimeService/StartContainer\") 具体过程 // pkg/kubelet/kuberuntime/kuberuntime_container.go func (m *kubeGenericRuntimeManager) startContainer(podSandboxID, podSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { container := spec.container // Step 1: 拉镜像 m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig) // Step 2: 通过 CRI 创建容器 containerConfig := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, podIPs, target) m.internalLifecycle.PreCreateContainer(pod, container, containerConfig) containerID := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) m.internalLifecycle.PreStartContainer(pod, container, containerID) // Step 3: 启动容器 m.runtimeService.StartContainer(containerID) legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) m.osInterface.Symlink(containerLog, legacySymlink) // Step 4: 执行 post start hook m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) } 过程： 拉镜像[64]。如果是私有镜像仓库，就会从 PodSpec 中寻找访问仓库用的 secrets。 通过 CRI 创建 container[65]。 从 parent PodSpec 的 ContainerConfig struct 中解析参数（command, image, labels, mounts, devices, env variables 等等）， 然后通过 protobuf 发送给 CRI plugin。例如对于 docker，收到请求后会反序列化，从中提取自己需要的参数，然后发送给 Daemon API。过程中它会给容器添加几个 metadata labels （例如 container type, log path, sandbox ID）。 然后通过 runtimeService.startContainer() 启动容器； 如果注册了 post-start hooks，接下来就执行这些 hooks。post Hook 类型： Exec：在容器内执行具体的 shell 命令。 HTTP：对容器内的服务（endpoint）发起 HTTP 请求。 如果 PostStart hook 运行时间过长，或者 hang 住或失败了，容器就无法进入 running状态。 7 结束 至此，应该已经有 3 个 pod 在运行了，取决于系统资源和调度策略，它们可能在一台 node 上，也可能分散在多台。 脚注 [1]What happens when ... Kubernetes edition!: https://github.com/jamiehannaford/what-happens-when-k8s[2]v1.21: https://github.com/kubernetes/kubernetes/tree/v1.21.1[3]generic apiserver: https://github.com/kubernetes/kubernetes/blob/v1.21.0/cmd/kube-apiserver/app/server.go#L219[4]Config.OpenAPIConfig 字段: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/server/config.go#L167[5]storage provider: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go#L204[6]配置 REST mappings: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go#L92[7]镜像名为空或格式不对: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go#L262[8]generator: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go#L300[9]文档: https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators[10]BasicPod: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kubectl/pkg/generate/versioned/run.go#L233[11]实现: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kubectl/pkg/generate/versioned/run.go#L259[12]搜索合适的 API group 和版本: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go#L610-L619[13]创建一个正确版本的客户端（versioned client）: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go#L641[14]缓存这份 OpenAPI schema: https://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/cli-runtime/pkg/genericclioptions/config_flags.go#L234[15]发送出去: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go#L654[16]预定义的路径: https://github.com/kubernetes/client-go/blob/v1.21.0/tools/clientcmd/loader.go#L52[17]TLS: https://github.com/kubernetes/client-go/blob/82aa063804cf055e16e8911250f888bc216e8b61/rest/transport.go#L80-L89[18]发送: https://github.com/kubernetes/client-go/blob/c6f8cf2c47d21d55fa0df928291b2580544886c8/transport/round_trippers.go#L314[19]发送: https://github.com/kubernetes/client-go/blob/c6f8cf2c47d21d55fa0df928291b2580544886c8/transport/round_trippers.go#L223[20]命令行参数: https://kubernetes.io/docs/admin/kube-apiserver/[21]x509 handler: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go#L60[22]bearer token handler: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/authentication/request/bearertoken/bearertoken.go#L38[23]basicauth handler: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/plugin/pkg/authenticator/request/basicauth/basicauth.go#L37[24]加上用户信息: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go#L71-L75[25]进一步处理: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go#L60[26]webhook: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go#L143[27]ABAC: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/auth/authorizer/abac/abac.go#L223[28]RBAC: https://github.com/kubernetes/kubernetes/blob/v1.21.0/plugin/pkg/auth/authorizer/rbac/rbac.go#L43[29]Node: https://github.com/kubernetes/kubernetes/blob/v1.21.0/plugin/pkg/auth/authorizer/node/node_authorizer.go#L67[30]admission controllers: https://kubernetes.io/docs/admin/admission-controllers/#what-are-they[31]plugin/pkg/admission 目录: https://github.com/kubernetes/kubernetes/tree/master/plugin/pkg/admission[32]POST handler: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go#L815[33]分派给相应的 handler: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/server/handler.go#L136[34]path-based handler: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/server/mux/pathrecorder.go#L146[35]createHandler: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go#L37[36]写到 etcd: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go#L401[37]storage provider: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L362[38]生成的: https://github.com/kubernetes/kubernetes/blob/v1.21.0/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go#L131-L142[39]initializers: https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers[40]注册的 addDeployment() 回调函数: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/controller/deployment/deployment_controller.go#L122[41]批处理的: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/controller/replicaset/replica_set.go#L487[42]这篇博客: http://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores[43]过滤 PodSpect 中 NodeName 字段为空的 pods: https://github.com/kubernetes/kubernetes/blob/v1.21.0/plugin/pkg/scheduler/factory/factory.go#L190[44]过滤: https://github.com/kubernetes/kubernetes/blob/v1.21.0/plugin/pkg/scheduler/core/generic_scheduler.go#L117[45]创建一个 Binding 对象: https://github.com/kubernetes/kubernetes/blob/v1.21.0/plugin/pkg/scheduler/scheduler.go#L336-L342[46]过滤: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/config/apiserver.go#L32[47]AppArmor profiles and NO_NEW_PRIVS: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kubelet.go#L883-L884[48]等待: https://github.com/kubernetes/kubernetes/blob/2723e06a251a4ec3ef241397217e73fa782b0b98/pkg/kubelet/volumemanager/volume_manager.go#L330[49]生成一个 v1.PodStatus: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kubelet_pods.go#L1287[50]将变为: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kubelet_pods.go#L1293-L1297[51]waiting: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kubelet_pods.go#L1244[52]Pending: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kubelet_pods.go#L1258-L1261[53]PodReady 字段置为 False: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/status/generate.go#L70-L81[54]protocol buffers: https://github.com/google/protobuf[55]发起 RunPodSandbox: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kuberuntime/kuberuntime_sandbox.go#L51[56]这篇博客: https://www.ianlewis.org/en/almighty-pause-container[57]What even is a Container?: https://jvns.ca/blog/2016/10/10/what-even-is-a-container/[58]github.com/containernetworking/cni: https://github.com/containernetworking/cni[59]github.com/containernetworking/plugins: https://github.com/containernetworking/plugins[60]plugins/main/bridge/bridge.go: https://github.com/containernetworking/plugins/blob/v0.9.1/plugins/main/bridge/bridge.go[61]plugins/ipam/host-local: https://github.com/containernetworking/plugins/tree/v0.9.1/plugins/ipam/host-local[62]plugins/test/noop/main.go: https://github.com/containernetworking/cni/blob/v0.8.1/plugins/test/noop/main.go#L184[63]代码: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L690[64]拉镜像: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L140[65]创建 container: https://github.com/kubernetes/kubernetes/blob/v1.21.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L179 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/readme.html":{"url":"blog/kubernetes/readme.html","title":"Readme","keywords":"","body":"Kubernetes Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/":{"url":"blog/kubernetes/task/","title":"Task","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/HPA.html":{"url":"blog/kubernetes/task/HPA.html","title":"HPA","keywords":"","body":"K8s: Horizontal Pod Autoscaler Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。 Pod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。 pod　水平扩缩实现机制 HPA　是一个周期采集指标后，通过指标对比算法，得出期望pod数，然后在时间窗口内根据建议信息进行平滑扩缩。 指标 在 Kubernetes 1.6 支持了基于多个指标进行缩放。 你可以使用 autoscaling/v2beta2 API 来为 Horizontal Pod Autoscaler 指定多个指标。 Horizontal Pod Autoscaler 会跟据每个指标计算，并生成一个缩放建议。 幅度最大的缩放建议会被采纳。 每个周期内，controller manager 根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。 controller manager 可以从 resource metrics API（每个pod 资源指标）和 custom metrics API（其他指标）获取指标。 周期 由 controller manager 的 --horizontal-pod-autoscaler-sync-period 参数 指定周期（默认值为15秒） 算法 从最基本的角度来看，pod 水平自动缩放控制器跟据当前指标和期望指标来计算缩放比例。 期望副本数 = ceil[当前副本数 * ( 当前指标 / 期望指标 )] 例如，当前指标为200m，目标设定值为100m,那么由于200.0 / 100.0 == 2.0， 副本数量将会翻倍。 如果当前指标为50m，副本数量将会减半，因为50.0 / 100.0 == 0.5。 如果计算出的缩放比例接近1.0（跟据--horizontal-pod-autoscaler-tolerance 参数全局配置的容忍值，默认为0.1）， 将会放弃本次缩放。 异常情况 由于受技术限制，pod 水平缩放控制器无法准确的知道 pod 什么时候就绪， 也就无法决定是否暂时搁置该 pod。 --horizontal-pod-autoscaler-initial-readiness-delay 参数（默认为30s），用于设置 pod 准备时间， 在此时间内的 pod 统统被认为未就绪。 --horizontal-pod-autoscaler-cpu-initialization-period参数（默认为5分钟），用于设置 pod 的初始化时间， 在此时间内的 pod，CPU 资源指标将不会被采纳。 如果有任何 pod 的指标缺失，我们会更保守地重新计算平均值， 在需要缩小时假设这些 pod 消耗了目标值的 100%， 在需要放大时假设这些 pod 消耗了0%目标值。 这可以在一定程度上抑制伸缩的幅度。 建议信息 最后，在 HPA 控制器执行缩放操作之前，会记录缩放建议信息（scale recommendation）。 控制器会在操作时间窗口中考虑所有的建议信息，并从中选择得分最高的建议。 这个值可通过 kube-controller-manager 服务的启动参数 --horizontal-pod-autoscaler-downscale-stabilization 进行配置， 默认值为 5min。 这个配置可以让系统更为平滑地进行缩容操作，从而消除短时间内指标值快速波动产生的影响。 冷却/延迟 当使用 Horizontal Pod Autoscaler 管理一组副本缩放时， 有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 抖动。 1.6: --horizontal-pod-autoscaler-downscale-stabilization: 这个 kube-controller-manager 的参数表示缩容冷却时间。 即自从上次缩容执行结束后，多久可以再次执行缩容，默认时间是5min。 HPA的配置 HPA通常会根据type从aggregated APIs (metrics.k8s.io, custom.metrics.k8s.io, external.metrics.k8s.io)的资源路径上拉取metrics HPA支持的metrics类型有4种(下述为v2beta2的格式)： resource pods object external resource：目前仅支持cpu和memory。target可以指定数值(targetAverageValue)和比例(targetAverageUtilization)进行扩缩容 kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta1 metadata: name: app-3412287192 namespace: app-1683527219 spec: scaleTargetRef: kind: StatefulSet name: app-3412287192 apiVersion: apps/v1 minReplicas: 1 maxReplicas: 3 metrics: - type: Resource resource: name: memory targetAverageUtilization: 20 - type: Resource resource: name: cpu targetAverageUtilization: 20 pods：custom metrics，这类metrics描述了pod类型，target仅支持按指定数值(targetAverageValue)进行扩缩容。targetAverageValue 用于计算所有相关pods上的metrics的平均值 ... type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k object：custom metrics，这类metrics描述了相同命名空间下的(非pod)类型。target支持通过value和AverageValue进行扩缩容，前者直接将metric与target比较进行扩缩容，后者通过metric相关的pod数目与target比较进行扩缩容 ... type: Object object: metric: name: requests-per-second describedObject: apiVersion: extensions/v1beta1 kind: Ingress name: main-route target: type: Value value: 2k external：kubernetes 1.10+。这类metrics与kubernetes集群无关(pods和object需要与kubernetes中的某一类型关联)。与object类似，target支持通过value和AverageValue进行扩缩容。由于external会尝试匹配所有kubernetes资源的metrics，因此实际中不建议使用该类型。 ... - type: External external: metric: name: queue_messages_ready selector: \"queue=worker_tasks\" target: type: AverageValue averageValue: 30 注：target的value的一个单位可以划分为1000份，每一份以m为单位，如500m表示1/2个单位。参见Quantity kubernetes HPA的算法如下： desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )] 当使用targetAverageValue 或targetAverageUtilization时，currentMetricValue会取HPA指定的所有pods的metric的平均值 操作实践 前提条件 API aggregation layer 已开启 相应的 API 已注册 资源指标会使用 metrics.k8s.io API，一般由 metrics-server 提供。 它可以做为集群组件启动。 用户指标会使用 custom.metrics.k8s.io API。 它由其他厂商的“适配器”API 服务器提供。 确认你的指标管道，或者查看 list of known solutions。 外部指标会使用 external.metrics.k8s.io API。可能由上面的用户指标适配器提供。 --horizontal-pod-autoscaler-use-rest-clients 参数设置为 true 或者不设置。 如果设置为 false，则会切换到基于 Heapster 的自动缩放，这个特性已经被弃用了。 deploy metric-server https://github.com/kubernetes-sigs/metrics-server kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml 安装后会报错，修改成如下参数 containers: - args: - --cert-dir=/tmp - --kubelet-insecure-tls - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP 一分钟后，度量服务器开始报告节点和pod的CPU和内存使用情况。 查看nodes metrics： kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq . 一）Resource 类型的HPA deploy one deployment kubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80 创建 Horizontal Pod Autoscaler kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 kubectl get hpa add workload kubectl run -i --tty load-generator --image=busybox /bin/sh Hit enter for command prompt while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done 二）Customer Metric HPA 自定义metric HPA原理： 首选需要注册一个apiservice(custom metrics API)。 当HPA请求metrics时，kube-aggregator(apiservice的controller)会将请求转发到adapter，adapter作为kubernentes集群的pod，实现了Kubernetes resource metrics API 和custom metrics API，它会根据配置的rules从Prometheus抓取并处理metrics，在处理(如重命名metrics等)完后将metric通过custom metrics API返回给HPA。最后HPA通过获取的metrics的value对Deployment/ReplicaSet进行扩缩容。 adapter作为extension-apiserver(即自己实现的pod)，充当了代理kube-apiserver请求Prometheus的功能。 如下是k8s-prometheus-adapter apiservice的定义，kube-aggregator通过下面的service将请求转发给adapter。v1beta1.custom.metrics.k8s.io是写在k8s-prometheus-adapter代码中的，因此不能任意改变。 apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1beta1.custom.metrics.k8s.io spec: service: name: custom-metrics-apiserver namespace: custom-metrics group: custom.metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使用 Prometheus Adapter，Prometheus 用于监控应用的负载和集群本身的各种指标，Prometheus Adapter 可以帮我们使用 Prometheus 收集的指标并使用它们来制定扩展策略，这些指标都是通过 APIServer 暴露的，而且 HPA 资源对象也可以很轻易的直接使用 实践目标： 对开启服务网格的应用根据请求数进行自动扩缩 原始指标： istio_requests_total 采集时间间隔： 1min HPA中采集的指标名称：istio_requests_per_min 思考： prometheus 中是否需要添加类似 istio_request__count 的指标收集？怎么添加指标？ HPA 是怎么锁定service级别的指标收集，而不是单个pods的？ HPA 采样达标多久后开始执行扩容？ 1. 根据 service 请求指标进行自动扩缩 cd $GOPATH git clone https://github.com/stefanprodan/k8s-prom-hpa 2. 制作证书 #!/bin/bash # 生成根秘钥及证书 openssl req -x509 -sha256 -newkey rsa:2048 -keyout ca.key -out ca.crt -days 3560 -nodes -subj '/CN=custom-metrics-apiserver LStack Authority' # 生成服务器密钥，证书并使用CA证书签名 openssl genrsa -out server.key 2048 openssl req -new -key server.key -subj \"/CN=custom-metrics-apiserver\" -out server.csr #echo subjectAltName = IP:oam.lstack.com.cn > extfile.cnf #openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile extfile.cnf -out server.crt -days 3650 openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 3650 kubectl create secret generic cm-adapter-serving-certs \\ --from-file=server.crt=server.crt \\ --from-file=server.key=server.key \\ --from-file=ca.crt=ca.crt \\ -n monitoring 3. 创建k8s-prometheus-adapter 3.1 查看当前集群中的namespace 和 pod labels 3.2 set custom metrics deployment configmap rules: - seriesQuery: '{__name__=~\"istio_requests_total\"}' seriesFilters: [] resources: overrides: kubernetes_namespace: resource: namespace kubernetes_pod_name: resource: pod destination_service_name: resource: service name: matches: \"^(.*)_total\" as: \"${1}_per_min\" metricsQuery: sum(increase(>{>}[1m])) by (>) seriesQuery：查询 Prometheus 的语句，通过这个查询语句查询到的所有指标都可以用于 HPA seriesFilters：查询到的指标可能会存在不需要的，可以通过它过滤掉。 resources：通过 seriesQuery 查询到的只是指标，如果需要查询某个 Pod 的指标，肯定要将它的名称和所在的命名空间作为指标的标签进行查询，resources 就是将指标的标签和 k8s 的资源类型关联起来，最常用的就是 pod 和 namespace。有两种添加标签的方式，一种是 overrides，另一种是 template。 overrides：它会将指标中的标签和 k8s 资源关联起来。上面示例中就是将指标中的 pod 和 namespace 标签和 k8s 中的 pod 和 namespace 关联起来，因为 pod 和 namespace 都属于核心 api 组，所以不需要指定 api 组。当我们查询某个 pod 的指标时，它会自动将 pod 的名称和名称空间作为标签加入到查询条件中。比如 nginx: {group: \"apps\", resource: \"deployment\"} 这么写表示的就是将指标中 nginx 这个标签和 apps 这个 api 组中的 deployment 资源关联起来； template：通过 go 模板的形式。比如template: \"kube>>\" 这么写表示，假如 > 为 apps，> 为 deployment，那么它就是将指标中 kube_apps_deployment 标签和 deployment 资源关联起来。 name：用来给指标重命名的，之所以要给指标重命名是因为有些指标是只增的，比如以 total 结尾的指标。这些指标拿来做 HPA 是没有意义的，我们一般计算它的速率，以速率作为值，那么此时的名称就不能以 total 结尾了，所以要进行重命名。 matches：通过正则表达式来匹配指标名，可以进行分组 as：默认值为 $1，也就是第一个分组。as 为空就是使用默认值的意思。 metricsQuery：这就是 Prometheus 的查询语句了，前面的 seriesQuery 查询是获得 HPA 指标。当我们要查某个指标的值时就要通过它指定的查询语句进行了。可以看到查询语句使用了速率和分组，这就是解决上面提到的只增指标的问题。 Series：表示指标名称 LabelMatchers：附加的标签，目前只有 pod 和 namespace 两种，因此我们要在之前使用 resources 进行关联 GroupBy：就是 pod 名称，同样需要使用 resources 进行关联。 3.3 执行部署prometheus-adapter命令 kubectl create -f ./custom-metrics-api 3.4 List the custom metrics provided by Prometheus: kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1\" | jq . response: { \"kind\": \"APIResourceList\", \"apiVersion\": \"v1\", \"groupVersion\": \"custom.metrics.k8s.io/v1beta1\", \"resources\": [ { \"name\": \"namespaces/istio_requests_per_min\", \"singularName\": \"\", \"namespaced\": false, \"kind\": \"MetricValueList\", \"verbs\": [ \"get\" ] }, { \"name\": \"pods/istio_requests_per_min\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"MetricValueList\", \"verbs\": [ \"get\" ] }, { \"name\": \"services/istio_requests_per_min\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"MetricValueList\", \"verbs\": [ \"get\" ] } ] } 由于我们在prometheus规则中复写了3种资源，所以这里显示同一个指标（istio_requests_per_min），对应三种资源（namespaces，pods，services）的采集 4.创建要自动扩缩的应用和HPA 下们我们为istio bookinfo 中的 productpage创建HPA，当productpage 服务在1分钟内请求达到100次的时候启动pod扩容机制 apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: productpage spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: productpage-v1 minReplicas: 1 maxReplicas: 5 metrics: - type: Pods pods: metricName: istio_requests_per_min targetAverageValue: 100 # 只支持平均值 5.prometheus 观察 istio_requests_total 1min连接数指标 查询公式： sum(increase(istio_request_bytes_count{destination_service_name=\"productpage-v1\"}[1m])) 6.压力测试 为了增加CPU使用率，请使用rakyll / hey运行负载测试： #install hey go get -u github.com/rakyll/hey #do 10K requests hey -n 10000 -q 10 -c 5 http:// /productpage 思考解答： 1）prometheus 需要修改配置，添加 单位时间内采样指标名称和规则。 2 ) 修改 prometheus-adapter 的配置文件，rules[index].resources.overrides, 其中key 是 prometheus中的label，value 是k8s中的资源名称。[详细配置请看上面的3.2] 3）自动缩放器不会立即对使用峰值做出反应。默认情况下，度量标准同步每30秒发生一次，只有在最后3-5分钟内没有重新缩放时才能进行扩展/缩小。通过这种方式，HPA可以防止快速执行冲突的决策，并为Cluster Autoscaler提供时间。 三）External Metric HPA github 阿里云 扩展指标项目 参考连接: 官方文档 自定义指标 适配器 开发自定义指标适配器指南 github HPA 测试demo custom-metrics-api 设计提案 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/api 源码分析.html":{"url":"blog/kubernetes/task/api 源码分析.html","title":"Api 源码分析","keywords":"","body":"kube-apiserver 的设计与实现 发表于 2020-02-24 | 2 kube-apiserver 是 kubernetes 中与 etcd 直接交互的一个组件，其控制着 kubernetes 中核心资源的变化。它主要提供了以下几个功能： 提供 Kubernetes API，包括认证授权、数据校验以及集群状态变更等，供客户端及其他组件调用； 代理集群中的一些附加组件组件，如 Kubernetes UI、metrics-server、npd 等； 创建 kubernetes 服务，即提供 apiserver 的 Service，kubernetes Service； 资源在不同版本之间的转换； kube-apiserver 处理流程 kube-apiserver 主要通过对外提供 API 的方式与其他组件进行交互，可以调用 kube-apiserver 的接口 $ curl -k https://:6443或者通过其提供的 swagger-ui 获取到，其主要有以下三种 API： core group：主要在 /api/v1 下； named groups：其 path 为 /apis/$NAME/$VERSION； 暴露系统状态的一些 API：如/metrics 、/healthz 等； API 的 URL 大致以 /apis/group/version/namespaces/my-ns/myresource 组成，其中 API 的结构大致如下图所示： 了解了 kube-apiserver 的 API 后，下面会介绍 kube-apiserver 如何处理一个 API 请求，一个请求完整的流程如下图所示： 此处以一次 POST 请求示例说明，当请求到达 kube-apiserver 时，kube-apiserver 首先会执行在 http filter chain 中注册的过滤器链，该过滤器对其执行一系列过滤操作，主要有认证、鉴权等检查操作。当 filter chain 处理完成后，请求会通过 route 进入到对应的 handler 中，handler 中的操作主要是与 etcd 的交互，在 handler 中的主要的操作如下所示： Decoder kubernetes 中的多数 resource 都会有一个 internal version，因为在整个开发过程中一个 resource 可能会对应多个 version，比如 deployment 会有 extensions/v1beta1，apps/v1。 为了避免出现问题，kube-apiserver 必须要知道如何在每一对版本之间进行转换（例如，v1⇔v1alpha1，v1⇔v1beta1，v1beta1⇔v1alpha1），因此其使用了一个特殊的internal version，internal version 作为一个通用的 version 会包含所有 version 的字段，它具有所有 version 的功能。 Decoder 会首先把 creater object 转换到 internal version，然后将其转换为 storage version，storage version 是在 etcd 中存储时的另一个 version。 在解码时，首先从 HTTP path 中获取期待的 version，然后使用 scheme 以正确的 version 创建一个与之匹配的空对象，并使用 JSON 或 protobuf 解码器进行转换，在转换的第一步中，如果用户省略了某些字段，Decoder 会把其设置为默认值。 Admission 在解码完成后，需要通过验证集群的全局约束来检查是否可以创建或更新对象，并根据集群配置设置默认值。在 k8s.io/kubernetes/plugin/pkg/admission 目录下可以看到 kube-apiserver 可以使用的所有全局约束插件，kube-apiserver 在启动时通过设置 --enable-admission-plugins 参数来开启需要使用的插件，通过 ValidatingAdmissionWebhook 或 MutatingAdmissionWebhook 添加的插件也都会在此处进行工作。 Validation 主要检查 object 中字段的合法性。 在 handler 中执行完以上操作后最后会执行与 etcd 相关的操作，POST 操作会将数据写入到 etcd 中，以上在 handler 中的主要处理流程如下所示： v1beta1 ⇒ internal ⇒ | ⇒ | ⇒ v1 ⇒ json/yaml ⇒ etcd admission validation kube-apiserver 中的组件 kube-apiserver 共由 3 个组件构成（Aggregator、KubeAPIServer、APIExtensionServer），这些组件依次通过 Delegation 处理请求： Aggregator：暴露的功能类似于一个七层负载均衡，将来自用户的请求拦截转发给其他服务器，并且负责整个 APIServer 的 Discovery 功能； KubeAPIServer ：负责对请求的一些通用处理，认证、鉴权等，以及处理各个内建资源的 REST 服务； APIExtensionServer：主要处理 CustomResourceDefinition（CRD）和 CustomResource（CR）的 REST 请求，也是 Delegation 的最后一环，如果对应 CR 不能被处理的话则会返回 404。 Aggregator 和 APIExtensionsServer 对应两种主要扩展 APIServer 资源的方式，即分别是 AA 和 CRD。 Aggregator Aggregator 通过 APIServices 对象关联到某个 Service 来进行请求的转发，其关联的 Service 类型进一步决定了请求转发形式。Aggregator 包括一个 GenericAPIServer 和维护自身状态的 Controller。其中 GenericAPIServer 主要处理 apiregistration.k8s.io 组下的 APIService 资源请求。 Aggregator 除了处理资源请求外还包含几个 controller： 1、apiserviceRegistrationController：负责 APIServices 中资源的注册与删除； 2、availableConditionController：维护 APIServices 的可用状态，包括其引用 Service 是否可用等； 3、autoRegistrationController：用于保持 API 中存在的一组特定的 APIServices； 4、crdRegistrationController：负责将 CRD GroupVersions 自动注册到 APIServices 中； 5、openAPIAggregationController：将 APIServices 资源的变化同步至提供的 OpenAPI 文档； kubernetes 中的一些附加组件，比如 metrics-server 就是通过 Aggregator 的方式进行扩展的，实际环境中可以通过使用 apiserver-builder 工具轻松以 Aggregator 的扩展方式创建自定义资源。 启用 API Aggregation 在 kube-apiserver 中需要增加以下配置来开启 API Aggregation： --proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt --proxy-client-key-file=/etc/kubernetes/certs/proxy.key --requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt --requestheader-allowed-names=aggregator --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User KubeAPIServer KubeAPIServer 主要是提供对 API Resource 的操作请求，为 kubernetes 中众多 API 注册路由信息，暴露 RESTful API 并且对外提供 kubernetes service，使集群中以及集群外的服务都可以通过 RESTful API 操作 kubernetes 中的资源。 APIExtensionServer APIExtensionServer 作为 Delegation 链的最后一层，是处理所有用户通过 Custom Resource Definition 定义的资源服务器。 其中包含的 controller 以及功能如下所示： 1、openapiController：将 crd 资源的变化同步至提供的 OpenAPI 文档，可通过访问 /openapi/v2 进行查看； 2、crdController：负责将 crd 信息注册到 apiVersions 和 apiResources 中，两者的信息可通过 $ kubectl api-versions 和 $ kubectl api-resources 查看； 3、namingController：检查 crd obj 中是否有命名冲突，可在 crd .status.conditions 中查看； 4、establishingController：检查 crd 是否处于正常状态，可在 crd .status.conditions 中查看； 5、nonStructuralSchemaController：检查 crd obj 结构是否正常，可在 crd .status.conditions 中查看； 6、apiApprovalController：检查 crd 是否遵循 kubernetes API 声明策略，可在 crd .status.conditions 中查看； 7、finalizingController：类似于 finalizes 的功能，与 CRs 的删除有关； kube-apiserver 启动流程分析 kubernetes 版本：v1.16 首先分析 kube-apiserver 的启动方式，kube-apiserver 也是通过其 Run 方法启动主逻辑的，在Run 方法调用之前会进行解析命令行参数、设置默认值等。 Run Run 方法的主要逻辑为： 1、调用 CreateServerChain 构建服务调用链并判断是否启动非安全的 http server，http server 链中包含 apiserver 要启动的三个 server，以及为每个 server 注册对应资源的路由； 2、调用 server.PrepareRun 进行服务运行前的准备，该方法主要完成了健康检查、存活检查和OpenAPI路由的注册工作； 3、调用 prepared.Run 启动 https server； server 的初始化使用委托模式，通过 DelegationTarget 接口，把基本的 API Server、CustomResource、Aggregator 这三种服务采用链式结构串联起来，对外提供服务。 k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:147 func Run(completeOptions completedServerRunOptions, stopCh CreateServerChain CreateServerChain 是完成 server 初始化的方法，里面包含 APIExtensionsServer、KubeAPIServer、AggregatorServer 初始化的所有流程，最终返回 aggregatorapiserver.APIAggregator 实例，初始化流程主要有：http filter chain 的配置、API Group 的注册、http path 与 handler 的关联以及 handler 后端存储 etcd 的配置。其主要逻辑为： 1、调用 CreateKubeAPIServerConfig 创建 KubeAPIServer 所需要的配置，主要是创建 master.Config，其中会调用 buildGenericConfig 生成 genericConfig，genericConfig 中包含 apiserver 的核心配置； 2、判断是否启用了扩展的 API server 并调用 createAPIExtensionsConfig 为其创建配置，apiExtensions server 是一个代理服务，用于代理 kubeapiserver 中的其他 server，比如 metric-server； 3、调用 createAPIExtensionsServer 创建 apiExtensionsServer 实例； 4、调用 CreateKubeAPIServer初始化 kubeAPIServer； 5、调用 createAggregatorConfig 为 aggregatorServer 创建配置并调用 createAggregatorServer 初始化 aggregatorServer； 6、配置并判断是否启动非安全的 http server； k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:165 func CreateServerChain(completedOptions completedServerRunOptions, stopCh CreateKubeAPIServerConfig 在 CreateKubeAPIServerConfig 中主要是调用 buildGenericConfig 创建 genericConfig 以及构建 master.Config 对象。 k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:271 func CreateKubeAPIServerConfig( s completedServerRunOptions, nodeTunneler tunneler.Tunneler, proxyTransport *http.Transport, ) (......) { // 1、构建 genericConfig genericConfig, versionedInformers, insecureServingInfo, serviceResolver, pluginInitializers, admissionPostStartHook, storageFactory, lastErr = buildGenericConfig(s.ServerRunOptions, proxyTransport) if lastErr != nil { return } ...... // 2、初始化所支持的 capabilities capabilities.Initialize(capabilities.Capabilities{ AllowPrivileged: s.AllowPrivileged, PrivilegedSources: capabilities.PrivilegedSources{ HostNetworkSources: []string{}, HostPIDSources: []string{}, HostIPCSources: []string{}, }, PerConnectionBandwidthLimitBytesPerSec: s.MaxConnectionBytesPerSec, }) // 3、获取 service ip range 以及 api server service IP serviceIPRange, apiServerServiceIP, lastErr := master.DefaultServiceIPRange(s.PrimaryServiceClusterIPRange) if lastErr != nil { return } ...... // 4、构建 master.Config 对象 config = &master.Config{......} if nodeTunneler != nil { config.ExtraConfig.KubeletClientConfig.Dial = nodeTunneler.Dial } if config.GenericConfig.EgressSelector != nil { config.ExtraConfig.KubeletClientConfig.Lookup = config.GenericConfig.EgressSelector.Lookup } return } buildGenericConfig 主要逻辑为： 1、调用 genericapiserver.NewConfig 生成默认的 genericConfig，genericConfig 中主要配置了 DefaultBuildHandlerChain，DefaultBuildHandlerChain 中包含了认证、鉴权等一系列 http filter chain； 2、调用 master.DefaultAPIResourceConfigSource 加载需要启用的 API Resource，集群中所有的 API Resource 可以在代码的 k8s.io/api 目录中看到，随着版本的迭代也会不断变化； 3、为 genericConfig 中的部分字段设置默认值； 4、调用 completedStorageFactoryConfig.New 创建 storageFactory，后面会使用 storageFactory 为每种API Resource 创建对应的 RESTStorage； k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:386 func buildGenericConfig( s *options.ServerRunOptions, proxyTransport *http.Transport, ) (......) { // 1、为 genericConfig 设置默认值 genericConfig = genericapiserver.NewConfig(legacyscheme.Codecs) genericConfig.MergedResourceConfig = master.DefaultAPIResourceConfigSource() if lastErr = s.GenericServerRunOptions.ApplyTo(genericConfig); lastErr != nil { return } ...... genericConfig.OpenAPIConfig = genericapiserver.DefaultOpenAPIConfig(......) genericConfig.OpenAPIConfig.Info.Title = \"Kubernetes\" genericConfig.LongRunningFunc = filters.BasicLongRunningRequestCheck( sets.NewString(\"watch\", \"proxy\"), sets.NewString(\"attach\", \"exec\", \"proxy\", \"log\", \"portforward\"), ) kubeVersion := version.Get() genericConfig.Version = &kubeVersion storageFactoryConfig := kubeapiserver.NewStorageFactoryConfig() storageFactoryConfig.ApiResourceConfig = genericConfig.MergedResourceConfig completedStorageFactoryConfig, err := storageFactoryConfig.Complete(s.Etcd) if err != nil { lastErr = err return } // 初始化 storageFactory storageFactory, lastErr = completedStorageFactoryConfig.New() if lastErr != nil { return } if genericConfig.EgressSelector != nil { storageFactory.StorageConfig.Transport.EgressLookup = genericConfig.EgressSelector.Lookup } // 2、初始化 RESTOptionsGetter，后期根据其获取操作 Etcd 的句柄，同时添加 etcd 的健康检查方法 if lastErr = s.Etcd.ApplyWithStorageFactoryTo(storageFactory, genericConfig); lastErr != nil { return } // 3、设置使用 protobufs 用来内部交互，并且禁用压缩功能 genericConfig.LoopbackClientConfig.ContentConfig.ContentType = \"application/vnd.kubernetes.protobuf\" genericConfig.LoopbackClientConfig.DisableCompression = true // 4、创建 clientset kubeClientConfig := genericConfig.LoopbackClientConfig clientgoExternalClient, err := clientgoclientset.NewForConfig(kubeClientConfig) if err != nil { lastErr = fmt.Errorf(\"failed to create real external clientset: %v\", err) return } versionedInformers = clientgoinformers.NewSharedInformerFactory(clientgoExternalClient, 10*time.Minute) // 5、创建认证实例，支持多种认证方式：请求 Header 认证、Auth 文件认证、CA 证书认证、Bearer token 认证、 // ServiceAccount 认证、BootstrapToken 认证、WebhookToken 认证等 genericConfig.Authentication.Authenticator, genericConfig.OpenAPIConfig.SecurityDefinitions, err = BuildAuthenticator(s, clientgoExternalClient, versionedInformers) if err != nil { lastErr = fmt.Errorf(\"invalid authentication config: %v\", err) return } // 6、创建鉴权实例，包含：Node、RBAC、Webhook、ABAC、AlwaysAllow、AlwaysDeny genericConfig.Authorization.Authorizer, genericConfig.RuleResolver, err = BuildAuthorizer(s, versionedInformers) ...... serviceResolver = buildServiceResolver(s.EnableAggregatorRouting, genericConfig.LoopbackClientConfig.Host, versionedInformers) authInfoResolverWrapper := webhook.NewDefaultAuthenticationInfoResolverWrapper(proxyTransport, genericConfig.LoopbackClientConfig) // 7、审计插件的初始化 lastErr = s.Audit.ApplyTo(......) if lastErr != nil { return } // 8、准入插件的初始化 pluginInitializers, admissionPostStartHook, err = admissionConfig.New(proxyTransport, serviceResolver) if err != nil { lastErr = fmt.Errorf(\"failed to create admission plugin initializer: %v\", err) return } err = s.Admission.ApplyTo(......) if err != nil { lastErr = fmt.Errorf(\"failed to initialize admission: %v\", err) } return } 以上主要分析 KubeAPIServerConfig 的初始化，其他两个 server config 的初始化暂且不详细分析，下面接着继续分析 server 的初始化。 createAPIExtensionsServer APIExtensionsServer 是最先被初始化的，在 createAPIExtensionsServer 中调用 apiextensionsConfig.Complete().New 来完成 server 的初始化，其主要逻辑为： 1、首先调用 c.GenericConfig.New 按照go-restful的模式初始化 Container，在 c.GenericConfig.New 中会调用 NewAPIServerHandler 初始化 handler，APIServerHandler 包含了 API Server 使用的多种http.Handler 类型，包括 go-restful 以及 non-go-restful，以及在以上两者之间选择的 Director 对象，go-restful 用于处理已经注册的 handler，non-go-restful 用来处理不存在的 handler，API URI 处理的选择过程为：FullHandlerChain-> Director ->{GoRestfulContainer， NonGoRestfulMux}。在 c.GenericConfig.New 中还会调用 installAPI来添加包括 /、/debug/*、/metrics、/version 等路由信息。三种 server 在初始化时首先都会调用 c.GenericConfig.New 来初始化一个 genericServer，然后进行 API 的注册； 2、调用 s.GenericAPIServer.InstallAPIGroup 在路由中注册 API Resources，此方法的调用链非常深，主要是为了将需要暴露的 API Resource 注册到 server 中，以便能通过 http 接口进行 resource 的 REST 操作，其他几种 server 在初始化时也都会执行对应的 InstallAPI； 3、初始化 server 中需要使用的 controller，主要有 openapiController、crdController、namingController、establishingController、nonStructuralSchemaController、apiApprovalController、finalizingController； 4、将需要启动的 controller 以及 informer 添加到 PostStartHook 中； k8s.io/kubernetes/cmd/kube-apiserver/app/apiextensions.go:94 func createAPIExtensionsServer(apiextensionsConfig *apiextensionsapiserver.Config, delegateAPIServer genericapiserver.DelegationTarget) (* apiextensionsapiserver.CustomResourceDefinitions, error) { return apiextensionsConfig.Complete().New(delegateAPIServer) } k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/apiserver.go:132 func (c completedConfig) New(delegationTarget genericapiserver.DelegationTarget) (*CustomResourceDefinitions, error) { // 1、初始化 genericServer genericServer, err := c.GenericConfig.New(\"apiextensions-apiserver\", delegationTarget) if err != nil { return nil, err } s := &CustomResourceDefinitions{ GenericAPIServer: genericServer, } // 2、初始化 APIGroup Info，APIGroup 指该 server 需要暴露的 API apiResourceConfig := c.GenericConfig.MergedResourceConfig apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(apiextensions.GroupName, Scheme, metav1.ParameterCodec, Codecs) if apiResourceConfig.VersionEnabled(v1beta1.SchemeGroupVersion) { storage := map[string]rest.Storage{} customResourceDefintionStorage := customresourcedefinition.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter) storage[\"customresourcedefinitions\"] = customResourceDefintionStorage storage[\"customresourcedefinitions/status\"] = customresourcedefinition.NewStatusREST(Scheme, customResourceDefintionStorage) apiGroupInfo.VersionedResourcesStorageMap[v1beta1.SchemeGroupVersion.Version] = storage } if apiResourceConfig.VersionEnabled(v1.SchemeGroupVersion) { ...... } // 3、注册 APIGroup if err := s.GenericAPIServer.InstallAPIGroup(&apiGroupInfo); err != nil { return nil, err } // 4、初始化需要使用的 controller crdClient, err := internalclientset.NewForConfig(s.GenericAPIServer.LoopbackClientConfig) if err != nil { return nil, fmt.Errorf(\"failed to create clientset: %v\", err) } s.Informers = internalinformers.NewSharedInformerFactory(crdClient, 5*time.Minute) ...... establishingController := establish.NewEstablishingController(s.Informers.Apiextensions().InternalVersion(). CustomResourceDefinitions(), crdClient.Apiextensions()) crdHandler, err := NewCustomResourceDefinitionHandler(......) if err != nil { return nil, err } s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(\"/apis\", crdHandler) s.GenericAPIServer.Handler.NonGoRestfulMux.HandlePrefix(\"/apis/\", crdHandler) crdController := NewDiscoveryController(s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions(), versionDiscoveryHandler, groupDiscoveryHandler) namingController := status.NewNamingConditionController(s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions(), crdClient.Apiextensions()) nonStructuralSchemaController := nonstructuralschema.NewConditionController(s.Informers.Apiextensions().InternalVersion(). CustomResourceDefinitions(), crdClient.Apiextensions()) apiApprovalController := apiapproval.NewKubernetesAPIApprovalPolicyConformantConditionController(s.Informers.Apiextensions(). InternalVersion().CustomResourceDefinitions(), crdClient.Apiextensions()) finalizingController := finalizer.NewCRDFinalizer( s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions(), crdClient.Apiextensions(), crdHandler, ) var openapiController *openapicontroller.Controller if utilfeature.DefaultFeatureGate.Enabled(apiextensionsfeatures.CustomResourcePublishOpenAPI) { openapiController = openapicontroller.NewController(s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions()) } // 5、将 informer 以及 controller 添加到 PostStartHook 中 s.GenericAPIServer.AddPostStartHookOrDie(\"start-apiextensions-informers\", func(context genericapiserver.PostStartHookContext) error { s.Informers.Start(context.StopCh) return nil }) s.GenericAPIServer.AddPostStartHookOrDie(\"start-apiextensions-controllers\", func(context genericapiserver.PostStartHookContext) error { ...... go crdController.Run(context.StopCh) go namingController.Run(context.StopCh) go establishingController.Run(context.StopCh) go nonStructuralSchemaController.Run(5, context.StopCh) go apiApprovalController.Run(5, context.StopCh) go finalizingController.Run(5, context.StopCh) return nil }) s.GenericAPIServer.AddPostStartHookOrDie(\"crd-informer-synced\", func(context genericapiserver.PostStartHookContext) error { return wait.PollImmediateUntil(100*time.Millisecond, func() (bool, error) { return s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions().Informer().HasSynced(), nil }, context.StopCh) }) return s, nil } 以上是 APIExtensionsServer 的初始化流程，其中最核心方法是 s.GenericAPIServer.InstallAPIGroup，也就是 API 的注册过程，三种 server 中 API 的注册过程都是其核心。 CreateKubeAPIServer 本节继续分析 KubeAPIServer 的初始化，在CreateKubeAPIServer 中调用了 kubeAPIServerConfig.Complete().New 来完成相关的初始化操作。 kubeAPIServerConfig.Complete().New 主要逻辑为： 1、调用 c.GenericConfig.New 初始化 GenericAPIServer，其主要实现在上文已经分析过； 2、判断是否支持 logs 相关的路由，如果支持，则添加 /logs 路由； 3、调用 m.InstallLegacyAPI 将核心 API Resource 添加到路由中，对应到 apiserver 就是以 /api 开头的 resource； 4、调用 m.InstallAPIs 将扩展的 API Resource 添加到路由中，在 apiserver 中即是以 /apis 开头的 resource； k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:214 func CreateKubeAPIServer(......) (*master.Master, error) { kubeAPIServer, err := kubeAPIServerConfig.Complete().New(delegateAPIServer) if err != nil { return nil, err } kubeAPIServer.GenericAPIServer.AddPostStartHookOrDie(\"start-kube-apiserver-admission-initializer\", admissionPostStartHook) return kubeAPIServer, nil } k8s.io/kubernetes/pkg/master/master.go:325 func (c completedConfig) New(delegationTarget genericapiserver.DelegationTarget) (*Master, error) { ...... // 1、初始化 GenericAPIServer s, err := c.GenericConfig.New(\"kube-apiserver\", delegationTarget) if err != nil { return nil, err } // 2、注册 logs 相关的路由 if c.ExtraConfig.EnableLogsSupport { routes.Logs{}.Install(s.Handler.GoRestfulContainer) } m := &Master{ GenericAPIServer: s, } // 3、安装 LegacyAPI if c.ExtraConfig.APIResourceConfigSource.VersionEnabled(apiv1.SchemeGroupVersion) { legacyRESTStorageProvider := corerest.LegacyRESTStorageProvider{ StorageFactory: c.ExtraConfig.StorageFactory, ProxyTransport: c.ExtraConfig.ProxyTransport, ...... } if err := m.InstallLegacyAPI(&c, c.GenericConfig.RESTOptionsGetter, legacyRESTStorageProvider); err != nil { return nil, err } } restStorageProviders := []RESTStorageProvider{ auditregistrationrest.RESTStorageProvider{}, authenticationrest.RESTStorageProvider{Authenticator: c.GenericConfig.Authentication.Authenticator, APIAudiences: c.GenericConfig. Authentication.APIAudiences}, ...... } // 4、安装 APIs if err := m.InstallAPIs(c.ExtraConfig.APIResourceConfigSource, c.GenericConfig.RESTOptionsGetter, restStorageProviders...); err != nil { return nil, err } if c.ExtraConfig.Tunneler != nil { m.installTunneler(c.ExtraConfig.Tunneler, corev1client.NewForConfigOrDie(c.GenericConfig.LoopbackClientConfig).Nodes()) } m.GenericAPIServer.AddPostStartHookOrDie(\"ca-registration\", c.ExtraConfig.ClientCARegistrationHook.PostStartHook) return m, nil } m.InstallLegacyAPI 此方法的主要功能是将 core API 注册到路由中，是 apiserver 初始化流程中最核心的方法之一，不过其调用链非常深，下面会进行深入分析。将 API 注册到路由其最终的目的就是对外提供 RESTful API 来操作对应 resource，注册 API 主要分为两步，第一步是为 API 中的每个 resource 初始化 RESTStorage 以此操作后端存储中数据的变更，第二步是为每个 resource 根据其 verbs 构建对应的路由。m.InstallLegacyAPI 的主要逻辑为： 1、调用 legacyRESTStorageProvider.NewLegacyRESTStorage 为 LegacyAPI 中各个资源创建 RESTStorage，RESTStorage 的目的是将每种资源的访问路径及其后端存储的操作对应起来； 2、初始化 bootstrap-controller，并将其加入到 PostStartHook 中，bootstrap-controller 是 apiserver 中的一个 controller，主要功能是创建系统所需要的一些 namespace 以及创建 kubernetes service 并定期触发对应的 sync 操作，apiserver 在启动后会通过调用 PostStartHook 来启动 bootstrap-controller； 3、在为资源创建完 RESTStorage 后，调用 m.GenericAPIServer.InstallLegacyAPIGroup 为 APIGroup 注册路由信息，InstallLegacyAPIGroup方法的调用链非常深，主要为InstallLegacyAPIGroup--> installAPIResources --> InstallREST --> Install --> registerResourceHandlers，最终核心的路由构造在registerResourceHandlers方法内，该方法比较复杂，其主要功能是通过上一步骤构造的 REST Storage 判断该资源可以执行哪些操作（如 create、update等），将其对应的操作存入到 action 中，每一个 action 对应一个标准的 REST 操作，如 create 对应的 action 操作为 POST、update 对应的 action 操作为PUT。最终根据 actions 数组依次遍历，对每一个操作添加一个 handler 方法，注册到 route 中去，再将 route 注册到 webservice 中去，webservice 最终会注册到 container 中，遵循 go-restful 的设计模式； 关于 legacyRESTStorageProvider.NewLegacyRESTStorage 以及 m.GenericAPIServer.InstallLegacyAPIGroup 方法的详细说明在后文中会继续进行讲解。 k8s.io/kubernetes/pkg/master/master.go:406 func (m *Master) InstallLegacyAPI(......) error { legacyRESTStorage, apiGroupInfo, err := legacyRESTStorageProvider.NewLegacyRESTStorage(restOptionsGetter) if err != nil { return fmt.Errorf(\"Error building core storage: %v\", err) } controllerName := \"bootstrap-controller\" coreClient := corev1client.NewForConfigOrDie(c.GenericConfig.LoopbackClientConfig) bootstrapController := c.NewBootstrapController(legacyRESTStorage, coreClient, coreClient, coreClient, coreClient.RESTClient()) m.GenericAPIServer.AddPostStartHookOrDie(controllerName, bootstrapController.PostStartHook) m.GenericAPIServer.AddPreShutdownHookOrDie(controllerName, bootstrapController.PreShutdownHook) if err := m.GenericAPIServer.InstallLegacyAPIGroup(genericapiserver.DefaultLegacyAPIPrefix, &apiGroupInfo); err != nil { return fmt.Errorf(\"Error in registering group versions: %v\", err) } return nil } InstallAPIs 与 InstallLegacyAPI 的主要流程是类似的，限于篇幅此处不再深入分析。 createAggregatorServer AggregatorServer 主要用于自定义的聚合控制器的，使 CRD 能够自动注册到集群中。 主要逻辑为： 1、调用 aggregatorConfig.Complete().NewWithDelegate 创建 aggregatorServer； 2、初始化 crdRegistrationController 和 autoRegistrationController，crdRegistrationController 负责注册 CRD，autoRegistrationController 负责将 CRD 对应的 APIServices 自动注册到 apiserver 中，CRD 创建后可通过 $ kubectl get apiservices 查看是否注册到 apiservices 中； 3、将 autoRegistrationController 和 crdRegistrationController 加入到 PostStartHook 中； k8s.io/kubernetes/cmd/kube-apiserver/app/aggregator.go:124 func createAggregatorServer(......) (*aggregatorapiserver.APIAggregator, error) { // 1、初始化 aggregatorServer aggregatorServer, err := aggregatorConfig.Complete().NewWithDelegate(delegateAPIServer) if err != nil { return nil, err } // 2、初始化 auto-registration controller apiRegistrationClient, err := apiregistrationclient.NewForConfig(aggregatorConfig.GenericConfig.LoopbackClientConfig) if err != nil { return nil, err } autoRegistrationController := autoregister.NewAutoRegisterController(......) apiServices := apiServicesToRegister(delegateAPIServer, autoRegistrationController) crdRegistrationController := crdregistration.NewCRDRegistrationController(......) err = aggregatorServer.GenericAPIServer.AddPostStartHook(\"kube-apiserver-autoregistration\", func(context genericapiserver.PostStartHookContext) error { go crdRegistrationController.Run(5, context.StopCh) go func() { if aggregatorConfig.GenericConfig.MergedResourceConfig.AnyVersionForGroupEnabled(\"apiextensions.k8s.io\") { crdRegistrationController.WaitForInitialSync() } autoRegistrationController.Run(5, context.StopCh) }() return nil }) if err != nil { return nil, err } err = aggregatorServer.GenericAPIServer.AddBootSequenceHealthChecks( makeAPIServiceAvailableHealthCheck( \"autoregister-completion\", apiServices, aggregatorServer.APIRegistrationInformers.Apiregistration().V1().APIServices(), ), ) if err != nil { return nil, err } return aggregatorServer, nil } aggregatorConfig.Complete().NewWithDelegate aggregatorConfig.Complete().NewWithDelegate 是初始化 aggregatorServer 的方法，主要逻辑为： 1、调用 c.GenericConfig.New 初始化 GenericAPIServer，其内部的主要功能在上文已经分析过； 2、调用 apiservicerest.NewRESTStorage 为 APIServices 资源创建 RESTStorage，RESTStorage 的目的是将每种资源的访问路径及其后端存储的操作对应起来； 3、调用 s.GenericAPIServer.InstallAPIGroup 为 APIGroup 注册路由信息； k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go:158 func (c completedConfig) NewWithDelegate(delegationTarget genericapiserver.DelegationTarget) (*APIAggregator, error) { openAPIConfig := c.GenericConfig.OpenAPIConfig c.GenericConfig.OpenAPIConfig = nil // 1、初始化 genericServer genericServer, err := c.GenericConfig.New(\"kube-aggregator\", delegationTarget) if err != nil { return nil, err } apiregistrationClient, err := clientset.NewForConfig(c.GenericConfig.LoopbackClientConfig) if err != nil { return nil, err } informerFactory := informers.NewSharedInformerFactory( apiregistrationClient, 5*time.Minute, ) s := &APIAggregator{ GenericAPIServer: genericServer, delegateHandler: delegationTarget.UnprotectedHandler(), ...... } // 2、为 API 注册路由 apiGroupInfo := apiservicerest.NewRESTStorage(c.GenericConfig.MergedResourceConfig, c.GenericConfig.RESTOptionsGetter) if err := s.GenericAPIServer.InstallAPIGroup(&apiGroupInfo); err != nil { return nil, err } // 3、初始化 apiserviceRegistrationController、availableController apisHandler := &apisHandler{ codecs: aggregatorscheme.Codecs, lister: s.lister, } s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(\"/apis\", apisHandler) s.GenericAPIServer.Handler.NonGoRestfulMux.UnlistedHandle(\"/apis/\", apisHandler) apiserviceRegistrationController := NewAPIServiceRegistrationController(informerFactory.Apiregistration().V1().APIServices(), s) availableController, err := statuscontrollers.NewAvailableConditionController( ...... ) if err != nil { return nil, err } // 4、添加 PostStartHook s.GenericAPIServer.AddPostStartHookOrDie(\"start-kube-aggregator-informers\", func(context genericapiserver.PostStartHookContext) error { informerFactory.Start(context.StopCh) c.GenericConfig.SharedInformerFactory.Start(context.StopCh) return nil }) s.GenericAPIServer.AddPostStartHookOrDie(\"apiservice-registration-controller\", func(context genericapiserver.PostStartHookContext) error { go apiserviceRegistrationController.Run(context.StopCh) return nil }) s.GenericAPIServer.AddPostStartHookOrDie(\"apiservice-status-available-controller\", func(context genericapiserver.PostStartHookContext) error { go availableController.Run(5, context.StopCh) return nil }) return s, nil } 以上是对 AggregatorServer 初始化流程的分析，可以看出，在创建 APIExtensionsServer、KubeAPIServer 以及 AggregatorServer 时，其模式都是类似的，首先调用 c.GenericConfig.New 按照go-restful的模式初始化 Container，然后为 server 中需要注册的资源创建 RESTStorage，最后将 resource 的 APIGroup 信息注册到路由中。 至此，CreateServerChain 中流程已经分析完，其中的调用链如下所示： |--> CreateNodeDialer | |--> CreateKubeAPIServerConfig | CreateServerChain --|--> createAPIExtensionsConfig | | |--> c.GenericConfig.New |--> createAPIExtensionsServer --> apiextensionsConfig.Complete().New --| | |--> s.GenericAPIServer.InstallAPIGroup | | |--> c.GenericConfig.New --> legacyRESTStorageProvider.NewLegacyRESTStorage | | |--> CreateKubeAPIServer --> kubeAPIServerConfig.Complete().New --|--> m.InstallLegacyAPI | | | |--> m.InstallAPIs | | |--> createAggregatorConfig | | |--> c.GenericConfig.New | | |--> createAggregatorServer --> aggregatorConfig.Complete().NewWithDelegate --|--> apiservicerest.NewRESTStorage | |--> s.GenericAPIServer.InstallAPIGroup prepared.Run 在 Run 方法中首先调用 CreateServerChain 完成各 server 的初始化，然后调用 server.PrepareRun 完成服务启动前的准备工作，最后调用 prepared.Run 方法来启动安全的 http server。server.PrepareRun 主要完成了健康检查、存活检查和OpenAPI路由的注册工作，下面继续分析 prepared.Run 的流程，在 prepared.Run 中主要调用 s.NonBlockingRun 来完成启动工作。 k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go:269 func (s preparedAPIAggregator) Run(stopCh s.NonBlockingRun s.NonBlockingRun 的主要逻辑为： 1、判断是否要启动审计日志服务； 2、调用 s.SecureServingInfo.Serve 配置并启动 https server； 3、执行 postStartHooks； 4、向 systemd 发送 ready 信号； k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:351 func (s preparedGenericAPIServer) NonBlockingRun(stopCh 以上就是 server 的初始化以及启动流程过程的分析，上文已经提到各 server 初始化过程中最重要的就是 API Resource RESTStorage 的初始化以及路由的注册，由于该过程比较复杂，下文会单独进行讲述。 storageFactory 的构建 上文已经提到过，apiserver 最终实现的 handler 对应的后端数据是以 Store 的结构保存的，这里以 /api 开头的路由举例，通过NewLegacyRESTStorage方法创建各个资源的RESTStorage。RESTStorage 是一个结构体，具体的定义在k8s.io/apiserver/pkg/registry/generic/registry/store.go下，结构体内主要包含NewFunc返回特定资源信息、NewListFunc返回特定资源列表、CreateStrategy特定资源创建时的策略、UpdateStrategy更新时的策略以及DeleteStrategy删除时的策略等重要方法。在NewLegacyRESTStorage内部，可以看到创建了多种资源的 RESTStorage。 NewLegacyRESTStorage 的调用链为 CreateKubeAPIServer --> kubeAPIServerConfig.Complete().New --> m.InstallLegacyAPI --> legacyRESTStorageProvider.NewLegacyRESTStorage。 NewLegacyRESTStorage 一个 API Group 下的资源都有其 REST 实现，k8s.io/kubernetes/pkg/registry下所有的 Group 都有一个rest目录，存储的就是对应资源的 RESTStorage。在NewLegacyRESTStorage方法中，通过NewREST或者NewStorage会生成各种资源对应的 Storage，此处以 pod 为例进行说明。 k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:102 func (c LegacyRESTStorageProvider) NewLegacyRESTStorage(restOptionsGetter generic.RESTOptionsGetter) (LegacyRESTStorage, genericapiserver. APIGroupInfo, error) { apiGroupInfo := genericapiserver.APIGroupInfo{ PrioritizedVersions: legacyscheme.Scheme.PrioritizedVersionsForGroup(\"\"), VersionedResourcesStorageMap: map[string]map[string]rest.Storage{}, Scheme: legacyscheme.Scheme, ParameterCodec: legacyscheme.ParameterCodec, NegotiatedSerializer: legacyscheme.Codecs, } var podDisruptionClient policyclient.PodDisruptionBudgetsGetter if policyGroupVersion := (schema.GroupVersion{Group: \"policy\", Version: \"v1beta1\"}); legacyscheme.Scheme. IsVersionRegistered(policyGroupVersion) { var err error podDisruptionClient, err = policyclient.NewForConfig(c.LoopbackClientConfig) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } } // 1、LegacyAPI 下的 resource RESTStorage 的初始化 restStorage := LegacyRESTStorage{} podTemplateStorage, err := podtemplatestore.NewREST(restOptionsGetter) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } eventStorage, err := eventstore.NewREST(restOptionsGetter, uint64(c.EventTTL.Seconds())) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } limitRangeStorage, err := limitrangestore.NewREST(restOptionsGetter) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } ...... endpointsStorage, err := endpointsstore.NewREST(restOptionsGetter) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } nodeStorage, err := nodestore.NewStorage(restOptionsGetter, c.KubeletClientConfig, c.ProxyTransport) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } // 2、pod RESTStorage 的初始化 podStorage, err := podstore.NewStorage(......) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } ...... serviceClusterIPAllocator, err := ipallocator.NewAllocatorCIDRRange(&serviceClusterIPRange, func(max int, rangeSpec string) (allocator. Interface, error) { ...... }) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, fmt.Errorf(\"cannot create cluster IP allocator: %v\", err) } restStorage.ServiceClusterIPAllocator = serviceClusterIPRegistry var secondaryServiceClusterIPAllocator ipallocator.Interface if utilfeature.DefaultFeatureGate.Enabled(features.IPv6DualStack) && c.SecondaryServiceIPRange.IP != nil { ...... } var serviceNodePortRegistry rangeallocation.RangeRegistry serviceNodePortAllocator, err := portallocator.NewPortAllocatorCustom(c.ServiceNodePortRange, func(max int, rangeSpec string) (allocator.Interface, error) { ...... }) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, fmt.Errorf(\"cannot create cluster port allocator: %v\", err) } restStorage.ServiceNodePortAllocator = serviceNodePortRegistry controllerStorage, err := controllerstore.NewStorage(restOptionsGetter) if err != nil { return LegacyRESTStorage{}, genericapiserver.APIGroupInfo{}, err } serviceRest, serviceRestProxy := servicestore.NewREST(......) // 3、restStorageMap 保存 resource http path 与 RESTStorage 对应关系 restStorageMap := map[string]rest.Storage{ \"pods\": podStorage.Pod, \"pods/attach\": podStorage.Attach, \"pods/status\": podStorage.Status, \"pods/log\": podStorage.Log, \"pods/exec\": podStorage.Exec, \"pods/portforward\": podStorage.PortForward, \"pods/proxy\": podStorage.Proxy, ...... \"componentStatuses\": componentstatus.NewStorage(componentStatusStorage{c.StorageFactory}.serversToValidate), } ...... } podstore.NewStorage podstore.NewStorage 是为 pod 生成 storage 的方法，该方法主要功能是为 pod 创建后端存储最终返回一个 RESTStorage 对象，其中调用 store.CompleteWithOptions 来创建后端存储的。 k8s.io/kubernetes/pkg/registry/core/pod/storage/storage.go:71 func NewStorage(......) (PodStorage, error) { store := &genericregistry.Store{ NewFunc: func() runtime.Object { return &api.Pod{} }, NewListFunc: func() runtime.Object { return &api.PodList{} }, ...... } options := &generic.StoreOptions{ RESTOptions: optsGetter, AttrFunc: pod.GetAttrs, TriggerFunc: map[string]storage.IndexerFunc{\"spec.nodeName\": pod.NodeNameTriggerFunc}, } // 调用 store.CompleteWithOptions if err := store.CompleteWithOptions(options); err != nil { return PodStorage{}, err } statusStore := *store statusStore.UpdateStrategy = pod.StatusStrategy ephemeralContainersStore := *store ephemeralContainersStore.UpdateStrategy = pod.EphemeralContainersStrategy bindingREST := &BindingREST{store: store} // PodStorage 对象 return PodStorage{ Pod: &REST{store, proxyTransport}, Binding: &BindingREST{store: store}, LegacyBinding: &LegacyBindingREST{bindingREST}, Eviction: newEvictionStorage(store, podDisruptionBudgetClient), Status: &StatusREST{store: &statusStore}, EphemeralContainers: &EphemeralContainersREST{store: &ephemeralContainersStore}, Log: &podrest.LogREST{Store: store, KubeletConn: k}, Proxy: &podrest.ProxyREST{Store: store, ProxyTransport: proxyTransport}, Exec: &podrest.ExecREST{Store: store, KubeletConn: k}, Attach: &podrest.AttachREST{Store: store, KubeletConn: k}, PortForward: &podrest.PortForwardREST{Store: store, KubeletConn: k}, }, nil } 可以看到最终返回的对象里对 pod 的不同操作都是一个 REST 对象，REST 中自动集成了 genericregistry.Store 对象，而 store.CompleteWithOptions 方法就是对 genericregistry.Store 对象中存储实例就行初始化的。 type REST struct { *genericregistry.Store proxyTransport http.RoundTripper } type BindingREST struct { store *genericregistry.Store } ...... store.CompleteWithOptions store.CompleteWithOptions 主要功能是为 store 中的配置设置一些默认的值以及根据提供的 options 更新 store，其中最主要的就是初始化 store 的后端存储实例。 在CompleteWithOptions方法内，调用了options.RESTOptions.GetRESTOptions 方法，其最终返回generic.RESTOptions 对象，generic.RESTOptions 对象中包含对 etcd 初始化的一些配置、数据序列化方法以及对 etcd 操作的 storage.Interface 对象。其会依次调用StorageWithCacher-->NewRawStorage-->Create方法创建最终依赖的后端存储。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go:1192 func (e *Store) CompleteWithOptions(options *generic.StoreOptions) error { ...... var isNamespaced bool switch { case e.CreateStrategy != nil: isNamespaced = e.CreateStrategy.NamespaceScoped() case e.UpdateStrategy != nil: isNamespaced = e.UpdateStrategy.NamespaceScoped() default: return fmt.Errorf(\"store for %s must have CreateStrategy or UpdateStrategy set\", e.DefaultQualifiedResource.String()) } ...... // 1、调用 options.RESTOptions.GetRESTOptions opts, err := options.RESTOptions.GetRESTOptions(e.DefaultQualifiedResource) if err != nil { return err } // 2、设置 ResourcePrefix prefix := opts.ResourcePrefix if !strings.HasPrefix(prefix, \"/\") { prefix = \"/\" + prefix } if prefix == \"/\" { return fmt.Errorf(\"store for %s has an invalid prefix %q\", e.DefaultQualifiedResource.String(), opts.ResourcePrefix) } if e.KeyRootFunc == nil && e.KeyFunc == nil { ...... } keyFunc := func(obj runtime.Object) (string, error) { ...... } // 3、以下操作主要是将 opts 对象中的值赋值到 store 对象中 if e.DeleteCollectionWorkers == 0 { e.DeleteCollectionWorkers = opts.DeleteCollectionWorkers } e.EnableGarbageCollection = opts.EnableGarbageCollection if e.ObjectNameFunc == nil { ...... } if e.Storage.Storage == nil { e.Storage.Codec = opts.StorageConfig.Codec var err error e.Storage.Storage, e.DestroyFunc, err = opts.Decorator( opts.StorageConfig, prefix, keyFunc, e.NewFunc, e.NewListFunc, attrFunc, options.TriggerFunc, ) if err != nil { return err } e.StorageVersioner = opts.StorageConfig.EncodeVersioner if opts.CountMetricPollPeriod > 0 { stopFunc := e.startObservingCount(opts.CountMetricPollPeriod) previousDestroy := e.DestroyFunc e.DestroyFunc = func() { stopFunc() if previousDestroy != nil { previousDestroy() } } } } return nil } options.RESTOptions 是一个 interface，想要找到其 GetRESTOptions 方法的实现必须知道 options.RESTOptions 初始化时对应的实例，其初始化是在 CreateKubeAPIServerConfig --> buildGenericConfig --> s.Etcd.ApplyWithStorageFactoryTo 方法中进行初始化的，RESTOptions 对应的实例为 StorageFactoryRestOptionsFactory，所以 PodStorage 初始时构建的 store 对象中genericserver.Config.RESTOptionsGetter 实际的对象类型为 StorageFactoryRestOptionsFactory，其 GetRESTOptions 方法如下所示： k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/options/etcd.go:253 func (f *StorageFactoryRestOptionsFactory) GetRESTOptions(resource schema.GroupResource) (generic.RESTOptions, error) { storageConfig, err := f.StorageFactory.NewConfig(resource) if err != nil { return generic.RESTOptions{}, fmt.Errorf(\"unable to find storage destination for %v, due to %v\", resource, err.Error()) } ret := generic.RESTOptions{ StorageConfig: storageConfig, Decorator: generic.UndecoratedStorage, DeleteCollectionWorkers: f.Options.DeleteCollectionWorkers, EnableGarbageCollection: f.Options.EnableGarbageCollection, ResourcePrefix: f.StorageFactory.ResourcePrefix(resource), CountMetricPollPeriod: f.Options.StorageConfig.CountMetricPollPeriod, } if f.Options.EnableWatchCache { sizes, err := ParseWatchCacheSizes(f.Options.WatchCacheSizes) if err != nil { return generic.RESTOptions{}, err } cacheSize, ok := sizes[resource] if !ok { cacheSize = f.Options.DefaultWatchCacheSize } // 调用 generic.StorageDecorator ret.Decorator = genericregistry.StorageWithCacher(cacheSize) } return ret, nil } 在 genericregistry.StorageWithCacher 中又调用了不同的方法最终会调用 factory.Create 来初始化存储实例，其调用链为：genericregistry.StorageWithCacher --> generic.NewRawStorage --> factory.Create。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory.go:30 func Create(c storagebackend.Config) (storage.Interface, DestroyFunc, error) { switch c.Type { case \"etcd2\": return nil, nil, fmt.Errorf(\"%v is no longer a supported storage backend\", c.Type) // 目前 k8s 只支持使用 etcd v3 case storagebackend.StorageTypeUnset, storagebackend.StorageTypeETCD3: return newETCD3Storage(c) default: return nil, nil, fmt.Errorf(\"unknown storage type: %s\", c.Type) } } newETCD3Storage 在 newETCD3Storage 中，首先通过调用 newETCD3Client 创建 etcd 的 client，client 的创建最终是通过 etcd 官方提供的客户端工具 clientv3 进行创建的。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:209 func newETCD3Storage(c storagebackend.Config) (storage.Interface, DestroyFunc, error) { stopCompactor, err := startCompactorOnce(c.Transport, c.CompactionInterval) if err != nil { return nil, nil, err } client, err := newETCD3Client(c.Transport) if err != nil { stopCompactor() return nil, nil, err } var once sync.Once destroyFunc := func() { once.Do(func() { stopCompactor() client.Close() }) } transformer := c.Transformer if transformer == nil { transformer = value.IdentityTransformer } return etcd3.New(client, c.Codec, c.Prefix, transformer, c.Paging), destroyFunc, nil } 至此对于 pod resource 中 store 的构建基本分析完成，不同 resource 对应一个 REST 对象，其中又引用了 genericregistry.Store 对象，最终是对 genericregistry.Store 的初始化。在分析完 store 的初始化后还有一个重要的步骤就是路由的注册，路由注册主要的流程是为 resource 根据不同 verbs 构建 http path 以及将 path 与对应 handler 进行绑定。 路由注册 上文 RESTStorage 的构建对应的是 InstallLegacyAPI 中的 legacyRESTStorageProvider.NewLegacyRESTStorage 方法，下面继续分析 InstallLegacyAPI 中的 m.GenericAPIServer.InstallLegacyAPIGroup 方法的实现。 k8s.io/kubernetes/pkg/master/master.go:406 func (m *Master) InstallLegacyAPI(......) error { legacyRESTStorage, apiGroupInfo, err := legacyRESTStorageProvider.NewLegacyRESTStorage(restOptionsGetter) if err != nil { return fmt.Errorf(\"Error building core storage: %v\", err) } ...... if err := m.GenericAPIServer.InstallLegacyAPIGroup(genericapiserver.DefaultLegacyAPIPrefix, &apiGroupInfo); err != nil { return fmt.Errorf(\"Error in registering group versions: %v\", err) } return nil } m.GenericAPIServer.InstallLegacyAPIGroup 的调用链非常深，最终是为 Group 下每一个 API resources 注册 handler 及路由信息，其调用链为：m.GenericAPIServer.InstallLegacyAPIGroup --> s.installAPIResources --> apiGroupVersion.InstallREST --> installer.Install --> a.registerResourceHandlers。其中几个方法的作用如下所示： s.installAPIResources：为每一个 API resource 调用 apiGroupVersion.InstallREST 添加路由； apiGroupVersion.InstallREST：将 restful.WebServic 对象添加到 container 中； installer.Install：返回最终的 restful.WebService 对象 a.registerResourceHandlers 该方法实现了 rest.Storage 到 restful.Route 的转换，其首先会判断 API Resource 所支持的 REST 接口，然后为 REST 接口添加对应的 handler，最后将其注册到路由中。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:181 func (a *APIInstaller) registerResourceHandlers(path string, storage rest.Storage, ws *restful.WebService) (*metav1.APIResource, error) { admit := a.group.Admit ...... // 1、判断该 resource 实现了哪些 REST 操作接口，以此来判断其支持的 verbs 以便为其添加路由 creater, isCreater := storage.(rest.Creater) namedCreater, isNamedCreater := storage.(rest.NamedCreater) lister, isLister := storage.(rest.Lister) getter, isGetter := storage.(rest.Getter) getterWithOptions, isGetterWithOptions := storage.(rest.GetterWithOptions) gracefulDeleter, isGracefulDeleter := storage.(rest.GracefulDeleter) collectionDeleter, isCollectionDeleter := storage.(rest.CollectionDeleter) updater, isUpdater := storage.(rest.Updater) patcher, isPatcher := storage.(rest.Patcher) watcher, isWatcher := storage.(rest.Watcher) connecter, isConnecter := storage.(rest.Connecter) storageMeta, isMetadata := storage.(rest.StorageMetadata) storageVersionProvider, isStorageVersionProvider := storage.(rest.StorageVersionProvider) if !isMetadata { storageMeta = defaultStorageMetadata{} } exporter, isExporter := storage.(rest.Exporter) if !isExporter { exporter = nil } ...... // 2、为 resource 添加对应的 actions 并根据是否支持 namespace switch { case !namespaceScoped: ...... actions = appendIf(actions, action{\"LIST\", resourcePath, resourceParams, namer, false}, isLister) actions = appendIf(actions, action{\"POST\", resourcePath, resourceParams, namer, false}, isCreater) actions = appendIf(actions, action{\"DELETECOLLECTION\", resourcePath, resourceParams, namer, false}, isCollectionDeleter) actions = appendIf(actions, action{\"WATCHLIST\", \"watch/\" + resourcePath, resourceParams, namer, false}, allowWatchList) actions = appendIf(actions, action{\"GET\", itemPath, nameParams, namer, false}, isGetter) if getSubpath { actions = appendIf(actions, action{\"GET\", itemPath + \"/{path:*}\", proxyParams, namer, false}, isGetter) } actions = appendIf(actions, action{\"PUT\", itemPath, nameParams, namer, false}, isUpdater) actions = appendIf(actions, action{\"PATCH\", itemPath, nameParams, namer, false}, isPatcher) actions = appendIf(actions, action{\"DELETE\", itemPath, nameParams, namer, false}, isGracefulDeleter) actions = appendIf(actions, action{\"WATCH\", \"watch/\" + itemPath, nameParams, namer, false}, isWatcher) actions = appendIf(actions, action{\"CONNECT\", itemPath, nameParams, namer, false}, isConnecter) actions = appendIf(actions, action{\"CONNECT\", itemPath + \"/{path:*}\", proxyParams, namer, false}, isConnecter && connectSubpath) default: ...... actions = appendIf(actions, action{\"LIST\", resourcePath, resourceParams, namer, false}, isLister) actions = appendIf(actions, action{\"POST\", resourcePath, resourceParams, namer, false}, isCreater) actions = appendIf(actions, action{\"DELETECOLLECTION\", resourcePath, resourceParams, namer, false}, isCollectionDeleter) actions = appendIf(actions, action{\"WATCHLIST\", \"watch/\" + resourcePath, resourceParams, namer, false}, allowWatchList) actions = appendIf(actions, action{\"GET\", itemPath, nameParams, namer, false}, isGetter) ...... } // 3、根据 action 创建对应的 route kubeVerbs := map[string]struct{}{} reqScope := handlers.RequestScope{ Serializer: a.group.Serializer, ParameterCodec: a.group.ParameterCodec, Creater: a.group.Creater, Convertor: a.group.Convertor, ...... } ...... // 4、从 rest.Storage 到 restful.Route 映射 // 为每个操作添加对应的 handler for _, action := range actions { ...... verbOverrider, needOverride := storage.(StorageMetricsOverride) switch action.Verb { case \"GET\": ...... case \"LIST\": case \"PUT\": case \"PATCH\": // 此处以 POST 操作进行说明 case \"POST\": var handler restful.RouteFunction // 5、初始化 handler if isNamedCreater { handler = restfulCreateNamedResource(namedCreater, reqScope, admit) } else { handler = restfulCreateResource(creater, reqScope, admit) } handler = metrics.InstrumentRouteFunc(action.Verb, group, version, resource, subresource, requestScope, metrics.APIServerComponent, handler) article := GetArticleForNoun(kind, \" \") doc := \"create\" + article + kind if isSubresource { doc = \"create \" + subresource + \" of\" + article + kind } // 6、route 与 handler 进行绑定 route := ws.POST(action.Path).To(handler). Doc(doc). Param(ws.QueryParameter(\"pretty\", \"If 'true', then the output is pretty printed.\")). Operation(\"create\"+namespaced+kind+strings.Title(subresource)+operationSuffix). Produces(append(storageMeta.ProducesMIMETypes(action.Verb), mediaTypes...)...). Returns(http.StatusOK, \"OK\", producedObject). Returns(http.StatusCreated, \"Created\", producedObject). Returns(http.StatusAccepted, \"Accepted\", producedObject). Reads(defaultVersionedObject). Writes(producedObject) if err := AddObjectParams(ws, route, versionedCreateOptions); err != nil { return nil, err } addParams(route, action.Params) // 7、添加到路由中 routes = append(routes, route) case \"DELETE\": case \"DELETECOLLECTION\": case \"WATCH\": case \"WATCHLIST\": case \"CONNECT\": default: } ...... return &apiResource, nil } restfulCreateNamedResource restfulCreateNamedResource 是 POST 操作对应的 handler，最终会调用 createHandler 方法完成。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:1087 func restfulCreateNamedResource(r rest.NamedCreater, scope handlers.RequestScope, admit admission.Interface) restful.RouteFunction { return func(req *restful.Request, res *restful.Response) { handlers.CreateNamedResource(r, &scope, admit)(res.ResponseWriter, req.Request) } } func CreateNamedResource(r rest.NamedCreater, scope *RequestScope, admission admission.Interface) http.HandlerFunc { return createHandler(r, scope, admission, true) } createHandler createHandler 是将数据写入到后端存储的方法，对于资源的操作都有相关的权限控制，在 createHandler 中首先会执行 decoder 和 admission 操作，然后调用 create 方法完成 resource 的创建，在 create 方法中会进行 validate 以及最终将数据保存到后端存储中。admit 操作即执行 kube-apiserver 中的 admission-plugins，admission-plugins 在 CreateKubeAPIServerConfig 中被初始化为了 admissionChain，其初始化的调用链为 CreateKubeAPIServerConfig --> buildGenericConfig --> s.Admission.ApplyTo --> a.GenericAdmission.ApplyTo --> a.Plugins.NewFromPlugins，最终在 a.Plugins.NewFromPlugins 中将所有已启用的 plugins 封装为 admissionChain，此处要执行的 admit 操作即执行 admission-plugins 中的 admit 操作。 createHandler 中调用的 create 方法是genericregistry.Store 对象的方法，在每个 resource 初始化 RESTStorage 都会引入 genericregistry.Store 对象。 createHandler 中所有的操作就是本文开头提到的请求流程，如下所示： v1beta1 ⇒ internal ⇒ | ⇒ | ⇒ v1 ⇒ json/yaml ⇒ etcd admission validation k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go:46 func createHandler(r rest.NamedCreater, scope *RequestScope, admit admission.Interface, includeName bool) http.HandlerFunc { return func(w http.ResponseWriter, req *http.Request) { trace := utiltrace.New(\"Create\", utiltrace.Field{\"url\", req.URL.Path}) defer trace.LogIfLong(500 * time.Millisecond) ...... gv := scope.Kind.GroupVersion() // 1、得到合适的SerializerInfo s, err := negotiation.NegotiateInputSerializer(req, false, scope.Serializer) if err != nil { scope.err(err, w, req) return } // 2、找到合适的 decoder decoder := scope.Serializer.DecoderToVersion(s.Serializer, scope.HubGroupVersion) body, err := limitedReadBody(req, scope.MaxRequestBodyBytes) if err != nil { scope.err(err, w, req) return } ...... defaultGVK := scope.Kind original := r.New() trace.Step(\"About to convert to expected version\") // 3、decoder 解码 obj, gvk, err := decoder.Decode(body, &defaultGVK, original) ...... ae := request.AuditEventFrom(ctx) admit = admission.WithAudit(admit, ae) audit.LogRequestObject(ae, obj, scope.Resource, scope.Subresource, scope.Serializer) userInfo, _ := request.UserFrom(ctx) if len(name) == 0 { _, name, _ = scope.Namer.ObjectName(obj) } // 4、执行 admit 操作，即执行 kube-apiserver 启动时加载的 admission-plugins， admissionAttributes := admission.NewAttributesRecord(......) if mutatingAdmission, ok := admit.(admission.MutationInterface); ok && mutatingAdmission.Handles(admission.Create) { err = mutatingAdmission.Admit(ctx, admissionAttributes, scope) if err != nil { scope.err(err, w, req) return } } ...... // 5、执行 create 操作 result, err := finishRequest(timeout, func() (runtime.Object, error) { return r.Create( ctx, name, obj, rest.AdmissionToValidateObjectFunc(admit, admissionAttributes, scope), options, ) }) ...... } } 总结 本文主要分析 kube-apiserver 的启动流程，kube-apiserver 中包含三个 server，分别为 KubeAPIServer、APIExtensionsServer 以及 AggregatorServer，三个 server 是通过委托模式连接在一起的，初始化过程都是类似的，首先为每个 server 创建对应的 config，然后初始化 http server，http server 的初始化过程为首先初始化 GoRestfulContainer，然后安装 server 所包含的 API，安装 API 时首先为每个 API Resource 创建对应的后端存储 RESTStorage，再为每个 API Resource 支持的 verbs 添加对应的 handler，并将 handler 注册到 route 中，最后将 route 注册到 webservice 中，启动流程中 RESTFul API 的实现流程是其核心，至于 kube-apiserver 中认证鉴权等 filter 的实现、多版本资源转换、kubernetes service 的实现等一些细节会在后面的文章中继续进行分析。 参考： https://mp.weixin.qq.com/s/hTEWatYLhTnC5X0FBM2RWQ https://bbbmj.github.io/2019/04/13/Kubernetes/code-analytics/kube-apiserver/ https://mp.weixin.qq.com/s/TQuqAAzBjeWHwKPJZ3iJhA https://blog.openshift.com/kubernetes-deep-dive-api-server-part-1/ https://www.jianshu.com/p/daa4ff387a78 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/dashboard.html":{"url":"blog/kubernetes/task/dashboard.html","title":"Dashboard","keywords":"","body":"dashboard install kubectl apply -f https://gitee.com/mirrors_kubernetes/mirrors_kubernetes_dashboard/raw/master/aio/deploy/recommended.yaml # Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: v1 kind: Namespace metadata: name: kubernetes-dashboard --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboard type: Opaque --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboard type: Opaque data: csrf: \"\" --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboard type: Opaque --- kind: ConfigMap apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard rules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mcp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.3.1 imagePullPolicy: Always ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kubernetes-dashboard # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard nodeSelector: \"kubernetes.io/os\": linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- kind: Service apiVersion: v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: ports: - port: 8000 targetPort: 8000 selector: k8s-app: dashboard-metrics-scraper --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dashboard-metrics-scraper template: metadata: labels: k8s-app: dashboard-metrics-scraper annotations: seccomp.security.alpha.kubernetes.io/pod: 'runtime/default' spec: containers: - name: dashboard-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.6 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /tmp name: tmp-volume securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 serviceAccountName: kubernetes-dashboard nodeSelector: \"kubernetes.io/os\": linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: tmp-volume emptyDir: {} token kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token kubectl -n $NAMESPACE get secret $(kubectl -n $NAMESPACE get sa $SERVICE_ACCOUNT_NAME -o=json | jq -r '.secrets[0].name') -o=json | jq -r '.data[\"token\"]' | base64 --decode Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/kubectl patch.html":{"url":"blog/kubernetes/task/kubectl patch.html","title":"Kubectl Patch","keywords":"","body":"Update API Objects in Place Using kubectl patch This task shows how to use kubectl patch to update an API object in place. The exercises in this task demonstrate a strategic merge patch and a JSON merge patch. Before you begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds: Katacoda Play with Kubernetes To check the version, enter kubectl version. kubectl Patch usage Command kubectl patch (-f FILENAME | TYPE NAME) -p PATCH [options] Examples: 下面是三种类型的Patch 例子 Partially update a node using a strategic merge patch. Specify the patch as JSON. kubectl patch node k8s-node-1 -p '{\"spec\":{\"unschedulable\":true}}' Update a container's image using a json patch with positional arrays. kubectl patch pod valid-pod --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"newimage\"}]' Update a container's image using a merge patch with positional arrays. kubectl patch deployment patch-demo --patch '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"patch-demo-ctr-4\",\"image\": \"redis\"}]}}}}' --type merge Options: --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats. --dry-run=false: If true, only print the object that would be sent, without sending it. -f, --filename=[]: Filename, directory, or URL to files identifying the resource to update -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R. --local=false: If true, patch will operate on the content of the file, not the server-side resource. -o, --output='': Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-file. -p, --patch='': The patch to be applied to the resource JSON file. --record=false: Record current kubectl command in the resource annotation. If set to false, do not record the command. If set to true, record the command. If not set, default to updating the existing annotation value only if one already exists. -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory. --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview]. --type='strategic': The type of patch being provided; one of [json merge strategic] Use a strategic merge patch to update a Deployment Here's the configuration file for a Deployment that has two replicas. Each replica is a Pod that has one container: application/deployment-patch.yaml apiVersion: apps/v1 kind: Deployment metadata: name: patch-demo spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: patch-demo-ctr image: nginx tolerations: - effect: NoSchedule key: dedicated value: test-team Create the Deployment: kubectl apply -f https://k8s.io/examples/application/deployment-patch.yaml View the Pods associated with your Deployment: kubectl get pods The output shows that the Deployment has two Pods. The 1/1 indicates that each Pod has one container: NAME READY STATUS RESTARTS AGE patch-demo-28633765-670qr 1/1 Running 0 23s patch-demo-28633765-j5qs3 1/1 Running 0 23s Make a note of the names of the running Pods. Later, you will see that these Pods get terminated and replaced by new ones. At this point, each Pod has one Container that runs the nginx image. Now suppose you want each Pod to have two containers: one that runs nginx and one that runs redis. Create a file named patch-file.yaml that has this content: spec: template: spec: containers: - name: patch-demo-ctr-2 image: redis Patch your Deployment: Bash PowerShell kubectl patch deployment patch-demo --patch \"$(cat patch-file.yaml)\" View the patched Deployment: kubectl get deployment patch-demo --output yaml The output shows that the PodSpec in the Deployment has two Containers: containers: - image: redis imagePullPolicy: Always name: patch-demo-ctr-2 ... - image: nginx imagePullPolicy: Always name: patch-demo-ctr ... View the Pods associated with your patched Deployment: kubectl get pods The output shows that the running Pods have different names from the Pods that were running previously. The Deployment terminated the old Pods and created two new Pods that comply with the updated Deployment spec. The 2/2 indicates that each Pod has two Containers: NAME READY STATUS RESTARTS AGE patch-demo-1081991389-2wrn5 2/2 Running 0 1m patch-demo-1081991389-jmg7b 2/2 Running 0 1m Take a closer look at one of the patch-demo Pods: kubectl get pod --output yaml The output shows that the Pod has two Containers: one running nginx and one running redis: containers: - image: redis ... - image: nginx ... Notes on the strategic merge patch The patch you did in the preceding exercise is called a strategic merge patch. Notice that the patch did not replace the containers list. Instead it added a new Container to the list. In other words, the list in the patch was merged with the existing list. This is not always what happens when you use a strategic merge patch on a list. In some cases, the list is replaced, not merged. With a strategic merge patch, a list is either replaced or merged depending on its patch strategy. The patch strategy is specified by the value of the patchStrategy key in a field tag in the Kubernetes source code. For example, the Containers field of PodSpec struct has a patchStrategy of merge: type PodSpec struct { ... Containers []Container `json:\"containers\" patchStrategy:\"merge\" patchMergeKey:\"name\" ...` You can also see the patch strategy in the OpenApi spec: \"io.k8s.api.core.v1.PodSpec\": { ... \"containers\": { \"description\": \"List of containers belonging to the pod. ... }, \"x-kubernetes-patch-merge-key\": \"name\", \"x-kubernetes-patch-strategy\": \"merge\" }, And you can see the patch strategy in the Kubernetes API documentation. Create a file named patch-file-tolerations.yaml that has this content: spec: template: spec: tolerations: - effect: NoSchedule key: disktype value: ssd Patch your Deployment: kubectl patch deployment patch-demo --patch \"$(cat patch-file-tolerations.yaml)\" View the patched Deployment: kubectl get deployment patch-demo --output yaml The output shows that the PodSpec in the Deployment has only one Toleration: tolerations: - effect: NoSchedule key: disktype value: ssd Notice that the tolerations list in the PodSpec was replaced, not merged. This is because the Tolerations field of PodSpec does not have a patchStrategy key in its field tag. So the strategic merge patch uses the default patch strategy, which is replace. type PodSpec struct { ... Tolerations []Toleration `json:\"tolerations,omitempty\" protobuf:\"bytes,22,opt,name=tolerations\"` Use a JSON merge patch to update a Deployment A strategic merge patch is different from a JSON merge patch. With a JSON merge patch, if you want to update a list, you have to specify the entire new list. And the new list completely replaces the existing list. The kubectl patch command has a type parameter that you can set to one of these values: Parameter value Merge type 备注 json JSON Patch, RFC 6902 The JSON Patch format is similar to a database transaction: it is an array of mutating operations on a JSON document, which is executed atomically by a proper implementation. It is basically a series of \"add\", \"remove\", \"replace\", \"move\" and \"copy\" operations. merge JSON Merge Patch, RFC 7386 对json 进行了简化，但是仍然有缺陷。1. delete must set null； 2. add new element must report entire array。3. 缺少JSON Schema验证。 strategic Strategic merge patch 不必提供完整的字段，新字段会添加，出现的已有字段会更新，没有出现的已有字段不变不支持 custom resource， custom resource 需要用\"application/merge-patch+json\" For a comparison of JSON patch and JSON merge patch, see JSON Patch and JSON Merge Patch. The default value for the type parameter is strategic. So in the preceding exercise, you did a strategic merge patch. Next, do a JSON merge patch on your same Deployment. Create a file named patch-file-2.yaml that has this content: spec: template: spec: containers: - name: patch-demo-ctr-3 image: gcr.io/google-samples/node-hello:1.0 In your patch command, set type to merge: kubectl patch deployment patch-demo --type \"$(cat patch-file-2.yaml)\" View the patched Deployment: kubectl get deployment patch-demo --output yaml The containers list that you specified in the patch has only one Container. The output shows that your list of one Container replaced the existing containers list. spec: containers: - image: gcr.io/google-samples/node-hello:1.0 ... name: patch-demo-ctr-3 List the running Pods: kubectl get pods In the output, you can see that the existing Pods were terminated, and new Pods were created. The 1/1 indicates that each new Pod is running only one Container. NAME READY STATUS RESTARTS AGE patch-demo-1307768864-69308 1/1 Running 0 1m patch-demo-1307768864-c86dc 1/1 Running 0 1m Use strategic merge patch to update a Deployment using the retainKeys strategy Here's the configuration file for a Deployment that uses the RollingUpdate strategy: application/deployment-retainkeys.yaml apiVersion: apps/v1 kind: Deployment metadata: name: retainkeys-demo spec: selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 30% template: metadata: labels: app: nginx spec: containers: - name: retainkeys-demo-ctr image: nginx Create the deployment: kubectl apply -f https://k8s.io/examples/application/deployment-retainkeys.yaml At this point, the deployment is created and is using the RollingUpdate strategy. Create a file named patch-file-no-retainkeys.yaml that has this content: spec: strategy: type: Recreate Patch your Deployment: Bash PowerShell kubectl patch deployment retainkeys-demo --patch \"$(cat patch-file-no-retainkeys.yaml)\" In the output, you can see that it is not possible to set type as Recreate when a value is defined for spec.strategy.rollingUpdate: The Deployment \"retainkeys-demo\" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate' The way to remove the value for spec.strategy.rollingUpdate when updating the value for type is to use the retainKeys strategy for the strategic merge. Create another file named patch-file-retainkeys.yaml that has this content: spec: strategy: $retainKeys: - type type: Recreate With this patch, we indicate that we want to retain only the type key of the strategy object. Thus, the rollingUpdate will be removed during the patch operation. Patch your Deployment again with this new patch: Bash PowerShell kubectl patch deployment retainkeys-demo --patch \"$(cat patch-file-retainkeys.yaml)\" Examine the content of the Deployment: kubectl get deployment retainkeys-demo --output yaml The output shows that the strategy object in the Deployment does not contain the rollingUpdate key anymore: spec: strategy: type: Recreate template: Notes on the strategic merge patch using the retainKeys strategy The patch you did in the preceding exercise is called a strategic merge patch with retainKeys strategy. This method introduces a new directive $retainKeys that has the following strategies: It contains a list of strings. All fields needing to be preserved must be present in the $retainKeys list. The fields that are present will be merged with live object. All of the missing fields will be cleared when patching. All fields in the $retainKeys list must be a superset or the same as the fields present in the patch. The retainKeys strategy does not work for all objects. It only works when the value of the patchStrategy key in a field tag in the Kubernetes source code contains retainKeys. For example, the Strategy field of the DeploymentSpec struct has a patchStrategy of retainKeys: type DeploymentSpec struct { ... // +patchStrategy=retainKeys Strategy DeploymentStrategy `json:\"strategy,omitempty\" patchStrategy:\"retainKeys\" ...` You can also see the retainKeys strategy in the OpenApi spec: \"io.k8s.api.apps.v1.DeploymentSpec\": { ... \"strategy\": { \"$ref\": \"#/definitions/io.k8s.api.apps.v1.DeploymentStrategy\", \"description\": \"The deployment strategy to use to replace existing pods with new ones.\", \"x-kubernetes-patch-strategy\": \"retainKeys\" }, And you can see the retainKeys strategy in the Kubernetes API documentation. Alternate forms of the kubectl patch command The kubectl patch command takes YAML or JSON. It can take the patch as a file or directly on the command line. Create a file named patch-file.json that has this content: { \"spec\": { \"template\": { \"spec\": { \"containers\": [ { \"name\": \"patch-demo-ctr-2\", \"image\": \"redis\" } ] } } } } The following commands are equivalent: kubectl patch deployment patch-demo --patch \"$(cat patch-file.yaml)\" kubectl patch deployment patch-demo --patch 'spec:\\n template:\\n spec:\\n containers:\\n - name: patch-demo-ctr-2\\n image: redis' kubectl patch deployment patch-demo --patch \"$(cat patch-file.json)\" kubectl patch deployment patch-demo --patch '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"patch-demo-ctr-2\",\"image\": \"redis\"}]}}}}' Summary In this exercise, you used kubectl patch to change the live configuration of a Deployment object. You did not change the configuration file that you originally used to create the Deployment object. Other commands for updating API objects include kubectl annotate, kubectl edit, kubectl replace, kubectl scale, and kubectl apply. Note: Strategic merge patch is not supported for custom resources. Link: Update API Objects in Place Using kubectl patch Json and merge Patch pk Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:22:34 "},"blog/kubernetes/task/kustomize.html":{"url":"blog/kubernetes/task/kustomize.html","title":"Kustomize","keywords":"","body":"kustomize kustomize lets you customize raw, template-free YAML files for multiple purpose, leaving the original YAML untouched and unabl as is. Kustomize targets tubernetes; it understands and can patch kubernetes style API objects. It's like make, in that what it does is declared in a file, and it's like sed, in that it emits text. Usage: 1) Make a kustomization file In some directory containing your YAML resource files (deployments, services, configmaps, etc.), create a kustomization file. This file should declare those resources, and any customization to apply to them, e.g. add a common label. 1) Make a customizeation file File Struct ~/someApp ├── deployment.yaml ├── kustomization.yaml └── service.yaml The resources in this directory could be a fork of someone else's configuration. If so, you can easily rebase from the source material to capture improvements, because you don't modify the resources directly. Generate customized YAML with: kustomize build ~/someApp The YAML can be directly applied to a cluster: kustomize build ~/someApp | kubectl apply -f - 2) Create variants using overlays Manage traditional variants of a configuration - like development, staging and production - using overlays that modify a common base. File struct: ~/someApp ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── development │ ├── cpu_count.yaml │ ├── kustomization.yaml │ └── replica_count.yaml └── production ├── cpu_count.yaml ├── kustomization.yaml └── replica_count.yaml Take the work from step (1) above, move it into a someApp subdirectory called base, then place overlays in a sibling directory. An overlay is just another kustomization, referring to the base, and referring to patches to apply to that base. This arrangement makes it easy to manage your configuration with git. The base could have files from an upstream repository managed by someone else. The overlays could be in a repository you own. Arranging the repo clones as siblings on disk avoids the need for git submodules (though that works fine, if you are a submodule fan). Generate YAML with kustomize build ~/someApp/overlays/production The YAML can be directly applied to a cluster: kustomize build ~/someApp/overlays/production | kubectl apply -f - Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/local-path.html":{"url":"blog/kubernetes/task/local-path.html","title":"Local Path","keywords":"","body":"Storage-class local 本地 FEATURE STATE: Kubernetes v1.14 [stable] kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 本地卷还不支持动态制备，然而还是需要创建 StorageClass 以延迟卷绑定， 直到完成 Pod 的调度。这是由 WaitForFirstConsumer 卷绑定模式指定的。 延迟卷绑定使得调度器在为 PersistentVolumeClaim 选择一个合适的 PersistentVolume 时能考虑到所有 Pod 的调度限制。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/metrics-server.html":{"url":"blog/kubernetes/task/metrics-server.html","title":"Metrics Server","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/pod 信息注入.html":{"url":"blog/kubernetes/task/pod 信息注入.html","title":"Pod 信息注入","keywords":"","body":"k8s: pod 信息注入 描述 Docker 字段名称 Kubernetes 字段名称 容器执行的命令 Entrypoint command 传给命令的参数 Cmd args 如果要覆盖默认的 Entrypoint 与 Cmd，需要遵循如下规则： 如果在容器配置中没有设置 command 或者 args，那么将使用 Docker 镜像自带的命令及其入参。 如果在容器配置中只设置了 command 但是没有设置 args，那么容器启动时只会执行该命令，Docker 镜像中自带的命令及其入参会被忽略。 如果在容器配置中只设置了 args，那么 Docker 镜像中自带的命令会使用该新入参作为其执行时的入参。 如果在容器配置中同时设置了 command 与 args，那么 Docker 镜像中自带的命令及其入参会被忽略。容器启动时只会执行配置中设置的命令，并使用配置中设置的入参作为命令的入参。 在配置中使用环境变量 您在 Pod 的配置中定义的环境变量可以在配置的其他地方使用，例如可用在为 Pod 的容器设置的命令和参数中。在下面的示例配置中，环境变量 GREETING ，HONORIFIC 和 NAME 分别设置为 Warm greetings to ，The Most Honorable 和 Kubernetes。然后这些环境变量在传递给容器 env-print-demo 的 CLI 参数中使用。 apiVersion: v1 kind: Pod metadata: name: print-greeting spec: containers: - name: env-print-demo image: bash env: - name: GREETING value: \"Warm greetings to\" - name: HONORIFIC value: \"The Most Honorable\" - name: NAME value: \"Kubernetes\" command: [\"echo\"] args: [\"$(GREETING) $(HONORIFIC) $(NAME)\"] 使用 PodPreset 将信息注入 Pods 在 pod 创建时，用户可以使用 podpreset 对象将 secrets、卷挂载和环境变量等信息注入其中。 本文展示了一些 PodPreset 资源使用的示例。 用户可以从理解 Pod Presets 中了解 PodPresets 的整体情况。 用户提交的 pod spec： apiVersion: v1 kind: Pod metadata: name: website labels: app: website role: frontend spec: containers: - name: website image: nginx ports: - containerPort: 80 用户提交的 ConfigMap： apiVersion: v1 kind: ConfigMap metadata: name: etcd-env-config data: number_of_members: \"1\" initial_cluster_state: new initial_cluster_token: DUMMY_ETCD_INITIAL_CLUSTER_TOKEN discovery_token: DUMMY_ETCD_DISCOVERY_TOKEN discovery_url: http://etcd_discovery:2379 etcdctl_peers: http://etcd:2379 duplicate_key: FROM_CONFIG_MAP REPLACE_ME: \"a value\" PodPreset 示例： apiVersion: settings.k8s.io/v1alpha1 kind: PodPreset metadata: name: allow-database spec: selector: matchLabels: role: frontend env: - name: DB_PORT value: \"6379\" - name: duplicate_key value: FROM_ENV - name: expansion value: $(REPLACE_ME) envFrom: - configMapRef: name: etcd-env-config volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: {} 使用 Secret 安全地分发凭证 将 secret 数据转换为 base-64 形式 假设用户想要有两条 secret 数据：用户名 my-app 和密码 39528$vdg7Jb。 首先使用 Base64 编码 将用户名和密码转化为 base-64 形式。 这里是一个 Linux 示例： echo -n 'my-app' | base64 echo -n '39528$vdg7Jb' | base64 apiVersion: v1 kind: Secret metadata: name: test-secret data: username: bXktYXBw password: Mzk1MjgkdmRnN0pi 注意： 如果想要跳过 Base64 编码的步骤，可以使用 kubectl create secret 命令来创建 Secret： kubectl create secret generic test-secret --from-literal=username='my-app' --from-literal=password='39528$vdg7Jb' 创建可以通过卷访问 secret 数据的 Pod 这里是一个可以用来创建 pod 的配置文件： apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: nginx volumeMounts: # name must match the volume name below - name: secret-volume mountPath: /etc/secret-volume # The secret data is exposed to Containers in the Pod through a Volume. volumes: - name: secret-volume secret: secretName: test-secret secret 数据通过挂载在 /etc/secret-volume 目录下的卷暴露在容器中。 在 shell 中，进入 secret 数据被暴露的目录： root@secret-test-pod:/etc/secret-volume# ls password username 通过文件将Pod信息呈现给容器 pods/inject/dapi-volume.yaml apiVersion: v1 kind: Pod metadata: name: kubernetes-downwardapi-volume-example labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 annotations: build: two builder: john-doe spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en '\\n\\n'; cat /etc/podinfo/annotations; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo volumes: - name: podinfo downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels - path: \"annotations\" fieldRef: fieldPath: metadata.annotations Downward　API 　存储POD字段 apiVersion: v1 kind: Pod metadata: name: kubernetes-downwardapi-volume-example labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 annotations: build: two builder: john-doe spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en '\\n\\n'; cat /etc/podinfo/annotations; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo volumes: - name: podinfo downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels - path: \"annotations\" fieldRef: fieldPath: metadata.annotations 在配置文件中，你可以看到Pod有一个downwardAPI类型的Volume，并且挂载到容器中的/etc。 查看downwardAPI下面的items数组。每个数组元素都是一个DownwardAPIVolumeFile。 第一个元素指示Pod的metadata.labels字段的值保存在名为labels的文件中。 第二个元素指示Pod的annotations字段的值保存在名为annotations的文件中。 存储容器字段 apiVersion: v1 kind: Pod metadata: name: kubernetes-downwardapi-volume-example-2 spec: containers: - name: client-container image: k8s.gcr.io/busybox:1.24 command: [\"sh\", \"-c\"] args: - while true; do echo -en '\\n'; if [[ -e /etc/podinfo/cpu_limit ]]; then echo -en '\\n'; cat /etc/podinfo/cpu_limit; fi; if [[ -e /etc/podinfo/cpu_request ]]; then echo -en '\\n'; cat /etc/podinfo/cpu_request; fi; if [[ -e /etc/podinfo/mem_limit ]]; then echo -en '\\n'; cat /etc/podinfo/mem_limit; fi; if [[ -e /etc/podinfo/mem_request ]]; then echo -en '\\n'; cat /etc/podinfo/mem_request; fi; sleep 5; done; resources: requests: memory: \"32Mi\" cpu: \"125m\" limits: memory: \"64Mi\" cpu: \"250m\" volumeMounts: - name: podinfo mountPath: /etc/podinfo volumes: - name: podinfo downwardAPI: items: - path: \"cpu_limit\" resourceFieldRef: containerName: client-container resource: limits.cpu divisor: 1m - path: \"cpu_request\" resourceFieldRef: containerName: client-container resource: requests.cpu divisor: 1m - path: \"mem_limit\" resourceFieldRef: containerName: client-container resource: limits.memory divisor: 1Mi - path: \"mem_request\" resourceFieldRef: containerName: client-container resource: requests.memory divisor: 1Mi Capabilities of the Downward API 下面这些信息可以通过环境变量和DownwardAPIVolumeFiles提供给容器： 能通过fieldRef获得的： metadata.name - Pod名称 metadata.namespace - Pod名字空间 metadata.uid - Pod的UID, 版本要求 v1.8.0-alpha.2 metadata.labels[''] - 单个 pod 标签值 (例如, metadata.labels['mylabel']); 版本要求 Kubernetes 1.9+ * metadata.annotations[''] - 单个 pod 的标注值 (例如, metadata.annotations['myannotation']); 版本要求 Kubernetes 1.9+ 能通过resourceFieldRef获得的： 容器的CPU约束值 容器的CPU请求值 容器的内存约束值 容器的内存请求值 容器的临时存储约束值, 版本要求 v1.8.0-beta.0 容器的临时存储请求值, 版本要求 v1.8.0-beta.0 此外，以下信息可通过DownwardAPIVolumeFiles从fieldRef获得： metadata.labels - all of the pod’s labels, formatted as label-key=\"escaped-label-value\" with one label per line metadata.annotations - all of the pod’s annotations, formatted as annotation-key=\"escaped-annotation-value\" with one annotation per line metadata.labels - 所有Pod的标签，以label-key=\"escaped-label-value\"格式显示，每行显示一个label metadata.annotations - Pod的注释，以annotation-key=\"escaped-annotation-value\"格式显示，每行显示一个标签 以下信息可通过环境变量从fieldRef获得： status.podIP - 节点IP spec.serviceAccountName - Pod服务帐号名称, 版本要求 v1.4.0-alpha.3 spec.nodeName - 节点名称, 版本要求 v1.4.0-alpha.3 status.hostIP - 节点IP, 版本要求 v1.7.0-alpha.1 通过环境变量将Pod信息呈现给容器 用Pod字段作为环境变量的值 apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"sh\", \"-c\"] args: - while true; do echo -en '\\n'; printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE; printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT; sleep 10; done; env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: MY_POD_SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName restartPolicy: Never 这个配置文件中，你可以看到五个环境变量。env字段是一个EnvVars类型的数组。 数组中第一个元素指定MY_NODE_NAME这个环境变量从Pod的spec.nodeName字段获取变量值。同样，其它环境变量也是从Pod的字段获取它们的变量值。 用容器字段作为环境变量的值 apiVersion: v1 kind: Pod metadata: name: dapi-envars-resourcefieldref spec: containers: - name: test-container image: k8s.gcr.io/busybox:1.24 command: [ \"sh\", \"-c\"] args: - while true; do echo -en '\\n'; printenv MY_CPU_REQUEST MY_CPU_LIMIT; printenv MY_MEM_REQUEST MY_MEM_LIMIT; sleep 10; done; resources: requests: memory: \"32Mi\" cpu: \"125m\" limits: memory: \"64Mi\" cpu: \"250m\" env: - name: MY_CPU_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.cpu - name: MY_CPU_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.cpu - name: MY_MEM_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.memory - name: MY_MEM_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.memory restartPolicy: Never 这个配置文件中，你可以看到四个环境变量。env字段是一个EnvVars 类型的数组。数组中第一个元素指定MY_CPU_REQUEST这个环境变量从容器的requests.cpu字段获取变量值。同样，其它环境变量也是从容器的字段获取它们的变量值。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/pod 垂直扩缩容.html":{"url":"blog/kubernetes/task/pod 垂直扩缩容.html","title":"Pod 垂直扩缩容","keywords":"","body":"Autopilot: workload autoscaling at Google 在云计算平台上部署应用程序，经常出现资源分配不合理的现象，单个应用计算资源分配过高属于浪费，分配过低影响性能甚至OOM。 为了解决这个问题，Google使用Autopilot自动配置资源，同时调整作业中的并发任务数（水平缩放）和单个任务的CPU /内存限制（垂直缩放） 很难估算一个工作负载实例需要多少资源才是合理的 资源使用是个动态值，即使当前设定是合理的，随着业务的变化也可能是不合理的 总结： 手动配置的资源永远都不准确 Autopilot 工作机制 算法 依赖于历史用量的指数平滑滑动窗口 元学习：具备自学能力，能够充分利用过去的经验来指导未来的任务。被认为是实现通用人工智能的关键 架构 Autopilot的功能体系结构是三合一的闭环控制系统 水平缩放 CPU垂直缩放 内存垂直缩放 Autopilot的实现（上图）采用架构上的一组标准作业的形式：每个集群都有自己的Autopilot。Recommenders可以看作是高可用的Autopilot服务， 该服务将估算出的最佳资源分配大小推荐给BrogMaster（可以看成是K8s master), BrogMaster 修改配额， 工作负载实际运行的集群节点Broglet（可以看成是node kubelet）调用cgroup 限制容器配额。 垂直自动缩放 Autopilot服务会根据任务选择作业推荐器（cpu 或 内存 推荐器） 预处理（汇总输入信号） --> 推荐算法处理 --> 云计算平台实施资源管理 预处理 低级任务监控记录原始信号是针对一项工作的每个任务的时间测量序列。 我们将监视系统在任务time的时间recorded记录为𝑟𝑖[𝜏]。 此时间序列通常每1秒包含一个样本。为了减少设置作业限制时存储和处理的数据量，我们的监视系统将𝑟𝑖[𝜏]预处理为汇总信号𝑠[𝑡]汇总信号𝑠[𝑡]的一个样本是一个直方图，总结了这5分钟内所有工作任务的资源使用情况。 推荐算法 略 实施资源管理 Borg允许作业在作业运行时修改其资源需求。在水平缩放中，作业可以动态添加或删除任务。在垂直扩展中，作业可以更改其任务的RAM和CPU限制。增加作业的RAM和CPU限制是一项潜在的高成本操作，因为某些任务可能不再适合他们的计算机。在这种情况下，这些计算机上的Borglet将终止一些优先级较低的任务；反过来，这些任务将重新安排到其他计算机上，并可能触发其他优先级更低的任务的终止。 在承载workload 的节点资源足够的情况下，能不停止运行workload的情况下修改资源配额 在承载workload 机节点源不足的情况下，会kill 本节点的其他低级别workload，或者是迁移本workload到其他节点上 水平自动缩放 单个工作负载使用的资源是无法超过承载它的计算机的。为了解决这个问题，Autopilot水平缩放可根据作业的负载动态更改工作负载中的任务数量（副本） CPU使用率：作业所有者指定 CPU使用率信号的平均窗口（默认为5分钟 视界长度𝑇（默认视界为72小时） 统计𝑆：max或𝑃95，即95％ile； 目标平均利用率。 Autopilot根据最新T利用率样本的值time在时间time处计算副本数number [𝑡] = 𝑆𝜏∈[𝑡−𝑇，𝑡] {𝑖𝑟𝑖[𝜏]}。 然后，原始建议的副本数为𝑛𝑟[𝑡] =𝑟𝑆[𝑡] /𝑟∗。 目标大小：作业所有者指定用于计算任务数量的函数，即𝑛𝑟[𝑡] =𝑓[𝑡]。 该功能使用来自作业监视系统的数据。 例如，使用排队系统管理请求的作业可以按请求处理时间的95％缩放； 文件系统服务器可能会根据其管理的文件空间量进行扩展。 当前行业应用背景 阿里云 更新正在运行的Pod资源配置是VPA的一项试验性功能，会导致Pod的重建和重启，而且有可能被调度到其他的节点上。 VPA不会驱逐没有在副本控制器管理下的Pod。目前对于这类Pod，Auto模式等同于Initial模式。 目前VPA不能和监控CPU和内存度量的Horizontal Pod Autoscaler （HPA）同时运行，除非HPA只监控其他定制化的或者外部的资源度量。 VPA使用admission webhook作为其准入控制器。如果集群中有其他的admission webhook，需要确保它们不会与VPA发生冲突。准入控制器的执行顺序定义在API Server的配置参数中。 VPA会处理绝大多数OOM（Out Of Memory）的事件，但不保证所有的场景下都有效。 VPA的性能还没有在大型集群中测试过。 VPA对Pod资源requests的修改值可能超过实际的资源上限，例如节点资源上限、空闲资源或资源配额，从而造成Pod处于Pending状态无法被调度。同时使用集群自动伸缩（ClusterAutoscaler）可以一定程度上解决这个问题。 多个VPA同时匹配同一个Pod会造成未定义的行为。 应用前景 自动扩缩对于云来说效率、可靠性、运维工作量至关重要。手动设置的限制不仅浪费资源（平均配置限制太高），而且随着负载增加或新版本的服务推出时，会导致频繁违反限制。Autopilot是Google使用的垂直和水平自动缩放工具。通过自动提高限制设置的精度，它减少了资源浪费并提高了可靠性：内存不足错误不那么常见，不那么严重，并且减少任务间项目影响。使用简单的时间加权滑动窗口算法可以实现其中一些增益，但是切换到受强化学习启发的更复杂的算法可以实现更大的增益。 收益 在应用界面去除资源配置项，可以获得更愉快的用户体验。 如果算法可靠，可以减少资源闲置 可以减少OOM频次 link 中文翻译 滑动窗口算法 论文地址 阿里云VPA Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/readme.html":{"url":"blog/kubernetes/task/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/server-side-apply.html":{"url":"blog/kubernetes/task/server-side-apply.html","title":"Server Side Apply","keywords":"","body":"服务器端应用（Server-Side Apply） FEATURE STATE: Kubernetes v1.16 [beta] 简介 服务器端应用协助用户、控制器通过声明式配置的方式管理他们的资源。 它发送完整描述的目标（A fully specified intent）， 声明式地创建和/或修改 对象。 一个完整描述的目标并不是一个完整的对象，仅包括能体现用户意图的字段和值。 该目标（intent）可以用来创建一个新对象， 也可以通过服务器来实现与现有对象的合并。 系统支持多个应用者（appliers）在同一个对象上开展协作。 “字段管理（field management）”机制追踪对象字段的变化。 当一个字段值改变时，其所有权从当前管理器（manager）转移到施加变更的管理器。 当尝试将新配置应用到一个对象时，如果字段有不同的值，且由其他管理器管理， 将会引发冲突。 冲突引发警告信号：此操作可能抹掉其他协作者的修改。 冲突可以被刻意忽略，这种情况下，值将会被改写，所有权也会发生转移。 当你从配置文件中删除一个字段，然后应用这个配置文件， 这将触发服务端应用检查此字段是否还被其他字段管理器拥有。 如果没有，那就从活动对象中删除该字段；如果有，那就重置为默认值。 该规则同样适用于 list 或 map 项目。 服务器端应用既是原有 kubectl apply 的替代品， 也是控制器发布自身变化的一个简化机制。 如果你启用了服务器端应用，控制平面就会跟踪被所有新创建对象管理的字段。 字段管理 相对于通过 kubectl 管理的注解 last-applied， 服务器端应用使用了一种更具声明式特点的方法： 它持续的跟踪用户的字段管理，而不仅仅是最后一次的执行状态。 这就意味着，作为服务器端应用的一个副作用， 关于用哪一个字段管理器负责管理对象中的哪个字段的这类信息，都要对外界开放了。 用户管理字段这件事，在服务器端应用的场景中，意味着用户依赖并期望字段的值不要改变。 最后一次对字段值做出断言的用户将被记录到当前字段管理器。 这可以通过发送 POST、 PUT、 或非应用（non-apply）方式的 PATCH 等命令来修改字段值的方式实现， 或通过把字段放在配置文件中，然后发送到服务器端应用的服务端点的方式实现。 当使用服务器端应用，尝试着去改变一个被其他人管理的字段， 会导致请求被拒绝（在没有设置强制执行时，参见冲突）。 如果两个或以上的应用者均把同一个字段设置为相同值，他们将共享此字段的所有权。 后续任何改变共享字段值的尝试，不管由那个应用者发起，都会导致冲突。 共享字段的所有者可以放弃字段的所有权，这只需从配置文件中删除该字段即可。 字段管理的信息存储在 managedFields 字段中，该字段是对象的 metadata中的一部分。 服务器端应用创建对象的简单示例如下： apiVersion: v1 kind: ConfigMap metadata: name: test-cm namespace: default labels: test-label: test managedFields: - manager: kubectl operation: Apply apiVersion: v1 time: \"2010-10-10T0:00:00Z\" fieldsType: FieldsV1 fieldsV1: f:metadata: f:labels: f:test-label: {} f:data: f:key: {} data: key: some value 上述对象在 metadata.managedFields 中包含了唯一的管理器。 管理器由管理实体自身的基本信息组成，比如操作类型、API 版本、以及它管理的字段。 说明： 该字段由 API 服务器管理，用户不应该改动它。 不过，执行 Update 操作修改 metadata.managedFields 也是可实现的。 强烈不鼓励这么做，但当发生如下情况时， 比如 managedFields 进入不一致的状态（显然不应该发生这种情况）， 这么做也是一个合理的尝试。 managedFields 的格式在 API 文档中描述。 冲突 冲突是一种特定的错误状态， 发生在执行 Apply 改变一个字段，而恰巧该字段被其他用户声明过主权时。 这可以防止一个应用者不小心覆盖掉其他用户设置的值。 冲突发生时，应用者有三种办法来解决它： 覆盖前值，成为唯一的管理器： 如果打算覆盖该值（或应用者是一个自动化部件，比如控制器）， 应用者应该设置查询参数 force 为 true，然后再发送一次请求。 这将强制操作成功，改变字段的值，从所有其他管理器的 managedFields 条目中删除指定字段。 不覆盖前值，放弃管理权： 如果应用者不再关注该字段的值， 可以从配置文件中删掉它，再重新发送请求。 这就保持了原值不变，并从 managedFields 的应用者条目中删除该字段。 不覆盖前值，成为共享的管理器： 如果应用者仍然关注字段值，并不想覆盖它， 他们可以在配置文件中把字段的值改为和服务器对象一样，再重新发送请求。 这样在不改变字段值的前提下， 就实现了字段管理被应用者和所有声明了管理权的其他的字段管理器共享。 管理器 管理器识别出正在修改对象的工作流程（在冲突时尤其有用）, 管理器可以通过修改请求的参数 fieldManager 指定。 虽然 kubectl 默认发往 kubectl 服务端点，但它则请求到应用的服务端点（apply endpoint）。 对于其他的更新，它默认的是从用户代理计算得来。 应用和更新 此特性涉及两类操作，分别是 Apply （内容类型为 application/apply-patch+yaml 的 PATCH 请求） 和 Update （所有修改对象的其他操作）。 这两类操作都会更新字段 managedFields，但行为表现有一点不同。 说明： 不管你提交的是 JSON 数据还是 YAML 数据， 都要使用 application/apply-patch+yaml 作为 Content-Type 的值。 所有的 JSON 文档 都是合法的 YAML。 例如，在冲突发生的时候，只有 apply 操作失败，而 update 则不会。 此外，apply 操作必须通过提供一个 fieldManager 查询参数来标识自身， 而此查询参数对于 update 操作则是可选的。 最后，当使用 apply 命令时，你不能在应用中的对象中持有 managedFields。 一个包含多个管理器的对象，示例如下： apiVersion: v1 kind: ConfigMap metadata: name: test-cm namespace: default labels: test-label: test managedFields: - manager: kubectl operation: Apply apiVersion: v1 fields: f:metadata: f:labels: f:test-label: {} - manager: kube-controller-manager operation: Update apiVersion: v1 time: '2019-03-30T16:00:00.000Z' fields: f:data: f:key: {} data: key: new value 在这个例子中， 第二个操作被管理器 kube-controller-manager 以 Update 的方式运行。 此 update 更改 data 字段的值， 并使得字段管理器被改为 kube-controller-manager。 如果把 update 操作改为 Apply，那就会因为所有权冲突的原因，导致操作失败。 合并策略 由服务器端应用实现的合并策略，提供了一个总体更稳定的对象生命周期。 服务器端应用试图依据谁管理它们来合并字段，而不只是根据值来否决。 这么做是为了多个参与者可以更简单、更稳定的更新同一个对象，且避免引起意外干扰。 当用户发送一个“完整描述的目标”对象到服务器端应用的服务端点， 服务器会将它和活动对象做一次合并，如果两者中有重复定义的值，那就以配置文件的为准。 如果配置文件中的项目集合不是此用户上一次操作项目的超集， 所有缺少的、没有其他应用者管理的项目会被删除。 关于合并时用来做决策的对象规格的更多信息，参见 sigs.k8s.io/structured-merge-diff. Kubernetes 1.16 和 1.17 中添加了一些标记， 允许 API 开发人员描述由 list、map、和 structs 支持的合并策略。 这些标记可应用到相应类型的对象，在 Go 文件或在 CRD 的 OpenAPI 的模式中定义： Golang 标记 OpenAPI extension 可接受的值 描述 引入版本 //+listType x-kubernetes-list-type atomic/set/map 适用于 list。 atomic 和 set 适用于只包含标量元素的 list。 map 适用于只包含嵌套类型的 list。 如果配置为 atomic, 合并时整个列表会被替换掉; 任何时候，唯一的管理器都把列表作为一个整体来管理。如果是 set 或 map ，不同的管理器也可以分开管理条目。 1.16 //+listMapKey x-kubernetes-list-map-keys 用来唯一标识条目的 map keys 切片，例如 [\"port\", \"protocol\"] 仅当 +listType=map 时适用。组合值的字符串切片必须唯一标识列表中的条目。尽管有多个 key，listMapKey 是单数的，这是因为 key 需要在 Go 类型中单独的指定。 1.16 //+mapType x-kubernetes-map-type atomic/granular 适用于 map。 atomic 指 map 只能被单个的管理器整个的替换。 granular 指 map 支持多个管理器各自更新自己的字段。 1.17 //+structType x-kubernetes-map-type atomic/granular 适用于 structs；否则就像 //+mapType 有相同的用法和 openapi 注释. 1.17 自定义资源 默认情况下，服务器端应用把自定义资源看做非结构化数据。 所有的键值（keys）就像 struct 的字段一样被处理， 所有的 list 被认为是原子性的。 如果自定义资源定义（Custom Resource Definition，CRD）定义了一个 模式， 它包含类似以前“合并策略”章节中定义过的注解， 这些注解将在合并此类型的对象时使用。 在控制器中使用服务器端应用 控制器的开发人员可以把服务器端应用作为简化控制器的更新逻辑的方式。 读-改-写 和/或 patch 的主要区别如下所示： 应用的对象必须包含控制器关注的所有字段。 对于在控制器没有执行过应用操作之前就已经存在的字段，不能删除。 （控制器在这种用例环境下，依然可以发送一个 PATCH/UPDATE） 对象不必事先读取，resourceVersion 不必指定。 强烈推荐：设置控制器在冲突时强制执行，这是因为冲突发生时，它们没有其他解决方案或措施。 转移所有权 除了通过冲突解决方案提供的并发控制， 服务器端应用提供了一些协作方式来将字段所有权从用户转移到控制器。 最好通过例子来说明这一点。 让我们来看看，在使用 HorizontalPodAutoscaler 资源和与之配套的控制器， 且开启了 Deployment 的自动水平扩展功能之后， 怎么安全的将 replicas 字段的所有权从用户转移到控制器。 假设用户定义了 Deployment，且 replicas 字段已经设置为期望的值： application/ssa/nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 并且，用户使用服务器端应用，像这样创建 Deployment： kubectl apply -f https://k8s.io/examples/application/ssa/nginx-deployment.yaml --server-side 然后，为 Deployment 启用 HPA，例如： kubectl autoscale deployment nginx-deployment --cpu-percent=50 --min=1 --max=10 现在，用户希望从他们的配置中删除 replicas，所以他们总是和 HPA 控制器冲突。 然而，这里存在一个竟态： 在 HPA 需要调整 replicas 之前会有一个时间窗口， 如果在 HPA 写入字段成为所有者之前，用户删除了replicas， 那 API 服务器就会把 replicas 的值设为1， 也就是默认值。 这不是用户希望发生的事情，即使是暂时的。 这里有两个解决方案： （容易） 把 replicas 留在配置文件中；当 HPA 最终写入那个字段， 系统基于此事件告诉用户：冲突发生了。在这个时间点，可以安全的删除配置文件。 （高级）然而，如果用户不想等待，比如他们想为合作伙伴保持集群清晰， 那他们就可以执行以下步骤，安全的从配置文件中删除 replicas。 首先，用户新定义一个只包含 replicas 字段的配置文件： application/ssa/nginx-deployment-replicas-only.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 用户使用名为 handover-to-hpa 的字段管理器，应用此配置文件。 kubectl apply -f https://k8s.io/examples/application/ssa/nginx-deployment-replicas-only.yaml \\ --server-side --field-manager=handover-to-hpa \\ --validate=false 如果应用操作和 HPA 控制器产生冲突，那什么都不做。 冲突只是表明控制器在更早的流程中已经对字段声明过所有权。 在此时间点，用户可以从配置文件中删除 replicas 。 application/ssa/nginx-deployment-no-replicas.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 注意，只要 HPA 控制器为 replicas 设置了一个新值， 该临时字段管理器将不再拥有任何字段，会被自动删除。 这里不需要执行清理工作。 在用户之间转移所有权 通过在配置文件中把一个字段设置为相同的值，用户可以在他们之间转移字段的所有权， 从而共享了字段的所有权。 当用户共享了字段的所有权，任何一个用户可以从他的配置文件中删除该字段， 并应用该变更，从而放弃所有权，并实现了所有权向其他用户的转移。 与客户端应用的对比 由服务器端应用实现的冲突检测和解决方案的一个结果就是， 应用者总是可以在本地状态中得到最新的字段值。 如果得不到最新值，下次执行应用操作时就会发生冲突。 解决冲突三个选项的任意一个都会保证：此应用过的配置文件是服务器上对象字段的最新子集。 这和客户端应用（Client Side Apply） 不同，如果有其他用户覆盖了此值， 过期的值被留在了应用者本地的配置文件中。 除非用户更新了特定字段，此字段才会准确， 应用者没有途径去了解下一次应用操作是否会覆盖其他用户的修改。 另一个区别是使用客户端应用的应用者不能改变他们正在使用的 API 版本，但服务器端应用支持这个场景。 从客户端应用升级到服务器端应用 客户端应用方式时，用户使用 kubectl apply 管理资源， 可以通过使用下面标记切换为使用服务器端应用。 kubectl apply --server-side [--dry-run=server] 默认情况下，对象的字段管理从客户端应用方式迁移到 kubectl 触发的服务器端应用时，不会发生冲突。 注意： 保持注解 last-applied-configuration 是最新的。 从注解能推断出字段是由客户端应用管理的。 任何没有被客户端应用管理的字段将引发冲突。 举例说明，比如你在客户端应用之后， 使用 kubectl scale 去更新 replicas 字段， 可是该字段并没有被客户端应用所拥有， 在执行 kubectl apply --server-side 时就会产生冲突。 此操作以 kubectl 作为字段管理器来应用到服务器端应用。 作为例外，可以指定一个不同的、非默认字段管理器停止的这种行为，如下面的例子所示。 对于 kubectl 触发的服务器端应用，默认的字段管理器是 kubectl。 kubectl apply --server-side --field-manager=my-manager [--dry-run=server] 从服务器端应用降级到客户端应用 如果你用 kubectl apply --server-side 管理一个资源， 可以直接用 kubectl apply 命令将其降级为客户端应用。 降级之所以可行，这是因为 kubectl server-side apply 会保存最新的 last-applied-configuration 注解。 此操作以 kubectl 作为字段管理器应用到服务器端应用。 作为例外，可以指定一个不同的、非默认字段管理器停止这种行为，如下面的例子所示。 对于 kubectl 触发的服务器端应用，默认的字段管理器是 kubectl。 kubectl apply --server-side --field-manager=my-manager [--dry-run=server] API 端点 启用了服务器端应用特性之后， PATCH 服务端点接受额外的内容类型 application/apply-patch+yaml。 服务器端应用的用户就可以把 YAMl 格式的 部分定义对象（partially specified objects）发送到此端点。 当一个配置文件被应用时，它应该包含所有体现你意图的字段。 清除 ManagedFields 可以从对象中剥离所有 managedField， 实现方法是通过使用 MergePatch、 StrategicMergePatch、 JSONPatch、 Update、以及所有的非应用方式的操作来覆盖它。 这可以通过用空条目覆盖 managedFields 字段的方式实现。 PATCH /api/v1/namespaces/default/configmaps/example-cm Content-Type: application/merge-patch+json Accept: application/json Data: {\"metadata\":{\"managedFields\": [{}]}} PATCH /api/v1/namespaces/default/configmaps/example-cm Content-Type: application/json-patch+json Accept: application/json Data: [{\"op\": \"replace\", \"path\": \"/metadata/managedFields\", \"value\": [{}]}] 这一操作将用只包含一个空条目的 list 覆写 managedFields， 来实现从对象中整个的去除 managedFields。 注意，只把 managedFields 设置为空 list 并不会重置字段。 这么做是有目的的，所以 managedFields 将永远不会被与该字段无关的客户删除。 在重置操作结合 managedFields 以外其他字段更改的场景中， 将导致 managedFields 首先被重置，其他改变被押后处理。 其结果是，应用者取得了同一个请求中所有字段的所有权。 注意： 对于不接受资源对象类型的子资源（sub-resources）， 服务器端应用不能正确地跟踪其所有权。 如果你对这样的子资源使用服务器端应用，变更的字段将不会被跟踪。 禁用此功能 服务器端应用是一个 beta 版特性，默认启用。 要关闭此特性门控， 你需要在启动 kube-apiserver 时包含参数 --feature-gates ServerSideApply=false。 如果你有多个 kube-apiserver 副本，他们都应该有相同的标记设置。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 08:24:44 "},"blog/kubernetes/task/service.html":{"url":"blog/kubernetes/task/service.html","title":"Service","keywords":"","body":"Service 概述 ​ 摘自： https://www.cnblogs.com/xzkzzz/p/9559362.html kubernetes 中的pod是有生生灭灭的，时刻都有可能被新的pod所代替，而不可复活（pod的生命周期）。一旦一个pod生命终止，通过ReplicaSets动态创建和销毁pod（Pod的动态扩缩容，滚动升级 等）。 每个pod都有自己的IP,这IP随着pod的生生灭灭而变化，不能被依赖。这样导致一个问题，如果这个POD作为后端（backend）提供一些功能供给一些前端POD（frontend）,在kubernete集群中是如何实现让这些前台能够持续的追踪到这些后台的?所以之间需要一个服务作为后端的服务负载------service Kubernetes Service 是一个定义了一组Pod的策略的抽象，这些被服务标记的Pod都是（一般）通过label Selector实现的 举个例子，考虑一个图片处理 backend，它运行了3个副本。这些副本是可互换的 —— frontend 不需要关心它们调用了哪个 backend 副本。 然而组成这一组 backend 程序的 Pod 实际上可能会发生变化，frontend 客户端不应该也没必要知道，而且也不需要跟踪这一组 backend 的状态。 Service 定义的抽象能够解耦这种关联。 Service 实现的三种方式 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式，在 Kubernetes v1.0 版本，代理完全在 userspace。在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。 从 Kubernetes v1.2 起，默认就是 iptables 代理。在Kubernetes v1.8.0-beta.0中，添加了ipvs代理。在 Kubernetes v1.0 版本，Service 是 “4层”（TCP/UDP over IP）概念。 在 Kubernetes v1.1 版本，新增了 Ingress API（beta 版），用来表示 “7层”（HTTP）服务。 kube-proxy 这个组件始终监视着apiserver中有关service的变动信息，获取任何一个与service资源相关的变动状态，通过watch监视，一旦有service资源相关的变动和创建，kube-proxy都要转换为当前节点上的能够实现资源调度规则（例如：iptables、ipvs） userspace 代理模式 这种模式，当客户端Pod请求内核空间的service iptables后，把请求转到给用户空间监听的kube-proxy 的端口，由kube-proxy来处理后，再由kube-proxy打请求转给内核空间的 service iptalbes，再由service iptalbes根据请求转给各节点中的的service pod。由此可见这个模式有很大的问题，由客户端请求先进入内核空间的，又进去用户空间访问kube-proxy，由kube-proxy封装完成后再进去内核空间的iptables，再根据iptables的规则分发给各节点的用户空间的pod。这样流量从用户空间进出内核带来的性能损耗是不可接受的 iptables 代理模式 客户端IP请求时，直接求情本地内核service ip，根据iptables的规则求情到各pod上，因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。 ipvs 代理模式 客户端IP请求时，直接求情本地内核service ipvs，根据ipvs的规则求情到各pod上。kube-proxy会监视Kubernetes Service对象和Endpoints，调用netlink接口以相应地创建ipvs规则并定期与Kubernetes Service对象和Endpoints对象同步ipvs规则，以确保ipvs状态与期望一致。访问服务时，流量将被重定向到其中一个后端Pod。 与iptables类似，ipvs基于netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如： rr：轮询调度 lc：最小连接数 dh：目标哈希 sh：源哈希 sed：最短期望延迟 nq：不排队调度 注意： ipvs模式假定在运行kube-proxy之前在节点上都已经安装了IPVS内核模块。当kube-proxy以ipvs代理模式启动时，kube-proxy将验证节点上是否安装了IPVS模块，如果未安装，则kube-proxy将回退到iptables代理模式。 如果某个服务后端pod发生变化，标签选择器适应的pod有多一个，适应的信息会立即放映到apiserver上,而kube-proxy一定可以watch到etc中的信息变化，而将他立即转为ipvs或者iptables中的规则，这一切都是动态和实时的，删除一个pod也是同样的原理。 service 定义 kubectl explain svc.spec ports 建立哪些端口，暴露的端口是哪些 selector 把哪些容器通过这个service暴露出去 type 有四种 (ExternalName ClusterIP NodePort LoadBalancer) 默认是ClusterIP ports 的定义 kubectl explain svc.spec.ports name 指定的port的名称 nodePort 指定节点上的端口 port 暴露给服务的端口 targetPort 容器的端口 protocol 执行协议（TCP or UDP） ClusterIP方式 apiVersion: v1 kind: Service metadata: name: redis namespace: default spec: selector: app: redis role: log-store type: ClusterIP ports: - port: 6379 targetPort: 6379 查看一下详细 $ kubectl describe svc redis Name: redis Namespace: default Labels: Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"name\":\"redis\",\"namespace\":\"default\"},\"spec\":{\"ports\":[{\"port\":6379,\"targetPort\":6379}... Selector: app=redis,role=log-store Type: ClusterIP IP: 10.43.164.114 Port: 6379/TCP Endpoints: 10.42.0.219:6379 Session Affinity: None Events: 资源记录格式： SVC_NAME.NS_NAME.DOMAIN.LTD. 默认的service的a记录 svc.cluster.local. 刚创建的service的a记录 redis.default.cluster.local. NodePort方式 apiVersion: v1 kind: Service metadata: name: myapp namespace: default spec: selector: app: myapp release: dev type: NodePort ports: - port: 80 targetPort: 80 nodePort: 30080 $ kubectl describe svc myapp Name: myapp Namespace: default Labels: Annotations: field.cattle.io/publicEndpoints=[{\"addresses\":[\"172.16.138.170\"],\"port\":30080,\"protocol\":\"TCP\",\"serviceName\":\"default:myapp\",\"allNodes\":true}] kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"name\":\"myapp\",\"namespace\":\"default\"},\"spec\":{\"ports\":[{\"nodePort\":30080,\"port\":80,\"ta... Selector: app=myapp,release=dev Type: NodePort IP: 10.43.162.175 Port: 80/TCP NodePort: 30080/TCP Endpoints: 10.42.0.218:80,10.42.1.107:80,10.42.3.210:80 Session Affinity: None Events: #可以看到他负责均衡的效果 $ for a in {1..10}; do curl http://172.16.138.170:30080/hostname.html && sleep 1s; done myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-7shh9 myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-7shh9 myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-7shh9 myapp-deploy-869b888f66-vwgj2 myapp-deploy-869b888f66-7shh9 myapp-deploy-869b888f66-4l4cv LoadBalancer类型 使用支持外部负载均衡器的云提供商的服务，设置 type 的值为 \"LoadBalancer\"，将为 Service 提供负载均衡器。 负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的 status.loadBalancer 字段被发布出去。 来自外部负载均衡器的流量将直接打到 backend Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。 在这些情况下，将根据用户设置的 loadBalancerIP 来创建负载均衡器。 某些云提供商允许设置 loadBalancerIP。如果没有设置 loadBalancerIP，将会给负载均衡器指派一个临时 IP。 如果设置了 loadBalancerIP，但云提供商并不支持这种特性，那么设置的 loadBalancerIP 值将会被忽略掉。 ExternalName 类型 提供访问发布服务的，像使用集群内部一样使用外部服务。 会话粘性（常说的会话保持） kubectl explain svc.spec.sessionAffinity 支持ClientIP和None 两种方式，默认是None（随机调度） ClientIP是来自于同一个客户端的请求调度到同一个pod中 apiVersion: v1 kind: Service metadata: name: myapp namespace: default spec: selector: app: myapp release: dev sessionAffinity: ClientIP type: NodePort ports: - port: 80 targetPort: 80 nodePort: 30080 查看来自同一客户端的请求始终访问同一个Pod $ kubectl describe svc myapp Name: myapp Namespace: default Labels: Annotations: field.cattle.io/publicEndpoints=[{\"addresses\":[\"172.16.138.170\"],\"port\":30080,\"protocol\":\"TCP\",\"serviceName\":\"default:myapp\",\"allNodes\":true}] kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"name\":\"myapp\",\"namespace\":\"default\"},\"spec\":{\"ports\":[{\"nodePort\":30080,\"port\":80,\"ta... Selector: app=myapp,release=dev Type: NodePort IP: 10.43.162.175 Port: 80/TCP NodePort: 30080/TCP Endpoints: 10.42.0.218:80,10.42.1.107:80,10.42.3.210:80 Session Affinity: ClientIP Events: $ for a in {1..10}; do curl http://172.16.138.170:30080/hostname.html && sleep 1s; done myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv myapp-deploy-869b888f66-4l4cv Headless service(就是没有Cluster IP 的Service ) 有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（spec.clusterIP）的值为 \"None\" 来创建 Headless Service。它会给一个集群内部的每个成员提供一个唯一的DNS域名来作为每个成员的网络标识，集群内部成员之间使用域名通信 这个选项允许开发人员自由寻找他们自己的方式，从而降低与 Kubernetes 系统的耦合性。 应用仍然可以使用一种自注册的模式和适配器，对其它需要发现机制的系统能够很容易地基于这个 API 来构建。 对这类 Service 并不会分配 Cluster IP，kube-proxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了 selector。 apiVersion: v1 kind: Service metadata: name: myapp-headless namespace: default spec: selector: app: myapp release: dev clusterIP: \"None\" ports: - port: 80 targetPort: 80 验证 $ dig -t A myapp-headless.default.svc.cluster.local. @10.42.0.5 ; > DiG 9.9.4-RedHat-9.9.4-61.el7 > -t A myapp-headless.default.svc.cluster.local. @10.42.0.5 ;; global options: +cmd ;; Got answer: ;; ->>HEADERlink： ​ https://www.cnblogs.com/xzkzzz/p/9559362.html Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/storage-class.html":{"url":"blog/kubernetes/task/storage-class.html","title":"Storage Class","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/token.html":{"url":"blog/kubernetes/task/token.html","title":"Token","keywords":"","body":"token create token https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-token/ 【现网问题 】ahs路由管理修改名称无效https://www.tapd.cn/57974601/bugtrace/bugs/view?bug_id=1157974601001006154 【现网问题】线上修改路由证书，不生效https://www.tapd.cn/57974601/bugtrace/bugs/view?bug_id=1157974601001006153 【现网问题】【cnops】当有代码源配置后，效率看板中点击“代码仓库”，没有提示“视图配置” 导致以为数据获取不到，请添加提示语https://www.tapd.cn/57974601/bugtrace/bugs/view?bug_id=1157974601001006155 【现网问题】cnops-效率看板中，若流水线同步接口请求后，在ci/cd查看流水线数据时会报错https://www.tapd.cn/57974601/bugtrace/bugs/view?bug_id=1157974601001006157 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/准入控制.html":{"url":"blog/kubernetes/task/准入控制.html","title":"准入控制","keywords":"","body":"kubernetes webhook 什么是准入控制插件 如何启用一个准入控制器 怎么关闭准入控制器 哪些插件是默认启用的 每个准入控制器的作用是什么 动态准入控制 什么是 admission webhook 生成CA证书什么是准入控制插件 准入控制器是一段代码，它会在请求通过认证和授权之后、对象被持久化之前拦截到达 API 服务器的请求 有两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 如何启用一个准入控制器 Kubernetes API 服务器的 enable-admission-plugins 标志，它指定了一个用于在集群修改对象之前调用的（以逗号分隔的）准入控制插件顺序列表。 例如，下面的命令就启用了 NamespaceLifecycle 和 LimitRanger 准入控制插件： kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ... 怎么关闭准入控制器 Kubernetes API 服务器的 disable-admission-plugins 标志，会将传入的（以逗号分隔的）准入控制插件列表禁用，即使是默认启用的插件也会被禁用。 kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ... 哪些插件是默认启用的 kube-apiserver -h | grep enable-admission-plugins 每个准入控制器的作用是什么 AlwaysPullImages 该准入控制器会修改每一个新创建的 Pod 的镜像拉取策略为 Always 。 这在多租户集群中是有用的，这样用户就可以放心，他们的私有镜像只能被那些有凭证的人使用。 如果没有这个准入控制器，一旦镜像被拉取到节点上，任何用户的 pod 都可以通过已了解到的镜像的名称（假设 pod 被调度到正确的节点上）来使用它，而不需要对镜像进行任何授权检查。 当启用这个准入控制器时，总是在启动容器之前拉取镜像，这意味着需要有效的凭证。 MutatingAdmissionWebhook FEATURE STATE: Kubernetes v1.13 该准入控制器调用任何与请求匹配的变更 webhook。匹配的 webhook 将被串行调用。每一个 webhook 都可以根据需要修改对象。 MutatingAdmissionWebhook ，顾名思义，仅在变更阶段运行。 如果由此准入控制器调用的 Webhook 有副作用（如降低配额）， 则它 必须 具有协调系统，因为不能保证后续的 Webhook 和验证准入控制器都会允许完成请求。 如果你禁用了 MutatingAdmissionWebhook，那么还必须使用 --runtime-config 标志禁止 admissionregistration.k8s.io/v1beta1 组/版本中的 MutatingWebhookConfiguration 对象（版本 >=1.9 时，这两个对象都是默认启用的） ValidatingAdmissionWebhook FEATURE STATE: Kubernetes v1.13 该准入控制器调用与请求匹配的所有验证 webhook。匹配的 webhook 将被并行调用。如果其中任何一个拒绝请求，则整个请求将失败。 该准入控制器仅在验证阶段运行；与 MutatingAdmissionWebhook 准入控制器所调用的 webhook 相反，它调用的 webhook 应该不会使对象出现变更。 如果以此方式调用的 webhook 有其它作用（如，配额递减），则它必须具有协调系统，因为不能保证后续的 webhook 或其他有效的准入控制器都允许请求完成。 如果您禁用了 ValidatingAdmissionWebhook，还必须在 admissionregistration.k8s.io/v1beta1 组/版本中使用 --runtime-config 标志来禁用 ValidatingWebhookConfiguration 对象（默认情况下在 1.9 版和更高版本中均处于启用状态）。 ServiceAccount FEATURE STATE: Kubernetes v1.16 alpha 该准入控制器实现了 serviceAccounts 的自动化。 如果您打算使用 Kubernetes 的 ServiceAccount 对象，我们强烈建议您使用这个准入控制器。 容器运行时类 容器运行时类定义描述了与运行 Pod 相关的开销。此准入控制器将相应地设置 pod.Spec.Overhead 字段。 详情请参见 Pod 开销。 有推荐的准入控制器吗 有，对于 Kubernetes 1.10 以上的版本，推荐使用的准入控制器默认情况下都处于启用状态（查看这里）。 因此您无需显式指定它们。您可以使用 --enable-admission-plugins 标志（ 顺序不重要 ）来启用默认设置以外的其他准入控制器。 注意 : --admission-control 在 1.10 中已废弃，已由 --enable-admission-plugins 取代。 对于 Kubernetes 1.9 及更早版本，我们建议使用 --admission-control 标志（顺序很重要）运行下面的一组准入控制器 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota 什么是 admission webhook Admission webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的 admission webhook，即 validating admission webhook 和 mutating admission webhook。 Mutating admission webhook 会先被调用。它们可以更改发送到 API 服务器的对象以执行自定义的设置默认值操作。 在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后，validating admission webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。 注意： 如果 admission webhook 需要保证它们所看到的是对象的最终状态以实施某种策略。则应使用 validating admission webhook，因为对象被 mutating webhook 看到之后仍然可能被修改。 required 确保 Kubernetes 集群版本至少为 v1.16（以便使用 admissionregistration.k8s.io/v1 API） 或者 v1.9 （以便使用 admissionregistration.k8s.io/v1beta1 API）。 确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 这里 是一组推荐的 admission 控制器，通常可以启用。 确保启用了 admissionregistration.k8s.io/v1beta1 API。 - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key - --requestheader-allowed-names=front-proxy-client - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt - --requestheader-extra-headers-prefix=X-Remote-Extra- - --requestheader-group-headers=X-Remote-Group - --requestheader-username-headers=X-Remote-User - --secure-port=6443 - --service-account-key-file=/etc/kubernetes/pki/sa.pub - --service-cluster-ip-range=10.96.0.0/12 - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --enable-admission-plugins=NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook 上面的enable-admission-plugins参数中带上了MutatingAdmissionWebhook和ValidatingAdmissionWebhook两个准入控制插件，如果没有的，需要添加上这两个参数，然后重启 apiserver。 然后通过运行下面的命令检查集群中是否启用了准入注册 API： [root@wyh ~]# kubectl api-versions |grep admission admissionregistration.k8s.io/v1beta1 write admission webhook server 请参阅 Kubernetes e2e 测试中的 admission webhook 服务器 的实现。webhook 处理由 apiserver 发送的 AdmissionReview 请求，并且将其决定作为 AdmissionReview 对象以相同版本发送回去。 有关发送到 webhook 的数据的详细信息，请参阅 webhook 请求。 要获取来自 webhook 的预期数据，请参阅 webhook 响应。 示例 admission webhook 服务器置 ClientAuth 字段为空，默认为 NoClientCert 。这意味着 webhook 服务器不会验证客户端的身份，认为其是 apiservers。 如果您需要双向 TLS 或其他方式来验证客户端，请参阅如何对 apiservers 进行身份认证。 deploy admission webhook server webhook 配置 要注册 admssion webhook，请创建 MutatingWebhookConfiguration 或 ValidatingWebhookConfiguration API 对象。 每种配置可以包含一个或多个 webhook。如果在单个配置中指定了多个 webhook，则应为每个 webhook 赋予一个唯一的名称。 这在 admissionregistration.k8s.io/v1 中是必需的，但是在使用 admissionregistration.k8s.io/v1beta1 时强烈建议使用，以使生成的审核日志和指标更易于与活动配置相匹配。 每个 webhook 定义以下内容。 匹配请求-规则 每个 webhook 必须指定用于确定是否应将对 apiserver 的请求发送到 webhook 的规则列表。 每个规则都指定一个或多个 operations、apiGroups、apiVersions 和 resources 以及资源的 scope： operations 列出一个或多个要匹配的操作。可以是 CREATE、UPDATE、DELETE、CONNECT 或 * 以匹配所有内容。 apiGroups 列出了一个或多个要匹配的 API 组。\"\" 是核心 API 组。\"*\" 匹配所有 API 组。 apiVersions 列出了一个或多个要匹配的 API 版本。\"*\" 匹配所有 API 版本。 resources 列出了一个或多个要匹配的资源。 \"*\" 匹配所有资源，但不包括子资源。 \"/\" 匹配所有资源，包括子资源。 \"pods/*\" 匹配 pod 的所有子资源。 \"*/status\" 匹配所有 status 子资源。 scope 指定要匹配的范围。有效值为 \"Cluster\"、\"Namespaced\" 和 \"\"。子资源匹配其父资源的范围。在 Kubernetes v1.14+ 版本中才被支持。默认值为 \"\"，对应 1.14 版本之前的行为。 \"Cluster\" 表示只有集群作用域的资源才能匹配此规则（API 对象 Namespace 是集群作用域的）。 \"Namespaced\" 意味着仅具有命名空间的资源才符合此规则。 \"*\" 表示没有范围限制。 如果传入请求与任何 Webhook 规则的指定操作、组、版本、资源和范围匹配，则该请求将发送到 Webhook。 以下是可用于指定应拦截哪些资源的规则的其他示例。 匹配针对 apps/v1 和 apps/v1beta1 组中 deployments 和 replicasets 资源的 CREATE 或 UPDATE 请求： apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\"CREATE\", \"UPDATE\"] apiGroups: [\"apps\"] apiVersions: [\"v1\", \"v1beta1\"] resources: [\"deployments\", \"replicasets\"] scope: \"Namespaced\" ... # v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\"CREATE\", \"UPDATE\"] apiGroups: [\"apps\"] apiVersions: [\"v1\", \"v1beta1\"] resources: [\"deployments\", \"replicasets\"] scope: \"Namespaced\" ... 生成CA证书 webhook-create-signed-cert.sh #!/bin/bash set -e usage() { cat > ${tmpdir}/csr.conf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = ${service} DNS.2 = ${service}.${namespace} DNS.3 = ${service}.${namespace}.svc EOF openssl genrsa -out ${tmpdir}/server-key.pem 2048 openssl req -new -key ${tmpdir}/server-key.pem -subj \"/CN=${service}.${namespace}.svc\" -out ${tmpdir}/server.csr -config ${tmpdir}/csr.conf # clean-up any previously created CSR for our service. Ignore errors if not present. kubectl delete csr ${csrName} 2>/dev/null || true # create server cert/key CSR and send to k8s API cat &2 exit 1 fi echo ${serverCert} | openssl base64 -d -A -out ${tmpdir}/server-cert.pem # create the secret with CA cert and server cert/key kubectl create secret generic ${secret} \\ --from-file=key.pem=${tmpdir}/server-key.pem \\ --from-file=cert.pem=${tmpdir}/server-cert.pem \\ --dry-run -o yaml | kubectl -n ${namespace} apply -f - webhook-patch-ca-bundle.sh #!/bin/bash ROOT=$(cd $(dirname $0)/../../; pwd) set -o errexit set -o nounset set -o pipefail export CA_BUNDLE=$(kubectl config view --raw --flatten -o json | jq -r '.clusters[] | select(.name == \"kubernetes\") | .cluster.\"certificate-authority-data\"') if command -v envsubst >/dev/null 2>&1; then envsubst else sed -e \"s|\\${CA_BUNDLE}|${CA_BUNDLE}|g\" fi Question: MutatingWebhookConfiguration 是否一定要绑定ca caBundle 怎么生成 参考文献: https://kubernetes.io/zh/docs/reference/access-authn-authz/webhook/ https://www.qikqiak.com/post/k8s-admission-webhook/ https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#how-do-i-turn-on-an-admission-controller https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/聚合api.html":{"url":"blog/kubernetes/task/聚合api.html","title":"聚合Api","keywords":"","body":"聚合PAI CRD apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1.custom-gateway spec: insecureSkipTLSVerify: true group: custom-gateway groupPriorityMinimum: 1000 versionPriority: 5 service: name: canary namespace: default version: v1 --- apiVersion: apps/v1 kind: Deployment metadata: name: canary namespace: default spec: replicas: 1 selector: matchLabels: app: canary version: v1 template: metadata: labels: app: canary version: v1 spec: containers: - image: xishengcai/canary imagePullPolicy: IfNotPresent name: canary ports: - containerPort: 80 name: port protocol: TCP --- apiVersion: v1 kind: Service metadata: name: canary namespace: default spec: ports: - name: port port: 443 protocol: TCP targetPort: 443 selector: app: canary sessionAffinity: None type: NodePort required： 后端服务 必须开通https 服务 后端服务的api 必须以 “/apis/{group}.{version}/v1/\" 开头 后端服务的API 通过k8s API 访问效果： 注意后端服务的api路径必须符合规范： /apis/group/version/。。。。 curl localhost:8080/apis/custom-gateway/v1/hello2 实验1: ​ 场景：后端服务 自带https ​ 结果： 可行 实验2: ​ 场景：后端服务只有http，通过nginx 转发 ​ 结果： 不可行 疑问： 1. 聚合api 背后的实现原理 ​ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/资源预留配置.html":{"url":"blog/kubernetes/task/资源预留配置.html","title":"资源预留配置","keywords":"","body":"Kubernetes 资源预留配置 Kubernetes 的节点可以按照节点的资源容量进行调度，默认情况下 Pod 能够使用节点全部可用容量。这样就会造成一个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。除非为这些系统守护进程留出资源，否则它们将与 Pod 争夺资源并导致节点资源短缺问题。 当我们在线上使用 Kubernetes 集群的时候，如果没有对节点配置正确的资源预留，我们可以考虑一个场景，由于某个应用无限制的使用节点的 CPU 资源，导致节点上 CPU 使用持续100%运行，而且压榨到了 kubelet 组件的 CPU 使用，这样就会导致 kubelet 和 apiserver 的心跳出问题，节点就会出现 Not Ready 状况了。默认情况下节点 Not Ready 过后，5分钟后会驱逐应用到其他节点，当这个应用跑到其他节点上的时候同样100%的使用 CPU，是不是也会把这个节点搞挂掉，同样的情况继续下去，也就导致了整个集群的雪崩，集群内的节点一个一个的 Not Ready 了，后果是非常严重的，或多或少的人遇到过 Kubernetes 集群雪崩的情况，这个问题也是面试的时候镜像询问的问题。 要解决这个问题就需要为 Kubernetes 集群配置资源预留，kubelet 暴露了一个名为 Node Allocatable 的特性，有助于为系统守护进程预留计算资源，Kubernetes 也是推荐集群管理员按照每个节点上的工作负载来配置 Node Allocatable。 本文的操作环境为 Kubernetes V1.17.11 版本，Docker 和 Kubelet 采用的 cgroup 驱动为 systemd。 Node Allocatable Kubernetes 节点上的 Allocatable 被定义为 Pod 可用计算资源量，调度器不会超额申请 Allocatable,目前支持 CPU, memory 和 ephemeral-storage 这几个参数。 我们可以通过 kubectl describe node 命令查看节点可分配资源的数据： $ kubectl describe node ydzs-node4 ...... Capacity: cpu: 4 ephemeral-storage: 17921Mi hugepages-2Mi: 0 memory: 8008820Ki pods: 110 Allocatable: cpu: 4 ephemeral-storage: 16912377419 hugepages-2Mi: 0 memory: 7906420Ki pods: 110 ...... 可以看到其中有 Capacity 与 Allocatable 两项内容，其中的 Allocatable 就是节点可被分配的资源，我们这里没有配置资源预留，所以默认情况下 Capacity 与 Allocatable 的值基本上是一致的。下图显示了可分配资源和资源预留之间的关系： Node Allocatable Kubelet Node Allocatable 用来为 Kube 组件和 System 进程预留资源，从而保证当节点出现满负荷时也能保证 Kube 和 System 进程有足够的资源。 目前支持 cpu, memory, ephemeral-storage 三种资源预留。 Node Capacity 是节点的所有硬件资源，kube-reserved 是给 kube 组件预留的资源，system-reserved 是给系统进程预留的资源，eviction-threshold 是 kubelet 驱逐的阈值设定，allocatable 才是真正调度器调度 Pod 时的参考值（保证节点上所有 Pods 的 request 资源不超过Allocatable）。 节点可分配资源的计算方式为： Node Allocatable Resource = Node Capacity - Kube-reserved - system-reserved - eviction-threshold 配置资源预留 Kube 预留值 首先我们来配置 Kube 预留值，kube-reserved 是为了给诸如 kubelet、容器运行时、node problem detector 等 kubernetes 系统守护进程争取资源预留。要配置 Kube 预留，需要把 kubelet 的 --kube-reserved-cgroup 标志的值设置为 kube 守护进程的父控制组。 不过需要注意，如果 --kube-reserved-cgroup 不存在，Kubelet 不会创建它，启动 Kubelet 将会失败。 比如我们这里修改 node-ydzs4 节点的 Kube 资源预留，我们可以直接修改 /var/lib/kubelet/config.yaml 文件来动态配置 kubelet，添加如下所示的资源预留配置： apiVersion: kubelet.config.k8s.io/v1beta1 ...... enforceNodeAllocatable: - pods - kube-reserved # 开启 kube 资源预留 kubeReserved: cpu: 500m memory: 1Gi ephemeral-storage: 1Gi kubeReservedCgroup: /kubelet.slice # 指定 kube 资源预留的 cgroup 修改完成后，重启 kubelet，如果没有创建上面的 kubelet 的 cgroup，启动会失败： $ systemctl restart kubelet $ journalctl -u kubelet -f ...... Aug 11 15:04:13 ydzs-node4 kubelet[28843]: F0811 15:04:13.653476 28843 kubelet.go:1380] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kubelet.slice\": [\"kubelet\"] cgroup does not exist 上面的提示信息很明显，我们指定的 kubelet 这个 cgroup 不存在，但是由于子系统较多，具体是哪一个子系统不存在不好定位，我们可以将 kubelet 的日志级别调整为 v=4，就可以看到具体丢失的 cgroup 路径： $ vi /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=\"--v=4 --cgroup-driver=systemd --network-plugin=cni\" 然后再次重启 kubelet： $ systemctl daemon-reload $ systemctl restart kubelet 再次查看 kubelet 日志： $ journalctl -u kubelet -f ...... Sep 09 17:57:36 ydzs-node4 kubelet[20427]: I0909 17:57:36.382811 20427 cgroup_manager_linux.go:273] The Cgroup [kubelet] has some missing paths: [/sys/fs/cgroup/cpu,cpuacct/kubelet.slice /sys/fs/cgroup/memory/kubelet.slice /sys/fs/cgroup/systemd/kubelet.slice /sys/fs/cgroup/pids/kubelet.slice /sys/fs/cgroup/cpu,cpuacct/kubelet.slice /sys/fs/cgroup/cpuset/kubelet.slice] Sep 09 17:57:36 ydzs-node4 kubelet[20427]: I0909 17:57:36.383002 20427 factory.go:170] Factory \"systemd\" can handle container \"/system.slice/run-docker-netns-db100461211c.mount\", but ignoring. Sep 09 17:57:36 ydzs-node4 kubelet[20427]: I0909 17:57:36.383025 20427 manager.go:908] ignoring container \"/system.slice/run-docker-netns-db100461211c.mount\" Sep 09 17:57:36 ydzs-node4 kubelet[20427]: F0909 17:57:36.383046 20427 kubelet.go:1381] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kubelet.slice\": [\"kubelet\"] cgroup does not exist 注意：systemd 的 cgroup 驱动对应的 cgroup 名称是以 .slice 结尾的，比如如果你把 cgroup 名称配置成 kubelet.service，那么对应的创建的 cgroup 名称应该为 kubelet.service.slice。如果你配置的是 cgroupfs 的驱动，则用配置的值即可。无论哪种方式，通过查看错误日志都是排查问题最好的方式。 现在可以看到具体的 cgroup 不存在的路径信息了： The Cgroup [kubelet] has some missing paths: [/sys/fs/cgroup/cpu,cpuacct/kubelet.slice /sys/fs/cgroup/memory/kubelet.slice /sys/fs/cgroup/systemd/kubelet.slice /sys/fs/cgroup/pids/kubelet.slice /sys/fs/cgroup/cpu,cpuacct/kubelet.slice /sys/fs/cgroup/cpuset/kubelet.slice] 所以要解决这个问题也很简单，我们只需要创建上面的几个路径即可： $ mkdir -p /sys/fs/cgroup/cpu,cpuacct/kubelet.slice $ mkdir -p /sys/fs/cgroup/memory/kubelet.slice $ mkdir -p /sys/fs/cgroup/systemd/kubelet.slice $ mkdir -p /sys/fs/cgroup/pids/kubelet.slice $ mkdir -p /sys/fs/cgroup/cpu,cpuacct/kubelet.slice $ mkdir -p /sys/fs/cgroup/cpuset/kubelet.slice 创建完成后，再次重启： $ systemctl restart kubelet $ journalctl -u kubelet -f ...... Sep 09 17:59:41 ydzs-node4 kubelet[21462]: F0909 17:59:41.291957 21462 kubelet.go:1381] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kubelet.slice\": failed to set supported cgroup subsystems for cgroup [kubelet]: failed to set config for supported subsystems : failed to write 0 to hugetlb.2MB.limit_in_bytes: open /sys/fs/cgroup/hugetlb/kubelet.slice/hugetlb.2MB.limit_in_bytes: no such file or directory 可以看到还有一个 hugetlb 的 cgroup 路径不存在，所以继续创建这个路径： $ mkdir -p /sys/fs/cgroup/hugetlb/kubelet.slice $ systemctl restart kubelet 重启完成后就可以正常启动了，启动完成后我们可以通过查看 cgroup 里面的限制信息校验是否配置成功，比如我们查看内存的限制信息： $ cat /sys/fs/cgroup/memory/kubelet.slice/memory.limit_in_bytes 1073741824 # 1Gi 现在再次查看节点的信息： $ kubectl describe node ydzs-node4 ...... Addresses: InternalIP: 10.151.30.59 Hostname: ydzs-node4 Capacity: cpu: 4 ephemeral-storage: 17921Mi hugepages-2Mi: 0 memory: 8008820Ki pods: 110 Allocatable: cpu: 3500m ephemeral-storage: 15838635595 hugepages-2Mi: 0 memory: 6857844Ki pods: 110 ...... 可以看到可以分配的 Allocatable 值就变成了 Kube 预留过后的值了，证明我们的 Kube 预留成功了。 系统预留值 我们也可以用同样的方式为系统配置预留值，system-reserved 用于为诸如 sshd、udev 等系统守护进程争取资源预留，system-reserved 也应该为 kernel 预留 内存，因为目前 kernel 使用的内存并不记在 Kubernetes 的 pod 上。但是在执行 system-reserved 预留操作时请加倍小心，因为它可能导致节点上的关键系统服务 CPU 资源短缺或因为内存不足而被终止，所以如果不是自己非常清楚如何配置，可以不用配置系统预留值。 同样通过 kubelet 的参数 --system-reserved 配置系统预留值，但是也需要配置 --system-reserved-cgroup 参数为系统进程设置 cgroup。 请注意，如果 --system-reserved-cgroup 不存在，kubelet 不会创建它，kubelet 会启动失败。 驱逐阈值 上面我们还提到可分配的资源还和 kubelet 驱逐的阈值有关。节点级别的内存压力将导致系统内存不足，这将影响到整个节点及其上运行的所有 Pod，节点可以暂时离线直到内存已经回收为止，我们可以通过配置 kubelet 驱逐阈值来防止系统内存不足。驱逐操作只支持内存和 ephemeral-storage 两种不可压缩资源。当出现内存不足时，调度器不会调度新的 Best-Effort QoS Pods 到此节点，当出现磁盘压力时，调度器不会调度任何新 Pods 到此节点。 我们这里为 ydzs-node4 节点配置如下所示的硬驱逐阈值： # /var/lib/kubelet/config.yaml ...... evictionHard: # 配置硬驱逐阈值 memory.available: \"300Mi\" nodefs.available: \"10%\" enforceNodeAllocatable: - pods - kube-reserved kubeReserved: cpu: 500m memory: 1Gi ephemeral-storage: 1Gi kubeReservedCgroup: /kubelet.slice ...... 我们通过 --eviction-hard 预留一些内存后，当节点上的可用内存降至保留值以下时，kubelet 将尝试驱逐 Pod， $ kubectl describe node ydzs-node4 ...... Addresses: InternalIP: 10.151.30.59 Hostname: ydzs-node4 Capacity: cpu: 4 ephemeral-storage: 17921Mi hugepages-2Mi: 0 memory: 8008820Ki pods: 110 Allocatable: cpu: 3500m ephemeral-storage: 15838635595 hugepages-2Mi: 0 memory: 6653044Ki pods: 110 ...... 配置生效后再次查看节点可分配的资源可以看到内存减少了，临时存储没有变化是因为硬驱逐的默认值就是 10%。也是符合可分配资源的计算公式的： Node Allocatable Resource = Node Capacity - Kube-reserved - system-reserved - eviction-threshold 到这里我们就完成了 Kubernetes 资源预留的配置。 可分配资源的计算公式：可分配资源（Allocatable） = 总资源（Capacity）-预留资源（Reserved）-驱逐阈值（Eviction-Threshold） 公式说明： 总资源对应查询节点命令输出中的Capacity字段。 关于预留资源的相关信息，请参见资源预留策略。 关于驱逐阈值的相关信息，请参见节点压力驱逐 /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf #jinja2: trim_blocks:False # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpu,cpuacct/kubelet.slice ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/memory/kubelet.slice ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/systemd/kubelet.slice ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/pids/kubelet.slice ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpu,cpuacct/kubelet.slice ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuset/kubelet.slice ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/hugetlb/kubelet.slice Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --enforce-node-allocatable=pods,kube-reserved --kube-reserved=cpu=300m,memory=500Mi --eviction-hard=memory.available Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/task/配置热更新.html":{"url":"blog/kubernetes/task/配置热更新.html","title":"配置热更新","keywords":"","body":"Kubernetes配置热更新的两种方式 2021-08-12阅读 5050 1 背景 任何应用都需要一些特定的配置项，用来自定义应用的特性。这些配置通常可以分为两类： 一类是诸如运行环境和外部依赖等非敏感配置 一类是诸如密钥和 SSH 证书等敏感配置。 这些配置不应该直接放到容器镜像中，而是应该配配置与容器分离，通过数据卷、环境变量等方式在运行时动态挂载。 在我们使用kubernetes的过程中，通常都会将应用的配置文件放到ConfigMap或/和Secret中，但是也经常碰到配置文件更新后如何让其生效的问题。 用户定义Kubernetes的资源对象（例如Deployment、Daemonset 等），配置文件以configmap定义，通过Volumemounts进行挂载到Pod里，配置文件修改以后，服务可以自动reload加载更新配置。 2 解决方案 2.1 Reloader 限制条件：Kubernetes版本在1.9以及以上 集群安装reloader 通过添加注解annotation的方式实现 kubectl apply -f https://raw.githubusercontent.com/stakater/Reloader/master/deployments/kubernetes/reloader.yaml 复制 2.1.1 全局 configmap 触发更新 apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: log labels: k8s-app: filebeat annotations: reloader.stakater.com/auto: \"true\" 复制 2.1.2 按照指定的 configmap 变更自动触发资源对象的配置更新 单 ConfigMap 更新 apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: log labels: k8s-app: filebeat annotations: configmap.reloader.stakater.com/reload: \"filebeat-config\" 复制 多 configmap，以逗号对多个 configmap 进行隔离 apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: log labels: k8s-app: filebeat annotations: configmap.reloader.stakater.com/reload: \"filebeat-config,foo-config\" 复制 2.2 checksum 注解 checksum 注解是 Helm Charts 中最常用的滚动更新方法，即在 Deployment 的 annotations 中加上 Secret 或者 ConfigMap 的 sha256sum，这样已有的 Pod 就会随着 Secret 或者 ConfigMap 的变更而更新。 kind: Deployment spec: template: metadata: annotations: checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }} [...] 复制 添加这一节的效果就是，在/configmap.yaml中有任何内容改变，都会导致Deployment的sepc下的annotation被更新，进而驱动重建pod，达到我们想要的效果。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/telepresence.html":{"url":"blog/kubernetes/telepresence.html","title":"Telepresence","keywords":"","body":"使用telepresence在k8s中调试 作者： 吴江法 公司：杭州朗澈科技 前言 关于golang程序在k8s中的远程调试，可以参考使用dlv进行，但是这种方式缺陷也很明显，已部署的工作负载，需要重新制作镜像，重新部署，对业务也有一定侵入性，也不够灵活。 本文介绍一种更契合远程调试部署在k8s中的业务的方式，这种方式也是k8s在官方文档中推荐使用的：telepresence 1.准备 telepresence下载 kubectl下载 2.版本检测 下载完毕后，执行以下命令查看telepresence版本 $telepresence version Client: v2.5.3 (api v3) Root Daemon: not running User Daemon: not running 注意：如果版本小于v2.0.3,则需要升级telepresence 3.连接k8s集群 执行以下命令连接k8s集群： $telepresence connect Launching Telepresence Root Daemon Need root privileges to run: /usr/local/bin/telepresence daemon-foreground /Users/xxx/Library/Logs/telepresence '/Users/xxx/Library/Application Support/telepresence' Password: Launching Telepresence User Daemon Connected to context kubernetes-admin@kubernetes (https://8.16.0.211:6443) 注意：连接的集群为kubeconfig中指定的集群，需要能真实可访问。 同时，telepresence会自动打开浏览器，要求登录： 该步骤不能省略，否则后续的步骤执行时，都会要求先登录才能继续执行。 完成上述步骤后，查看k8s集群，能发现在该集群中会创建了名为traffic-manager的控制器： $kubectl get po -n ambassador NAME READY STATUS RESTARTS AGE traffic-manager-5bcfc9766f-lbrsz 1/1 Running 0 15m 4.拦截器 如上图所示，在k8s中部署了两个service，分别是Users和Orders。 这里以service Orders为例，正常情况下，一个访问Orders的请求，会被正常的收发。而telepresence的功能，就是拦截发送到Orders的请求，并将其转发到用户指定的地址（一般为本地)。 因此在开始配置前，需要了解telepresence中拦截器的概念： 全局拦截（Global intercept）：将访问k8s中某个service的流量全部拦截，并转发到本地。 如图所示，使用全局拦截，能将访问Orders服务的全部流量拦截，全部转发到本地。当然，我们需要将本地代码运行起来，用于接收转发过来的请求，同时，可以使用任意的debug的工具在本地进行调试。 个人拦截（Personal intercept）：有选择性地仅拦截某个service的部分流量，而不会干扰其余流量。 可以通过以下参数设置是否拦截请求的标识： --http-match=key=value 基于请求头识别请求是否需要拦截转发 --http-path-equal 基于请求路径 --http-path-prefix 基于请求路径前缀 --http-path-regex 基于请求路径是否匹配给定的正则表达式 5.实践 在开始前，需要把用来远程调试的服务部署到k8s集群： $kubectl get po,svc -lk8s-app=lsh-mcp-idp-cd-test NAME READY STATUS RESTARTS AGE pod/lsh-mcp-idp-cd-6c68876d48-v6c88 1/1 Running 0 30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/lsh-mcp-idp-cd NodePort 20.102.1.158 9090:30323/TCP,2345:30886/TCP 30s 并在本地debug运行lsh-mcp-idp-cd代码: 接着使用以下命令找到要拦截转发的service，即lsh-mcp-idp-cd： $telepresence list lsh-mcp-idp-cd: ready to intercept (traffic-agent not yet installed) 注意，要指定命名空间时，可以添加--namespace参数，如下所示： $telepresence list --namespace=kube-system 添加全局拦截器： telepresence intercept --port [:] --http-match=all --env-file [--namespace 可选] 对应到实践场景： $telepresence intercept lsh-mcp-idp-cd --port 9090:9090 --http-match=all --env-file ~/lsh-mcp-idp-cd-intercept.env Flag --http-match has been deprecated, use --http-header Using Deployment lsh-mcp-idp-cd intercepted Intercept name : lsh-mcp-idp-cd State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:9090 Service Port Identifier: 9090 Volume Mount Error : sshfs is not installed on your local machine Intercepting : matching all HTTP requests Preview URL : https://sad-thompson-7927.preview.edgestack.me Layer 5 Hostname : lsh-mcp-idp-cd.default.svc.cluster.local 执行完成后，会发现工作负载被注入了一个sidecar: $kubectl get po -lk8s-app=lsh-mcp-idp-cd-test -oyaml | grep -A 5 containerID - containerID: docker://6aea792f32af00b2e71f643ea41630de9bb6b0ebbe91251877fd79f67630efa1 image: registry.cn-beijing.aliyuncs.com/launcher-agent-only-dev/idp:v1 imageID: docker-pullable://registry.cn-beijing.aliyuncs.com/launcher-agent-only-dev/idp@sha256:c3be2545c30eb75fb652d383e9ec5545df9142e40d3b6f7f78633316b0db8103 lastState: {} name: idp-cd ready: true -- - containerID: docker://5acc04048950fdd38be3a8012c4cc0edbfd83079883717e34992f6f31036176f image: datawire/ambassador-telepresence-agent:1.11.10 imageID: docker-pullable://datawire/ambassador-telepresence-agent@sha256:9008fc1a6a91dd27baf3da9ebd0aee024f0d6d6a3f9c24611476474f6583e7f8 lastState: {} name: traffic-agent ready: true 增加了一个名为traffic-agent的容器，正是该容器，负责拦截发送到该pod的流量，并负责转发。 在k8s集群内执行以下命令,请求lsh-mcp-idp-cd服务： $curl 20.102.1.158:9090/version 再看本地代码，发现已经收到了请求: 以上就是全局拦截的实践部分，个人拦截gan兴趣的同学自己实践吧，另外关于个人拦截，似乎每个账号存在使用次数限制，超过次数后创建个人拦截器时会报错： telepresence: error: Failed to establish intercept: intercept in error state AGENT_ERROR: You’ve reached your limit of personal intercepts available for your subscription. See usage and available plans at https://app.getambassador.io/cloud/subscriptions See logs for details (1 error found): \"/Users/xxx/Library/Logs/telepresence/daemon.log\" See logs for details (13609 errors found): \"/Users/xxx/Library/Logs/telepresence/connector.log\" If you think you have encountered a bug, please run `telepresence gather-logs` and attach the telepresence_logs.zip to your github issue or create a new one: https://github.com/telepresenceio/telepresence/issues/new?template=Bug_report.md . 6.卸载 删除拦截器：执行后，会删除注入工作负载的sidecar $telepresence leave lsh-mcp-idp-cd 删除telepresence agents and manager，执行后清除所有sidecar，以及traffic-manager控制器，并关闭本地telepresence的后台进程 $telepresence uninstall --everything Telepresence Network quitting...done Telepresence Traffic Manager quitting...done Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/webhook.html":{"url":"blog/kubernetes/webhook.html","title":"Webhook","keywords":"","body":"K8s: 准入控制 什么是准入控制插件 准入控制器是一段代码，它会在请求通过认证和授权之后、对象被持久化之前拦截到达 API 服务器的请求 有两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 如何启用一个准入控制器 Kubernetes API 服务器的 enable-admission-plugins 标志，它指定了一个用于在集群修改对象之前调用的（以逗号分隔的）准入控制插件顺序列表。 例如，下面的命令就启用了 NamespaceLifecycle 和 LimitRanger 准入控制插件： kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ... 怎么关闭准入控制器 Kubernetes API 服务器的 disable-admission-plugins 标志，会将传入的（以逗号分隔的）准入控制插件列表禁用，即使是默认启用的插件也会被禁用。 kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ... 哪些插件是默认启用的 kube-apiserver -h | grep enable-admission-plugins 每个准入控制器的作用是什么 AlwaysPullImages 该准入控制器会修改每一个新创建的 Pod 的镜像拉取策略为 Always 。 这在多租户集群中是有用的，这样用户就可以放心，他们的私有镜像只能被那些有凭证的人使用。 如果没有这个准入控制器，一旦镜像被拉取到节点上，任何用户的 pod 都可以通过已了解到的镜像的名称（假设 pod 被调度到正确的节点上）来使用它，而不需要对镜像进行任何授权检查。 当启用这个准入控制器时，总是在启动容器之前拉取镜像，这意味着需要有效的凭证。 MutatingAdmissionWebhook FEATURE STATE: Kubernetes v1.13 该准入控制器调用任何与请求匹配的变更 webhook。匹配的 webhook 将被串行调用。每一个 webhook 都可以根据需要修改对象。 MutatingAdmissionWebhook ，顾名思义，仅在变更阶段运行。 如果由此准入控制器调用的 Webhook 有副作用（如降低配额）， 则它 必须 具有协调系统，因为不能保证后续的 Webhook 和验证准入控制器都会允许完成请求。 如果你禁用了 MutatingAdmissionWebhook，那么还必须使用 --runtime-config 标志禁止 admissionregistration.k8s.io/v1beta1 组/版本中的 MutatingWebhookConfiguration 对象（版本 >=1.9 时，这两个对象都是默认启用的） ValidatingAdmissionWebhook FEATURE STATE: Kubernetes v1.13 该准入控制器调用与请求匹配的所有验证 webhook。匹配的 webhook 将被并行调用。如果其中任何一个拒绝请求，则整个请求将失败。 该准入控制器仅在验证阶段运行；与 MutatingAdmissionWebhook 准入控制器所调用的 webhook 相反，它调用的 webhook 应该不会使对象出现变更。 如果以此方式调用的 webhook 有其它作用（如，配额递减），则它必须具有协调系统，因为不能保证后续的 webhook 或其他有效的准入控制器都允许请求完成。 如果您禁用了 ValidatingAdmissionWebhook，还必须在 admissionregistration.k8s.io/v1beta1 组/版本中使用 --runtime-config 标志来禁用 ValidatingWebhookConfiguration 对象（默认情况下在 1.9 版和更高版本中均处于启用状态）。 ServiceAccount FEATURE STATE: Kubernetes v1.16 alpha 该准入控制器实现了 serviceAccounts 的自动化。 如果您打算使用 Kubernetes 的 ServiceAccount 对象，我们强烈建议您使用这个准入控制器。 容器运行时类 容器运行时类定义描述了与运行 Pod 相关的开销。此准入控制器将相应地设置 pod.Spec.Overhead 字段。 详情请参见 Pod 开销。 有推荐的准入控制器吗 有，对于 Kubernetes 1.10 以上的版本，推荐使用的准入控制器默认情况下都处于启用状态（查看这里）。 因此您无需显式指定它们。您可以使用 --enable-admission-plugins 标志（ 顺序不重要 ）来启用默认设置以外的其他准入控制器。 注意 : --admission-control 在 1.10 中已废弃，已由 --enable-admission-plugins 取代。 对于 Kubernetes 1.9 及更早版本，我们建议使用 --admission-control 标志（顺序很重要）运行下面的一组准入控制器 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota 什么是 admission webhook Admission webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的 admission webhook，即 validating admission webhook 和 mutating admission webhook。 Mutating admission webhook 会先被调用。它们可以更改发送到 API 服务器的对象以执行自定义的设置默认值操作。 在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后，validating admission webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。 注意： 如果 admission webhook 需要保证它们所看到的是对象的最终状态以实施某种策略。则应使用 validating admission webhook，因为对象被 mutating webhook 看到之后仍然可能被修改。 required 确保 Kubernetes 集群版本至少为 v1.16（以便使用 admissionregistration.k8s.io/v1 API） 或者 v1.9 （以便使用 admissionregistration.k8s.io/v1beta1 API）。 确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 这里 是一组推荐的 admission 控制器，通常可以启用。 确保启用了 admissionregistration.k8s.io/v1beta1 API。 - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key - --requestheader-allowed-names=front-proxy-client - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt - --requestheader-extra-headers-prefix=X-Remote-Extra- - --requestheader-group-headers=X-Remote-Group - --requestheader-username-headers=X-Remote-User - --secure-port=6443 - --service-account-key-file=/etc/kubernetes/pki/sa.pub - --service-cluster-ip-range=10.96.0.0/12 - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --enable-admission-plugins=NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook 上面的enable-admission-plugins参数中带上了MutatingAdmissionWebhook和ValidatingAdmissionWebhook两个准入控制插件，如果没有的，需要添加上这两个参数，然后重启 apiserver。 然后通过运行下面的命令检查集群中是否启用了准入注册 API： [root@wyh ~]# kubectl api-versions |grep admission admissionregistration.k8s.io/v1beta1 write admission webhook server 请参阅 Kubernetes e2e 测试中的 admission webhook 服务器 的实现。webhook 处理由 apiserver 发送的 AdmissionReview 请求，并且将其决定作为 AdmissionReview 对象以相同版本发送回去。 有关发送到 webhook 的数据的详细信息，请参阅 webhook 请求。 要获取来自 webhook 的预期数据，请参阅 webhook 响应。 示例 admission webhook 服务器置 ClientAuth 字段为空，默认为 NoClientCert 。这意味着 webhook 服务器不会验证客户端的身份，认为其是 apiservers。 如果您需要双向 TLS 或其他方式来验证客户端，请参阅如何对 apiservers 进行身份认证。 deploy admission webhook server webhook 配置 要注册 admssion webhook，请创建 MutatingWebhookConfiguration 或 ValidatingWebhookConfiguration API 对象。 每种配置可以包含一个或多个 webhook。如果在单个配置中指定了多个 webhook，则应为每个 webhook 赋予一个唯一的名称。 这在 admissionregistration.k8s.io/v1 中是必需的，但是在使用 admissionregistration.k8s.io/v1beta1 时强烈建议使用，以使生成的审核日志和指标更易于与活动配置相匹配。 每个 webhook 定义以下内容。 匹配请求-规则 每个 webhook 必须指定用于确定是否应将对 apiserver 的请求发送到 webhook 的规则列表。 每个规则都指定一个或多个 operations、apiGroups、apiVersions 和 resources 以及资源的 scope： operations 列出一个或多个要匹配的操作。可以是 CREATE、UPDATE、DELETE、CONNECT 或 * 以匹配所有内容。 apiGroups 列出了一个或多个要匹配的 API 组。\"\" 是核心 API 组。\"*\" 匹配所有 API 组。 apiVersions 列出了一个或多个要匹配的 API 版本。\"*\" 匹配所有 API 版本。 resources 列出了一个或多个要匹配的资源。 \"*\" 匹配所有资源，但不包括子资源。 \"/\" 匹配所有资源，包括子资源。 \"pods/*\" 匹配 pod 的所有子资源。 \"*/status\" 匹配所有 status 子资源。 scope 指定要匹配的范围。有效值为 \"Cluster\"、\"Namespaced\" 和 \"\"。子资源匹配其父资源的范围。在 Kubernetes v1.14+ 版本中才被支持。默认值为 \"\"，对应 1.14 版本之前的行为。 \"Cluster\" 表示只有集群作用域的资源才能匹配此规则（API 对象 Namespace 是集群作用域的）。 \"Namespaced\" 意味着仅具有命名空间的资源才符合此规则。 \"*\" 表示没有范围限制。 如果传入请求与任何 Webhook 规则的指定操作、组、版本、资源和范围匹配，则该请求将发送到 Webhook。 以下是可用于指定应拦截哪些资源的规则的其他示例。 匹配针对 apps/v1 和 apps/v1beta1 组中 deployments 和 replicasets 资源的 CREATE 或 UPDATE 请求： apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\"CREATE\", \"UPDATE\"] apiGroups: [\"apps\"] apiVersions: [\"v1\", \"v1beta1\"] resources: [\"deployments\", \"replicasets\"] scope: \"Namespaced\" ... # v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\"CREATE\", \"UPDATE\"] apiGroups: [\"apps\"] apiVersions: [\"v1\", \"v1beta1\"] resources: [\"deployments\", \"replicasets\"] scope: \"Namespaced\" ... 参考文献: https://kubernetes.io/zh/docs/reference/access-authn-authz/webhook/ https://www.qikqiak.com/post/k8s-admission-webhook/ https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#how-do-i-turn-on-an-admission-controller https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/一致性认证.html":{"url":"blog/kubernetes/一致性认证.html","title":"一致性认证","keywords":"","body":"kubernetes一致性认证 CNCF宣布启动Kubernetes一致性认证计划，以推动Kubernetes产品的一致性和可移植性，践行Kubernetes被创立时的初心。 点击查看已认证厂家 下面是lastack 基于1.18.0版本进行的认证实践过程，希望对大家有所帮助 e2e test install test tool sonobuoy download sonobuoy release page # download binar wget https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.18.0/sonobuoy_0.18.0_linux_amd64.tar.gz tar -zxvf sonobuoy_0.18.0_linux_amd64.tar.gz mv sonobuoy /usr/local/bin/ run test sonobuoy run --mode=certified-conformance query status sonobuoy status --json | jq 由于测试事件非常长，每次1个多小时，建议2个节点，3个节点会更长。 测试过程中用上面的命令查看测试进度，发现失败就停止，查找原因。 output test result outfile=$(sonobuoy retrieve) mkdir ./results; tar xzf $outfile -C ./results copy test result to your code repo scp -r root@xxxxx:/root/results/plugins/e2e/results/global/ ./ troubshouting 1. flannel 网络插件不支持 错误信息： {\"msg\":\"FAILED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]\",\"total\":275,\"completed\":19,\"skipped\":280,\"failed\":1,\"failures\":[\"[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]\"]} 解决方案：use calico replace flannel 2. IPVS 在1.18.0环境，pod ip变更后， svc 访问失败 错误信息： { \"plugins\": [ \"failures\": [ \"[sig-network] Services should be able to create a functioning NodePort service [Conformance]\", \"[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]\", \"[sig-network] DNS should provide DNS for the cluster [Conformance]\", \"[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]\", \"[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]\", \"[sig-cli] Kubectl client Guestbook application should create and stop a working application [Conformance]\", \"[sig-network] DNS should provide DNS for ExternalName services [Conformance]\", \"[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]\", \"[sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]\", \"[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]\", \"[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]\", \"[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]\" ] } 解决方案： 使用iptables替换 ipvs Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/专利名词.html":{"url":"blog/kubernetes/专利名词.html","title":"专利名词","keywords":"","body":"专利名次解释 Docker： 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。 Containerd：是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性。Containerd 可以在宿主机中管理完整的容器生命周期：容器镜像的传输和存储、容器的执行和管理、存储和网络等，可以被 Kubernets CRI 等项目使用。 Dragonfly：中文名“蜻蜓”，是一个基于P2P的智能文件分发系统。解决了应用部署，大规模缓存文件分发，数据文件分发，镜像分发等大规模文件分发场景中低效率，低成功率，浪费网络带宽等问题。 Kubernetes：一种容器编排框架。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/云原生开源项目大全.html":{"url":"blog/kubernetes/云原生开源项目大全.html","title":"云原生开源项目大全","keywords":"","body":"Awesome Cloud Native A curated list of open-source cloud native tools, software, and tutorials. 云原生开源工具、软件、教程大全。 Cloud Native is a behavior and design philosophy. At its essence, any behavior or approach that improves resource utilization and application delivery efficiency in the cloud is called Cloud Native. Join the Cloud Native Community (China) 加入云原生社区 Contents AI API Gateway Application Delivery Big Data Container Runtime Database Edge Computing Kubernetes Operators Logging Message Broker Miscellaneous Monitoring Network Observability Orchestration and Scheduler Proxy RPC Security and Audit Service Mesh Service Registry and Discovery Serverless Stability Storage Tools Tracing Tutorials UI Community AI allennlp - An open-source NLP research library, built on PyTorch. caffe2 - Caffe2 is a lightweight, modular, and scalable deep learning framework. elasticdl - Kubernetes-native Deep Learning Framework. h2o-3 - Open Source Fast Scalable Machine Learning API For Smarter Applications (Deep Learning, Gradient Boosting, Random Forest, Generalized Linear Modeling (Logistic Regression, Elastic Net), K-Means, PCA, Stacked Ensembles.) keras - Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. kubeflow - Machine Learning Toolkit for Kubernetes. leaf - Open Machine Intelligence Framework for Hackers. (GPU/CPU). paddlepaddle - PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice （『飞桨』核心框架，深度学习&机器学习高性能单机、分布式训练和跨平台部署）. predictionio - PredictionIO, a machine learning server for developers and ML engineers. pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration. seldon-core - A framework to deploy, manage and scale your production machine learning to thousands of models. sqlflow - Brings SQL and AI together. tensorflow - Computation using data flow graphs for scalable machine learning. API Gateway apisix - The Cloud-Native API Gateway. ambassador - Ambassador: a self-service API gateway for microservices built on Lyft Envoy. express-gateway - A microservices API Gateway built on top of ExpressJS. hango-gateway - Hango API Gateway, build on Envoy & Istio. kong - The Microservice API Gateway. lura - Ultra performant API Gateway with middlewares. A project hosted at The Linux Foundation. orange - OpenResty/Nginx Gateway for API Monitoring and Management. ocelot - .NET core API Gateway. tyk - Tyk Open Source API Gateway written in Go, supporting REST, GraphQL, TCP and gRPC protocols. Application Delivery argo-cd - Declarative continuous deployment for Kubernetes. argo - Get stuff done with container-native workflows for Kubernetes. arkade - Kubernetes apps for developers. armada - A python orchestrator for a installing, upgrading, and managing a collection of helm charts, dependencies, and values overrides. autoapply - Automatically apply changes from a git repository to Kubernetes. ballerina-lang - Ballerina is a new programming language for integration built on a sequence diagram metaphor. beetle - Kubernetes multi-cluster deployment automation service. binderhub - Run your code in the cloud, with technology so advanced, it feels like magic! build - A Kubernetes-native Build resource. capact - A framework to manage applications and infrastructure in a unified way. cdk8s - Define Kubernetes native apps and abstractions using object-oriented programming. cds - Enterprise-Grade Continuous Delivery & DevOps Automation Open Source Platform. charitify - Generate Helm Charts from Kubernetes objects. circleci - Continuous Integration and Deployment. cloudbase-framework - 🚀 A front-end and back-end integrated deployment tool 🔥 One-click deploy to serverless architecture. 云原生一体化部署工具 CloudBase Framework. cnab-spec - Cloud Native Application Bundle Specification. commandeer - Cloud management desktop app for macOS, Windows, and Linux. containerops - DevOps Orchestration Platform. couler - Unified Interface for Constructing and Managing Workflows crane - Yet another control plane based on docker built-in swarmkit. crossplane - An Open Source Multicloud Control Plane. cross-cloud - Cross Cloud Continuous Integration. cue - Validate and define text-based and dynamic configuration. cyclone - Powerful workflow engine and end-to-end pipeline solutions implemented with native Kubernetes resources. devtron - Software Delivery Workflow For Kubernetes draft - A tool for developers to create cloud-native applications on Kubernetes. drone - Drone is a Continuous Delivery platform built on Docker, written in Go. fabric8 - fabric8 is an open source microservices platform based on Docker, Kubernetes and Jenkins. flagger - Progressive delivery Kubernetes operator (Canary, A/B Testing and Blue/Green deployments) . flux - A tool for turning container images into running Kubernetes services. gitkube - Gitkube: Build and deploy docker images to Kubernetes using git push. gockerize - Package golang service into minimal docker containers. habitus - A build flow tool for Docker. helm - The Kubernetes Package Manager. helmfile - Deploy Kubernetes Helm Charts. helmsman - Helm Charts as Code. hiboot - Hiboot is a high performance web and cli application framework with dependency injection support. hygieia - CapitalOne DevOps Dashboard. hyscale - All things HyScale. jenkins - Jenkins automation server. jib - Build container images for your Java applications. jsonnet - Jsonnet - The data templating language. jx - A command line tool for installing and working with Jenkins X. kaniko - Build Container Images In Kubernetes. kd - Minimalistic kubernetes resources deployment tool with templating. kdo - Deployless Development on Kubernetes. kedge - Kedge - Concise Application Definition for Kubernetes. kenyata - Automated Canary Service. keptn - Keptn is a control-plane for continuous delivery and operations enable cloud-native applications to run autonomously. kismatic - Kismatic Enterprise Toolkit: Fully-Automated, Production-Grade Kubernetes Operations. kompose - Go from Docker Compose to Kubernetes. kpt - Kpt is a toolkit to help you manage, manipulate, customize, and apply Kubernetes Resource configuration data files. kubeapps - A web-based UI for deploying and managing applications in Kubernetes clusters. kubegen - Kubegen – simple way to describe Kubernetes resources. kubernetes-deploy - A command-line tool that helps you ship changes to a Kubernetes namespace and understand the result. kubevela - Make shipping applications more enjoyable. kustomize - Customization of kubernetes YAML configurations. lastbackend - Container orchestration with CI&CD, cli and amazing UI. mkit - MKIT is a Managed Kubernetes Inspection Tool that validates several common security-related configuration settings of managed Kubernetes cluster objects and the workloads/resources running inside the cluster. opencompose - OpenCompose - A higher level abstraction for Kubernetes Resource. pipeline - REST API to provision or reuse managed Kubernetes clusters in the cloud and deploy cloud native apps. pipeline - A K8s-native Pipeline resource. pulumi - A multi-language, multi-cloud development platform -- your code, your cloud, your team. rudr - A Kubernetes implementation of the Open Application Model specification. sealer - Seal your applications all dependencies and kubernetes into CloudImage! Build Deliver and Run user-defined clusters in one command. skaffold - Easy and Repeatable Kubernetes Development. smith - Smith: A microcontainer builder. source-to-image - A tool for building/building artifacts from source and injecting into docker images. spec - The Open Application Model specification. spinnaker - Spinnaker is an open source, multi-cloud continuous delivery platform for releasing software changes with high velocity and confidence. terraform - Terraform is a tool for building, changing, and combining infrastructure safely and efficiently. tilt - A multi-service dev environment for teams on Kubernetes. wercker - The Wercker CLI can be used to execute pipelines locally for both local development and easy introspection. woodpecker - Fork of drone.io v0.8 since drone is not fully opensource anymore. Big Data fast-data-dev - Kafka Docker for development. Kafka, Zookeeper, Schema Registry, Kafka-Connect, Landoop Tools, 20+ connectors. pachyderm - Reproducible Data Science at Scale! spark - Apache Spark enhanced with native Kubernetes scheduler back-end. spark-on-kubernetes-helm - Spark on Kubernetes infrastructure Helm charts repo. wallaroo - Ultrafast and elastic data processing. v6d - vineyard (v6d), an in-memory immutable data manager. Container Runtime clear-containers - OCI (Open Containers Initiative) compatible runtime using Virtual Machines. containerd - An open and reliable container runtime. cri-containerd - Containerd-based implementation of Kubernetes Container Runtime Interface. cri-o - Open Container Initiative-based implementation of Kubernetes Container Runtime Interface. frakti - The hypervisor-based container runtime for Kubernetes. gvisor - Sandboxed Container Runtime. hyperd - HyperContainer Daemon. img - Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. lima - Linux virtual machines, on macOS (aka \"Linux-on-Mac\", \"macOS subsystem for Linux\", \"containerd for Mac\", unofficially). katacontainers - Kata Containers is a new open source project building extremely lightweight virtual machines that seamlessly plug into the containers ecosystem. moby - Moby Project - a collaborative project for the container ecosystem to assemble container-based systems. podman - A tool for managing OCI containers and pods. pouch - Pouch is an open-source project created to promote the container technology movement. railcar - RailCar: Rust implementation of the Open Containers Initiative oci-runtime. rkt - Rkt is a pod-native container engine for Linux. It is composable, secure, and built on standards. wasmCloud - wasmCloud is a universal host runtime for actors built with WebAssembly and capability providers. Database arangodb - ArangoDB is a native multi-model database with flexible data models for documents, graphs, and key-values. Build high performance applications using a convenient SQL-like query language or JavaScript extensions. beringei - Beringei is a high performance, in-memory storage engine for time series data. cockroachdb - CockroachDB - the open source, cloud-native SQL database. couchdb - Apache CouchDB is one of a new breed of database management systems. etcd - Distributed reliable key-value store for the most critical data of a distributed system. influxdb - Scalable datastore for metrics, events, and real-time analytics. kvrocks - Kvrocks is a distributed key value NoSQL database based on RocksDB and compatible with Redis protocol. leveldb - LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. m3 - M3 monorepo - Distributed TSDB, Aggregator and Query Engine, Prometheus Sidecar, Graphite Compatible, Metrics Platform. mehdb - Educational Kubernetes-native NoSQL datastore using StatefulSet and persistent volumes. mongodb - MongoDB is an open source database that uses a document-oriented data model. montydb - Monty, Mongo tinified. MongoDB implemented in Python. nebula - A distributed, fast open-source graph database featuring horizontal scalability and high availability. nocodb - The Open Source Airtable alternative. oceanbase - A distributed, banking suitable, open-source related database featuring high scalability and high compatibility. opentsdb - A scalable, distributed Time Series Database. polardb-for-postgresql - PolarDB for PostgreSQL (PolarDB for short) is an open source database system based on PostgreSQL. redis - Redis is an in-memory database that persists on disk. The data model is key-value, but many different kind of values are supported: Strings, Lists, Sets, Sorted Sets, Hashes, HyperLogLogs, Bitmaps. rethinkdb - The open-source database for the realtime web. sharding-sphere - Distributed database middleware. stolon - PostgreSQL cloud native High Availability and more. tidb - TiDB is a distributed NewSQL database compatible with MySQL protocol. tikv - Distributed transactional key-value database, originally created to complement TiDB. tinydb - TinyDB is a lightweight document oriented database optimized for your happiness. Edge Computing akri - A Kubernetes Resource Interface for the Edge. baetyl - Extend cloud computing, data and service seamlessly to edge devices. eliot - Open source system for managing containerized applications in IoT device. iotedge - The IoT Edge OSS project. k0s - k0s - Zero Friction Kubernetes. k3s - Lightweight Kubernetes. 5 less than k8s. kubeedge - Kubernetes Native Edge Computing Framework (project under CNCF). octopus - Lightweight device management system for Kubernetes/k3s. openyurt - Extending your native Kubernetes to the edge. superedge - An edge-native container management system for edge computing. Kubernetes Operators banzaicloud/bank-vaults - A Vault swiss-army knife: a K8s operator, Go client with automatic token renewal, automatic configuration, multiple unseal options and more. A CLI tool to init, unseal and configure Vault (auth methods, secret engines). Direct secret injection into Pods. eunomia - A GitOps Operator for Kubernetes. fabedge - Secure Edge Networking Based On Kubernetes And KubeEdge. flagger - Istio progressive delivery Kubernetes operator. keel - Kubernetes Operator to automate Helm, DaemonSet, StatefulSet & Deployment updates. kopf - A Python framework to write Kubernetes operators in just few lines of code. kudo - Kubernetes Universal Declarative Operator (KUDO). kubevirt - Kubernetes Virtualization Operator with API and runtime in order to define and manage virtual machines. operator-lifecycle-manager - A management framework for extending Kubernetes with Operators. operator-sdk - SDK for building Kubernetes applications. Provides high level APIs, useful abstractions, and project scaffolding. prometheus-operator - Prometheus Operator creates/configures/manages Prometheus clusters atop Kubernetes. spark-on-k8s-operator - Kubernetes operator for managing the lifecycle of Apache Spark applications on Kubernetes. strimzi-kafka-operator - Apache Kafka running on Kubernetes. tidb-operator - TiDB operator creates and manages TiDB clusters running in Kubernetes. Logging beats - Beats - Lightweight shippers for Elasticsearch & Logstash. collectbeat - Beats with discovery capabilities for environments like Kubernetes. dagger - Dagger 是一个基于 Loki 的日志查询和管理系统. egg - The simple error aggregator. elasticsearch - Open Source, Distributed, RESTful Search Engine. fluent-bit - Fast and Lightweight Log/Data Forwarder for Linux, BSD and macOS. fluentd-pilot - Collect logs in docker containers. fluentd - Fluentd: Unified Logging Layer (project under CNCF). flume - Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. heapster - Compute Resource Usage Analysis and Monitoring of Container Clusters. log-pilot - Collect logs in docker containers. loki - Like Prometheus, but for logs. telegraf - The plugin-driven server agent for collecting & reporting metrics. Message Broker emqx - EMQ X Broker - Scalable Distributed MQTT Message Broker for IoT in 5G Era. eventmesh - EventMesh is a dynamic cloud-native eventing infrastructure used to decouple the application and backend middleware layer, which supports a wide range of use cases that encompass complex multi-cloud, widely distributed topologies using diverse technology stacks. flume - Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. gnatsd - High-Performance server for NATS, the cloud native messaging system. jocko - Kafka implemented in Golang with built-in coordination (No ZK dep, single binary install, Cloud Native). pulsar - A distributed messaging and streaming platform. kafka - A distributed streaming platform. kubemq - KubeMQ is Enterprise-grade message broker native for Docker and Kubernetes. nsq - A realtime distributed messaging platform. rabbitmq - RabbitMQ is the most widely deployed open source message broker. rocketmq - Apache RocketMQ is a distributed messaging and streaming platform with low latency, high performance and reliability, trillion-level capacity and flexible scalability. Miscellaneous agones - Dedicated Game Server Hosting and Scaling for Multiplayer Games on Kubernetes. Monitoring cadvisor - Analyzes resource usage and performance characteristics of running containers. cortex - A multitenant, horizontally scalable Prometheus as a Service. elasticsearch-hq - Monitoring and Management Web Application for ElasticSearch instances and clusters. grafana - The tool for beautiful monitoring and metric analytics & dashboards for Graphite, InfluxDB & Prometheus & More. hawkular-metrics - Time Series Metrics Engine based on Cassandra. kibana - Kibana analytics and search dashboard for Elasticsearch. kubernetes-zabbix - Kubernetes Zabbix/Grafana cluster (bare metal, Google Computer Engine - GCE, Google Container Engine - GKE). kubenurse - Kubernetes network monitoring. nexclipper - An open source software for monitoring Kubernetes and containers. open-falcon - Enterprise Internet monitoring system from Xiaomi. owl - Distributed monitoring system from TalkingData. prometheus - The Prometheus monitoring system and time series database. scope - Monitoring, visualisation & management for Docker & Kubernetes. sofa-lookout - Lookout can help you to measure and monitor the status of the target system with its multi-dimensional metrics. statsd - Daemon for easy but powerful stats aggregation. Network calico - A Pure Layer 3 Approach to Virtual Networking for Highly Scalable Data Centers. cilium - API-aware Networking and Security using eBPF and XDP. cni - Container Network Interface - networking for Linux containers. cni-genie - CNI-Genie for choosing pod network of your choice during deployment time. Supported pod networks - Calico, Flannel, Romana, Weave. contiv - Container networking for various use cases. flannel - Flannel is a network fabric for containers, designed for Kubernetes. hubble - Hubble - Network, Service & Security Observability for Kubernetes. istio-cni - Istio CNI to setup kubernetes pod namespaces to redirect traffic to sidecar proxy. knitter - Kubernetes network solution. kube-router - Kube-router, a turnkey solution for Kubernetes networking. kube-ovn - Kube-OVN, a Kubernetes network fabric for enterprises that is rich in functions and easy in operations. matchbox - Network boot and provision Container Linux clusters (e.g. etcd3, Kubernetes, more). submariner - Connect all your Kubernetes clusters, no matter where they are in the world. weave - Simple, resilient multi-host Docker networking and more. Observability envoy-ui - Dead simple server-side UI for Envoy proxy (like HAproxy stats). goldpinger - Debugging tool for Kubernetes which tests and displays connectivity between nodes in the cluster. istio-ui - Istio config management backend. kiali - Kiali project to help istio service mesh observability. konstellate - Free and Open Source GUI to Visualize Kubernetes Applications. kube-ops-view - Kubernetes Operational View - read-only system dashboard for multiple K8s clusters. naftis - An excellent dashboard for Istio built with love. octant - Highly extensible platform for developers to better understand the complexity of Kubernetes clusters. vistio - Visualize your Istio mesh using Netflix's Vizceral. vizceral - WebGL visualization for displaying animated traffic graphs. Orchestration and Scheduler alameda - Intelligent Resources Orchestrator for Kubernetes by using machine learning. blox - Open source tools for building custom schedulers on Amazon ECS. clusterset - Managing your Kubernetes clusters (including public, private, edge, etc) as easily as visiting the Internet. compose - Define and run multi-container applications with Docker. conductor - Conductor is a microservices orchestration engine. dc/os - Datacenter Operating System. deis - Deis v1, the CoreOS and Docker PaaS: Your PaaS. Your Rules. descheduler - Descheduler for Kubernetes. eks-distro - Amazon EKS Distro (EKS-D) is a Kubernetes distribution based on and used by Amazon Elastic Kubernetes Service (EKS) to create reliable and secure Kubernetes clusters. fleet - Fleet ties together systemd and etcd into a distributed init system. karmada - Open, Multi-Cloud, Multi-Cluster Kubernetes Orchestration kruise - Automate application workloads management on Kubernetes. kubernetes - Production-Grade Container Scheduling and Management. marathon - Deploy and manage containers (including Docker) on top of Apache Mesos at scale. mesos - Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively. pixie - Instant Kubernetes-Native Application Observability serf - Service orchestration and management tool by hashicorp. service-fabric - Service Fabric is a distributed systems platform for packaging, deploying, and managing stateless and stateful distributed applications and containers at large scale. supergiant - Automatically scale hardware and easily run stateful applications using Kubernetes. swan - A Distributed, Highly Available Mesos Scheduler, Inspired by the design of Google Borg. swarm - Swarm: a Docker-native clustering system. vamp - Vamp - canary releasing and autoscaling for microservice systems. volcano - A Kubernetes Native Batch System (Project under CNCF). Proxy apisix-ingress-controller - Ingress controller for K8s. caddy - Fast, cross-platform HTTP/2 web server with automatic HTTPS. contour - Contour is a Kubernetes ingress controller for Lyft's Envoy proxy. envoy-docker-shim - Run Envoy in place of docker-proxy. envoy - C++ front/service proxy. func-e - func-e (pronounced funky) makes running Envoy easy. gimbal - Heptio Gimbal is an ingress load balancing platform capable of routing traffic to multiple Kubernetes and OpenStack clusters. Built by Heptio in partnership with Actapio. gobetween - Modern & minimalistic load balancer for the Сloud era. haproxy - HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. inlets-operator - Add public LoadBalancers to your local Kubernetes clusters. kedge - kEdge - Kubernetes Edge Proxy for gRPC and HTTP Microservices. katran - A high performance layer 4 load balancer. kong-ingress - A Kubernetes Ingress for Kong. kong/kubernetes-ingress-controller - Deploy Kong in a native Kubernetes Ingress Controller. metallb - A network load-balancer implementation for Kubernetes using standard routing protocols. mosn - MOSN is a cloud native proxy for edge or service mesh. nginx-kubernetes-ingress - NGINX and NGINX Plus Ingress Controllers for Kubernetes. nginx - Nginx is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server, originally written by Igor Sysoev. reverse-proxy - A toolkit for developing high-performance HTTP reverse proxy applications. ribbon - Ribbon is a Inter Process Communication (remote procedure calls) library with built in software load balancers. The primary usage model involves REST calls with various serialization scheme support. skipper - An HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress. traefik - Træfik, a modern reverse proxy. voyager - Secure Ingress Controller for Kubernetes. RPC brpc - Most common RPC framework used throughout Baidu, with 600,000+ instances and 500+ kinds of services, called \"baidu-rpc\" inside Baidu. drpc - drpc is a lightweight, drop-in replacement for gRPC. finagle - A fault tolerant, protocol-agnostic RPC system. grpc - A high performance, open source, general-purpose RPC framework. kitex - A high-performance and strong-extensibility Golang RPC framework that helps developers build microservices. proxygen - A collection of C++ HTTP libraries including an easy to use HTTP server. rsocket - Streaming message protocol with Reactive Extension/Stream semantics. sofa-bolt - SOFABolt is a lightweight, easy to use and high performance remoting framework based on Netty. sofa-rpc - SOFARPC is a high-performance, high-extensibility, production-level Java RPC framework. tars - Tars is a high-performance RPC framework based on name service and Tars protocol, also integrated administration platform, and implemented hosting-service via flexible schedule. thrift - Apache thrift. Security and Audit apparmor - AppArmor is an effective and easy-to-use Linux application security system. authenticator - A tool for using AWS IAM credentials to authenticate to a Kubernetes cluster. awacs - Next-gen behavior analysis server (think Mixpanel, Google Analytics) with built-in encryption cert-manager - Automatically provision and manage TLS certificates in Kubernetes. checkov - A static analysis tool for infrastructure as code - to prevent misconfigs at build time. clair - Vulnerability Static Analysis for Containers. cost-model - Cross-cloud cost allocation models for workloads running on Kubernetes. curiefense - Adds a broad set of automated web security tools to Envoy. dex - OpenID Connect Identity (OIDC) and OAuth 2.0 Provider with Pluggable Connectors. docker-bench-security - The Docker Bench for Security is a script that checks for dozens of common best-practices around deploying Docker containers in production. dockscan - Dockscan is security vulnerability and audit scanner for Docker installations. drydock - Drydock provides a flexible way of assessing the security of your Docker daemon configuration and containers using editable audit templates. falco - Behavioral Activity Monitoring With Container Support. goldfish - A HashiCorp Vault UI panel written with VueJS and Vault native Go API. grafeas - Cloud artifact metadata CRUD API and resource specifications. guard - Kubernetes Authentication WebHook Server. k8guard - An auditing system for Kubernetes. keycloak - Open Source Identity and Access Management For Modern Applications and Services. kratos - Next-gen identity server (think Auth0, Okta, Firebase) with Ory-hardened authentication, MFA, FIDO2, profile management, identity schemas, social sign in, registration, account recovery, service-to-service and IoT auth. Can work as an OAuth2 / OpenID Connect Provider. Golang, headless, API-only - without templating or theming headaches. kritis - Deploy-time Policy Enforcer for Kubernetes applications. kube-bench - The Kubernetes Bench for Security is a Go application that checks whether Kubernetes is deployed according to security best practices. kube-lego - Automatically request certificates for Kubernetes Ingress resources from Let's Encrypt. kube2iam - kube2iam provides different AWS IAM roles for pods running on Kubernetes. kubed - A Kubernetes Cluster Operator Daemon. kubescape - Kubescape is the first tool for testing if Kubernetes is deployed securely as defined in Kubernetes Hardening Guidance by to NSA and CISA. kyverno - Kubernetes Native Policy Management. notary - Notary is a Docker project that allows anyone to have trust over arbitrary collections of data. opa - An open source project to policy-enable your service. pomerium - Pomerium is a zero-trust context and identity aware access gateway inspired by BeyondCorp. spiffe - The SPIFFE Project. trivy - A Simple and Comprehensive Vulnerability Scanner for Containers, Suitable for CI. vault - A tool for managing secrets. vilicus - Vilicus is an open source tool that orchestrates security scans of container images(docker/oci) and centralizes all results into a database for further analysis and metrics. Service Mesh aeraki - Manage any layer 7 traffic in an Istio service mesh. amalgam8 - Content and Version-based Routing Fabric for Polyglot Microservices. consul - Consul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure. easemesh - A service mesh implementation for connecting, control, and observe services in spring-cloud. getmesh - An integration, and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio. istio - Connect, secure, control, and observe services. kuma - Universal Control Plane for your Service Mesh. slime - Slime is a CRD controller for istio. linkerd - Resilient service mesh for cloud native apps. linkerd2 - Ultralight, security-first service mesh for Kubernetes. Main repo for Linkerd 2.x. maesh - Simpler Service Mesh. nginmesh - Service Mesh using Nginx. nginx-unit - NGINX Unit is a new, lightweight, open source application server built to meet the demands of dynamic and distributed applications. osm - Open Service Mesh (OSM) is a lightweight, extensible, cloud native service mesh that allows users to uniformly manage, secure, and get out-of-the-box observability features for highly dynamic microservice environments. secretscanner - Find secrets and passwords in container images and file systems. servicecomb - ServiceComb is a microservice framework that provides an easy way to develop and deploy applications in the cloud. supergloo - The Service Mesh Orchestration Platform. Service Registry and Discovery admiral - Admiral provides automatic configuration generation, syncing and service discovery for multicluster Istio service mesh. apollo - Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景. confd - Manage local application configuration files using templates and data from etcd or consul. coredns - CoreDNS is a DNS server that chains middleware. eureka - AWS Service registry for resilient mid-tier load balancing and failover. open-service-broker-sdk - A starting point for creating service brokers implementing the Open Service Broker API. polaris - Service discovery and governance center for distributed and microservice architecture. registrator - Service registry bridge for Docker with pluggable adapters. rotor - Rotor is a fast, lightweight bridge between your service discovery and the configuration APIs of Envoy. Rotor supports Kubernetes, Consul, AWS (EC2 and ECS), DC/OS, flat files, and even other EDS/CDS implementations. service-broker - Open Service Broker API Specification. service-catalog - Consume services in Kubernetes using the Open Service Broker API. skydns - DNS for skynet or any other service discovery. steward - The Kubernetes-native Service Broker. synapse - A transparent service discovery framework for connecting an SOA. vulcand - Programmatic load balancer backed by Etcd. zookeeper - Apache ZooKeeper is an effort to develop and maintain an open-source server which enables highly reliable distributed coordination. Serverless booster - Booster is a framework for building and deploying reliable and scalable event-driven serverless applications. dapr - Dapr is a portable, event-driven, runtime for building distributed applications across cloud and edge. dispatch - Dispatch is a framework for deploying and managing serverless style applications. easyfaas - EasyFaaS 是一个依赖轻、适配性强、资源占用少、无状态且高性能的函数计算服务引擎. eventing - Open source specification and implementation of Knative event binding and delivery. faas-netes - Enable Kubernetes as a backend for Functions as a Service (OpenFaaS). firecamp - Serverless Platform for the stateful services. firecracker - Secure and fast microVMs for serverless computing. fission - Fast Serverless Functions for Kubernetes. fn - The container native, cloud agnostic serverless platform. funktion - A CLI tool for working with funktion. fx - Poor man's serverless framework based on Docker, Function as a Service with painless. gloo - The Function Gateway built on top of Envoy. ironfunctions - IronFunctions - the serverless microservices platform. keda - KEDA is a Kubernetes-based Event Driven Autoscaling component. It provides event driven scale for any container running in Kubernetes. knative-lambda-runtime - Running AWS Lambda Functions on Knative/Kubernetes Clusters. knix - KNIX MicroFunctions is a serverless computing platform that combines container-based resource isolation with a lightweight execution model using processes to significantly improve resource efficiency and decrease the function startup latency. KNIX MicroFunctions works in Knative as well as bare metal or virtual machine-based environments. kubeless - Kubernetes Native Serverless Framework. layotto - A fast and efficient cloud native application runtime. nuclio - High-Performance Serverless event and data processing platform. openfaas - OpenFaaS - Serverless Functions Made Simple for Docker & Kubernetes. openwhisk - Apache OpenWhisk (Incubating) is a serverless, open source cloud platform that executes functions in response to events at any scale. osiris - A general purpose, scale-to-zero component for Kubernetes. riff - Riff is for functions. serverless - Serverless Framework – Build web, mobile and IoT applications with serverless architectures using AWS Lambda, Azure Functions, Google CloudFunctions & more! serving - Kubernetes-based, scale-to-zero, request-driven compute. spec - CloudEvents Specification. sqoop - The GraphQL Engine powered by Gloo. thanos - Highly available Prometheus setup with long term storage capabilities. Stability chaosblade - An easy to use and powerful chaos engineering experiment toolkit（阿里巴巴开源的一款简单易用、功能强大的混沌实验注入工具）. chaosmonkey - Chaos Monkey is a resiliency tool that helps applications tolerate random instance failures. chaos-mesh - A Chaos Engineering Platform for Kubernetes. concurrency-limits - Java Library that implements and integrates concepts from TCP congestion control to auto-detect concurrency limits to achieve optimal throughput with optimal latency. hystrix - Hystrix is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure and enable resilience in complex distributed systems where failure is inevitable. kubedoom - Kill Kubernetes pods by playing Id's DOOM! metersphere - MeterSphere is an End-to-End open source continuous testing platform. MeterSphere 是一站式开源持续测试平台，涵盖测试跟踪、接口测试、性能测试、团队协作等功能，全面兼容 JMeter、Postman、Swagger 等开源、主流标准. ratelimit - Go/gRPC service designed to enable generic rate limit scenarios from different types of applications. sentinel - A powerful flow control component enabling reliability, resilience and monitoring for microservices. (面向云原生微服务的高可用流控防护组件) toxiproxy - A TCP proxy to simulate network and system conditions for chaos and resiliency testing. Storage ceph - Ceph is a distributed object, block, and file storage platform. chubaofs - A distributed storage system for cloud native applications. convoy - A Docker volume plugin, managing persistent container volumes. fastdfs - FastDFS is an open source high performance distributed file system (DFS). It's major functions include: file storing, file syncing and file accessing, and design for high capacity and load balance. flocker - Container data volume manager for your Dockerized application. glusterd2 - GlusterD-2.0 is the distributed management framework to be used for GlusterFS-4.0. glusterfs - Gluster is a software defined distributed storage that can scale to several petabytes. It provides interfaces for object, block and file storage. harbor - An open source trusted cloud native registry project that stores, signs, and scans content. heketi - RESTful based volume management framework for GlusterFS. juicefs - A distributed POSIX file system built on top of Redis and S3. k8ssandra - K8ssandra is a collection of Helm charts for running Apache Cassandra on Kubernetes in production. kubefs - Mount kubernetes metadata storage as a filesystem. infinit - The Infinit policy-based software-defined storage platform. leofs - The LeoFS Storage System. longhorn - We put storage on cows and move them around from rancher. minio - Minio is an open source object storage server compatible with Amazon S3 APIs. openebs - OpenEBS is containerized block storage written in Go for cloud native and other environments w/ per container (or pod) QoS SLAs, tiering and replica policies across AZs and environments, and predictable and scalable performance. rook - File, Block, and Object Storage Services for your Cloud-Native Environment. storageos - Enterprise persistent storage for containers and the cloud. torus - Torus Distributed Storage. vitess - Vitess is a database clustering system for horizontal scaling of MySQL. zenko - Because everyone should be in control of their data. Tools aglio - An API Blueprint renderer with theme support that outputs static HTML. ansible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications — automate in a language that approaches plain English, using SSH, with no agents to install on remote systems. ark - Heptio Ark is a utility for managing disaster recovery, specifically for your Kubernetes cluster resources and persistent volumes. Brought to you by Heptio. buildx - Docker CLI plugin for extended build capabilities with BuildKit. chaostoolkit - An Open API to Chaos Engineering. che - Eclipse Che: Next-generation Eclipse IDE. Open source workspace server and cloud IDE. client-go - Go client for Kubernetes. cloud-native-sandbox - Cloud Native Sandbox can help you setup a standalone Kubernetes and Istio environment with Docker on you own laptop. cluster-lifecycle-manager - Cluster Lifecycle Manager (CLM) to provision and update multiple Kubernetes clusters. clusternet - Managing your Kubernetes clusters (including public, private, edge, etc) as easily as visiting the Internet. compass - A Debugging Tool for your Kubernetes Deployments. container-structure-test - Validate the structure of your container images. container-transform - Transforms docker-compose, ECS, and Marathon configurations. cost-model - Cross-cloud cost allocation models for workloads running on Kubernetes. crashcart - CrashCart: sideload binaries into a running container. cri-tools - CLI and validation tools for Kubelet Container Runtime Interface (CRI). datree - CLI tool that automatically scans Kubernetes manifests and Helm charts to ensure they follow best practices as well as your organization’s policies. devspace - Cloud Native Software Development with Kubernetes and Docker - simply run \"devspace up\" in any of your projects and start coding directly on top of Kubernetes (works with minikube, self-hosted and cloud-based clusters). docker-elk - The ELK stack powered by Docker and Compose. docker-pushrm - A Docker CLI plugin that that lets you push the README.md file from the current directory to Docker Hub. Also supports Quay and Harbor. docker-wine - Docker image that includes Wine and Winetricks for running Windows applications on Linux and macOS. dockersh - A shell which places users into individual docker containers. dotmesh - Dotmesh (dm) is like git for your data volumes (databases, files etc) in Docker and Kubernetes. dragonfly - Dragonfly is an intelligent P2P based file distribution system. drakov - Mock Server that implements the API Blueprint specification. eksctl - A CLI for Amazon EKS. erda - An enterprise-grade application building, deploying, monitoring platform (An iPaaS). escalator - Escalator is a batch or job optimized horizontal autoscaler for Kubernetes. fleet - Manage large fleets of Kubernetes clusters. freshpod - Restart Pods on Minikube automatically on image rebuilds. fubectl - Reduces repetitive interactions with kubectl. garden - Development orchestrator for Kubernetes, containers and serverless functions. gardener - Kubernetes API server extension and controller manager providing conformant Kubernetes clusters (a.k.a. (off)shoot clusters) as a service (with day-2 ops) on Alibaba, AWS, Azure, GCP, and OpenStack. go-kubectx - 5x-10x faster alternative to kubectx. Uses client-go. istio-pod-network-controller - Controller to manage Istio Pod Network. k - Exec into kubernetes pod easy (via kubectl). k8s-mirror - Creates a local mirror of a Kubernetes cluster in a docker container to support offline reviewing. k8s-snapshots - Automatic Volume Snapshots on Kubernetes. kail - Kubernetes log viewer. kcg - Kubernetes config generator. kconmon - A Kubernetes node connectivity monitoring tool. kpack - Kubernetes Native Container Build Service. kind - Kubernetes IN Docker - local clusters for testing Kubernetes. kip - Virtual-kubelet provider running pods in cloud instances. kops - Kubernetes Operations (kops) - Production Grade K8s Installation, Upgrades, and Management. krane - A command-line tool that helps you ship changes to a Kubernetes namespace and understand the result. krustlet - Kubernetes Rust Kubelet. ksonnet-lib - (technical preview) Simplify working with Kubernetes. ksonnet - A CLI-supported framework that streamlines writing and deployment of Kubernetes configurations to multiple clusters. ksync - Sync files between your local system and a kubernetes cluster. kt-connect - Manage and Integration with your Kubernetes dev environment more efficient. ktmpl - Parameterized templates for Kubernetes manifests. kube-capacity - A simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. kube-downscaler - Scale down Kubernetes deployments after work hours. kube-fledged - A kubernetes add-on for creating and managing a cache of container images in a kubernetes cluster. kube-linter - KubeLinter is a static analysis tool that checks Kubernetes YAML files and Helm charts to ensure the applications represented in them adhere to best practices. kube-ps1 - Kubernetes prompt info for bash and zsh. kube-shell - Kubernetes shell: An integrated shell for working with the Kubernetes CLI. kube-version-converter - Convert API Object file into specified version. kubeasz - 使用Ansible脚本安装K8S集群，介绍组件交互原理，方便直接，不受国内网络环境影响. kubeadm-offline-installer - Setup a cluster with kubeadm, without internet connections. kubeadm - Aggregator for issues filed against kubeadm. kubebox - Terminal console for Kubernetes clusters. kubebuilder - Kubebuilder - SDK for building Kubernetes APIs using CRDs. kubecarrier - KubeCarrier - Service Management at Scale. kubecdn - Self-hosted CDN based on Kubernetes. kubecfg - A tool for managing complex enterprise Kubernetes environments as code. kubectl-doctor - Kubectl cluster triage plugin for Kubernetes (brew doctor equivalent). kubectl-trace - Schedule bpftrace programs on your kubernetes cluster using the kubectl. kubectl-tree - kubectl plugin to browse Kubernetes object hierarchies as a tree 🎄 (using? star the repo!) kubedb - KubeDB CLI to manage kubernetes ready production-grade Databases. kubedirector - Kubernetes Director (aka KubeDirector) for deploying and managing stateful applications on Kubernetes. kubefwd - Bulk port forwarding Kubernetes services for local development. kubehandler - A framework for writing Kubernetes controllers. kubeiql - A GraphQL interface for Kubernetes. kubeletctl - A client for kubelet. kubelibrary - Kubernetes library for Robot Framework. kubeload - Jobs managing K8S operator for IAC-oriented load tests. kubeonoff - A simple web UI for managing Kubernetes deployments. kuberlr - A tool that simplifies the management of multiple versions of kubectl. kubernetes-client - Java client for Kubernetes & OpenShift 3. kubernetes-vagrant-centos-cluster - Setting up a distributed Kubernetes cluster along with Istio service mesh locally with Vagrant and VirtualBox. kubespray - Setup a kubernetes cluster also mentioned as kargo. kubespy - Tools for observing Kubernetes resources in real time, powered by Pulumi. kubesql - A tool using sql to query the resources of kubernetes, such as pod, node and so on. kubetap - Kubectl plugin to interactively proxy Kubernetes Services with ease. kubeup - Cluster operation the Kubernetes way. kubeutr - Cookie cutter templating tool for scaffolding K8s manifests. kubie - A more powerful alternative to kubectx and kubens. KubiScan - A tool to scan Kubernetes cluster for risky permissions. kuui - UI that can be used to edit configmaps/secrets of your kubernetes cluster. kvdi - A Kubernetes-native Virtual Desktop Infrastructure. microconfig - Modern and simple way of microservice configuration management. microk8s - A kubernetes cluster in a snap. mindaro - Bridge to Kubernetes - for Visual Studio and Visual Studio Code minikube - Run Kubernetes locally. monday - A dev tool for microservice developers that run local applications and/or forward some others from Kubernetes or over SSH. nocalhost - Nocalhost is Cloud Native Dev Environment. okteto - Local development experience for Kubernetes apps. packer - Packer is a tool for creating identical machine images for multiple platforms from a single source configuration. pangolin - An enhanced Horizontal Pod Autoscaler for Kubernetes. pluto - A cli tool to help discover deprecated apiVersions in Kubernetes. podtnl - A Powerful CLI that makes your pod available to online without exposing a Kubernetes service. portainer - Simple management UI for Docker. powerfulseal- A powerful testing tool for Kubernetes clusters. rafter - Kubernetes-native S3-like files/assets store based on CRDs and powered by MinIO. rback - RBAC in Kubernetes visualizer. reloader - A Kubernetes controller to watch changes in ConfigMap and Secrets and do rolling upgrades on Pods with their associated Deployment, StatefulSet, DaemonSet and DeploymentConfig. searchlight - Alerts for Kubernetes. seaworthy - A CLI to verify Kubernetes resource health. skopeo - Work with remote images registries - retrieving information, images, signing content. sloop - Kubernetes History Visualization. sonobuoy - Heptio Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests in an accessible and non-destructive manner. squash - The debugger for microservices. stash - Backup your Kubernetes Volumes. statusbay - Kubernetes deployment visibility like a pro. stern - Multi pod and container log tailing for Kubernetes. swagger - Swagger UI is a collection of HTML, JavaScript, and CSS assets that dynamically generate beautiful documentation from a Swagger-compliant API. talos - A modern OS for Kubernetes. tectonic-installer - Install a Kubernetes cluster the CoreOS Tectonic Way: HA, self-hosted, RBAC, etcd Operator, and more. telepresence - Local development against a remote Kubernetes or OpenShift cluster. terminus - Graceful shutdown and Kubernetes readiness / liveness checks for any Node.js HTTP applications. test-infra - Test infrastructure for the Kubernetes project. tensile-kube - A Kubernetes Provider. tini - A tiny but valid init for containers. tor-controller - Run Tor onion services on Kubernetes. usernetes - Kubernetes installable under $HOME, without the root privileges. vagrant - Vagrant is a tool for building and distributing development environments. watchtower - Automatically update running Docker containers. wksctl - Open Source Weaveworks Kubernetes System. xlskubectl - A spreadsheet to control your Kubernetes cluster. Tracing appdash - Application tracing system for Go, based on Google's Dapper. jaeger - Jaeger, a Distributed Tracing System. opencensus - A single distribution of libraries that automatically collect traces and metrics from your app, display them locally, and send them to any backend. opentelemetry - An observability framework for cloud-native software. opentracing - Consistent, expressive, vendor-neutral APIs for distributed tracing and context propagation. pinpoint - Pinpoint is an open source APM (Application Performance Management) tool for large-scale distributed systems written in Java. sentry - Sentry is a cross-platform crash reporting and aggregation platform. skywalking - An APM system for tracing, monitoring, diagnosing distributed systems, especially based on microservices, cloud native and container. sofa-tracker - SOFATracer is a component for the distributed system call trace. And through a unified traceId logging the logs of various network calls in the invoking link . These logs can be used for quick discovery of faults, service governance, etc. zipkin - Zipkin is a distributed tracing system. Tutorials aws-eks-best-practices - A best practices guide for day 2 operations, including operational excellence, security, reliability, performance efficiency, and cost optimization. aws-workshop-for-kubernetes - AWS Workshop for Kubernetes. envoy-steps - Envoy Step by Step. envoy-tutorial - Envoy mesh in kubernetes tutorial. falco-analyze-audit-log-from-k3s-cluster - Detect intrusions that happened in your Kubernetes cluster through audit logs using Falco. istio-handbook - Istio Service Mesh Advanced Practical Istio服务网格进阶实战. istio-index-conf2018 - Istio is not just for Microservices: Secure your Kubernetes services using Istio Service Mesh. istio-ingress-tutorial - How to run the Istio Ingress Controller on Kubernetes. istio-service-mesh-workshop - Using Istio Workshop. istio-tutorial - Istio Tutorial for Java Microservices. istio101 - Istio 101 workshop from IBM. ks - A series of Kubernetes walk-throughs. kube-ladder - Learning Kubernetes, The Chinese Taoist Way. kubeadm-workshop - Showcasing a bare-metal multi-platform kubeadm setup with persistent storage and monitoring. kubernetes-handbook - Kubernetes中文指南/云原生应用架构实践手册. kubernetes-java-simple - Kubernetes Hands-on Workshop for Java Developers. kubernetes-on-aws - Deploying Kubernetes on AWS with CloudFormation and Ubuntu. kubernetes-security-best-practice - Kubernetes Security - Best Practice Guide. kubernetes-the-hard-way - Bootstrap Kubernetes the hard way on Google Cloud Platform. No scripts. kubicorn - Create, manage, snapshot, and scale Kubernetes infrastructure in the public cloud. mosn-tutorial - Tutorial for MOSN and Istio Service Mesh. rpi-handbook - Raspberry Pi Handbook/树莓派实践手册. UI breeze - Wise2C ansible playbook for Kubernetes cluster installation. choerodon - The open source PaaS for Kubernetes. cloudfoundry - Cloud Foundry is an open source, multi cloud application platform as a service (PaaS) governed by the Cloud Foundry Foundation. conjure-up - Deploying complex solutions, magically. dashboard - General-purpose web UI for Kubernetes clusters. kdash - A simple and fast dashboard for Kubernetes. kqeen - Kubernetes queen - cluster manager. kubermatic - The Central Kubernetes Management Platform For Any Infrastructure. kubernator - Alternative Kubernetes UI. kubesphere - Enterprise Container Managent Platform. kubevious - Kubevious - application centric Kubernetes UI and continuous assurance provider. oneinfra - Kubernetes as a Service. opendcp - Docker platform developed by weibo. openshift - Enterprise Kubernetes for Developers. rainbond - Serverless PaaS , A new generation of easy-to-use cloud management platforms based on kubernetes. rancher - Complete container management platform. wayne - Web UI for Kubernetes multi-clusters. Community Cloud Native Community (China) Cloud Native Computing Foundation ServiceMesher Contribute This website is hosted on GitHub Pages within rootsongjc/awesome-cloud-native repository. Please take a quick gander at the contribution guidelines first. Thanks to all contributors, you rock 🤟! 云原生社区 meetup 第八期上海站✕ 10 月 23 日（周六）下午，上海蚂蚁 S 空间见！线下开源聚会，欢迎报名参加！ 立即报名 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/删除namespace.html":{"url":"blog/kubernetes/删除namespace.html","title":"删除Namespace","keywords":"","body":"namespace Terminating 一般性的删除 kubectl delete --grace-period=0 --force --wait=false 强制删除 kubectl patch -p '{\"metadata\":{\"finalizers\":null}}' 在k8s集群中进行测试删除namespace是经常的事件，而为了方便操作，一般都是直接对整个名称空间进行删除操作。 相信道友们在进行此步操作的时候，会遇到要删除的namespace一直处于Terminating。下面我将给出一个完美的解决方案， 测试demo 创建demo namespace # kubectl create ns test namespace/test created 删除demo namespace # kubectl delete ns test namespace \"test\" deleted 一直处于deleted不见exit 查看状态 可见test namespace 处于Terminating # kubectl get ns -w NAME STATUS AGE test Terminating 18s 下面给出一种完美的解决方案：调用接口删除 开启一个代理终端 kubectl proxy 将test namespace的配置文件输出保存, kubectl get ns test -o json > test.json 删除spec及status部分的内容还有metadata字段后的\",\"号，切记 3.调接口删除 curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @test.json \\ http://127.0.0.1:8001/api/v1/namespaces/test/finalize 查看结果 1、delete 状态终止 kubectl delete ns test namespace \"test\" deleted 2、Terminating状态终止 kubectl get ns -w test Terminating 18s test Terminating 17m 名称空间被删除掉 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/多集群管理/Karmada.html":{"url":"blog/kubernetes/多集群管理/Karmada.html","title":"Karmada","keywords":"","body":"Karmada Kamada 是华为主导的开源 kubernetes 多云容器编排管理系统，可以跨多个kubernets集群和云运行你的云原生应用程序，而无需更改应用程序。通过kubernetes原声API并提供高级调度功能，Karmada可以实现真正的开放式多云集群管理。 1. 特性 兼容k8s 原生API 开箱即用 避免锁定供应商 集中化管理 高效的多集群调度策略 开放和中立 2. 架构 ETCD：存储Karmada API对象。 Karmada Scheduler：提供高级的多集群调度策略。 Karmada Controller Manager: 包含多个Controller，Controller监听karmada对象并且与成员集群API server进行通信并创建成员集群的k8s对象。 Cluster Controller：成员集群的生命周期管理与对象管理。 Policy Controller：监听PropagationPolicy对象，创建ResourceBinding，配置资源分发策略。 Binding Controller：监听ResourceBinding对象，并创建work对象响应资源清单。 Execution Controller：监听work对象，并将资源分发到成员集群中。 2.1 控制面组件 Karmada API server karmada scheduler karmada controller manager ETCD stores the karmada API objects, the API Server is the REST endpoint all other components talk to, and the Karmada Controller Manager perform operations based on the API objects you create through the API server. The Karmada Controller Manager runs the various controllers, the controllers watch karmada objects and then talk to the underlying clusters’ API servers to create regular Kubernetes resources. Cluster Controller: attach kubernetes clusters to Karmada for managing the lifecycle of the clusters by creating cluster object. Policy Controller: the controller watches PropagationPolicy objects. When PropagationPolicy object is added, it selects a group of resources matching the resourceSelector and create ResourceBinding with each single resource object. Binding Controller: the controller watches ResourceBinding object and create Work object corresponding to each cluster with single resource manifest. Execution Controller: the controller watches Work objects.When Work objects are created, it will distribute the resources to member clusters. ETCD存储karmada API 对象，API 服务是其他组件通信的REST 终端，karmada 控制器执行操作的对象是基于用户通过API server创建的。 集群控制器： 将集群附着到 Karmada控制器，通过创建集群对象来管理集群的生命周期 策略控制器 角色绑定 执行控制器 资源分发流程 基本概念 资源模板（Resource Template）：Karmada使用K8s原生API定义作为资源模板，便于快速对接K8s生态工具链。 分发策略（Propagaion Policy）：Karmada提供独立的策略API，用来配置资源分发策略。 差异化策略（Override Policy）：Karmada提供独立的差异化API，用来配置与集群相关的差异化配置。比如配置不同集群使用不同的镜像。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/多集群管理/OCM.html":{"url":"blog/kubernetes/多集群管理/OCM.html","title":"OCM","keywords":"","body":"OCM 为了让开发者、用户在多集群和混合环境下也能像在单个 Kubernetes 集群平台上一样，使用自己熟悉的开源项目和产品轻松开发功能，RedHat 和蚂蚁、阿里云共同发起并开源了 OCM（Open Cluster Management）旨在解决多集群、混合环境下资源、应用、配置、策略等对象的生命周期管理问题。目前，OCM 已向 CNCF TOC 提交 Sandbox 级别项目的孵化申请。 项目官网：https://open-cluster-management.io/ OCM 的主要功能和架构 OCM 旨在简化部署在混合环境下的多 Kubernetes 集群的管理工作。可以用来为 Kubernetes 生态圈不同管理工具拓展多集群管理能力。OCM 总结了多集群管理所需的基础概念，认为在多集群管理中，任何管理工具都需要具备以下几点能力： 1.理解集群的定义； 2.通过某种调度方式选择一个或多个集群； 3.分发配置或者工作负载到一个或多个集群； 4.治理用户对集群的访问控制； 5.部署管理探针到多个集群中。 OCM 采用了 hub-agent 的架构，包含了几项多集群管理的原语和基础组件来达到以上的要求： ●通过 ManagedCluster API 定义被管理的集群，同时 OCM 会安装名为 Klusterlet 的 agent 在每个集群里来完成集群注册，生命周期管理等功能。 ●通过 Placement API 定义如何将配置或工作负载调度到哪些集群中。调度结果会存放在 PlacementDecision API 中。其他的配置管理和应用部署工具可以通过 PlacementDecisiono 决定哪些集群需要进行配置和应用部署。 ●通过 ManifestWork API 定义分发到某个集群的配置和资源信息。 ●通过 ManagedClusterSet API 对集群进行分组，并提供用户访问集群的界限。 ●通过 ManagedClusterAddon API 定义管理探针如何部署到多个集群中以及其如何与 hub 端的控制面进行安全可靠的通信。 架构如下图所示，其中 registration 负责集群注册、集群生命周期管理、管理插件的注册和生命周期管理；work 负责资源的分发；placement 负责集群负载的调度。在这之上，开发者或者 SRE 团队能够基于 OCM 提供的 API 原语在不同的场景下方便的开发和部署管理工具。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/多集群管理/cluster-life-manage.html":{"url":"blog/kubernetes/多集群管理/cluster-life-manage.html","title":"Cluster Life Manage","keywords":"","body":" meeting record https://docs.google.com/document/d/1y6YLVC-v7cmVAdbjedoyR5WL0-q45DBRXTvz5_I7bkA/edit# Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/多集群管理/clusternet.html":{"url":"blog/kubernetes/多集群管理/clusternet.html","title":"Clusternet","keywords":"","body":"clusternet [toc] 架构介绍 Clusternet ( Cluster Internet ) 是腾讯开源的多集群和应用管理软件，无论集群是运行在公有云、私有云、混合云还是边缘云上，Clusternet 都可以让您像在本地运行一样管理/访问它们，用 Kubernetes API 集中部署和协调多集群的应用程序和服务。通过 Addon 插件方式，用户可以一键安装、运维及集成，轻松地管理数以百万计的 Kubernetes 集群，就像访问 Internet 一样自由便捷。 Clusternet 面向未来混合云、分布式云和边缘计算场景设计，支持海量集群的接入和管理，灵活的集群注册能力可以适应各种复杂网络条件下的集群管理需求，通过兼容云原生的 Kubernetes API 简化用户的管理和运维成本，加快用户业务的云原生转型。 clusternet-agent is responsible for auto-registering current cluster to a parent cluster as a child cluster, which is also been called ManagedCluster; 自动将当前集群注册到父集群（也叫管理集群） reporting heartbeats of current cluster, including Kubernetes version, running platform, healthz/readyz/livez status, etc; 自动上报当前集群心跳，包括集群版本，运行平台，健康/就绪/存活状态等 setting up a websocket connection that provides full-duplex communication channels over a single TCP connection to parent cluster; 启动一个websocket连接，通过连接到当前集群的一个TCP连接提供全双工通信 clusternet-hub is responsible for approving cluster registration requests and creating dedicated resources, such as namespaces, serviceaccounts and RBAC rules, for each child cluster; 允许集群注册请求和创建专用资源，比如 为子集群创建 namespaces, serviceaccounts and RBAC rules等; serving as an aggregated apiserver (AA), which is used to serve as a websocket server that maintain multiple active websocket connections from child clusters; 提供聚合api服务，作为webshocket服务用于维持和多个子集群websocket连接的活动 providing Kubernstes-styled API to redirect/proxy/upgrade requests to each child cluster; 提供k8s风格的API 直连/代理/升级 请求到子集群 coordinating and deploying applications to multiple clusters from a single set of APIs; 从一组API协调和发布应用到多个集群 相关概念 For every Kubernetes cluster that wants to be managed, we call it child cluster. The cluster where child clusters are registerring to, we call it parent cluster. clusternet-agent runs in child cluster, while clusternet-hub runs in parent cluster. ClusterRegistrationRequest is an object that clusternet-agent creates in parent cluster for child cluster registration. 集群注册申请，用于子集群申请注册 ManagedCluster is an object that clusternet-hub creates in parent cluster after approving ClusterRegistrationRequest. 管理集群 HelmChart is an object contains a helm chart configuration. Subscription defines the resources that subscribers want to install into clusters. For every matched cluster, a corresponding Base object will be created in its dedicated namespace. “Subscription”定义了订阅者想要安装到集群的资源。对于每一个匹配的集群，一个对应的Base对象将会被创建在专用的命名空间。 Clusternet provides a two-stage priority based override strategy. Localization and Globalization will define the overrides with priority, where lower numbers are considered lower priority. Localization is namespace-scoped resource, while Globalization is cluster-scoped. Refer to Deploying Applications to Multiple Clusters on how to use these. “Clusternet”提供了一种基于优先级的两阶段覆盖策略。 “本地化”和“全局化”将定义优先级覆盖，数字越低优先级越低。“本地化”是命名空间作用域的资源，而“全球化”是集群作用域的资源。关于如何使用它们，请参阅将应用程序部署到多个集群。 Base objects will be rendered to Description objects with Globalization and Localization settings applied. Description is the final resources to be deployed into target child clusters. “基础”对象将被渲染为“描述”对象，并应用“全球化”和“本地化”设置。“Description”是部署到目标子集群的最终资源。 部署 Clusternet You need to deploy clusternet-agent and clusternet-hub in child cluster and parent cluster respectively. Deploying clusternet-hub in parent cluster $ kubectl apply -f deploy/hub Next, you need to create a token for cluster registration, which will be used later by clusternet-agent. Either a bootstrap token or a service account token is okay. If bootstrapping authentication is supported, i.e. --enable-bootstrap-token-auth=true is explicitly set in the kube-apiserver running in parent cluster, $ # this will create a bootstrap token 07401b.f395accd246ae52d $ kubectl apply -f manifests/samples/cluster_bootstrap_token.yaml If bootstrapping authentication is not supported by the kube-apiserver in parent cluster (like k3s) , i.e. --enable-bootstrap-token-auth=false (which defaults to be false), please use serviceaccount token instead. $ # this will create a serviceaccount token $ kubectl apply -f manifests/samples/cluster_serviceaccount_token.yaml $ kubectl get secret -n clusternet-system -o=jsonpath='{.items[?(@.metadata.annotations.kubernetes\\.io/service-account\\.name==\"cluster-bootstrap-use\")].data.token}' | base64 --decode; echo HERE WILL OUTPUTS A LONG STRING. PLEASE REMEMBER THIS. Deploying clusternet-agent in child cluster clusternet-agent runs in child cluster and helps register self-cluster to parent cluster. clusternet-agent could be configured with below three kinds of SyncMode (configured by flag --cluster-sync-mode), Push means that all the resource changes in the parent cluster will be synchronized, pushed and applied to child clusters by clusternet-hub automatically. Push（推） 模式是指父集群的所有资源变化将由 clusternet-hub 自动同步、推送并应用到子集群 Pull means clusternet-agent will watch, synchronize and apply all the resource changes from the parent cluster to child cluster. Pull（拉） 模式表示 clusternet-agent 将自动 watch、同步和应用所有从父集群到子集群的资源变化 Dual combines both Push and Pull mode. This mode is strongly recommended, which is usually used together with feature gate AppPusher. Dual 推拉结合模式，这种模式强烈推荐，通常与特性 AppPusher 一起使用 Feature gate AppPusher works on agent side, which is introduced mainly for below two reasons, SyncMode is not suggested getting changed after registration, which may bring in inconsistent settings and behaviors. That's why Dual mode is strong recommended. When Dual mode is set, feature gate AppPusher provides a way to help switch Push mode to Pull mode without really changing flag --cluster-sync-mode, and vice versa. 不建议在注册后改变同步模式，这可能会带来不一致的配置和行为，这就是为什么强烈推荐双模式。当双模式被设置后，AppPusher 提供了一种方法来帮助将 Push 模式切换到 Pull 模式，而无需真正更改标志 --cluster-sync-mode，反之亦然。 For security concerns, such as child cluster security risks, etc. When a child cluster has disabled feature gate AppPusher, the parent cluster won't deploy any applications to it, even if SyncMode Push or Dual is set. At this time, this child cluster is working like Pull mode. Resources to be deployed are represented as Description, you can run your own controllers as well to watch changes of Description objects, then distribute and deploy resources. 当一个子集群禁用 AppPusher 时，父集群不会向其部署任何应用程序，即使设置为 Push 或 Dual 模式，这个时候，这个子集群的工作方式就像 Pull 模式。 要部署的资源被表示为 Description 对象，你也可以运行你自己的控制器来 watch 该对象的变化，然后来分发和部署资源。 Upon deploying clusternet-agent, a secret that contains token for cluster registration should be created firstly. $ # create namespace clusternet-system if not created $ kubectl create ns clusternet-system $ # here we use the token created above $ PARENTURL=https://192.168.10.10 REGTOKEN=07401b.f395accd246ae52d envsubst 📌 📌 Note: If you're creating service account token above, please replace 07401b.f395accd246ae52d with above long string token that outputs. The PARENTURL above is the apiserver address of the parent cluster that you want to register to, the https scheme must be specified and it is the only one supported at the moment. If the apiserver is not listening on the standard https port (:443), please specify the port number in the URL to ensure the agent connects to the right endpoint, for instance, https://192.168.10.10:6443. $ # before deploying, you could update the SyncMode if needed $ kubectl apply -f deploy/agent Visit ManagedCluster With RBAC Clusternet supports visiting all your managed clusters with RBAC. There is one prerequisite here, that is kube-apiserver should allow anonymous requests. The flag --anonymous-auth is set to be true by default. So you can just ignore this unless this flag is set to false explicitly # Here the token is base64 decoded and from your child cluster. export CHILDCLUSTERTOKEN=\"eyJhbGciOiJSUzI1NiIsImtpZCI6IkhSaVJtdERIOEdhTkYzVndXMnEyNk02SWsxMnM3UTM3bFFBZHJ5Q2FLM3MifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1zcWMyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjAwZTI2NjIwLTZkYzgtNDkwOC1hMjcxLTQ0YTBkMTQ1NDIzYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.Zwnpr86dghryoVXIU11IegOuiw-CULvCd7A03TXmE1nOopJweR9SAzPAnrVC-O6gMVGSNXmYPFYtCW_4zCjoTlhxnAgv4WiNOndQQdOmNPMLHdcTjrtG_LT29IuHvgdDWSOQ-p2bk5wl0K5dlRNIpqex5LpzU-JzyJY8ek5Ug2gnA80HF-HMyBM_qBpDxiJvf_kOzpOc1m4P1q1oDqc86y8S5tS9EYV68T9I-NAmmh3XqjKVpUfcCMvN3YSzxFWX9XYGEPwIiqRKFPug946cKKtqaLHJ7GTkkpCKV9Ls_59WFJPkdsYuO5Czm63joLhfdSfaGkPWWe3BhKPiJ8EcXA\" # The Parent Cluster APIServer Address username: system:anonymous as: clusternet as-user-extra: clusternet-token: - BASE64-DECODED-PLEASE-CHANGE-ME export APISERVER=\"https://47.243.203.89:6443\" # specify the child cluster id export CHILDCLUSTERID=\"703c80cf-34cd-481d-b477-d692032103da\" curl -k -XGET -H \"Accept: application/json\" -H \"Impersonate-User: clusternet\" -H \"Impersonate-Extra-Clusternet-Token: ${CHILDCLUSTERTOKEN}\" -H \"Authorization: Basic system:anonymous\" \"${APISERVER}/apis/proxies.clusternet.io/v1alpha1/sockets/${CHILDCLUSTERID}/proxy/direct/api/v1/namespaces\" 发布应用 Clusternet supports deploying applications to multiple clusters from a single set of APIs in a hosting cluster. 📌 📌 Note: Feature gate Deployer should be enabled by clusternet-hub. First, let's see an exmaple application. Below Subscription \"app-demo\" defines the target child clusters to be distributed to, and the resources to be deployed with. # examples/applications/subscription.yaml apiVersion: apps.clusternet.io/v1alpha1 kind: Subscription metadata: name: app-demo namespace: default spec: subscribers: # defines the clusters to be distributed to - clusterAffinity: matchLabels: clusters.clusternet.io/cluster-id: dc91021d-2361-4f6d-a404-7c33b9e01118 # PLEASE UPDATE THIS CLUSTER-ID TO YOURS!!! feeds: # defines all the resources to be deployed with - apiVersion: apps.clusternet.io/v1alpha1 kind: HelmChart name: mysql namespace: default - apiVersion: v1 kind: Namespace name: foo - apiVersion: apps/v1 kind: Service name: my-nginx-svc namespace: foo - apiVersion: apps/v1 kind: Deployment name: my-nginx namespace: foo Before applying this Subscription, please modify examples/applications/subscription.yaml with your clusterID. Clusternet also provides a two-stage priority based override strategy. You can define namespace-scoped Localization and cluster-scoped Globalization with priorities (ranging from 0 to 1000, default to be 500), where lower numbers are considered lower priority. These Globalization(s) and Localization(s) will be applied by order from lower priority to higher. That means override values in lower Globalization will be overridden by those in higher Globalization. Globalization(s) come first and then Localization(s). “Clusternet”还提供了一个基于优先级的两阶段覆盖策略。您可以定义具有优先级的名称空间作用域的“本地化”和集群作用域的“全球化”(范围从0到1000，默认为500)，其中较低的数字被认为是较低的优先级。这些全球化和本地化将按优先级从低到高的顺序应用。这意味着较低“全球化”中的覆盖值将被较高“全球化”中的覆盖值所覆盖。首先是全球化，然后是本地化。 💫 💫 For example, Globalization (priority: 100) -> Globalization (priority: 600) -> Localization (priority: 100) -> Localization (priority 500) Meanwhile, below override policies are supported, ApplyNow will apply overrides for matched objects immediately, including those are already populated. Default override policy ApplyLater will only apply override for matched objects on next updates (including updates on Subscription, HelmChart, etc) or new created objects. Before applying these Localization(s), please modify examples/applications/localization.yaml with your ManagedCluster namespace, such as clusternet-5l82l. After installing kubectl plugin kubectl-clusternet, you could run below commands to distribute this application to child clusters. $ kubectl clusternet apply -f examples/applications/ helmchart.apps.clusternet.io/mysql created namespace/foo created deployment.apps/my-nginx created service/my-nginx-svc created subscription.apps.clusternet.io/app-demo created $ # or $ # kubectl-clusternet apply -f examples/applications/ Then you can view the resources just created, $ # list Subscription $ kubectl clusternet get subs -A NAMESPACE NAME AGE default app-demo 6m4s $ kubectl clusternet get chart NAME CHART VERSION REPO STATUS AGE mysql mysql 8.6.2 https://charts.bitnami.com/bitnami Found 71s $ kubectl clusternet get ns NAME CREATED AT foo 2021-08-07T08:50:55Z $ kubectl clusternet get svc -n foo NAME CREATED AT my-nginx-svc 2021-08-07T08:50:57Z $ kubectl clusternet get deploy -n foo NAME CREATED AT my-nginx 2021-08-07T08:50:56Z Clusternet will help deploy and coordinate applications to multiple clusters. You can check the status by following commands, $ kubectl clusternet get mcls -A NAMESPACE NAME CLUSTER ID SYNC MODE KUBERNETES READYZ AGE clusternet-5l82l clusternet-cluster-hx455 dc91021d-2361-4f6d-a404-7c33b9e01118 Dual v1.21.0 true 5d22h $ # list Descriptions $ kubectl clusternet get desc -A NAMESPACE NAME DEPLOYER STATUS AGE clusternet-5l82l app-demo-generic Generic Success 2m55s clusternet-5l82l app-demo-helm Helm Success 2m55s $ kubectl describe desc -n clusternet-5l82l app-demo-generic ... Status: Phase: Success Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfullyDeployed 2m55s clusternet-hub Description clusternet-5l82l/app-demo-generic is deployed successfully $ # list Helm Release $ # hr is an alias for HelmRelease $ kubectl clusternet get hr -n clusternet-5l82l NAME CHART VERSION REPO STATUS AGE helm-demo-mysql mysql 8.6.2 https://charts.bitnami.com/bitnami deployed 2m55s You can also verify the installation with Helm command line in your child cluster, $ helm ls -n abc NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION helm-demo-mysql abc 1 2021-07-06 14:34:44.188938 +0800 CST deployed mysql-8.6.2 8.0.25 📌 📌 Note: Admission webhooks could be configured in parent cluster, but please make sure that dry-run mode is supported in these webhooks. At the same time, a webhook must explicitly indicate that it will not have side-effects when running with dryRun. That is sideEffects must be set to None or NoneOnDryRun. While, these webhooks could be configured per child cluster without above limitations as well. 优点 一站式管理各类 Kubernetes 集群 Clusternet 支持 Pull 模式和 Push 模式管理集群。即使集群运行在 VPC 内网中、边缘或防火墙后时，Clusternet 仍可建立网络隧道连接管理集群。 支持跨集群的服务发现及服务互访 在无专网通道的情况下，仍可提供跨集群的访问路由。 完全兼容原生 Kubernetes API 完全兼容 Kubernetes 的标准 API，比如：Deployment，StatefulSet，DaemonSet，同时也包括用户自定义的 CRD 等，用户从单集群应用升级到多集群只需做简单的配置，无需学习复杂的多集群 API。 支持部署 Helm Chart、Kubernetes 原生的应用以及自定义的 CRD 支持 Helm chart 类型应用，包括 Chart 的分发、差异化配置、状态的汇聚等，和原生 Kubernetes API 的能力一致。 丰富、灵活的配置管理 提供了多种类型的配置策略，用户可灵活的搭配这些配置来实现复杂的业务场景，比如多集群灰度发布。 Addon 能力，架构简单 采用了 Aggregated ApiServer 的方式，且不依赖额外的存储，架构简单，便于部署，大大降低了运维复杂度。 便捷接入 Clusternet 提供了完善的对接能力，支持 kubectl plugin[1] 以及 client-go[2]，方便业务一键接入，具备管理多集群的能力。 实现原理 略 link https://mp.weixin.qq.com/s/y8z8VYk-K28M99uC_bqwRQ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/多集群管理/hello.html":{"url":"blog/kubernetes/多集群管理/hello.html","title":"Hello","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/灰度发布/flagger 架构.html":{"url":"blog/kubernetes/灰度发布/flagger 架构.html","title":"Flagger 架构","keywords":"","body":"[toc] flagger 需求分析 1. 业务场景 前提条件 组件介绍 业务逻辑 设计缺陷 改进建议 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/灰度发布/flagger.html":{"url":"blog/kubernetes/灰度发布/flagger.html","title":"Flagger","keywords":"","body":"简介 Flagger是一个能使运行在k8s体系上的应用发布流程全自动(无人参与)的工具, 它能减少发布的人为关注时间, 并且在发布过程中能自动识别一些风险(例如:RT,成功率,自定义metrics)并回滚. 简单介绍下上图含义: • primary service: 服务稳定版本. 可以理解为已发布在线的服务 • canary service: 即将发布的新版本服务. • Ingress: 服务网关. • Flagger: 会通过flagger spec(下面会介绍), 以ingress/service mesh的规范来调整primary和canary的流量策略.以此来达到A/B testing, blue/green, canary(金丝雀)发布效果. 在调整流量过程中, 根据prometheus采集的各项指标(RT,成功率等)来决策是否回滚发布或者继续调整流量比例。在此过程中,用户可以自定义是否人工干预,审核,收到通知等. User create canary， ingress(podInfo) flagger initialize create podinfo-svc create podinfo-svc-canary create podinfo-svc-primary create deployment podinfo-primary flagger create ingress podinfo-canary flagger modify ingress podinfo-canary annotation weight // create primary workload err = canaryController.Initialize(cd) if err != nil { c.recordEventWarningf(cd, \"%v\", err) return } // change the apex service pod selector to primary if err := kubeRouter.Reconcile(cd); err != nil { c.recordEventWarningf(cd, \"%v\", err) return } // Initialize creates or updates the primary and canary services to prepare for the canary release process targeted on the K8s service func (c *ServiceController) Initialize(cd *flaggerv1.Canary) (err error) { targetName := cd.Spec.TargetRef.Name primaryName := fmt.Sprintf(\"%s-primary\", targetName) canaryName := fmt.Sprintf(\"%s-canary\", targetName) svc, err := c.kubeClient.CoreV1().Services(cd.Namespace).Get(context.TODO(), targetName, metav1.GetOptions{}) if err != nil { return fmt.Errorf(\"service %s.%s get query error: %w\", primaryName, cd.Namespace, err) } if err = c.reconcileCanaryService(cd, canaryName, svc); err != nil { return fmt.Errorf(\"reconcileCanaryService failed: %w\", err) } if err = c.reconcilePrimaryService(cd, primaryName, svc); err != nil { return fmt.Errorf(\"reconcilePrimaryService failed: %w\", err) } return nil } // Reconcile creates or updates the main service func (c *KubernetesDefaultRouter) Reconcile(canary *flaggerv1.Canary) error { apexName, _, _ := canary.GetServiceNames() // main svc err := c.reconcileService(canary, apexName, fmt.Sprintf(\"%s-primary\", c.labelValue), canary.Spec.Service.Apex) if err != nil { return fmt.Errorf(\"reconcileService failed: %w\", err) } return nil } func (c *KubernetesDefaultRouter) reconcileService(canary *flaggerv1.Canary, name string, podSelector string, metadata *flaggerv1.CustomMetadata) error { portName := canary.Spec.Service.PortName if portName == \"\" { portName = \"http\" } targetPort := intstr.IntOrString{ Type: intstr.Int, IntVal: canary.Spec.Service.Port, } if canary.Spec.Service.TargetPort.String() != \"0\" { targetPort = canary.Spec.Service.TargetPort } // set pod selector and apex port svcSpec := corev1.ServiceSpec{ Type: corev1.ServiceTypeClusterIP, Selector: map[string]string{c.labelSelector: podSelector}, Ports: []corev1.ServicePort{ { Name: portName, Protocol: corev1.ProtocolTCP, Port: canary.Spec.Service.Port, TargetPort: targetPort, }, }, } // set additional ports for n, p := range c.ports { cp := corev1.ServicePort{ Name: n, Protocol: corev1.ProtocolTCP, Port: p, TargetPort: intstr.IntOrString{ Type: intstr.Int, IntVal: p, }, } svcSpec.Ports = append(svcSpec.Ports, cp) } if metadata == nil { metadata = &flaggerv1.CustomMetadata{} } if metadata.Labels == nil { metadata.Labels = make(map[string]string) } metadata.Labels[c.labelSelector] = name if metadata.Annotations == nil { metadata.Annotations = make(map[string]string) } // create service if it doesn't exists svc, err := c.kubeClient.CoreV1().Services(canary.Namespace).Get(context.TODO(), name, metav1.GetOptions{}) if errors.IsNotFound(err) { svc = &corev1.Service{ ObjectMeta: metav1.ObjectMeta{ Name: name, Namespace: canary.Namespace, Labels: metadata.Labels, Annotations: filterMetadata(metadata.Annotations), OwnerReferences: []metav1.OwnerReference{ *metav1.NewControllerRef(canary, schema.GroupVersionKind{ Group: flaggerv1.SchemeGroupVersion.Group, Version: flaggerv1.SchemeGroupVersion.Version, Kind: flaggerv1.CanaryKind, }), }, }, Spec: svcSpec, } _, err := c.kubeClient.CoreV1().Services(canary.Namespace).Create(context.TODO(), svc, metav1.CreateOptions{}) if err != nil { return fmt.Errorf(\"service %s.%s create error: %w\", svc.Name, canary.Namespace, err) } c.logger.With(\"canary\", fmt.Sprintf(\"%s.%s\", canary.Name, canary.Namespace)). Infof(\"Service %s.%s created\", svc.GetName(), canary.Namespace) return nil } else if err != nil { return fmt.Errorf(\"service %s get query error: %w\", name, err) } // update existing service pod selector and ports if svc != nil { sortPorts := func(a, b interface{}) bool { return a.(corev1.ServicePort).Port 0 { svcSpec.Ports[i].NodePort = port.NodePort break } } } updateService := false svcClone := svc.DeepCopy() portsDiff := cmp.Diff(svcSpec.Ports, svc.Spec.Ports, cmpopts.SortSlices(sortPorts)) selectorsDiff := cmp.Diff(svcSpec.Selector, svc.Spec.Selector) if portsDiff != \"\" || selectorsDiff != \"\" { svcClone.Spec.Ports = svcSpec.Ports svcClone.Spec.Selector = svcSpec.Selector updateService = true } // update annotations and labels only if the service has been created by Flagger if _, owned := c.isOwnedByCanary(svc, canary.Name); owned { if svc.ObjectMeta.Annotations == nil { svc.ObjectMeta.Annotations = make(map[string]string) } if diff := cmp.Diff(filterMetadata(metadata.Annotations), svc.ObjectMeta.Annotations); diff != \"\" { svcClone.ObjectMeta.Annotations = filterMetadata(metadata.Annotations) updateService = true } if diff := cmp.Diff(metadata.Labels, svc.ObjectMeta.Labels); diff != \"\" { svcClone.ObjectMeta.Labels = metadata.Labels updateService = true } } if updateService { if svcClone.ObjectMeta.Annotations == nil { svcClone.ObjectMeta.Annotations = make(map[string]string) } svcClone.ObjectMeta.Annotations = filterMetadata(svcClone.ObjectMeta.Annotations) _, err = c.kubeClient.CoreV1().Services(canary.Namespace).Update(context.TODO(), svcClone, metav1.UpdateOptions{}) if err != nil { return fmt.Errorf(\"service %s update error: %w\", name, err) } c.logger.With(\"canary\", fmt.Sprintf(\"%s.%s\", canary.Name, canary.Namespace)). Infof(\"Service %s updated\", svc.GetName()) } } return nil } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/版本特性/1.17.html":{"url":"blog/kubernetes/版本特性/1.17.html","title":"1.17","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/版本特性/1.19.html":{"url":"blog/kubernetes/版本特性/1.19.html","title":"1.19","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/版本特性/1.20.html":{"url":"blog/kubernetes/版本特性/1.20.html","title":"1.20","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/版本特性/1.21.html":{"url":"blog/kubernetes/版本特性/1.21.html","title":"1.21","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/版本特性/1.22.html":{"url":"blog/kubernetes/版本特性/1.22.html","title":"1.22","keywords":"","body":"k8s Version 1.22 Change https://help.aliyun.com/document_detail/390170.html 版本升级说明 ACK针对Kubernetes 1.22版本提供了组件的升级和优化。 核心组件 版本号 升级注意事项 Kubernetes 1.22.3-aliyun.1 Kubernetes 1.22版本弃用了大量测试（Beta）版本的API，在操作集群升级之前，请注意：通过测试版本API创建的资源（集群内已经存在的资源），不受升级影响。升级后，可以使用稳定版本API进行交互。与测试版本API交互的控制器或应用，需要在集群升级前完成调整，以免集群升级后控制器或应用无法正常工作。更多信息，请参见版本解读。Kubernetes 1.22版本进行了大量优化。更多信息，请参见版本解读。Dockershim从Kubernetes 1.20版本开始废弃，计划在Kubernetes 1.24版本移除，目前仍然可用，过渡建议如下：推荐新建的节点使用ContainerD作为容器运行时。由于ContainerD Windows功能受限，若您使用的是Windows容器，建议您继续使用Docker EE。升级到1.24版本之前，请做好节点运行时的调整。更多信息，请参见版本解读。容器组安全策略（PodSecurityPolicy）从Kubernetes 1.21版本开始废弃，计划在Kubernetes 1.25版本移除，目前仍然可用。推荐使用容器组安全准入（PodSecurity）替代。更多信息，请参见版本解读。 etcd 3.5.1 无 CoreDNS 1.8.4.1-3a376cc-aliyun 此次升级不会对业务造成影响，支持的新特性如下：支持EndpointSlices资源的监听。支持以IPv6地址进行DNS查询。 CRI Docker CE 19.03.15Docker EE（跟随Windows系统） 无 ContainerD 1.4.8 无 CSI 1.20.7-aafce42-aliyun 无 CNI Flannel 0.15.1.4-e02c8f12-aliyun 此次升级不会对业务造成影响，支持的新特性如下：更新Authorization等资源的APIVersion，以支持Kubernetes 1.22版本。支持以HostPort方式暴露服务。支持开启Hairpin模式。 Terway 无 NVIDIA Container Runtime 3.7.0 无 Ingress Controller 1.1.0-aliyun.1 Nginx Ingress Controller 0.44.0及以下版本使用v1beta1版本的Ingress API，因此无法在1.22版本集群中正常运行，您需要先在1.20版本集群中升级Nginx Ingress Controller到1.1.0版本，再进行集群的升级。组件升级可能会造成业务的瞬断和配置兼容性问题，请您在组件升级过程中充分验证无误后，再进行集群的升级。 版本解读 资源变更和废弃 【变更】MutatingWebhookConfiguration和ValidatingWebhookConfiguration资源不再支持admissionregisration.k8s.io/v1beta1 API。如果使用旧版本API创建准入或变换Webhook配置，会导致配置创建失败，进而影响Webhook服务的使用，请尽快使用admissionregisration.k8s.io/v1替代。 【变更】CustomResourceDefinition资源不再支持apiextensions.k8s.io/v1beta1 API。如果使用旧版本API创建自定义资源定义，会导致定义创建失败，进而影响调和（reconcile）该自定资源的控制器，请尽快使用apiextensions.k8s.io/v1替代。 【变更】APIService资源不再支持apiregistration.k8s.io/v1beta1API。如果使用旧版本API管理Kubernetes扩展API服务，会影响Kubernetes扩展API的服务，请尽快使用apiregistration.k8s.io/v1替代。 【变更】TokenReview资源不再支持authentication.k8s.io/v1beta1 API。如果使用旧版本API进行授权的验证，会导致验证失败，进而影响应用的正常工作，请尽快使用authentication.k8s.io/v1替代。 【变更】SubjectAccessReview资源不再支持authorization.k8s.io/v1beta1 API。如果使用旧版本API进行授权的验证，会导致验证失败，进而影响应用的正常工作，请尽快使用authorization.k8s.io/v1替代。 【变更】CertificateSigningRequest资源不再支持certificate.k8s.io/v1beta1 API。如果使用旧版本API请求签发证书，会导致签发失败，请使用certificate.k8s.io/v1替代。 【变更】Lease资源不再支持coordination.k8s.io/v1beta1 API。如果使用旧版本API进行选主操作，会导致选主失败，进而影响应用的正常工作，请尽快使用coordination.k8s.io/v1替代。 【变更】Ingress和IngressClass资源不再支持networking.k8s.io/v1beta1和extensions/v1beta1 API。如果使用旧版本API管理Ingress，会影响应用对外暴露服务，请尽快使用networking.k8s.io/v1替代。 【变更】ClusterRole、ClusterRoleBinding、Role和RoleBinding资源不再支持rbac.authorization.k8s.io/v1beta1 API。如果使用旧版本API管理RBAC资源，会影响应用的权限服务，甚至无法在集群内正常使用，请尽快使用rbac.authorization.k8s.io/v1替代。 【变更】CSIDriver、CSINode、StorageClass和VolumeAttachment资源不再支持storage.k8s.io/v1beta1 API。如果使用旧版本API管理CSI资源，会影响集群内的存储服务提供，导致集群内无法正常使用CSI，请尽快使用storage.k8s.io/v1替代。 【变更】PriorityClass资源不再支持scheduling.k8s.io/v1beta1 API。如果使用旧版本API管理集群内的容器组优先类型，会导致操作失败，请使用scheduling.k8s.io/v1替代。 【废弃】Dockershim目前被标记为废弃，计划于1.24版本后移除。更多信息，请参见 EP-2221 和 cri-containerd 。 在升级到1.24版本之前，请参考以下步骤，做好节点运行时的调整： 根据容器组数量，规划好非Docker运行时的节点容量（规格、数目）。 选择业务低峰期，扩容相应容量的节点。 逐一对使用Docker作为运行时的节点做排水操作。每次排完一个节点，请确认业务容器组都恢复后，再继续操作下一节点的排水。 待所有Docker运行时的节点都结束排水，且无业务容器组运行，再进行最后的移除。 特性增强 1.21版本后，默认开启不可修改ConfigMap和Secret （ImmutableEphemeralVolumes）特性。通过标记ConfigMap和Secret为不可修改，可以显著的降低APIServer的压力。更多信息，请参见Secret和ConfigMap。 1.21版本后，默认开启IPv4/IPv6双栈（IPv6DualStack）特性。除了在创建集群时配置正确的IPv4和IPv6无类别域间路由外，还需要集群内安装支持双栈的CNI插件。更多信息，请参见IPv4/IPv6双协议栈。 1.21版本后，默认开启节点优雅下线（GracefulNodeShutdown）特性。该特性目前仅支持Linux节点，在Kubelet感知节点将要（主动进入）停止并且在特定的停止周期（Shutdown Period）内完成对Pod的下线。更多信息，请参见节点。 1.21版本后，默认开启快速恢复监听缓存（EfficientWatchResumption）的特性。在APIServer发生重启后，将更高效的恢复监听型的缓存，更好的支持大规模的集群。更多信息，请参见KEP-1904。 1.22版本后，默认开启CSI存储容量（CSIStorageCapacity）特性。基于该特性，调度器（Kube Scheduler） 可以比较卷（Volume）的大小和节点上的存储容量，有助于更快的调度使用对应卷的容器组。更多信息，请参见存储容量。 1.22版本后，默认开启可配置守护进程集的滚动更新峰值（DaemonSetUpdateSurge）特性。对支持滚动更新的守护进程集的进行滚动更新时，可以声明.spec.strategy.rollingUpdate.maxSurge来指定更新过程的最大更新峰值。更多信息，请参见Deployments。 1.22版本后，默认开启并行任务支持索引（IndexedJob）特性。在任务（Job）中声明.spec.completionMode为Indexed，即可在运行的容器组（Pod）的中得到一个新注解（Annotation）batch.kubernetes.io/job-completion-index，同时容器会被注入一个新的环境变量（Environment Variable）JOB_COMPLETION_INDEX。更多信息，请参见Kubernetes。 1.22版本后，默认开启内存管理（MemoryManager）特性。目前该特性仅适用于Linux节点，实现NUMA感知的内存管理，对有内存使用质量保证要求的应用，能带来显著的使用性能提高。ACK 暂不配置任何和该特性相关的内存预留值。更多信息，请参见运行时的内存映射和使用NUMA感知内存管理器。 1.22版本后，默认开启容器组亲和性配置命名空间选择器（PodAffinityNamespaceSelector）特性。容器组的亲和性策略不再局限于同命名空间（Namespace）的标签选择，可以进行跨命名空间（Namespace）的标签选择，实现更完善的亲和性调度策略。更多信息，请参见KEP-2249。 1.22版本后，默认开启容器组删除开销（PodDeletionCost）特性。容器组可以根据其利用率，调整对应的删除开销，使利用率越低的容器组得到更低的删除开销。更多信息，请参见ReplicaSet。 1.22版本后，默认开启可配置容器组调度抢占提名节点（PreferNominatedNode）特性。调度器（Kube Scheduler）会优先尝试调度容器组到被提名的节点。只有被提名（Nominated）节点不符合被调度的要求时，调度器才会启动对剩余节点的评估流程。更多信息，请参见KEP-1923。 1.22版本后，默认开启可配置探针级别的优雅终止期限（ProbeTerminationGracePeriod）特性。该特性只能用在存活针中，配置探针级别（Pod-level）的teminationGracePeriodSeconds时长，缩短容器组失败后等待重启的时间。更多信息，请参见配置存活、就绪和启动探测器。 1.22版本后，默认开启网络策略末端端口可配置（NetworkPolicyEndPort）特性。基于该特性，可以配置网络策略（NetworkPolicy）来支持范围性的端口值。更多信息，请参见网络策略。 1.22版本后，默认开启基于对数化比较增加副本缩容的随机性（LogarithmicScaleDown）特性。基于该特性，可以增强容器组被缩容的随机性，缓解由于容器组拓扑分布约束带来的问题。更多信息，请参见在按比例缩小时应考虑Pod拓扑扩展约束和KEP-2185。 1.22版本后，默认开启支持任务挂起（SuspendJob）特性。基于该特性，可以更好的控制任务的生命周期，例如一个正在运行的任务挂起，之后再恢复执行。更多信息，请参见介绍暂停的工作。 1.22版本后，默认开启配置服务内部流量策略（ServiceInternalTrafficPolicy）特性。基于该特性，可以设置服务将内部流量路由到当前节点上就绪的端点（Local），或路由到集群范围（Cluster）的所有就绪端点。更多信息，请参见服务。 1.22版本后，默认开启配置服务负载均衡类型（ServiceLoadBalancerClass）特性，实现自定义的负载均衡。更多信息，请参见设置负载均衡器实现的类别。 1.22版本后，默认开启支持配置负载均衡类型的服务不分配节点端口（ServiceLBNodePortControl）特性。该特性适用于直接将流量路由到容器组的场景，基于该特性，可以通过配置类型为负载均衡的.spec.allocateLoadBalancerNodePorts为false，从而禁用节点端口的分配。更多信息，请参见设置负载均衡器实现的类别。 1.22版本后，默认开启可配置内存卷大小（SizeMemoryBackedVolumes）特性。该特性目前仅支持Linux节点，基于该特性，可以显式地通过emptyDir.sizeLimit定义需要的目录大小，提高容器组调度的透明性。更多信息，请参见：KEP-1967。 1.22版本后，默认开启服务端应用（Server-side Apply）特性。该特性可以更便捷的了解一个资源中字段变更的来源、时间和操作等。更多信息，请参见服务器端申请。 1.22版本后，CSI接口对Windows容器的支持进入稳定阶段。在不支持特权容器的操作系统（例如Windows Server 2019、WindowsServer Core version 2004等）上，Windows容器可以通过基于CSI代理操纵主机上的存储模块。由于该特性需要兼容CSI插件，请确认CSI插件的情况，再使用该功能。更多信息，请参csi-代理。 1.22版本后，默认开启请求签发更短时效证书（CSRDuration）特性。基于该特性，当提交的证书签发请求（CertificateSigningReqeust, CSR）内声明.spec.expirationSeconds时，证书签发的时效由该声明数值和控制管理器的参数--cluster-signing-duration的较小值决定。ACK 默认配置控制管理器为10年。更多信息，请参见签名者。 特性引入 1.21版本后，引入持久卷（PersistentVolume）健康度监控（Health Monitor）特性。持久卷健康度监控可以帮助工作负载（Workload）感知持久卷的健康程度，从而保证数据不会从受损的持久卷中读出或者写入。ACK默认开启CSI卷健康探测特性。由于该特性需要CSI插件的支持，只有使用的CSI插件支持该特性，才能正常使用该功能。更多信息，请参见卷健康监测。 1.22版本后，引入基于cgroups v2实现内存资源的服务质量（Quality of Service）保证特性。当资源使用紧张时（例如突发性的大资源量申请），CPU资源可以通过分配限速来提高资源的可用性，但无法实现内存资源的分配限速。为了支持内存资源的分配限速，Linux内核社区在cgroups v2中对相关接口进行了优化调整。ACK默认开启内存服务质量保证特性。由于该特性需要操作系统的内核支持，仅支持Linux节点，只有加入的节点支持该特性，才能正常使用该功能。更多信息，请参见cgroup v1接口支持memcg QoS功能和2570-memory-qos。 1.22版本后，引入基于主机进程容器（HostProcess containers）实现Windows特权（Privileged）容器的特性。ACK默认开启Windows主机进程容器特性。由于该特性需要操作系统的内核支持，只有加入的节点支持该特性，才能被正常使用。更多信息，请参见Windows Server 2022上的Windows容器的新增功能和创建Windows HostProcess Pod。 1.22版本后，引入工作负载（Workload）可使用节点交换内存特性，仅支持Linux节点。针对有对交换内存使用诉求的场景，例如节点管理员希望通过交换内存获得节点性能的调整和减少由于内存竞争带来的稳定性问题，应用开发者开发的应用可以通过交互内存获得更好的性能等。ACK暂不开启交换内存的特性。更多信息，请参见交换内存管理和KEP-2400。 1.22版本后，引入为工作负载配置默认seccomp配置特性，仅支持Linux节点。开启该特性后，将使用RuntimeDefault策略作为默认的seccomp配置。但由于某些工作负载可能相比其他工作负载需要更少的系统调用限制，启用默认配置可能会导致运行失败。ACK暂不开启seccomp默认配置的特性。更多信息，请参见启用RuntimeDefault作为所有工作负载的默认seccomp配置文件。 特性更替 1.21版本后，容器组安全策略（PodSecurityPolicy，PSP）进入废弃阶段，计划在1.25版本完全移除该资源定义。ACK默认开启容器组安全特性。您可以在1.22版本中逐步更换替代已有的PSP资源。更多信息，请参见Pod安全准入和PodSecurityPolicy弃用：过去、现在和未来。 1.21版本后，废弃通过在服务（Service）中描述拓扑关键字（topologyKeys）来实现的服务流量拓扑感知（ServiceTopology）特性，由拓扑感知提示特性替代。ACK默认不开启服务流量拓扑感知特性。如果已经开启该特性，可以在1.22版本中同时开启拓扑感知提示特性，并在该版本中逐步替换使用新的特性。更多信息，请参见使用拓扑键进行拓扑感知流量路由和拓扑感知提示。 ACK对Kubernetes 1.22版本的增强 可观测性 丰富了APIServer进行访问请求的指标信息，提高APIServer的可观测性。 对于ACK Pro版、ASK Pro版或边缘Pro版集群，可透出托管面组件的核心指标，提高托管面核心部件的可观测性。 稳定性 对于所有集群类型： 增加对存储层的保护，降低冷启动时对etcd的冲击。 可根据请求的来源、类型或路由的组合，开启APIServer的限流操作，降低冷启动时对APIServer的冲击。 性能优化 Kubelet：在原地升级kubelet时，最大程度保证不重启容器。更多信息，请参见kubelet计算容器是否发生变化会导致集群范围的中断。 KubeProxy：兼容Aliyun Linux2 （kernel-4.19.91-23）及之上的版本，在开启IPVS模式时，不设置conn_reuse_mode为0。更多信息，请参见[ipvs]在 Linux内核版本>=v5.9上设置conn_reuse_mode=1。 ASK集群：在Virtual Node未就绪时，不主动驱逐ECI Pod，减少业务损失。 ACK Pro版或边缘Pro版：调度器增强，支持Gang Scheduling、CPU拓扑感知、GPU拓扑感知等调度增强。更多信息，请参见ACK Pro版集群概述。 参考链接 CHANGELOG-1.21.md CHANGELOG-1.22.md Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/版本特性/1.23.html":{"url":"blog/kubernetes/版本特性/1.23.html","title":"1.23","keywords":"","body":" 「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。 Kubernetes v1.23 即将发布，这是 2021 年发布的第三个版本，也是今年最后一个正式发布的版本。 此版本中主要包括 47 项增强更新，其中 11 项达到 stable, 17 项达到 beta 还有 19 项达到 alpha 。 当然，也有 1 项被标记为废弃。相比于 v1.22 从数量上来说是少了一点（v1.22 有 53 项增强更新），但这并不影响这是一个很棒的版本！ 在 Kubernetes 的发布周期变更为 每4个月一个版本 后，很明显的感觉就是不用在升级上面花费太多时间了，毕竟 Kubernetes 的升级操作是个体力活，大家觉得呢？ 我们一起来看看这个版本中有哪些值得关注的变更吧！ 新增 kubectl alpha events 命令 在之前的 《K8S 生态周报| Helm 新版本发布增强对 OCI 的支持》 文章的上游进展中我曾为大家介绍了该功能。它是按照 KEP #1440 实施的。 增加此命令主要是由于在不修改 kubectl get 的前提下，查看 event 有一些限制，所以直接增加 kubectl events 命令可以更方便的去获取到需要的信息，尤其是 event 是在 Kubernetes 中经常需要查看的一个信息。kubectl get events 比较典型的一些问题, 比如排序（虽然可以通过加参数解决）， watch，以及无法按照时间线方式去查看 events 等。 我们来看看这个命令具体如何使用。 我们先来创建两个 Pod，分别叫 redis 和 redis2 。 (MoeLove) ➜ kubectl run redis --image=\"ghcr.io/tao12345666333/redis:alpine\" pod/redis created (MoeLove) ➜ kubectl run redis2 --image=\"ghcr.io/tao12345666333/redis:alpine\" pod/redis2 created (MoeLove) ➜ kubectl get pods NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 12m redis2 1/1 Running 0 2m23s 执行 kubectl alpha events 可以看到当前 namespace 下的所有 events 。如果增加 --for 条件可以用来筛选只展示特定资源相关的 events 。同时 默认情况下就是按时间排序的 (MoeLove) ➜ kubectl alpha events LAST SEEN TYPE REASON OBJECT MESSAGE 12m Normal Scheduled Pod/redis Successfully assigned default/redis to kind-control-plane 12m Normal Pulling Pod/redis Pulling image \"ghcr.io/tao12345666333/redis:alpine\" 12m Normal Pulled Pod/redis Successfully pulled image \"ghcr.io/tao12345666333/redis:alpine\" in 4.028873745s 12m Normal Created Pod/redis Created container redis 12m Normal Started Pod/redis Started container redis 3m5s Normal Scheduled Pod/redis2 Successfully assigned default/redis2 to kind-control-plane 3m5s Normal Pulled Pod/redis2 Container image \"ghcr.io/tao12345666333/redis:alpine\" already present on machine 3m4s Normal Created Pod/redis2 Created container redis2 3m4s Normal Started Pod/redis2 Started container redis2 (MoeLove) ➜ kubectl alpha events --for pod/redis2 LAST SEEN TYPE REASON OBJECT MESSAGE 3m23s Normal Scheduled Pod/redis2 Successfully assigned default/redis2 to kind-control-plane 3m23s Normal Pulled Pod/redis2 Container image \"ghcr.io/tao12345666333/redis:alpine\" already present on machine 3m22s Normal Created Pod/redis2 Created container redis2 3m22s Normal Started Pod/redis2 Started container redis2 IPv4/IPv6 双栈支持达到 GA 在配置双栈网络的 Kubernetes 时，需要同时指定 --node-cidr-mask-size-ipv4 和 --node-cidr-mask-size-ipv6 以便于设置每个 Node 上的子网大小。在此之前我们都是直接使用 --node-cidr-mask-size 进行设置即可。 如果我们仍然使用单栈 Kubernetes 集群的话，正常来说不需要做什么调整，当然我们也可以使用上面提到的选项，来单独设置集群的 IPv4/IPv6 子网。 PodSecurity Admission 达到 Beta PodSecurity Admission 是之前的 PSP 的代替，关于 Kubernetes Admission 可以参考我之前的文章 《理清 Kubernetes 中 Admission 机制》，这里就不展开了。 IngressClass 支持 namespace 级别的参数 IngressClass.Spec.Parameters.Namespace 字段当前达到 GA ，这样我们就可以为 IngressClass 设置参数为 namespace 级别了。比如： apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: external-lb spec: controller: example.com/ingress-controller parameters: apiGroup: k8s.example.com kind: IngressParameters name: external-lb namespace: external-configuration scope: Namespace Probe 中增加 gRPC 协议的支持 通过 KEP #2727 ，在此版本中为 Pod.Spec.Container.{Liveness,Readiness,Startup} 的 Probe 添加了 gRPC 协议的支持。 例如： readinessProbe: grpc: port: 9090 service: moelove-service initialDelaySeconds: 5 periodSeconds: 10 可通过 GRPCContainerProbe feature gate 开启此特性。具体细节可参考 #106463 新增 OpenAPI V3 这个特性是 Alpha 级别，可通过 OpenApiv3 feature gate 进行开启。 增加此特性主要是由于 CRD 目前可通过 OpenApi V3 进行定义，但是 api-server 目前还不支持。当从 OpenApi V3 转换为 V2 时，部分信息将会丢失。 更多详细信息可参考 KEP #2896 CRD Validation 表达式语言 这是一项 Alpha 级别的特性，默认是不开启的。可通过增加 CustomResourceValidationExpressions feature gate 来进行开启。单独介绍此 Alpha 级别的特性是因为目前基于 Custom Resource Definitions (CRDs) 的方式对 Kubernetes 进行扩展已经成为主流，但是在 CRD 中目前能添加的校验规则有限，更多的场景都需要通过额外的 Admission 来完成。 此功能使用一种叫做 Common Expression Language (CEL) 的语言进行规则定义，通过 x-kubernetes-validation-rules 字段进行规则的添加。 例如，某个 CRDs 的内容如下，其中定义了 minReplicas 小于 replicas 并且 replicas 小于 maxReplicas 。 ... openAPIV3Schema: type: object properties: spec: type: object x-kubernetes-validation-rules: - rule: \"self.minReplicas 那么，当有如下的自定义资源创建时，Kubernetes 将会拒绝其请求。 apiVersion: \"stable.example.com/v1\" kind: CustomDeployment metadata: name: my-new-deploy-object spec: minReplicas: 0 replicas: 20 maxReplicas: 10 并且返回如下错误： The CustomDeployment \"my-new-deploy-object\" is invalid: * spec: Invalid value: map[string]interface {}{\"maxReplicas\":10, \"minReplicas\":0, \"replicas\":20}: replicas should be smaller than or equal to maxReplicas. 这样相比原来我们通过 Admission 的方式来进行校验就会方便的多。关于 Kubernetes Admission 可以参考我之前的文章 《理清 Kubernetes 中 Admission 机制》。 HPA v2 API 达到 GA HPA v2 大约是在 5 年前首次提出，经过这 5 年的发展，终于在现在它达到了 GA 级别。 以上就是关于 Kubernetes v1.23 中我认为值得关注的一些主要特性，更多信息可参阅其 ReleaseNote Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/简化多集群.html":{"url":"blog/kubernetes/简化多集群.html","title":"简化多集群","keywords":"","body":"在Kubernetes中简化多集群 CNCF CNCF;) 今天 客座文章作者：Gianluca Arbezzano，Equinix Metal 软件工程师，CNCF 大使；Alex Palesandro，都灵理工学院研究助理 Kubernetes 集群在组织内部的数量和规模都在增长。这种扩散是由各种原因造成的：可伸缩性问题、地理限制、多提供者策略等等。不幸的是，现有的多集群方法在 pod 放置、集群设置和与新 API 的兼容性方面有很大的局限性。此外，它们需要大量的手动配置。 在第一次 CNCF 都灵 Meetup 上，Alex 和 Mattia 讨论了多集群管理问题，强调了当前方法的局限性。他们讨论了克服当前限制的可能的技术选择，并提出了Liqo[1]中可能的实现，Liqo 是一个通过透明地聚合多个现有集群来动态创建“大集群”的项目。在讨论的最后，他们展示了 Liqo 在云爆发（cloud-bursting）场景中的演示。 介绍——多集群的优点和缺点 Kubernetes 集群在数据中心中非常普遍，不同的区域已经成为现实。在容器化“革命”之后，Kubernetes 近年来已经成为事实上的基础设施管理标准。一方面，K8s 的普遍性是建立在云之上的。越来越多的提供者正在构建和交付作为服务的托管集群。另一方面，K8s 在本地安装（on-premise）也很受欢迎，Kubernetes 丰富的生态系统可以减少与公共云的“目录（catalog）”距离。此外，边缘设置也变得越来越流行：越来越多的项目专注于将 Kubernetes 引入轻量级和地理稀疏的基础设施。 尽管增加了所有的复杂性，但普遍存在的多集群拓扑引入了新的令人兴奋的潜力。这种潜力超越了目前所探索的通过多个集群进行的简单静态应用程序编排。事实上，多集群拓扑对于跨不同位置编排应用程序和统一对基础设施的访问非常有用。其中，这引入了一种令人兴奋的可能性，可以透明而快速地将应用程序从一个集群迁移到另一个集群。在处理集群灾难或关键基础设施干预、扩展或布局优化时，移动工作负载是可行的。 部分分类 多集群拓扑主要引入了两类挑战： 它们需要集群控制平面之间的一种同步形式。 它们需要一种互连形式，使服务可以在不同的集群中访问。 许多项目都解决了多集群问题；在这里，我们总结了最常见的方法。 多集群控制平面 专用 API 服务器 官方的 Kubernetes Cluster Federation（又名KubeFed[2]）就是这种方法的一个例子，它“允许你从一个托管集群中的一组 API 协调多个 Kubernetes 集群的配置”。为此，KubeFed 用一种新的语义扩展了传统的 Kubernetes API，该语义表示应该为特定的部署选择哪些集群（通过“覆盖”和“集群选择器”）。 GitOps GitOps 是一个建立良好的框架来编排 CI/CD 工作流程。其基本思想是使用 git 仓库作为应用程序部署的单一数据源，并更新集群的相应对象。面对多集群拓扑结构，GitOps 可以代表一个基本的多集群控制平面。我们可以举几个 GitOps 工具例子，如FluxCD[3]、Fleet[4]和ArgoCD[5]。 在这样的场景中，应用程序使用合适集群的正确值进行模板化，然后部署到目标集群上。这种方法结合适当的网络互连工具，允许你获得多集群编排，而无需处理额外 API 的复杂性。 然而，GitOps 方法缺乏跨多集群拓扑的动态 pod 放置。它们不支持任何主动灾难恢复策略或跨集群爆发。特别是，不可能跨集群自动迁移工作负载以响应意外故障或快速处理不可阻止的负载峰值。 基于 Virtual Kubelet 的方法 Virtual Kubelet（VK）[6]是一个“Kubernetes Kubelet[7]实现，它伪装成 Kubelet，将 Kubernetes 连接到其他 API”。初始的 VK 实现将远程服务建模为集群的节点，从而在 Kubernetes 集群中引入无服务器计算。后来，VK 在多集群上下文中变得流行起来：VK 提供者可以将远程集群映射到本地集群节点。包括Admiralty[8]、Tensile-kube[9]和 Liqo 在内的几个项目都采用了这种方法。 与专用 API 服务器相比，这种方法有几个优点。首先，它引入了多集群，不需要额外的 API，而且它对应用程序透明。其次，它灵活地将远程集群的资源集成到调度器的可用性中：用户可以以与本地 pod 相同的方式调度远程集群 pod。第三，它使分散治理成为可能。更准确地说，VK 可能不需要远程集群上的特权访问来调度 pod 和其他支持多所有权的 K8s 对象。 网络互连工具 网络互连是多集群拓扑的第二个重要方面。Pod 应该能够与其他集群和服务上的 Pod 无缝通信。集群间连接性可以通过 CNI（负责集群连接性的组件）的扩展，或专用工具提供。 互联工具的关键设计选择主要涉及三个方面：（1）不同集群配置的互操作性；（2）其他集群使用的网络参数的兼容性；（3）如何处理所有集群暴露的服务。 CNI 提供的互连 CiliumMesh[10]是一个 CNI 实现多集群互联的例子。更准确地说，CiliumMesh 扩展了流行的 Cilium CNI 的能力，以“联合”不同集群上的多个 Cilium 实例（ClusterMesh）。Cilium 支持通过隧道或直接路由跨多个 Kubernetes 组的 Pod IP 路由，而不需要任何网关或代理。此外，它还通过标准的 Kubernetes 服务和 coredns 来促进透明的服务发现。 像 CiliumMesh 这样的方法的主要缺点是严格依赖给定的 CNI，即 Cilium。Cilium 必须在两组集群中采用。此外，Cilium 在 pod CIDR 跨集群特性方面有一些关键的要求。 CNI 无感的互连 Submariner[11]支持在不同 Kubernetes 集群中的 Pod 和服务之间直接联网，可以是本地的，也可以是云端的。Submariner 是完全开源的，设计成网络插件（CNI）无感的。Submariner 有一个基于代理的集中式架构，该代理收集关于集群配置的信息并发回参数以供使用。 Submariner 不支持将端点分布在多个集群（多集群服务）中的服务。它提供了一种更直接的发现远程服务的机制，使所有后端 pod 都位于正确的位置。 Skupper[12]是一个七层业务的多集群互联服务。Skupper 通过定义一个特别的虚拟网络基底，实现了 Kubernetes 集群之间的安全通信。与 Submariner 和 Cilium 不同，Skupper 并不引入集群范围内的互连，而是只针对特定的命名空间集。Skupper 在 Skupper 网络中暴露的命名空间中实现了多集群服务。当一个服务被暴露时，Skupper 会创建特定的端点，使它们在整个集群上可用。 服务网格 服务网格框架是专用的基础架构层，用于简化基于微服务的应用程序的管理和配置。服务网格引入了一个边车（sidecar）容器作为代理，以提供多种功能（例如，使用相互 TLS 的安全连接、断路、canary 部署）。 一些最流行的服务网格架构（ISTIO[13]、Linkerd[14]）具有多集群支持，以支持多集群的微服务应用程序。不同集群之间的互连使用一个专用代理将流量从一个集群的网格路由到另一个。类似地，Istio 和 Linkerd 可以跨集群创建一个临时的相互 TLS 隧道，并提供原语来跨集群暴露服务，从而支持诸如跨集群流量分割等特性。 一般来说，服务网格框架中的多集群支持提供了广泛的特性。但是，它们需要许多步骤和几个新的特定 API 来配置以设置拓扑。 Liqo 上述方法的类别有几个局限性。首先，对于其中许多（Kubefed 和 GitOps），pod 的放置是静态的，不可能进行细粒度的集群优化。其次，这些项目要么处理网络平面，要么处理控制平面：这需要第三方工具来处理互连。总的来说，这种分离的方法排除了从现有拓扑中快速插入或删除集群的情况。例如，我们将在后面讨论，Liqo 集成方法支持实现与 CNI 无感的多集群服务支持，其中服务端点使用正确的 IP 地址添加到 K8s 中（即考虑到 natting 规则和网络拓扑）。 Liqo 背后的思想是使多集群拓扑成为集群管理员的单步操作。这是通过结合一种基于 Virtual Kubelet 的方法来处理多集群控制平面和一个与 CNI/IP 配置无感的专用网络结构来获得的。简而言之，Liqo 提供了对集群的统一访问，防止 Kubernetes 用户提前了解多集群拓扑结构。Kubernetes 管理员可以通过添加或删除集群来更改拓扑，而不会影响他们的用户，也可能不会影响正在运行的工作负载。 一方面，动态意味着可以在运行过程中添加和删除集群“同行（peering）”到拓扑上。另一方面，Liqo 通过实现 pod 卸载和依赖于专用 Virtual Kubelet 提供商的多集群服务，为应用程序提供了透明度。 Liqo 与其他项目的主要区别见表 1 和表 2。 表 1 – Comparison of Control-Plane Multi-cluster projects Criteria Liqo Admiralty Tensile-Kube Kubefed ArgoCD Fleet FluxCD Seamless Scheduling Yes Yes Yes No No No No Support for Decentralized Governance Yes Yes Yes No Yes Yes Yes No Need for application extra APIs Yes Yes Yes No No No No Dynamic Cluster Discovery Yes No No No, using Kubefed CLI No No No 表 2 – Comparison of Network Interconnection projects Criteria Liqo Cilium ClusterMesh Submariner Skupper Istio Multi-Cluster Linkerd Multi-cluster Architecture Overlay Network and Gateway Node to Node traffic Overlay Network and Gateway L7 Virtual Network Gateway-based Gateway-based Interconnection Set-Up Peer-To-Peer, Automatic Manual Broker-based, Manual Manual Manual Manual Secure Tunnel Technology Wireguard No IPSec (Strongswan, Libreswan), Wireguard TLS TLS TLS CNI Agnostic Yes No Yes Yes Yes Yes Multi-cluster Services (“East-West”) Yes Yes Limited Yes Yes, with traffic management Yes, with traffic splitting Seamless Cluster Extension Yes Yes Yes No No No Seamless Support for Overlapped IPs Yes No No, it relies on global IPs networking Yes Yes Yes Support for more than 2 clusters In Progress Yes Yes Yes Yes Yes Liqo 的主要特性 Liqo Discovery and Peering Liqo 实现了一种机制，只需一步就可以发现一个集群，并将其与另一个集群匹配（peer）。发现依赖于 DNS SRV 记录（如 SIP）、LAN mDNS 和作为最后手段的手动插入。当发现一个新的集群时，将在集群之间建立一个管理互连，称为 peering，模拟涉及不同互联网运营商之间互连的类似过程。Peering 进程依赖于 P2P 协议，该协议允许在两个集群之间交换参数，以定义安全的网络配置。 Liqo Network Fabric Liqo 组网的基本原则是保持单集群组网的基本功能。特别是，主要的重点是保持直接的 pod 到 pod 的流量，从用户的角度支持透明的扩展。作为服务端点发现的 pod 可以到达，即使它们在另一个集群上，或者它们的地址与“主”集群 pod 地址空间发生冲突。 在底层，通过覆盖网络建立集群互联，将流量路由到远程集群。Liqo 利用了一个“网关”pod，它使用 Wireguard 连接到远程节点。这种架构避免了要求（如在 CiliumMesh 中）让参与集群的所有节点完全可以从另一个集群到达。此外，Liqo 还处理可能重叠的 pod IP 地址，通过双 nat 进行处理。 Liqo 主要独立于连接集群或兼容 POD CIDR 的 CNI。CNI 可以独立选择，Liqo 还支持被管理的集群（即 AKS、GKE）及其网络架构。 Liqo Resource Sharing 在进行匹配（peering）检查之后，一个新节点被添加到集群中。这个虚拟节点将描述另一个可用于调度的集群的 CPU 和内存数量。普通的 Kubernetes 调度器可以直接将 pod 分配给这个创建的节点。匹配的过程定义了节点的大小，实际上引入了去中心化治理的可能性。集群管理员可以调整向其他集群暴露的资源数量。 使用 Liqo，对面向用户的 Kubernetes 没有中断。例如，当用户在 liq 标记的命名空间上部署应用程序时，命名空间内容反映在另一个集群上的孪生命名空间中。更准确地说，在“孪生（twin）”命名空间内，大部分 K8s 对象复制到远程命名空间上。这使得 pod 可以透明地远程执行并访问其配置对象。 这对于服务反射尤其有趣，它实现了“东西”的多集群服务。Pod 可以访问多集群拓扑中的任何位置的服务。在幕后，服务端点由 Liqo VK 操纵，精心设计还考虑 NAT 转换。 最后，Liqo pod 卸载是容忍裂脑的。当 pod 被卸载到远程集群时，它们被包装在 replicaset 对象中。这样，即使与原始集群的连接丢失，卸载的 pod 状态也会继续在远程集群上正确地协调。 未来的工作 Liqo 最近发布了它的第二个主要版本——0.2。下一个版本（v0.3，预计 2021 年 7 月中旬）计划的特性如下： 支持跨两个以上集群的部署：Liqo 提出的无缝集群集成将包含更复杂的拓扑结构，使部署（例如基于微服务的应用程序）能够跨三个或更多集群运行。 支持 Amazon Elastic Kubernetes（EKS）服务 支持对远程集群资源进行更细粒度的权限控制：到目前为止，Liqo 还没有处理权限管理，以限制远程集群上已卸载工作负载的权限。 结论 随着集群数量的增加，多集群拓扑将开始变得越来越流行。 Liqo 提出了一种有趣的方法来简化这个问题，它提供了一种创建虚拟集群抽象的方法，该抽象为集群提供统一和一致的视图，从而简化了多集群拓扑的创建和管理。 参考资料 [1]Liqo: https://liqo.io/[2]KubeFed: https://github.com/kubernetes-sigs/kubefed[3]FluxCD: https://fluxcd.io/[4]Fleet: https://rancher.com/docs/rancher/v2.x/en/deploy-across-clusters/fleet/[5]ArgoCD: https://argoproj.github.io/argo-cd/[6]Virtual Kubelet（VK）: https://virtual-kubelet.io/[7]Kubernetes Kubelet: https://kubernetes.io/docs/reference/generated/kubelet/[8]Admiralty: https://github.com/admiraltyio/admiralty/blob/master/README.md#readme[9]Tensile-kube: https://github.com/virtual-kubelet/tensile-kube/blob/master/README.md#readme[10]CiliumMesh: https://docs.cilium.io/en/v1.9/concepts/clustermesh/[11]Submariner: https://submariner.io/[12]Skupper: https://skupper.io/[13]ISTIO: https://istio.io/[14]Linkerd: https://linkerd.io/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/":{"url":"blog/kubernetes/网络插件/","title":"网络插件","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/Calico 路由反射模式权威指南.html":{"url":"blog/kubernetes/网络插件/Calico 路由反射模式权威指南.html","title":"Calico 路由反射模式权威指南","keywords":"","body":"Calico 路由反射模式权威指南 背景介绍 Calico 作为k8s的一种网络插件，具有很强的扩展性，较优的资源利用和较少的依赖。 为了提高网络的性能和灵活性，需要将k8s的工作节点和物理节点中的leaf交换机简历bgp邻居关系，同步bugp路由信息，可以将pod网路的路由发布到无力网络中。Calico 给出了三种类型的 BGP 互联方案，分别是 Full-mesh、Route reflectors 和 Top of Rack (ToR)。 FUll-mesh 全互联模式，启用了 BGP 之后，Calico 的默认行为是在每个节点彼此对等的情况下创建完整的内部 BGP（iBGP）连接，这使 Calico 可以在任何 L2 网络（无论是公有云还是私有云）上运行，或者说（如果配了 IPIP）可以在任何不禁止 IPIP 流量的网络上作为 overlay 运行。对于 vxlan overlay，Calico 不使用 BGP。 Full-mesh 模式对于 100 个以内的工作节点或更少节点的中小规模部署非常有用，但是在较大的规模上，Full-mesh 模式效率会降低，较大规模情况下，Calico 官方建议使用 Route reflectors。 Route reflectors 如果想构建内部 BGP（iBGP）大规模集群，可以使用 BGP 路由反射器来减少每个节点上使用 BGP 对等体的数量。在此模型中，某些节点充当路由反射器，并配置为在它们之间建立完整的网格。然后，将其他节点配置为与这些路由反射器的子集（通常为冗余，通常为 2 个）进行对等，从而与全网格相比减少了 BGP 对等连接的总数。 Top of Rack (ToR) 在本地部署中，可以将 Calico 配置为直接与物理网络基础结构对等。通常，这需要涉及到禁用 Calico 的默认 Full-mesh 行为，将所有 Calico 节点与 L3 ToR 路由器对等。 本篇文章重点会介绍如何在 BGP 网络环境下配置 Calico 路由反射器，本篇主要介绍将 K8S 工作节点作为路由反射器和物理交换机建立 BGP 连接。配置环境拓扑如下： 在本次环境中，分别有一台 spine 交换机和两台 leaf 交换机来建立 EBGP 连接。所有 leaf 交换机都属于一个独立的自治系统，所有 leaf 交换机下的 node 都属于一个独立的自治系统。Kubernetes 集群节点中每个 leaf 下由两台工作节点作为 CalicoRR（路由反射器），之所以用两台 node 作为路由反射器是考虑冗余性，所有 Calico RR 都跟自己上联的 leaf 交换机建立 EBGP 连接。Calico RR 和自己所属的 node 之间建立 iBGP 连接。 安装 calicoctl Calico RR 所有配置操作都需要通过 calicoctl 工具来完成， calicoctl 允许从命令创建，读取，更新和删除 Calico 对象，所以我们首先需要在 Kubernetes 所有的工作节点上安装 calicoctl 工具。 采用二进制方式安装 calicoctl 工具。 登录到主机，打开终端提示符，然后导航到安装二进制文件位置，一般情况下 calicoctl 安装到 /usr/local/bin/。 使用以下命令下载 calicoctl 二进制文件，版本号选择自己 calico 的版本。 curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.2/calicoctl 将文件设置为可执行文件。 chmod +x calicoctl 每次执行 calicoctl 之前需要设置环境变量。 export DATASTORE_TYPE=kubernetes export KUBECONFIG=~/.kube/config 如果不希望每次执行 calicoctl 之前都需要设置环境变量，可以将环境变量信息写到永久写入到/etc/calico/calicoctl.cfg 文件里，calicoctl.cfg 配置文件编辑如下 apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"kubernetes\" kubeconfig: \"/root/.kube/config\" 关闭 Full-mesh 模式 Calico 默认是 Full-mesh 全互联模式，Calico 集群中的的节点之间都会建立连接，进行路由交换。但是随着集群规模的扩大，mesh 模式将形成一个巨大服务网格，连接数成倍增加。这时就需要使用 Route Reflector（路由器反射）模式解决这个问题。确定一个或多个 Calico 节点充当路由反射器，让其他节点从这个 RR 节点获取路由信息。 关闭 node-to-node BGP 网络，具体操作步骤如下： 添加 default BGP 配置，调整 nodeToNodeMeshEnabled 和 asNumber： [root@node1 calico]# cat bgpconf.yaml apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 64512 直接应用一下，应用之后会马上禁用 Full-mesh， [root@node1 calico]# calicoctl apply -f bgpconf.yaml Successfully applied 1 'BGPConfiguration' resource(s) 查看 bgp 网络配置情况，false 为关闭 [root@node1 calico]# calicoctl get bgpconfig NAME LOGSEVERITY MESHENABLED ASNUMBER default Info false 64512 修改工作节点的calico 配置 通过 calicoctl get nodes --output=wide 可以获取各节点的 ASN 号， [root@node1 calico]# calicoctl get nodes --output=wide NAME ASN IPV4 IPV6 node1 (64512) 172.20.0.11/24 node2 (64512) 172.20.0.12/24 node3 (64512) 172.20.0.13/24 node4 (64512) 173.20.0.11/24 node5 (64512) 173.20.0.12/24 node6 (64512) 173.20.0.13/24 可以看到获取的 ASN 号都是“（64512）”，这是因为如果不给每个节点指定 ASN 号，默认都是 64512。我们可以按照拓扑图配置各个节点的 ASN 号，不同 leaf 交换机下的节点，ASN 号不一样，每个 leaf 交换机下的工作节点都是一个独立自治系统。 通过如下命令，获取工作节点的 calico 配置信息： calicoctl get node node1 -o yaml > node1.yaml 每一个工作节点的 calico 配置信息都需要获取一下，输出为 yaml 文件，“node1”为 calico 节点的名称。 按照如下格式进行修改： [root@node1 calico]# cat node1.yaml apiVersion: projectcalico.org/v3 kind: Node metadata: annotations: projectcalico.org/kube-labels: '{\"beta.kubernetes.io/arch\":\"amd64\",\"beta.kubernetes.io/os\":\"linux\",\"kubernetes.io/arch\":\"amd64\",\"kubernetes.io/hostname\":\"node1\",\"kubernetes.io/os\":\"linux\",\"node-role.kubernetes.io/master\":\"\",\"node-role.kubernetes.io/worker\":\"\",\"rr-group\":\"rr1\",\"rr-id\":\"rr1\"}' creationTimestamp: null labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: node1 kubernetes.io/os: linux node-role.kubernetes.io/master: \"\" node-role.kubernetes.io/worker: \"\" name: node1 spec: bgp: asNumber: 64512 ## asNumber根据自己需要进行修改 ipv4Address: 172.20.0.11/24 routeReflectorClusterID: 172.20.0.11 ## routeReflectorClusterID一般改成自己节点的IP地址 orchRefs: - nodeName: node1 orchestrator: k8s status: podCIDRs: - \"\" - 10.233.64.0/24 为node 节点进行分组（添加label） 为方便让 BGPPeer 轻松选择节点，在 Kubernetes 集群中，我们需要将所有节点通过打 label 的方式进行分组，这里，我们将 label 标签分为下面几种： rr-group 这里定义为节点所属的 Calico RR 组，主要有 rr1 和 rr2 两种，为不同 leaf 交换机下的 Calico RR rr-id 这里定义为所属 Calico RR 的 ID，节点添加了该标签说明该节点作为了路由反射器，主要有 rr1 和 rr2 两种，为不同 leaf 交换机下的 Calico RR Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/calico.html":{"url":"blog/kubernetes/网络插件/calico.html","title":"Calico","keywords":"","body":"1、Calico概述 Calico是Kubernetes生态系统中另一种流行的网络选择。虽然Flannel被公认为是最简单的选择，但Calico以其性能、灵活性而闻名。Calico的功能更为全面，不仅提供主机和pod之间的网络连接，还涉及网络安全和管理。Calico CNI插件在CNI框架内封装了Calico的功能。 Calico是一个基于BGP的纯三层的网络方案，与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。Calico在每个计算节点都利用Linux Kernel实现了一个高效的虚拟路由器vRouter来负责数据转发。每个vRouter都通过BGP1协议把在本节点上运行的容器的路由信息向整个Calico网络广播，并自动设置到达其他节点的路由转发规则。Calico保证所有容器之间的数据流量都是通过IP路由的方式完成互联互通的。Calico节点组网时可以直接利用数据中心的网络结构（L2或者L3），不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。 2 工具 1. 下载二进制文件 wget https://github.com/projectcalico/calicoctl/releases/download/v3.14.1/calicoctl chmod +x calicoctl mv calicoctl /usr/local/bin 2. 添加calicoctl配置文件 calicoctl通过读写calico的数据存储系统（datastore）进行查看或者其他各类管理操作，通常，它需要提供认证信息经由相应的数据存储完成认证。在使用Kubernetes API数据存储时，需要使用类似kubectl的认证信息完成认证。它可以通过环境变量声明的DATASTORE_TYPE和KUBECONFIG接入集群，例如以下命令格式运行calicoctl： [root@k8s-master ~]# DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes NAME k8s-master k8s-node1 k8s-node2 也可以直接将认证信息等保存于配置文件中，calicoctl默认加载 /etc/calico/calicoctl.cfg 配置文件读取配置信息，如下所示： [root@k8s-master ~]# cat /etc/calico/calicoctl.cfg apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"kubernetes\" kubeconfig: \"/root/.kube/config\" [root@k8s-master ~]# calicoctl get nodes NAME k8s-master k8s-node1 k8s-node2 3. 测试calicoctl命令 [root@k8s-master ~]# calicoctl node status Calico process is running. IPv4 BGP status +---------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +---------------+-------------------+-------+----------+-------------+ | 138.138.82.15 | node-to-node mesh | up | 09:03:56 | Established | | 138.138.82.16 | node-to-node mesh | up | 09:04:08 | Established | +---------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. [root@k8s-master ~]# calicoctl get ipPool -o yaml apiVersion: projectcalico.org/v3 items: - apiVersion: projectcalico.org/v3 kind: IPPool metadata: creationTimestamp: 2019-04-28T08:53:12Z name: default-ipv4-ippool resourceVersion: \"1799\" uid: 0df17422-6993-11e9-bde3-005056918222 spec: blockSize: 26 cidr: 192.168.0.0/16 ipipMode: Always natOutgoing: true nodeSelector: all() kind: IPPoolList metadata: …… Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/cilium.html":{"url":"blog/kubernetes/网络插件/cilium.html","title":"Cilium","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/ebpf.html":{"url":"blog/kubernetes/网络插件/ebpf.html","title":"Ebpf","keywords":"","body":"作者 狄卫华 发表于 2020年8月9日 “eBPF 是我见过的 Linux 中最神奇的技术，没有之一，已成为 Linux 内核中顶级子模块，从 tcpdump 中用作网络包过滤的经典 cbpf，到成为通用 Linux 内核技术的 eBPF，已经完成华丽蜕变，为应用与神奇的内核打造了一座桥梁，在系统跟踪、观测、性能调优、安全和网络等领域发挥重要的角色。为 Service Mesh 打造了具备 API 感知和安全高效的容器网络方案 Cilium，其底层正是基于 eBPF 技术” 1. BPF BPF（Berkeley Packet Filter ），中文翻译为伯克利包过滤器，是类 Unix 系统上数据链路层的一种原始接口，提供原始链路层封包的收发。1992 年，Steven McCanne 和 Van Jacobson 写了一篇名为《BSD数据包过滤：一种新的用户级包捕获架构》的论文。在文中，作者描述了他们如何在 Unix 内核实现网络数据包过滤，这种新的技术比当时最先进的数据包过滤技术快 20 倍。BPF 在数据包过滤上引入了两大革新： 一个新的虚拟机 (VM) 设计，可以有效地工作在基于寄存器结构的 CPU 之上； 应用程序使用缓存只复制与过滤数据包相关的数据，不会复制数据包的所有信息。这样可以最大程度地减少BPF 处理的数据； 由于这些巨大的改进，所有的 Unix 系统都选择采用 BPF 作为网络数据包过滤技术，直到今天，许多 Unix 内核的派生系统中（包括 Linux 内核）仍使用该实现。 tcpdump 的底层采用 BPF 作为底层包过滤技术，我们可以在命令后面增加 ”-d“ 来查看 tcpdump 过滤条件的底层汇编指令。 $ tcpdump -d 'ip and tcp port 8080' (000) ldh [12] (001) jeq #0x800 jt 2 jf 12 (002) ldb [23] (003) jeq #0x6 jt 4 jf 12 (004) ldh [20] (005) jset #0x1fff jt 12 jf 6 (006) ldxb 4*([14]&0xf) (007) ldh [x + 14] (008) jeq #0x1f90 jt 11 jf 9 (009) ldh [x + 16] (010) jeq #0x1f90 jt 11 jf 12 (011) ret #262144 (012) ret #0 图 1-1 tcpdump 底层汇编指令 BPF 工作在内核层，BPF 的架构图如下 [来自于bpf-usenix93]： 图 1-2 tcpdump 运行架构 2. eBPF 2.1 eBPF 介绍 2014 年初，Alexei Starovoitov 实现了 eBPF（extended Berkeley Packet Filter）。经过重新设计，eBPF 演进为一个通用执行引擎，可基于此开发性能分析工具、软件定义网络等诸多场景。eBPF 最早出现在 3.18 内核中，此后原来的 BPF 就被称为经典 BPF，缩写 cBPF（classic BPF），cBPF 现在已经基本废弃。现在，Linux 内核只运行 eBPF，内核会将加载的 cBPF 字节码透明地转换成 eBPF 再执行。 eBPF 新的设计针对现代硬件进行了优化，所以 eBPF 生成的指令集比旧的 BPF 解释器生成的机器码执行得更快。扩展版本也增加了虚拟机中的寄存器数量，将原有的 2 个 32 位寄存器增加到 10 个 64 位寄存器。由于寄存器数量和宽度的增加，开发人员可以使用函数参数自由交换更多的信息，编写更复杂的程序。总之，这些改进使 eBPF 版本的速度比原来的 BPF 提高了 4 倍。 维度 cBPF eBPF 内核版本 Linux 2.1.75（1997年） Linux 3.18（2014年）[4.x for kprobe/uprobe/tracepoint/perf-event] 寄存器数目 2个：A, X 10个： R0–R9, 另外 R10 是一个只读的帧指针 寄存器宽度 32位 64位 存储 16 个内存位: M[0–15] 512 字节堆栈，无限制大小的 “map” 存储 限制的内核调用 非常有限，仅限于 JIT 特定 有限，通过 bpf_call 指令调用 目标事件 数据包、 seccomp-BPF 数据包、内核函数、用户函数、跟踪点 PMCs 等 表格 1-1 cBPF 与 eBPF 对比 eBPF 在 Linux 3.18 版本以后引入，并不代表只能在内核 3.18+ 版本上运行，低版本的内核升级到最新也可以使用 eBPF 能力，只是可能部分功能受限，比如我就是在 Linux 发行版本 CentOS Linux release 7.7.1908 内核版本 3.10.0-1062.9.1.el7.x86_64 上运行 eBPF 在生产环境上搜集和排查网络问题。 eBPF 实现的最初目标是优化处理网络过滤器的内部 BPF 指令集。当时，BPF 程序仍然限于内核空间使用，只有少数用户空间程序可以编写内核处理的 BPF 过滤器，例如：tcpdump和 seccomp。时至今日，这些程序仍基于旧的 BPF 解释器生成字节码，但内核中会将这些指令转换为高性能的表示。 2014 年 6 月，eBPF 扩展到用户空间，这也成为了 BPF 技术的转折点。 正如 Alexei 在提交补丁的注释中写到：“这个补丁展示了 eBPF 的潜力”。当前，eBPF 不再局限于网络栈，已经成为内核顶级的子系统。eBPF 程序架构强调安全性和稳定性，看上去更像内核模块，但与内核模块不同，eBPF 程序不需要重新编译内核，并且可以确保 eBPF 程序运行完成，而不会造成系统的崩溃。 图 2-1 BPF 架构图 简述概括， eBPF 是一套通用执行引擎，提供了可基于系统或程序事件高效安全执行特定代码的通用能力，通用能力的使用者不再局限于内核开发者；eBPF 可由执行字节码指令、存储对象和 Helper 帮助函数组成，字节码指令在内核执行前必须通过 BPF 验证器 Verfier 的验证，同时在启用 BPF JIT 模式的内核中，会直接将字节码指令转成内核可执行的本地指令运行。 同时，eBPF 也逐渐在观测（跟踪、性能调优等）、安全和网络等领域发挥重要的角色。Facebook、NetFlix 、CloudFlare 等知名互联网公司内部广泛采用基于 eBPF 技术的各种程序用于性能分析、排查问题、负载均衡、防范 DDoS 攻击，据相关信息显示在 Facebook 的机器上内置一系列 eBPF 的相关工具。 相对于系统的性能分析和观测，eBPF 技术在网络技术中的表现，更是让人眼前一亮，BPF 技术与 XDP（eXpress Data Path） 和 TC（Traffic Control） 组合可以实现功能更加强大的网络功能，更可为 SDN 软件定义网络提供基础支撑。XDP 只作用与网络包的 Ingress 层面，BPF 钩子位于网络驱动中尽可能早的位置，无需进行原始包的复制就可以实现最佳的数据包处理性能，挂载的 BPF 程序是运行过滤的理想选择，可用于丢弃恶意或非预期的流量、进行 DDOS 攻击保护等场景；而 TC Ingress 比 XDP 技术处于更高层次的位置，BPF 程序在 L3 层之前运行，可以访问到与数据包相关的大部分元数据，是本地节点处理的理想的地方，可以用于流量监控或者 L3/L4 的端点策略控制，同时配合 TC egress 则可实现对于容器环境下更高维度和级别的网络结构。 图 2-2 XDP 技术架构 eBPF 相关的知名的开源项目包括但不限于以下： Facebook 高性能 4 层负载均衡器 Katran； Cilium 为下一代微服务 ServiceMesh 打造了具备API感知和安全高效的容器网络方案；底层主要使用 XDP 和 TC 等相关技术； IO Visor 项目开源的 BCC、 BPFTrace 和 Kubectl-Trace： BCC 提供了更高阶的抽象，可以让用户采用 Python、C++ 和 Lua 等高级语言快速开发 BPF 程序；BPFTrace 采用类似于 awk 语言快速编写 eBPF 程序；Kubectl-Trace 则提供了在 kubernetes 集群中使用 BPF 程序调试的方便操作； CloudFlare 公司开源的 eBPF Exporter 和 bpf-tools：eBPF Exporter 将 eBPF 技术与监控 Prometheus 紧密结合起来；bpf-tools 可用于网络问题分析和排查； 越来越多的基于 eBPF 的项目如雨后脆笋一样开始蓬勃发展，而且逐步在社区中异军突起，成为一道风景线。比如 IO Visor 项目的 BCC 工具，为性能分析和观察提供了更加丰富的工具集：图片来源 图 2-3 Linux bcc/BPF 观测工具 同时，IO Visor 的 bpf-docs 包含了日常的文档，可以用于学习。 由于 eBPF 还在快速发展期，内核中的功能也日趋增强，一般推荐基于Linux 4.4+ (4.9 以上会更好) 内核的来使用 eBPF。部分 Linux Event 和 BPF 版本支持见下图： 图 2-4 Linux 事件和 BPF 版本支持 2.2 eBPF 架构（观测） 基于 Linux 系统的观测工具中，eBPF 有着得天独厚的优势，高效、生产安全且内核中内置，特别的可以在内核中完成数据分析聚合比如直方图，与将数据发送到用户空间分析聚合相比，能够节省大量的数据复制传递带来的 CPU 消耗。 eBPF 整体结构图如下： 图 2-5 eBPF 观测架构 eBPF 分为用户空间程序和内核程序两部分： 用户空间程序负责加载 BPF 字节码至内核，如需要也会负责读取内核回传的统计信息或者事件详情； 内核中的 BPF 字节码负责在内核中执行特定事件，如需要也会将执行的结果通过 maps 或者 perf-event 事件发送至用户空间； 其中用户空间程序与内核 BPF 字节码程序可以使用 map 结构实现双向通信，这为内核中运行的 BPF 字节码程序提供了更加灵活的控制。 用户空间程序与内核中的 BPF 字节码交互的流程主要如下： 我们可以使用 LLVM 或者 GCC 工具将编写的 BPF 代码程序编译成 BPF 字节码； 然后使用加载程序 Loader 将字节码加载至内核；内核使用验证器（verfier） 组件保证执行字节码的安全性，以避免对内核造成灾难，在确认字节码安全后将其加载对应的内核模块执行；BPF 观测技术相关的程序程序类型可能是 kprobes/uprobes/tracepoint/perf_events 中的一个或多个，其中： kprobes：实现内核中动态跟踪。 kprobes 可以跟踪到 Linux 内核中的导出函数入口或返回点，但是不是稳定 ABI 接口，可能会因为内核版本变化导致，导致跟踪失效。 uprobes：用户级别的动态跟踪。与 kprobes 类似，只是跟踪用户程序中的函数。 tracepoints：内核中静态跟踪。tracepoints 是内核开发人员维护的跟踪点，能够提供稳定的 ABI 接口，但是由于是研发人员维护，数量和场景可能受限。 perf_events：定时采样和 PMC。 内核中运行的 BPF 字节码程序可以使用两种方式将测量数据回传至用户空间 maps 方式可用于将内核中实现的统计摘要信息（比如测量延迟、堆栈信息）等回传至用户空间； perf-event 用于将内核采集的事件实时发送至用户空间，用户空间程序实时读取分析； 如无特殊说明，本文中所说的 BPF 都是泛指 BPF 技术。 2.3 eBPF 的限制 eBPF 技术虽然强大，但是为了保证内核的处理安全和及时响应，内核中的 eBPF 技术也给予了诸多限制，当然随着技术的发展和演进，限制也在逐步放宽或者提供了对应的解决方案。 eBPF 程序不能调用任意的内核参数，只限于内核模块中列出的 BPF Helper 函数，函数支持列表也随着内核的演进在不断增加。 eBPF 程序不允许包含无法到达的指令，防止加载无效代码，延迟程序的终止。 eBPF 程序中循环次数限制且必须在有限时间内结束，这主要是用来防止在 kprobes 中插入任意的循环，导致锁住整个系统；解决办法包括展开循环，并为需要循环的常见用途添加辅助函数。Linux 5.3 在 BPF 中包含了对有界循环的支持，它有一个可验证的运行时间上限。 eBPF 堆栈大小被限制在 MAX_BPF_STACK，截止到内核 Linux 5.8 版本，被设置为 512；参见 include/linux/filter.h，这个限制特别是在栈上存储多个字符串缓冲区时：一个char[256]缓冲区会消耗这个栈的一半。目前没有计划增加这个限制，解决方法是改用 bpf 映射存储，它实际上是无限的。 /* BPF program can access up to 512 bytes of stack space. */ #define MAX_BPF_STACK 512 eBPF 字节码大小最初被限制为 4096 条指令，截止到内核 Linux 5.8 版本， 当前已将放宽至 100 万指令（ BPF_COMPLEXITY_LIMIT_INSNS），参见：include/linux/bpf.h，对于无权限的BPF程序，仍然保留 4096 条限制 ( BPF_MAXINSNS )；新版本的 eBPF 也支持了多个 eBPF 程序级联调用，虽然传递信息存在某些限制，但是可以通过组合实现更加强大的功能。 #define BPF_COMPLEXITY_LIMIT_INSNS 1000000 /* yes. 1M insns */ 2.4 eBPF 与内核模块对比 在 Linux 观测方面，eBPF 总是会拿来与 kernel 模块方式进行对比，eBPF 在安全性、入门门槛上比内核模块都有优势，这两点在观测场景下对于用户来讲尤其重要。 维度 Linux 内核模块 eBPF kprobes/tracepoints 支持 支持 安全性 可能引入安全漏洞或导致内核 Panic 通过验证器进行检查，可以保障内核安全 内核函数 可以调用内核函数 只能通过 BPF Helper 函数调用 编译性 需要编译内核 不需要编译内核，引入头文件即可 运行 基于相同内核运行 基于稳定 ABI 的 BPF 程序可以编译一次，各处运行 与应用程序交互 打印日志或文件 通过 perf_event 或 map 结构 数据结构丰富性 一般 丰富 入门门槛 高 低 升级 需要卸载和加载，可能导致处理流程中断 原子替换升级，不会造成处理流程中断 内核内置 视情况而定 内核内置支持 表格 2-1 eBPF 与 Linux 内核模块方式对比 3. 应用案例 大名鼎鼎的性能分析大师 Brendan Gregg 等编写了诸多的 BCC 或 BPFTrace 的工具集可以拿来直接使用，完全可以满足我们日常问题分析和排查。 BCC 在 CentOS 7 系统中可以通过 yum 快速安装 # yum install bcc -y Resolving Dependencies --> Running transaction check ---> Package bcc.x86_64 0:0.8.0-1.el7 will be updated --> Processing Dependency: bcc(x86-64) = 0.8.0-1.el7 for package: python-bcc-0.8.0-1.el7.x86_64 ---> Package bcc.x86_64 0:0.10.0-1.el7 will be an update --> Processing Dependency: bcc-tools = 0.10.0-1.el7 for package: bcc-0.10.0-1.el7.x86_64 --> Running transaction check ---> Package bcc-tools.x86_64 0:0.8.0-1.el7 will be updated ---> Package bcc-tools.x86_64 0:0.10.0-1.el7 will be an update ---> Package python-bcc.x86_64 0:0.8.0-1.el7 will be updated ---> Package python-bcc.x86_64 0:0.10.0-1.el7 will be an update --> Finished Dependency Resolution ... 其他系统的安装方式参见：INSTALL.md BCC 中每一个工具都有一个对应的使用样例，比如 execsnoop.py 和 execsnoop_example.txt，在使用样例中有详细的使用说明，而且 BCC 中的工具使用的帮助文档格式基本类似，上手非常方便。 BCC 的程序一般情况下都需要 root 用户来运行。 3.1 Linux 性能分析 60 秒 （BPF版本） 英文原文 Linux Performance Analysis in 60,000 Milliseconds，视频地址 uptime dmesg | tail vmstat 1 mpstat -P ALL 1 pidstat 1 iostat -xz 1 free -m sar -n DEV 1 sar -n TCP,ETCP 1 top 60s 系列 BPF 版本如下： 图 3-1 60s 排查之 BPF 版本 对于在系统中运行的 “闪电侠” 程序，运行周期非常短，但是可能会带来系统的抖动延时，我们采用 top 命令查看一般情况下难以发现，我们可以使用 BCC 提供的工具 execsnoop来进行排查： # Trace file opens with process and filename: opensnoop #/usr/share/bcc/tools/execsnoop PCOMM PID PPID RET ARGS sleep 3334 21029 0 /usr/bin/sleep 3 sleep 3339 21029 0 /usr/bin/sleep 3 conntrack 3341 1112 0 /usr/sbin/conntrack --stats conntrack 3342 1112 0 /usr/sbin/conntrack --count sleep 3344 21029 0 /usr/bin/sleep 3 iptables-save 3347 9211 0 /sbin/iptables-save -t filter iptables-save 3348 9211 0 /sbin/iptables-save -t nat 3.2 slab dentry 过大导致的网络抖动排查 现象 网络 ping 的延时间歇性有规律出现抖动 问题排查 采用 execsnoop 分析发现，某个运行命令cat /proc/slabinfo的运行时间间隔与抖动的频率完全吻合，顺着这个的线索定位，我们发现云厂商提供的 Java 版本的云监控会定期调用 cat /proc/slabinfo 来获取内核缓存的信息； 通过命令 slabtop 发现系统中的 dentry 项的内存占用非常大，系统内存 128G，dentry 占用 70G 以上，所以问题很快就定位到是系统在打开文件方面可能有相关问题； 根因分析 我们使用对于打开文件跟踪的 BCC 工具 opensnoop 很快就定位到是某个程序频繁创建和删除临时文件，最终定位为某个 PHP 程序设置的调用方式存在问题，导致每次请求会创建和删除临时文件；代码中将 http 调用中的 contentType 设置成了 Http::CONTENT_TYPE_UPLOAD，导致每次请求都会生成临时文件，修改成 application/x-www-form-urlencoded 问题解决。 问题的原理可参考 记一次对网络抖动经典案例的分析 和 systemtap脚本分析系统中dentry SLAB占用过高问题 3.3 生成火焰图 火焰图是帮助我们对系统耗时进行可视化的图表，能够对程序中那些代码经常被执行给出一个清晰的展现。Brendan Gregg 是火焰图的创建者，他在 GitHub 上维护了一组脚本可以轻松生成需要的可视化格式数据。使用 BCC 中的工具 profile 可很方面地收集道 CPU 路径的数据，基于数据采用工具可以轻松地生成火焰图，查找到程序的性能瓶颈。 使用 profile 搜集火焰图的程序没有任何限制和改造 profile 工具可以让我们轻松对于系统或者程序的 CPU 性能路径进行可视化分析： /usr/share/bcc/tools/profile -h usage: profile [-h] [-p PID | -L TID] [-U | -K] [-F FREQUENCY | -c COUNT] [-d] [-a] [-I] [-f] [--stack-storage-size STACK_STORAGE_SIZE] [-C CPU] [duration] Profile CPU stack traces at a timed interval positional arguments: duration duration of trace, in seconds optional arguments: -h, --help show this help message and exit -p PID, --pid PID profile process with this PID only -L TID, --tid TID profile thread with this TID only -U, --user-stacks-only show stacks from user space only (no kernel space stacks) -K, --kernel-stacks-only show stacks from kernel space only (no user space stacks) -F FREQUENCY, --frequency FREQUENCY sample frequency, Hertz -c COUNT, --count COUNT sample period, number of events -d, --delimited insert delimiter between kernel/user stacks -a, --annotations add _[k] annotations to kernel frames -I, --include-idle include CPU idle stacks -f, --folded output folded format, one line per stack (for flame graphs) --stack-storage-size STACK_STORAGE_SIZE the number of unique stack traces that can be stored and displayed (default 16384) -C CPU, --cpu CPU cpu number to run profile on examples: ./profile # profile stack traces at 49 Hertz until Ctrl-C ./profile -F 99 # profile stack traces at 99 Hertz ./profile -c 1000000 # profile stack traces every 1 in a million events ./profile 5 # profile at 49 Hertz for 5 seconds only ./profile -f 5 # output in folded format for flame graphs ./profile -p 185 # only profile process with PID 185 ./profile -L 185 # only profile thread with TID 185 ./profile -U # only show user space stacks (no kernel) ./profile -K # only show kernel space stacks (no user) profile 配合 FlameGraph 可以轻松帮我们绘制出 CPU 使用的火焰图。 $ profile -af 30 > out.stacks01 $ git clone https://github.com/brendangregg/FlameGraph $ cd FlameGraph $ ./flamegraph.pl --color=java out.svg 图 3-2 火焰图 3.3 排查网络调用来源 在生产场景下，会有些特定场景需要抓取连接到外网特定地址的程序，这时候我们可以采用 BCC 工具集中的 tcplife 来定位。 /usr/share/bcc/tools/tcplife -h usage: tcplife [-h] [-T] [-t] [-w] [-s] [-p PID] [-L LOCALPORT] [-D REMOTEPORT] Trace the lifespan of TCP sessions and summarize optional arguments: -h, --help show this help message and exit -T, --time include time column on output (HH:MM:SS) -t, --timestamp include timestamp on output (seconds) -w, --wide wide column output (fits IPv6 addresses) -s, --csv comma separated values output -p PID, --pid PID trace this PID only -L LOCALPORT, --localport LOCALPORT comma-separated list of local ports to trace. -D REMOTEPORT, --remoteport REMOTEPORT comma-separated list of remote ports to trace. examples: ./tcplife # trace all TCP connect()s ./tcplife -t # include time column (HH:MM:SS) ./tcplife -w # wider colums (fit IPv6) ./tcplife -stT # csv output, with times & timestamps ./tcplife -p 181 # only trace PID 181 ./tcplife -L 80 # only trace local port 80 ./tcplife -L 80,81 # only trace local ports 80 and 81 ./tcplife -D 80 # only trace remote port 80 通过在机器上使用 tcplife 来获取的网络连接信息，我们可以看到包括了 PID、COMM、本地 IP 地址、本地端口、远程 IP 地址和远程端口，通过这些信息非常方便排查到连接到特定 IP 地址的程序，尤其是连接的过程非常短暂，通过 netstat 等其他工具不容易排查的场景。 # /usr/share/bcc/tools/tcplife PID COMM IP LADDR LPORT RADDR RPORT TX_KB RX_KB MS 1776 blackbox_export 4 169.254.20.10 35830 169.254.20.10 53 0 0 0.36 27150 node-cache 4 169.254.20.10 53 169.254.20.10 35830 0 0 0.36 12511 coredns 4 127.0.0.1 58492 127.0.0.1 8080 0 0 0.32 ... 如果我们想知道更加详细的 TCP 状态情况，那么 tcptracer 可展示更加详细的 TCP 状态，其中 C 代表 Connect X 表示关闭， A 代表 Accept。 # /usr/share/bcc/tools/tcptracer Tracing TCP established connections. Ctrl-C to end. T PID COMM IP SADDR DADDR SPORT DPORT C 21066 ilogtail 4 10.81.128.12 100.100.49.128 40906 80 X 21066 ilogtail 4 10.81.128.12 100.100.49.128 40906 80 C 21066 ilogtail 4 10.81.128.12 100.100.49.128 40908 80 X 21066 ilogtail 4 10.81.128.12 100.100.49.128 40908 80 tcpstates 还能够展示出来 TCP 状态机的流转情况： # /usr/share/bcc/tools/tcpstates SKADDR C-PID C-COMM LADDR LPORT RADDR RPORT OLDSTATE -> NEWSTATE MS ffff9fd7e8192000 22384 curl 100.66.100.185 0 52.33.159.26 80 CLOSE -> SYN_SENT 0.000 ffff9fd7e8192000 0 swapper/5 100.66.100.185 63446 52.33.159.26 80 SYN_SENT -> ESTABLISHED 1.373 ffff9fd7e8192000 22384 curl 100.66.100.185 63446 52.33.159.26 80 ESTABLISHED -> FIN_WAIT1 176.042 同样，我们也可以实时获取到 TCP 连接超时或者重连的网络连接；也可以通过抓取 UDP包相关的连接信息，用于定位诸如 DNS 请求超时或者 DNS 请求的发起进程。 4. 编写 BPF 程序 对于大多数开发者而言，更多的是基于 BPF 技术之上编写解决我们日常遇到的各种问题，当前 BCC 和 BPFTrace 两个项目在观测和性能分析上已经有了诸多灵活且功能强大的工具箱，完全可以满足我们日常使用。 BCC 提供了更高阶的抽象，可以让用户采用 Python、C++ 和 Lua 等高级语言快速开发 BPF 程序； BPFTrace 采用类似于 awk 语言快速编写 eBPF 程序； 更早期的工具则是使用 C 语言来编写 BPF 程序，使用 LLVM clang 编译成 BPF 代码，这对于普通使用者上手有不少门槛当前仅限于对于 eBPF 技术更加深入的学习场景。 4.1 BCC 版本 HelloWorld 图 4-1 BCC 整体架构 使用 BCC 前端绑定语言 Python 编写的 Hello World 版本： #!/usr/bin/python3 from bcc import BPF # This may not work for 4.17 on x64, you need replace kprobe__sys_clone with kprobe____x64_sys_clone prog = \"\"\" int kprobe__sys_clone(void *ctx) { bpf_trace_printk(\"Hello, World!\\\\n\"); return 0; } \"\"\" b = BPF(text=prog, debug=0x04) b.trace_print() 运行程序前需要安装过 bcc 相关工具包，当运行正常的时候我们发现每当 sys_clone 系统调用时，运行的控制台上就会打印 “Hello, World!”，在打印文字前面还包含了调用程序的进程名称，进程 ID 等信息； 如果运行报错，可能是缺少头文件，一般安装 kernel-devel 包即可。 # python ./hello.py kubelet-8349 [006] d... 33637334.829981: : Hello, World! kubelet-8349 [006] d... 33637334.838594: : Hello, World! kubelet-8349 [006] d... 33637334.843788: : Hello, World! 4.3 BPFTrace BPFTrace 是基于 BPF 和 BCC 的开源项目，与 BCC 不同的是其提供了更高层次的抽象，可以使用类似 AWK 脚本语言来编写基于 BPF 的跟踪或者性能排查工具，更加易于入门和编写，该工具的主要灵感来自于 Solaris 的 D 语言。BPFTrace 更方便与编写单行的程序。BPFTrace 与 BCC 一样也是 IO Visor 组织下的项目，仓库参见 bpftrace。更加深入的学习资料参见：Reference Guide 和 One-Liner Tutorial。 BPFTrace 使用 LLVM 将脚本编译成 BPF 二进制码，后续使用 BCC 与 Linux 内核进行交互。从功能层面上讲，BPFTrace 的定制性和灵活性不如 BCC，但是比 BCC 工具更加易于理解和使用，降低了 BPF 技术的使用门槛。 使用样例： # 统计进程调用 sys_enter 的次数 #bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }' Attaching 1 probe... ^C @[bpftrace]: 6 @[systemd]: 24 @[snmp-pass]: 96 @[sshd]: 125 # 统计内核中函数堆栈的次数 # bpftrace -e 'profile:hz:99 { @[kstack] = count(); }' Attaching 1 probe... ^C [...] @[ filemap_map_pages+181 __handle_mm_fault+2905 handle_mm_fault+250 __do_page_fault+599 async_page_fault+69 ]: 12 [...] @[ cpuidle_enter_state+164 do_idle+390 cpu_startup_entry+111 start_secondary+423 secondary_startup_64+165 ]: 22122 4.3 C 语言原生方式 采用 LLVM Clang 的方式编译会涉及到内核编译环境搭建，而且还需要自己编译 Makefile 等操作，属于高级用户使用： bpf_program.c #include #define SEC(NAME) __attribute__((section(NAME), used)) static int (*bpf_trace_printk)(const char *fmt, int fmt_size, ...) = (void *)BPF_FUNC_trace_printk; SEC(\"tracepoint/syscalls/sys_enter_execve\") int bpf_prog(void *ctx) { char msg[] = \"Hello, BPF World!\"; bpf_trace_printk(msg, sizeof(msg)); return 0; } char _license[] SEC(\"license\") = \"GPL\"; loader.c #include \"bpf_load.h\" #include int main(int argc, char **argv) { if (load_bpf_file(\"bpf_program.o\") != 0) { printf(\"The kernel didn't load the BPF program\\n\"); return -1; } read_trace_pipe(); return 0; } Makefile 文件（部分） build: ${BPFCODE.c} ${BPFLOADER} $(CLANG) -O2 -target bpf -c $(BPFCODE:=.c) $(CCINCLUDE) -o ${BPFCODE:=.o} 其中 clang 编译中的选型 -target bpf 表明我们将代码编译成 bpf 的字节码。 完整的程序参见：hello_world；更多的样例代码可以参见对应内核中 kernel-src/samples/bpf/ 下的样例代码。 后续会持续进行 BPF 相关的内容总结和分享，Github bpf_study 仓库，欢迎提交 PR 和 Star 5. 参考资料 The BSD Packet Filter: A New Architecture for User-level Packet Capture [译] Cilium：BPF 和 XDP 参考指南（2019） Cillum BPF and XDP Reference Guide Cloudflare架构以及BPF如何占据世界 關於 BPF 和 eBPF 的筆記 Dive into BPF: a list of reading material 中文 eBPF 简史 https://www.youtube.com/watch?v=znBGt7oHJyQ BPF Documentation HOWTO interact with BPF subsystem Linux 内核 BPF 文档 Linux Extended BPF (eBPF) Tracing Tools Brendan Gregg 性能提升40%: 腾讯 TKE 用 eBPF绕过 conntrack 优化K8s Service SDN handbook Linux BPF 帮助文档 bpf(2) bpf-helpers(7) tc-bpf(8) Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/ipset.html":{"url":"blog/kubernetes/网络插件/ipset.html","title":"Ipset","keywords":"","body":"# ipset 作者：Linux_woniu 时间: 2021-02-05 08:53:35 标签：ipset防火墙 【摘要】ipset是iptables的扩展，可以让你添加规则来匹配地址集合。不同于常规的iptables链是线性的存储和遍历，ipset是用索引数据结构存储，甚至对于大型集合，查询效率非常都优秀。Besides the obvious situations where you might imagine this would be useful, such as blocking long list... ipset是iptables的扩展，可以让你添加规则来匹配地址集合。不同于常规的iptables链是线性的存储和遍历，ipset是用索引数据结构存储，甚至对于大型集合，查询效率非常都优秀。 Besides the obvious situations where you might imagine this would be useful, such as blocking long lists of \"bad\" hosts without worry of killing system resources or causing network congestion, IP sets also open up new ways of approaching certain aspects of firewall design and simplify many configuration scenarios. 除了可以看见的解决方案，例如防止长主机列表造成的系统资源耗尽，ipset还开创了新方式来设计防火墙和简化许多规则。 Getting ipset TODO iptables Overview TODO Enter ipset ipset is a \"match extension\" for iptables. To use it, you create and populate uniquely named \"sets\" using the ipset command-line tool, and then separately reference those sets in the match specification of one or more iptables rules. ipset用于iptables的\"匹配扩展\"。要使用它，需要通过ipset的命令行工具创建一个集合，然后分别在一个或多格iptables规则中引用这个set A set is simply a list of addresses stored efficiently for fast lookup. Take the following normal iptables commands that would block inbound traffic from 1.1.1.1 and 2.2.2.2: iptables -A INPUT -s 1.1.1.1 -j DROP iptables -A INPUT -s 2.2.2.2 -j DROP The match specification syntax -s 1.1.1.1 above means \"match packets whose source address is 1.1.1.1\". To block both 1.1.1.1 and 2.2.2.2, two separate iptables rules with two separate match specifications (one for 1.1.1.1 and one for 2.2.2.2) are defined above. Alternatively, the following ipset/iptables commands achieve the same result: ipset -N myset iphash ipset -A myset 1.1.1.1ipset -A myset 2.2.2.2iptables -A INPUT -m set --set myset src -j DROP The ipset commands above create a new set (myset of type iphash) with two addresses (1.1.1.1 and 2.2.2.2). The iptables command then references the set with the match specification -m set --set myset src, which means \"match packets whose source header matches (that is, is contained within) the set named myset\". The flag src means match on \"source\". The flag dst would match on \"destination\", and the flag src,dst would match on both source and destination. In the second version above, only one iptables command is required, regardless of how many additional IP addresses are contained within the set. Although this example uses only two addresses, you could just as easily define 1,000 addresses, and the ipset-based config still would require only a single iptables rule, while the previous approach, without the benefit of ipset, would require 1,000 iptables rules. 在上面的第二个版本，尽管这个例子只写了两个地址。但是不管ipset包含多少个ip地址，都只需要一条iptables命令。你可以定义1000个地址，iptables依然只需要一条规则。在第一个版本中，没有ipset，所以需要1000个iptables规则。 Set Types Each set is of a specific type, which defines what kind of values can be stored in it (IP addresses, networks, ports and so on) as well as how packets are matched (that is, what part of the packet should be checked and how it's compared to the set). Besides the most common set types, which check the IP address, additional set types are available that check the port, the IP address and port together, MAC address and IP address together and so on. 每一个set都需要一个特定的类型，用于定义它可以存储什么样的类型(IP地址、网络、端口等等)以及包如何匹配(即，包的什么部分应该被检查以及它是如何和set做匹配)。除了检查IP地址这个最常用的set类型，还有端口、IP地址和端口一起、MAC地址和IP地址一起，等等。 Each set type has its own rules for the type, range and distribution of values it can contain. Different set types also use different types of indexes and are optimized for different scenarios. The best/most efficient set type depends on the situation. The most flexible set types are iphash, which stores lists of arbitrary IP addresses, and nethash, which stores lists of arbitrary networks (IP/mask) of varied sizes. Refer to the ipset man page for a listing and description of all the set types (there are 11 in total at the time of this writing). 最灵活的set类型是iphash，可以存储任意的IP地址列表和任意的网络(IP/MASK)列表。ipset的man page列出和描述了所有的set类型(在写这篇文章时总共有11种类型) The special set type setlist also is available, which allows grouping several sets together into one. This is required if you want to have a single set that contains both single IP addresses and networks, for example. Advantages of ipset Besides the performance gains, ipset also allows for more straightforward configurations in many scenarios. 除开性能上的获取，ipset还允许在许多场景下做更多简洁的配置。 If you want to define a firewall condition that would match everything but packets from 1.1.1.1 or 2.2.2.2 and continue processing in mychain, notice that the following does not work: iptables -A INPUT -s ! 1.1.1.1 -g mychain iptables -A INPUT -s ! 2.2.2.2 -g mychain If a packet came in from 1.1.1.1, it would not match the first rule (because the source address is 1.1.1.1), but it would match the second rule (because the source address is not 2.2.2.2). If a packet came in from 2.2.2.2, it would match the first rule (because the source address is not 1.1.1.1). The rules cancel each other out—all packets will match, including 1.1.1.1 and 2.2.2.2. Although there are other ways to construct the rules properly and achieve the desired result without ipset, none are as intuitive or straightforward: ipset -N myset iphash ipset -A myset 1.1.1.1ipset -A myset 2.2.2.2iptables -A INPUT -m set ! --set myset src -g mychain In the above, if a packet came in from 1.1.1.1, it would not match the rule (because the source address 1.1.1.1 does match the set myset). If a packet came in from 2.2.2.2, it would not match the rule (because the source address 2.2.2.2 does match the set myset). Although this is a simplistic example, it illustrates the fundamental benefit associated with fitting a complete condition in a single rule. In many ways, separate iptables rules are autonomous from each other, and it's not always straightforward, intuitive or optimal to get separate rules to coalesce into a single logical condition, especially when it involves mixing normal and inverted tests. ipset just makes life easier in these situations. // TODO Another benefit of ipset is that sets can be manipulated independently of active iptables rules. Adding/changing/removing entries is a trivial matter because the information is simple and order is irrelevant. Editing a flat list doesn't require a whole lot of thought. In iptables, on the other hand, besides the fact that each rule is a significantly more complex object, the order of rules is of fundamental importance, so in-place rule modifications are much heavier and potentially error-prone operations. Excluding WAN, ××× and Other Routed Networks from the NAT—the Right Way Outbound NAT (SNAT or IP masquerade) allows hosts within a private LAN to access the Internet. An appropriate iptables NAT rule matches Internet-bound packets originating from the private LAN and replaces the source address with the address of the gateway itself (making the gateway appear to be the source host and hiding the private \"real\" hosts behind it). // TODO Outbound NAT? NAT automatically tracks the active connections so it can forward return packets back to the correct internal host (by changing the destination from the address of the gateway back to the address of the original internal host). NAT自动跟踪活动的链接，所以它可以返回包到正确的内部主机(通过网络地址改回目的地址到原来的内部主机地址) Here is an example of a simple outbound NAT rule that does this, where 10.0.0.0/24 is the internal LAN: iptables -t nat -A POSTROUTING \\ -s 10.0.0.0/24 -j MASQUERADE This rule matches all packets coming from the internal LAN and masquerades them (that is, it applies \"NAT\" processing). This might be sufficient if the only route is to the Internet, where all through traffic is Internet traffic. If, however, there are routes to other private networks, such as with ××× or physical WAN links, you probably don't want that traffic masqueraded. 这个规则匹配所有来至局域网内部的数据包，并且伪装他们(即，它适用NAT处理#TODO?)。如果唯一的路由是通向Internet，所有通过的流量都是Internet流量，也许会比较足够。但是，会有一些到私有网络的路由，比如×××或屋里WAN链路，你应该不希望流量伪装。 #TODO 这一段翻译的好蛋疼 One simple way (partially) to overcome this limitation is to base the NAT rule on physical interfaces instead of network numbers (this is one of the most popular NAT rules given in on-line examples and tutorials): iptables -t nat -A POSTROUTING \\ -o eth0 -j MASQUERADE This rule assumes that eth0 is the external interface and matches all packets that leave on it. Unlike the previous rule, packets bound for other networks that route out through different interfaces won't match this rule (like with Open××× links). 这条规则假设eth0是外网口并且匹配所有通过它出去的数据包。不同于前一条规则，绑定其他网络通过不同的网卡出去的数据包不会匹配这条规则(例如Open×××链路) Although many network connections may route through separate interfaces, it is not safe to assume that all will. A good example is KAME-based IPsec ××× connections (such as Openswan) that don't use virtual interfaces like other user-space ×××s (such as Open×××). 尽管许多网络连接可能会通过不同的网卡出去，但假设所有都会并不安全。一个很好的例子就是KAME-based IPsec ×××连接(例如Openswan)不会像其他user-space ×××s(例如Open×××)一样使用虚拟网卡。 Another situation where the above interface match technique wouldn't work is if the outward facing (\"external\") interface is connected to an intermediate network with routes to other private networks in addition to a route to the Internet. It is entirely plausible for there to be routes to private networks that are several hops away and on the same path as the route to the Internet. // TODO Designing firewall rules that rely on matching of physical interfaces can place artificial limits and dependencies on network topology, which makes a strong case for it to be avoided if it's not actually necessary. As it turns out, this is another great application for ipset. Let's say that besides acting as the Internet gateway for the local private LAN (10.0.0.0/24), your box routes directly to four other private networks (10.30.30.0/24, 10.40.40.0/24, 192.168.4.0/23 and 172.22.0.0/22). Run the following commands: ipset -N routed_nets nethash ipset -A routed_nets 10.30.30.0/24ipset -A routed_nets 10.40.40.0/24ipset -A routed_nets 192.168.4.0/23ipset -A routed_nets 172.22.0.0/22iptables -t nat -A POSTROUTING \\ -s 10.0.0.0/24 \\ -m set ! --set routed_nets dst \\ -j MASQUERADE As you can see, ipset makes it easy to zero in on exactly what you want matched and what you don't. This rule would masquerade all traffic passing through the box from your internal LAN (10.0.0.0/24) except those packets bound for any of the networks in your routed_nets set, preserving normal direct IP routing to those networks. Because this configuration is based purely on network addresses, you don't have to worry about the types of connections in place (type of ×××s, number of hops and so on), nor do you have to worry about physical interfaces and topologies. This is how it should be. Because this is a pure layer-3 (network layer) implementation, the underlying classifications required to achieve it should be pure layer-3 as well. Limiting Certain PCs to Have Access Only to Certain Public Hosts Let's say the boss is concerned about certain employees playing on the Internet instead of working and asks you to limit their PCs' access to a specific set of sites they need to be able to get to for their work, but he doesn't want this to affect all PCs (such as his). To limit three PCs (10.0.0.5, 10.0.0.6 and 10.0.0.7) to have outside access only to worksite1.com, worksite2.com and worksite3.com, run the following commands: ipset -N limited_hosts iphash ipset -A limited_hosts 10.0.0.5ipset -A limited_hosts 10.0.0.6ipset -A limited_hosts 10.0.0.7ipset -N allowed_sites iphash ipset -A allowed_sites worksite1.com ipset -A allowed_sites worksite2.com ipset -A allowed_sites worksite3.com iptables -I FORWARD \\ -m set --set limited_hosts src \\ -m set ! --set allowed_sites dst \\ -j DROP This example matches against two sets in a single rule. If the source matches limited_hosts and the destination does not match allowed_sites, the packet is dropped (because limited_hosts are allowed to communicate only with allowed_sites). Note that because this rule is in the FORWARD chain, it won't affect communication to and from the firewall itself, nor will it affect internal traffic (because that traffic wouldn't even involve the firewall). Blocking Access to Hosts for All but Certain PCs (Inverse Scenario) Let's say the boss wants to block access to a set of sites across all hosts on the LAN except his PC and his assistant's PC. For variety, in this example, let's match the boss and assistant PCs by MAC address instead of IP. Let's say the MACs are 11:11:11:11:11:11 and 22:22:22:22:22:22, and the sites to be blocked for everyone else are badsite1.com, badsite2.com and badsite3.com. In lieu of using a second ipset to match the MACs, let's utilize multiple iptables commands with the MARK target to mark packets for processing in subsequent rules in the same chain: ipset -N blocked_sites iphash ipset -A blocked_sites badsite1.com ipset -A blocked_sites badsite2.com ipset -A blocked_sites badsite3.com iptables -I FORWARD -m mark --mark 0x187 -j DROP iptables -I FORWARD \\ -m mark --mark 0x187 \\ -m mac --mac-source 11:11:11:11:11:11 \\ -j MARK --set-mark 0x0iptables -I FORWARD \\ -m mark --mark 0x187 \\ -m mac --mac-source 22:22:22:22:22:22 \\ -j MARK --set-mark 0x0iptables -I FORWARD \\ -m set --set blocked_sites dst \\ -j MARK --set-mark 0x187 As you can see, because you're not using ipset to do all the matching work as in the previous example, the commands are quite a bit more involved and complex. Because there are multiple iptables commands, it's necessary to recognize that their order is vitally important. Notice that these rules are being added with the -I option (insert) instead of -A (append). When a rule is inserted, it is added to the top of the chain, pushing all the existing rules down. Because each of these rules is being inserted, the effective order is reversed, because as each rule is added, it is inserted above the previous one. The last iptables command above actually becomes the first rule in the FORWARD chain. This rule matches all packets with a destination matching the blocked_sites ipset, and then marks those packets with 0x187 (an arbitrarily chosen hex number). The next two rules match only packets from the hosts to be excluded and that are already marked with 0x187. These two rules then set the marks on those packets to 0x0, which \"clears\" the 0x187 mark. Finally, the last iptables rule (which is represented by the first iptables command above) drops all packets with the 0x187 mark. This should match all packets with destinations in the blocked_sites set except those packets coming from either of the excluded MACs, because the mark on those packets is cleared before the DROP rule is reached. This is just one way to approach the problem. Other than using a second ipset, another way would be to utilize user-defined chains. If you wanted to use a second ipset instead of the mark technique, you wouldn't be able to achieve the exact outcome as above, because ipset does not have a machash set type. There is a macipmap set type, however, but this requires matching on IP and MACs together, not on MAC alone as above. Cautionary note: in most practical cases, this solution would not actually work for Web sites, because many of the hosts that might be candidates for the blocked_sites set (like Facebook, MySpace and so on) may have multiple IP addresses, and those IPs may change frequently. A general limitation of iptables/ipset is that hostnames should be specified only if they resolve to a single IP. Also, hostname lookups happen only at the time the command is run, so if the IP address changes, the firewall rule will not be aware of the change and still will reference the old IP. For this reason, a better way to accomplish these types of Web access policies is with an HTTP proxy solution, such as Squid. That topic is obviously beyond the scope of this article. Automatically Ban Hosts That Attempt to Access Invalid Services ipset also provides a \"target extension\" to iptables that provides a mechanism for dynamically adding and removing set entries based on any iptables rule. Instead of having to add entries manually with the ipset command, you can have iptables add them for you on the fly. For example, if a remote host tries to connect to port 25, but you aren't running an SMTP server, it probably is up to no good. To deny that host the opportunity to try anything else proactively, use the following rules: ipset -N banned_hosts iphash iptables -A INPUT \\ -p tcp --dport 25 \\ -j SET --add-set banned_hosts src iptables -A INPUT \\ -m set --set banned_hosts src \\ -j DROP If a packet arrives on port 25, say with source address 1.1.1.1, it instantly is added to banned_hosts, just as if this command were run: ipset -A banned_hosts 1.1.1.1 All traffic from 1.1.1.1 is blocked from that moment forward because of the DROP rule. Note that this also will ban hosts that try to run a port scan unless they somehow know to avoid port 25. Clearing the Running Config If you want to clear the ipset and iptables config (sets, rules, entries) and reset to a fresh open firewall state (useful at the top of a firewall script), run the following commands: iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT iptables -t filter -F iptables -t raw -F iptables -t nat -F iptables -t mangle -F ipset -F ipset -X Sets that are \"in use\", which means referenced by one or more iptables rules, cannot be destroyed (with ipset -X). So, in order to ensure a complete \"reset\" from any state, the iptables chains have to be flushed first (as illustrated above). Conclusion ipset adds many useful features and capabilities to the already very powerful netfilter/iptables suite. As described in this article, ipset not only provides new firewall configuration possibilities, but it also simplifies many setups that are difficult, awkward or less efficient to construct with iptables alone. Any time you want to apply firewall rules to groups of hosts or addresses at once, you should be using ipset. As I showed in a few examples, you also can combine ipset with some of the more exotic iptables features, such as packet marking, to accomplish all sorts of designs and network policies. The next time you're working on your firewall setup, consider adding ipset to the mix. I think you will be surprised at just how useful and flexible it can be. Resources Netfilter/iptables Project Home Page: http://www.netfilter.org ipset Home Page: http://ipset.netfilter.org Man IPSET Ref: [http://ipset.netfilter.org/ipset.man.html] ipset -- administration tool for IP sets ipset [ OPTIONS ] COMMAND [ COMMAND-OPTIONS ]COMMANDS := { create | add | del | test | destroy | list | save | restore | flush | rename | swap | help | version | - }OPTIONS := { -exist | -output { plain | save | xml } | -quiet | -resolve | -sorted }ipset create SETNAME TYPENAME [ CREATE-OPTIONS ]ipset add SETNAME ADD-ENTRY [ ADD-OPTIONS ]ipset del SETNAME DEL-ENTRY [ DEL-OPTIONS ]ipset test SETNAME TEST-ENTRY [ TEST-OPTIONS ]ipset destroy [ SETNAME ]ipset list [ SETNAME ]ipset save [ SETNAME ]ipset restore ipset flush [ SETNAME ]ipset rename SETNAME-FROM SETNAME-TO ipset swap SETNAME-FROM SETNAME-TO ipset help [ TYPENAME ]ipset version ipset - ipset有三种存储方式： bitmao hash list bitmap和list方式使用一个固定大小的存储。hash方式使用一个hash来存储元素。 ipset 配合ip/port/mac/net等可以组成多种类型 基本操作 create SETNAME TYPENAME [ CREATE-OPTIONS ]新建一个set add SETNAME ADD-ENTRY [ ADD-OPTIONS ]给已有的set添加一个entry(entry可以理解为一条匹配规则)del SETNAME DEL-ENTRY [ DEL-OPTIONS ]删除一条entry test SETNAME TEST-ENTRY [ TEST-OPTIONS ]测试一个地址是否匹配这个set(即set中有一条entry符合)x, destroy [ SETNAME ]删除set，如果没有指定set名，则删除所有set list [ SETNAME ]根据set名列出set相关信息，没有接set名称则列出所有 save [ SETNAME ]输出一个restory操作可读的格式到标准输出，用于导出数据 restore 通过标准输入恢复save操作产生的数据 flush [ SETNAME ]清空指定set的所有entry 一组类型包括该数据的存储方式和存储类型 TYPENAME = method:datatype[,datatype[,datatype]] 当前的存储方式有bitmap, hash, list 数据类型有ip, mac, port bitmap和list方式使用一个固定大小的存储。hash方式使用一个hash来存储元素 Set Type bitmap:ip 此集合类型使用内存段来存储 IPv4主机(默认) 或 IPv4网络地址 一个此类型可以存储最多 65536 个 entry CREATE-OPTIONS := range fromip-toip|ip/cidr [ netmask cidr ] [ timeout value ]ADD-ENTRY := { ip | fromip-toip | ip/cidr }ADD-OPTIONS := [ timeout value ]DEL-ENTRY := { ip | fromip-toip | ip/cidr }TEST-ENTRY := ip 必选参数: range fromip-toip|ip/cidr 创建一个指定IPv4地址范围的集合，范围的大小(entry个数)最大个数是65536个 可选参数: netmask cidr 当可选的netmask参数指定时，网络地址将取代IP主机地址存储在集合众。 举例: ipset create foo bitmap:ip range 192.168.0.0/16ipset add foo 192.168.1/24ipset test foo 192.168.1.1 bitmap:ip,mac 存储一个IPv4和MAC地址对。此类型最多存储65536个entry。 CREATE-OPTIONS := range fromip-toip|ip/cidr [ timeout value ]ADD-ENTRY := ip[,macaddr]ADD-OPTIONS := [ timeout value ]DEL-ENTRY := ip[,macaddr]TEST-ENTRY := ip[,macaddr] // TODO 举例： ipset create foo bitmap:ip,mac range 192.168.0.0/16ipset add foo 192.168.1.1,12:34:56:78:9A:BC ipset test foo 192.168.1.1 bitmap:port 端口集合最多可存储 65536 个 CREATE-OPTIONS := range fromport-toport [ timeout value ]ADD-ENTRY := { port | fromport-toport }ADD-OPTIONS := [ timeout value ]DEL-ENTRY := { port | fromport-toport }TEST-ENTRY := port 举例： ipset create foo bitmap:port range 0-1024ipset add foo 80ipset test foo 80% ipset add foo 1025ipset v6.17: Element is out of the range of the set hash:ip CREATE-OPTIONS := [ family { inet | inet6 } ] | [ hashsize value ] [ maxelem value ] [ netmask cidr ] [ timeout value ]ADD-ENTRY := ipaddr ADD-OPTIONS := [ timeout value ]DEL-ENTRY := ipaddr TEST-ENTRY := ipaddr// ipaddr := { ip | fromaddr-toaddr | ip/cidr } Note: netmask cidr When the optional netmask parameter specified, network addresses will be stored in the set instead of IP host addresses. The cidr prefix value must be between 1-32 for IPv4 and between 1-128 for IPv6. An IP address will be in the set if the network address, which is resulted by masking the address with the netmask calculated from the prefix, can be found in the set. 意思就是当默认是存储单个ip地址(ip address)，如果指定掩码，则默认是存储的网络地址，见下面的例子 举例： ipset create foo hash:ip netmask 30ipset add foo 192.168.1.0 // 则默认会添加network address: 192.168.1.0/30ipset add foo 192.168.1.0/24ipset test foo 192.168.1.2 hash:net hash:net用于使用hash来存储不同范围大小的ip网络地址。 CREATE-OPTIONS := [ family { inet | inet6 } ] | [ hashsize value ] [ maxelem value ] [ timeout value ]ADD-ENTRY := ip[/cidr]ADD-OPTIONS := [ timeout value ]DEL-ENTRY := ip[/cidr]TEST-ENTRY := ip[/cidr] When adding/deleting/testing entries, if the cidr prefix parameter is not specified, then the host prefix value is assumed. When adding/deleting entries, the exact element is added/deleted and overlapping elements are not checked by the kernel. When testing entries, if a host address is tested, then the kernel tries to match the host address in the networks added to the set and reports the result accordingly. 当添加/删除/测试一条entry时，如果cidr没有指定，则相当于一个host address。 当添加/删除entry时，精确的元素会添加/删除并覆盖原来的 当测试entry时，如果host address被测试，则内核尝试在添加的network中匹配这个host address。 举例： ipset create foo hash:net ipset add foo 192.168.0.0/24ipset add foo 10.1.0.0/16ipset test foo 192.168.0/24 hash:ip,port 此类型使用hash存储ip和port对。这个端口号随协议一起被解析(默认是tcp) CREATE-OPTIONS := [ family { inet | inet6 } ] | [ hashsize value ] [ maxelem value ] [ timeout value ]ADD-ENTRY := ipaddr,[proto:]port ADD-OPTIONS := [ timeout value ]DEL-ENTRY := ipaddr,[proto:]port TEST-ENTRY := ipaddr,[proto:]port//ipaddr := { ip | fromaddr-toaddr | ip/cidr } 举例： ipset create foo hash:ip,port ipset add foo 192.168.1.0/24,80-82ipset add foo 192.168.1.1,udp:53ipset add foo 192.168.1.1,vrrp:0ipset test foo 192.168.1.1,80 详细例子见下面的场景3 hash:net,port 相当于hash:net和hash:port的结合 hash:ip,port,ip 需要三个src/dst匹配，具体见下面的场景三 hash:ip,port,net 同上一个 list:set 此类型使用一个简单的list，可以存储如set名称 CREATE-OPTIONS := [ size value ] [ timeout value ]ADD-ENTRY := setname [ { before | after } setname ]ADD-OPTIONS := [ timeout value ]DEL-ENTRY := setname [ { before | after } setname ]TEST-ENTRY := setname [ { before | after } setname ] TODO 额外的评论 If you want to store same size subnets from a given network (say /24 blocks from a /8 network), use the bitmap:ip set type. If you want to store random same size networks (say random /24 blocks), use the hash:ip set type. If you have got random size of netblocks, use hash:net. 如果想在一个给定的网络范围里存储相同子网大小的段，可以使用bitmap:ip 如果想存储掩码长度相同，网络号随机的网络地址，可以使用hash:ip 如果想存储随机的网络范围(掩码长度和主机号都不一样)，可以使用hash:net iptree和iptreemap set类型都被移除。如果使用他们，则会自动被替换为hash:ip 应用场景 场景1 iptables -A INPUT -s 1.1.1.1 -j DROP iptables -A INPUT -s 2.2.2.2 -j DROP...iptables -A INPUT -s 100.100.100.100 -j DROP 这样会导致iptables规则非常多，降低效率 如果使用ipset ipset -N myset hash:ip ipset -A myset 1.1.1.1ipset -A myset 2.2.2.2...ipset -A myset 100.100.100.100iptables -A INPUT -m set --set myset src -j DROP iptables只需要一条规则，不但提高效率，还易于管理 场景2 比如有以下两条iptables规则： iptables -A INPUT -s ! 1.1.1.1 -g mychain iptables -A INPUT -s ! 2.2.2.2 -g mychain 如果packet来至1.1.1.1，则不会匹配第一条规则，但是会匹配第二条规则 如果packet来至2.2.2.2，则不会匹配第二条规则，但是会匹配第一条规则 这样就互相矛盾了，它实际会匹配所有的packet 用ipset就可以很简单的解决这个问题： ipset -N myset iphash ipset -A myset 1.1.1.1ipset -A myset 2.2.2.2iptables -A INPUT -m set ! --set myset src -g mychain 如果来至1.1.1.1和2.2.2.2的packet，都会不匹配这条规则，但其他packet可以匹配 场景3(*) 比如我们要设置一个ip黑名单，禁止访问本机80端口 方式1 参考:http://daemonkeeper.net/781/mass-blocking-ip-addresses-with-ipset/ ipset create blacklist hash:ip hashsize 4096iptables -I INPUT -m set --match-set blacklist src -p TCP \\ --destination-port 80 -j REJECT ipset add blacklist 192.168.0.5 ipset add blacklist 192.168.0.100 ipset add blacklist 192.168.0.220 这样blacklist里面的ip都回被禁止访问本机80端口 方式2 假设本机ip是192.168.0.5 ipset create foo hash:ip,port iptables -I INPUT -m set --match-set foo dst,dst -j REJECT// 上面这句就是把foo里面的ip,port都当目的ip和目的port来匹配ipset add foo 192.168.0.5,80// 这样当外面访问本机的80端口时，会被REJECT 这个的效果和上面类似，不过可以理解hash:ip,port的作用。 在hash:ip,port手册里有这么一句话： The hash:ip,port type of sets require two src/dst parameters of the set match and SET target kernel modules. 先开始一直不明白是什么意思，参考http://serverfault.com/questions/384132/iptables-limit-rate-of-a-specific-incoming-ip后才弄懂 如果使用hash:ip,port类型，就需要指定两个scr/dst，第一个指定ip是src还是dst的，第二个指定port是src还是dst的。这么控制真是太灵活and牛逼了 包括hash:ip,port,ip等需要三个src/dst都是一样的意思 补充 Size in memory 的单位是?(暂时猜测是字节) References 是指被iptables引用的个数，值为非0时表示有被引用的，这时不能用ipset destroy清除 timeout 属性用于设置规则多久自动清除 必须在set类型设置了timeout后, entry才能使用set设置的默认timeout或自己设置timeout nomatch 用于匹配 hash:net的类型, 表示这些entry不匹配, 相当于 逻辑非 操作 转载：http://bigsec.net/one/tool/ipset.html Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/nat.html":{"url":"blog/kubernetes/网络插件/nat.html","title":"Nat","keywords":"","body":"Nat 任务 vip（183.136.132.116） --->container ip (172.17.0.5) 前提条件（其实docker已经帮我开启转发了） echo \"1\" > /proc/sys/net/ipv4/ip_forward Step1: bind vip ifconfig em1:82 115.231.185.82 broadcast 115.231.185.82 netmask 255.255.255.0 up ip addr del 183.136.132.116 dev em1:116 Step2： set nat rule iptables -t nat -A PREROUTING -d 183.136.132.116 -p tcp --dport 80 -j DNAT --to-destination 172.17.0.5:80 iptables -t nat -A POSTROUTING -d 172.17.05.5 -p tcp --dport 80 -j SNAT --to-source 183.136.132.120 iptables -A INPUT -d 物理机ip地址 -j DROP 正确的做法 iptables -I INPUT 1 -m state --state RELATED,ESTABLISHED -j ACCEPT iptables -t filter -A OUTPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT #打开ping功能 iptables -A INPUT -d 物理机ip地址 -j DROP 单个vip策略 iptables -t nat -A DOCKER -d 183.136.132.124 -p tcp -m tcp --dport 3306 -j DNAT --to-destination 10.0.17.5:80 #实现转发 2.3.容灾ip漂移 删除规则 iptables -t nat -nvL --line-number iptables -t nat -D POSTROUTING 2 iptables -t nat -D PREROUTING 2 iptables -D INPUT 3 iptables -t nat -nvL --line-number | grep 183.136.132.116 | grep DNAT | awk '{print $1}' route | grep docker0 | awk '{print $1}' 获取docker0 4. troub shouting ssh 连接慢的问题 参考 http://blog.51cto.com/jasonyong/280993 iptables -I INPUT -p udp --sport 53 -j ACCEPT iptables -I INPUT -p tcp --sport 53 -j ACCEPT iptables -t nat -A PREROUTING -p tcp --dport -j REDIRECT --to-ports 8080 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:22:34 "},"blog/kubernetes/网络插件/readme.html":{"url":"blog/kubernetes/网络插件/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/网络插件/网络问题总结.html":{"url":"blog/kubernetes/网络插件/网络问题总结.html","title":"网络问题总结","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/证书升级.html":{"url":"blog/kubernetes/证书升级.html","title":"证书升级","keywords":"","body":"证书升级 https://bbs.huaweicloud.com/blogs/200460 总流程： 修改 controller-manage 手动批准证书 要使用内置的 API 方式来签名，首先我们需要配置 kube-controller-manager 组件的 --experimental-cluster-signing-duration 参数，将其调整为10年，我们这里是 kubeadm 安装的集群，所以直接修改静态 Pod 的 yaml 文件即可: $ vi /etc/kubernetes/manifests/kube-controller-manager.yaml ...... spec: containers: - command: - kube-controller-manager # 设置证书有效期为 10 年 - --experimental-cluster-signing-duration=87600h - --client-ca-file=/etc/kubernetes/pki/ca.crt ...... 签发证书 kubeadm alpha certs renew all --use-api --config kubeadm.yaml & 检查证书的有效期 [root@ningbo ~]# kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Aug 27, 2022 09:42 UTC 347d no apiserver Aug 27, 2022 09:42 UTC 347d ca no apiserver-etcd-client Aug 27, 2022 09:42 UTC 347d etcd-ca no apiserver-kubelet-client Aug 27, 2022 09:42 UTC 347d ca no controller-manager.conf Aug 27, 2022 09:42 UTC 347d no etcd-healthcheck-client Aug 27, 2022 09:42 UTC 347d etcd-ca no etcd-peer Aug 27, 2022 09:42 UTC 347d etcd-ca no etcd-server Aug 27, 2022 09:42 UTC 347d etcd-ca no front-proxy-client Aug 27, 2022 09:42 UTC 347d front-proxy-ca no scheduler.conf Aug 27, 2022 09:42 UTC 347d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca May 31, 2031 02:09 UTC 9y no etcd-ca May 31, 2031 02:09 UTC 9y no front-proxy-ca May 31, 2031 02:09 UTC 9y no https://github.com/kubeedge/kubeedge/issues/3019 # Generated by iptables-save v1.4.21 on Thu Sep 16 16:45:56 2021 *mangle :PREROUTING ACCEPT [3785940:1639624962] :INPUT ACCEPT [3703213:1626660186] :FORWARD ACCEPT [82727:12964776] :OUTPUT ACCEPT [3769323:2073222707] :POSTROUTING ACCEPT [3826412:2084082675] :KUBE-KUBELET-CANARY - [0:0] COMMIT # Completed on Thu Sep 16 16:45:56 2021 # Generated by iptables-save v1.4.21 on Thu Sep 16 16:45:56 2021 *nat :PREROUTING ACCEPT [108:13873] :INPUT ACCEPT [54:8277] :OUTPUT ACCEPT [77:14090] :POSTROUTING ACCEPT [129:19498] :DOCKER - [0:0] :KUBE-FIREWALL - [0:0] :KUBE-KUBELET-CANARY - [0:0] :KUBE-LOAD-BALANCER - [0:0] :KUBE-MARK-DROP - [0:0] :KUBE-MARK-MASQ - [0:0] :KUBE-NODE-PORT - [0:0] :KUBE-POSTROUTING - [0:0] :KUBE-SERVICES - [0:0] -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER -A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING -A POSTROUTING -s 169.254.123.0/24 ! -o docker0 -j MASQUERADE -A POSTROUTING -s 10.96.0.0/16 -d 10.96.0.0/16 -j RETURN -A POSTROUTING -s 10.96.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE -A POSTROUTING ! -s 10.96.0.0/16 -d 10.96.0.0/24 -j RETURN -A POSTROUTING ! -s 10.96.0.0/16 -d 10.96.0.0/16 -j MASQUERADE -A DOCKER -i docker0 -j RETURN -A KUBE-FIREWALL -j KUBE-MARK-DROP -A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 -A KUBE-NODE-PORT -p tcp -m comment --comment \"Kubernetes nodeport TCP port for masquerade purpose\" -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE -A KUBE-POSTROUTING -m comment --comment \"Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\" -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE -A KUBE-SERVICES ! -s 10.96.0.0/16 -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT -A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT COMMIT # Completed on Thu Sep 16 16:45:56 2021 # Generated by iptables-save v1.4.21 on Thu Sep 16 16:45:56 2021 *filter :INPUT ACCEPT [8700:1987324] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [8822:2493579] :DOCKER - [0:0] :DOCKER-ISOLATION-STAGE-1 - [0:0] :DOCKER-ISOLATION-STAGE-2 - [0:0] :DOCKER-USER - [0:0] :KUBE-FIREWALL - [0:0] :KUBE-FORWARD - [0:0] :KUBE-KUBELET-CANARY - [0:0] -A INPUT -j KUBE-FIREWALL -A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD -A FORWARD -j DOCKER-USER -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -o docker0 -j DOCKER -A FORWARD -i docker0 ! -o docker0 -j ACCEPT -A FORWARD -i docker0 -o docker0 -j ACCEPT -A FORWARD -s 10.96.0.0/16 -j ACCEPT -A FORWARD -d 10.96.0.0/16 -j ACCEPT -A OUTPUT -j KUBE-FIREWALL -A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2 -A DOCKER-ISOLATION-STAGE-1 -j RETURN -A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP -A DOCKER-ISOLATION-STAGE-2 -j RETURN -A DOCKER-USER -j RETURN -A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP -A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000/0x4000 -j ACCEPT -A KUBE-FORWARD -s 10.96.0.0/16 -m comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A KUBE-FORWARD -d 10.96.0.0/16 -m comment --comment \"kubernetes forwarding conntrack pod destination rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT COMMIT # Completed on Thu Sep 16 16:45:56 2021 # Generated by iptables-save v1.4.21 on Thu Sep 16 16:46:40 2021 *mangle :PREROUTING ACCEPT [68090052:41831148847] :INPUT ACCEPT [33998211:25943851208] :FORWARD ACCEPT [34091841:15887297639] :OUTPUT ACCEPT [33533180:38301554094] :POSTROUTING ACCEPT [67521800:54182324901] :KUBE-KUBELET-CANARY - [0:0] COMMIT # Completed on Thu Sep 16 16:46:40 2021 # Generated by iptables-save v1.4.21 on Thu Sep 16 16:46:40 2021 *nat :PREROUTING ACCEPT [9:2012] :INPUT ACCEPT [9:2012] :OUTPUT ACCEPT [8:1240] :POSTROUTING ACCEPT [16:2080] :DOCKER - [0:0] :KUBE-FIREWALL - [0:0] :KUBE-KUBELET-CANARY - [0:0] :KUBE-LOAD-BALANCER - [0:0] :KUBE-MARK-DROP - [0:0] :KUBE-MARK-MASQ - [0:0] :KUBE-NODE-PORT - [0:0] :KUBE-POSTROUTING - [0:0] :KUBE-SERVICES - [0:0] -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER -A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING -A POSTROUTING -s 169.254.123.0/24 ! -o docker0 -j MASQUERADE -A POSTROUTING -s 10.96.0.0/16 -d 10.96.0.0/16 -j RETURN -A POSTROUTING -s 10.96.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE -A POSTROUTING ! -s 10.96.0.0/16 -d 10.96.2.0/24 -j RETURN -A POSTROUTING ! -s 10.96.0.0/16 -d 10.96.0.0/16 -j MASQUERADE -A DOCKER -i docker0 -j RETURN -A KUBE-FIREWALL -j KUBE-MARK-DROP -A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 -A KUBE-NODE-PORT -p tcp -m comment --comment \"Kubernetes nodeport TCP port for masquerade purpose\" -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE -A KUBE-POSTROUTING -m comment --comment \"Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\" -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE -A KUBE-SERVICES ! -s 10.96.0.0/16 -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT -A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT COMMIT # Completed on Thu Sep 16 16:46:40 2021 # Generated by iptables-save v1.4.21 on Thu Sep 16 16:46:40 2021 *filter :INPUT ACCEPT [48:10831] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [46:10128] :DOCKER - [0:0] :DOCKER-ISOLATION-STAGE-1 - [0:0] :DOCKER-ISOLATION-STAGE-2 - [0:0] :DOCKER-USER - [0:0] :KUBE-FIREWALL - [0:0] :KUBE-FORWARD - [0:0] :KUBE-KUBELET-CANARY - [0:0] -A INPUT -j KUBE-FIREWALL -A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD -A FORWARD -j DOCKER-USER -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -o docker0 -j DOCKER -A FORWARD -i docker0 ! -o docker0 -j ACCEPT -A FORWARD -i docker0 -o docker0 -j ACCEPT -A FORWARD -s 10.96.0.0/16 -j ACCEPT -A FORWARD -d 10.96.0.0/16 -j ACCEPT -A OUTPUT -j KUBE-FIREWALL -A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2 -A DOCKER-ISOLATION-STAGE-1 -j RETURN -A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP -A DOCKER-ISOLATION-STAGE-2 -j RETURN -A DOCKER-USER -j RETURN -A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP -A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000/0x4000 -j ACCEPT -A KUBE-FORWARD -s 10.96.0.0/16 -m comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A KUBE-FORWARD -d 10.96.0.0/16 -m comment --comment \"kubernetes forwarding conntrack pod destination rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT COMMIT # Completed on Thu Sep 16 16:46:40 2021 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/":{"url":"blog/kubernetes/镜像/","title":"镜像","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/Containerd Stargz Snapshotter.html":{"url":"blog/kubernetes/镜像/Containerd Stargz Snapshotter.html","title":"Containerd Stargz Snapshotter","keywords":"","body":"Containerd Stargz Snapshotter 问题： 拉取镜像占用了容器启动时间的 76%，只有 6.4% 的时间用来读取数据。 这个问题一直困扰着各类工作负载，包括 serverless 函数的冷启动时间，镜像构建过程中基础镜像的拉取等。虽然有各种折中的解决方案，但这些方案都有缺陷： 缓存镜像 : 冷启动时仍然有性能损失。 减小镜像体积 : 无法避免某些场景需要用到大体积的镜像，比如机器学习。 Containerd 为了解决这个问题启动了一个非核心子项目 Stargz Snapshotter[2]，旨在提高镜像拉取的性能。该项目作为 Containerd 的一个插件，利用 Google 的 stargz 镜像格式[3]来延迟拉取镜像。这里的延迟拉取指的是 Containerd 在拉取时不会拉取整个镜像文件，而是按需获取必要的文件。 legacy shows the startup performance when we use containerd's default snapshotter (overlayfs) with images copied from docker.io/library without optimization. For this configuration, containerd pulls entire image contents and pull operation takes accordingly. When we use stargz snapshotter with eStargz-converted images but without any optimization (estargz-noopt) we are seeing performance improvement on the pull operation because containerd can start the container without waiting for the pull completion and fetch necessary chunks of the image on-demand. But at the same time, we see the performance drawback for run operation because each access to files takes extra time for fetching them from the registry. When we use eStargz with optimization (estargz), we can mitigate the performance drawback observed in estargz-noopt images. This is because stargz snapshotter prefetches and caches likely accessed files during running the container. On the first container creation, stargz snapshotter waits for the prefetch completion so create sometimes takes longer than other types of image. But it's still shorter than waiting for downloading all files of all layers. The above histogram is the benchmarking result on the commit ecdb227. We are constantly measuring the performance of this snapshotter so you can get the latest one through the badge shown top of this doc. Please note that we sometimes see dispersion among the results because of the NW condition on the internet and the location of the instance in the Github Actions, etc. Our benchmarking method is based on HelloBench. Stargz Snapshotter is a non-core sub-project of containerd. Quick Start with Kubernetes For more details about stargz snapshotter plugin and its configuration, refer to Containerd Stargz Snapshotter Plugin Overview. For using stargz snapshotter on kubernetes nodes, you need the following configuration to containerd as well as run stargz snapshotter daemon on the node. We assume that you are using containerd (> v1.4.2) as a CRI runtime. version = 2 # Plug stargz snapshotter into containerd # Containerd recognizes stargz snapshotter through specified socket address. # The specified address below is the default which stargz snapshotter listen to. [proxy_plugins] [proxy_plugins.stargz] type = \"snapshot\" address = \"/run/containerd-stargz-grpc/containerd-stargz-grpc.sock\" # Use stargz snapshotter through CRI [plugins.\"io.containerd.grpc.v1.cri\".containerd] snapshotter = \"stargz\" disable_snapshot_annotations = false Note that disable_snapshot_annotations = false is required since containerd > v1.4.2 This repo contains a Dockerfile as a KinD node image which includes the above configuration. You can use it with KinD like the following, $ docker build -t stargz-kind-node https://github.com/containerd/stargz-snapshotter.git $ kind create cluster --name stargz-demo --image stargz-kind-node Then you can create eStargz pods on the cluster. In this example, we create a stargz-converted Node.js pod (ghcr.io/stargz-containers/node:13.13.0-esgz) as a demo. apiVersion: v1 kind: Pod metadata: name: nodejs spec: containers: - name: nodejs-stargz image: ghcr.io/stargz-containers/node:13.13.0-esgz command: [\"node\"] args: - -e - var http = require('http'); http.createServer(function(req, res) { res.writeHead(200); res.end('Hello World!\\n'); }).listen(80); ports: - containerPort: 80 The following command lazily pulls ghcr.io/stargz-containers/node:13.13.0-esgz from Github Container Registry and creates the pod so the time to take for it is shorter than the original image library/node:13.13. $ kubectl --context kind-stargz-demo apply -f stargz-pod.yaml && kubectl get po nodejs -w $ kubectl --context kind-stargz-demo port-forward nodejs 8080:80 & $ curl 127.0.0.1:8080 Hello World! Stargz snapshotter also supports further configuration including private registry authentication, mirror registries, etc. DADI Block-Level Image Service ， Stargz Snapshotter， Nydus Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/OCI.html":{"url":"blog/kubernetes/镜像/OCI.html","title":"OCI","keywords":"","body":"OCI 镜像规范 OCI镜像规范是以Docker镜像规范v2为基础制定的，它定义了镜像的主要格式及内容，主要用于镜像仓库存放镜像及分发镜像等场景，与正在制定的OCI分发规范（参见1.5.2节）密切相关。OCI运行时在创建容器前，要把镜像下载并解压成符合运行时规范的文件系统包，并且把镜像中的配置转化成运行时配置，然后启动容器。 OCI 镜像包括4个部分： 镜像索引（Image Index）， 清单（Manifest），配置（configuration）和层文件（Layers）。 清单是JSON格式的描述文件，列出了镜像的配置和层文件。 配置是JSON格式的描述文件，说明了镜像运行参数。 层文件则是镜像内容，即镜像包含的文件，一般是二进制数据文件格式（Blob)。 一个镜像可以有一个或多个层文件，镜像索引不是必须的，如果存在，则指明了一组支持不同架构平台的相关镜像。镜像的4个部分之间是通过摘要（digest）来相互引用（reference）的。镜像各部分的关系如图1-13所示。 镜像分发 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/harbor/":{"url":"blog/kubernetes/镜像/harbor/","title":"Harbor","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/harbor/readme.html":{"url":"blog/kubernetes/镜像/harbor/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/readme.html":{"url":"blog/kubernetes/镜像/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/自动构建镜像.html":{"url":"blog/kubernetes/镜像/自动构建镜像.html","title":"自动构建镜像","keywords":"","body":"二进制自动构建 link buildpacks buildpacks registry Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/镜像加速方案.html":{"url":"blog/kubernetes/镜像/镜像加速方案.html","title":"镜像加速方案","keywords":"","body":"镜像背景知识 为了理解overlaybd的原理，首先需要了解容器镜像的分层机制。容器镜像由多个增量layer文件组成，在使用时进行叠加，这样在镜像分发时只需要对layer文件进行分发。每一层实质上都是与上一层的差异（包括文件的添加，修改或删除）的压缩包。 容器引擎可以通过其storage driver，按照约定的方式将差异叠加起来，然后以Read-Only的模式挂载到指定目录，该目录即称为lower_dir；而以Read/Write模式挂载的可写层，挂载目录则一般称为upper_dir。 Stargz Snapshotter 实现原理 Stargz： 使用可索引的压缩技术，之前构建镜像是打包成一整个的tar文件，现在是 打包成多个可索引的 gzip 文件 Snapshotter： 延迟拉取数据， 只拉取部分元数据，然后启动镜像，剩下的部分先远程挂载，再慢慢下载。 DADI ​ overlaybd本身没有文件的概念，它只是将镜像抽象为虚拟块设备，并在其上装载常规的文件系统。当用户应用读取数据时，该读取请求首先由常规的文件系统处理，将请求转换为虚拟块设备的一次或多次读取。这些读取请求会被转发到用户态的接收程序，即overlaybd的运行时载体，最后转换为对一个或多个layer的随机读取。 ​ ​ 与传统镜像一样，overlaybd在内部仍然保留着layer分层的结构，但每层的内容都是文件系统变更差异对应的一系列data block。overlaybd向上提供了一个合并视图，对layer的叠加规则很简单，即对于任意一个data block，总是使用最后的变更，在layer中未发生变更的块均视为全零块；向下又提供了将一系列data block导出成一个layer文件的功能，该文件高密度非稀疏、且可索引。因此，对块设备某个连续LBA范围进行读操作，可能包含了原本属于多层的小块数据段，我们将这些小块数据段称为segment。从segment的属性中找到层号，便能够继续映射到对这层的layer文件的读取上来。传统的容器镜像可以将它的layer文件保存在Registry或者对象存储上，那么overlaybd镜像自然也可以 为了更好的兼容性，overlaybd在layer文件的最外层，包装了一层tar文件的头和尾，这样伪装成一个tar文件。由于 tar内部仅一个文件，不影响按需读取。目前无论是docker、containerd或者buildkit，对镜像的下载或上传默认都有untar和tar的流程，不侵入代码是无法逾越的，所以增加tar伪装有利于兼容性和流程的统一，例如在镜像转换、构建、或者全量下载使用时，都无需修改代码，只需提供插件即可。 架构 Nydus 实现原理： ​ nydus 项目优化了现有的 OCI 镜像标准格式，并以此设计了一个用户态的文件系统。通过这些优化，nydus 能够提供这些特性： 容器镜像按需下载，用户不再需要下载完整镜像就能启动容器 块级别的镜像数据去重，最大限度为用户节省存储资源 镜像只有最终可用的数据，不需要保存和下载过期数据 端到端的数据一致性校验，为用户提供更好的数据保护 兼容 OCI 分发标准和 artifacts 标准，开箱即可用 支持不同的镜像存储后端，镜像数据不只可以存放在镜像仓库，还可以放到 NAS 或者类似 S3 的对象存储上 与 Dragonfly 的良好集成 架构上， nydus 主要包含一个新的镜像格式，和一个负责解析容器镜像的 FUSE 用户态文件系统进程。 ydus 能够解析 FUSE 或者 virtiofs 协议来支持传统的 runc 容器或者 Kata 容器。容器仓库，OSS 对象存储，NAS，以及 Dragonfly 的超级节点和 peer 节点都可以作为 nydus 的镜像数据源。同时， nydus 还可以配置一个本地缓存，从而避免每次启动都从远端数据源拉取数据。 镜像格式方面， nydus 把一个容器镜像分成元数据和数据两层。其中元数据层是一颗自校验的哈希树。每个文件和目录都是哈希树中的一个附带哈希值的节点。一个文件节点的哈希值是由文件的数据确定，一个目录节点的哈希值则是由该目录下所有文件和目录的哈希值确定。每个文件的数据被按照固定大小切片并保存到数据层中。数据切片可以在不同文件以及不同镜像中的不同文件共享。 Nydus 能为用户带来什么？ 用户如果部署了 nydus 镜像服务，最直观的一个感受就是，容器启动变快了，从以前的明显时间消耗，变成了几乎瞬间就能启动起来。在我们的测试中， nydus 能够把常见镜像的启动时间，从数分钟缩短到数秒钟。 另外一个不那么明显但也很重要的改进，是 nydus 能够为用户提供容器运行时数据一致性校验。在传统的镜像中，镜像数据会先被解压到本地文件系统，再由容器应用去访问使用。解压前，镜像数据是完整校验的。但是解压之后，镜像数据不再能够被校验。这带来的一个问题就是，如果解压后的镜像数据被无意或者恶意地修改，用户是无法感知的。而 nydus 镜像不会被解压到本地，同时可以对每一次数据访问进行校验，如果数据被篡改，则可以从远端数据源重新拉取。 性能上的横向对比暂时还没有具体数据，对比 stargz 来说 nydus 在 rootfs mount 时只有一层，stargz 有多层，nydus 优势是性能有提升，有数据端到端的校验。对比 dadi 来说，nydus 是文件系统粒度的，好处是能对镜像做文件级的操作，比如做镜像扫描，统计安全分析，镜像修复等，dadi 是块设备粒度的。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/镜像/镜像延迟加载技术解析.html":{"url":"blog/kubernetes/镜像/镜像延迟加载技术解析.html","title":"镜像延迟加载技术解析","keywords":"","body":"镜像延迟加载技术解析 1. 概述 现在越来越多的AI训练选择用镜像的方式进行，然而tensorflow等镜像的大小要远大于一般镜像（dockerhub上tensorflow/tensorflow:latest-devel-gpu有3.19GB，同比centos:latest只有约234MB），如此庞大的镜像给AI训练带来较大的镜像拉取时延。然而，在AI训练场景下，镜像内的许多文件可能是不需要去访问的（例如tensorflow镜像内的自带数据集等），基于这一切入点，一种基于“懒”加载的镜像延迟加载技术被提出，极大的降低了镜像的拉取时延。 1.1 overlayFS和容器镜像加载 为了介绍镜像延迟加载的技术，了解overlayFS和docker的镜像加载过程是必要的。 overlayFS是一种堆叠文件系统，并于2014年合并入Linux内核，在docker1.12后推出的overlay2在inode的利用方面比overlay更有效，至于为何有效我会放到这一小节末尾介绍，这里我先简要介绍一下overlay2。overlay2的基本结构如下图所示。 overlay2的读写场景如下，其中容器层Upper Dir，镜像层就是所有Lower Dir： 读的文件不在容器层：如果读的文件不在容器层，则从镜像层进行读 读的文件只存在在容器层：直接从容器层读 读的文件在容器层和镜像层：读容器层中的文件，因为容器层隐藏了镜像层同名的文件 写的文件不在容器层，在镜像层：由于文件不在容器层，因此overlay/overlay2存储驱动使用copy_up操作从镜像层拷贝文件到容器层，然后将写入的内容写入到文件新的拷贝中。如果是新增文件，其上层目录会和底层layer进行merge操作合并为新的目录 删除文件和目录：删除镜像层的文件，会在容器层创建一个whiteout文件来隐藏它；删除镜像层的目录，会创建opaque目录，它和whiteout文件有相同的效果 重命名目录：对一个目录调用rename仅仅在资源和目的地路径都在顶层时才被允许，否则返回EXDEV overlay是docker的存储驱动之一，docker里镜像驱动被抽象为graphdriver，每个镜像驱动（如overlay，aufs等）都实现了graphdriver的接口，通过fuse或内核态挂载对镜像的生命周期进行管理；而在containerd中，类似的模块叫做snapshotter，这个模块也是镜像延迟加载的关键，会在之后的章节详细介绍。 这里用ubuntu镜像来反应overlayFS的结构，执行docker pull ubuntu:latest后，镜像的拉取和解压会分层并行进行。docker在拉取镜像时会经过解析registry，解析镜像名，解析镜像tag，配置认证信息等步骤，在所有步骤执行完毕后docker的根目录内有如下的结构。 [root@VM_244_112_centos /var/lib/docker/overlay2]# tree -L 2 . ├── 91c6e5cc41f59fe3b914f368e7864ad4d708120069f7443c2c14e3ebb7adf688 │ ├── diff │ └── link ├── b4c8df66bd6bb6a06630bde87364d41cebcc7a311d7f3958159420d27ddee6a5 │ ├── diff │ ├── link │ ├── lower │ └── work ├── e45ca749ade1f4cf41cb81dd210a4761023bcefd8523039b22d8a5e768684a20 │ ├── diff │ ├── link │ ├── lower │ └── work └── l ├── 4SMJFTXKOSE3KIMJADFEHEIPOC -> ../b4c8df66bd6bb6a06630bde87364d41cebcc7a311d7f3958159420d27ddee6a5/diff ├── JMHFXUR2BPPIHDY53N7AWQFYHQ -> ../e45ca749ade1f4cf41cb81dd210a4761023bcefd8523039b22d8a5e768684a20/diff └── SSNCTFQYZMQSONPRWRXBQW4W6P -> ../91c6e5cc41f59fe3b914f368e7864ad4d708120069f7443c2c14e3ebb7adf688/diff 12 directories, 5 files ubuntu的镜像有3层layer，其中l目录中包含符号链接作为缩短的层标示符，这些标识符用来避免挂载时超过页面大小的限制，可以观察到每个link都是指向ubuntu其中一个镜像层的diff目录。同样，每个层目录的link文件也记录着该层的缩短标识符。 [root@VM_244_112_centos /var/lib/docker/overlay2/91c6e5cc41f59fe3b914f368e7864ad4d708120069f7443c2c14e3ebb7adf688]# cat link SSNCTFQYZMQSONPRWRXBQW4W6P [root@VM_244_112_centos /var/lib/docker/overlay2/91c6e5cc41f59fe3b914f368e7864ad4d708120069f7443c2c14e3ebb7adf688]# ll ../l | grep SSNCTFQYZMQSONPRWRXBQW4W6P lrwxrwxrwx 1 root root 72 Dec 22 19:16 SSNCTFQYZMQSONPRWRXBQW4W6P -> ../91c6e5cc41f59fe3b914f368e7864ad4d708120069f7443c2c14e3ebb7adf688/diff 层目录中的lower文件记录了下层layer的缩短标识符（即l目录下的标识符），注意底层layer是没有lower文件的。 [root@VM_244_112_centos /var/lib/docker/overlay2/b4c8df66bd6bb6a06630bde87364d41cebcc7a311d7f3958159420d27ddee6a5]# ls diff link lower work [root@VM_244_112_centos /var/lib/docker/overlay2/b4c8df66bd6bb6a06630bde87364d41cebcc7a311d7f3958159420d27ddee6a5]# cat lower l/SSNCTFQYZMQSONPRWRXBQW4W6P 每层目录下的work目录用来完成诸如copy-on-write的操作。 此时拉起一个ubuntu:latest的容器，cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0-init和cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0两个文件夹会出现在overlay2的目录下。查看这两个目录的内容。 [root@VM_244_112_centos /var/lib/docker/overlay2/cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0]# tree -L 2 /var/lib/docker/overlay2/cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0* /var/lib/docker/overlay2/cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0 ├── diff ├── link ├── lower ├── merged │ ├── bin -> usr/bin │ ├── boot │ ├── dev │ ├── etc │ ├── home │ ├── lib -> usr/lib │ ├── lib32 -> usr/lib32 │ ├── lib64 -> usr/lib64 │ ├── libx32 -> usr/libx32 │ ├── media │ ├── mnt │ ├── opt │ ├── proc │ ├── root │ ├── run │ ├── sbin -> usr/sbin │ ├── srv │ ├── sys │ ├── tmp │ ├── usr │ └── var └── work └── work /var/lib/docker/overlay2/cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0-init ├── diff │ ├── dev │ └── etc ├── link ├── lower └── work └── work 30 directories, 4 files 其中init为初始层，容器初始化时需要注入容器的信息都会放在初始层中（如主机信息，域名服务文件等），对容器作出改变的操作都在读写层完成。 # 容器内创建一个test文件 root@aa161ee6e328:/# echo test > test root@aa161ee6e328:/# ls bin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys test tmp usr var # 该容器镜像层下的验证 [root@VM_244_112_centos /var/lib/docker/overlay2/cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0]# cat ./diff/test test [root@VM_244_112_centos /var/lib/docker/overlay2/cc85d9b18e2e79ff63045ef531aef9e224db23662652c3fbb923f765e5d185a0]# cat ./merged/test test overlay2支持多层lower层（最多支持128个），而overlay只支持两层（一个lower层，一个upper层），在overlay中，下层文件在上层中是以hard link的形式存在，而在linux操作系统中hard link会消耗inode，这也是为何overlay2对比overlay能节省更多inode的原因。 1.2 为什么 在整个容器拉起过程中，镜像加载占据了大部分时间。在Harter[3]的调查中，拉取镜像文件占据了76%的容器拉起时间，但是镜像内只有6.4%的内容被读取，在AI训练场景下这一问题更加突出。如概述所述，AI训练相关镜像动辄达到GB数量级，拉取会产生大量时延，但大部分耗时却花销在了不会访问的镜像内容上，降低了AI训练的效率。 1.3 传统方案 镜像缓存/预加载 传统方案就是在母机上缓存拉取过的镜像层（当然，docker已经支持了这一功能），并配置pod的imagePolicy为IfNotPresent，镜像的分层让不同镜像之间可以对层进行共享，从而减少拉取其他镜像的时延。 同样的，预加载也是在母机上预先加载一部分关键镜像（例如tensorflow，ubuntu等），拉取其他镜像时，部分镜像层会命中镜像仓库的缓存，从而降低拉取时延。 但是这种方法在cold start（例如刚刚上架的母机上没有缓存的镜像）或imagePolicy为Always的场景下表现不佳，且预加载只能缓解部分镜像的时延，为了增加命中，也会消耗大量母机的磁盘空间。 减小镜像大小 这种方法成本较高（需要人工修改镜像），且不适用于部分AI镜像。 2. 原理 镜像延迟加载的原理一句话概括便是只拉取镜像的索引文件，在用户挂载访问镜像时“懒惰”的拉取镜像内容。 2.1 术语 CRFS：一种基于FUSE的文件系统，支持直接从远程registry镜像仓库挂载镜像到本地 stargz：CRFS依赖的镜像格式，由于传统的targz压缩无法索引且乱序，CRFS的挂载需要可索引、有序的stargz格式压缩文件 estargz：stargz的一种优化，在stargz基础上利用prefetch landmark区分高低优先文件，高优文件会直接拉取 snapshotter：管理镜像本地状态变化和挂载的containerd模块，每个snapshotter都有一个对应的文件系统 remote layer：可支持远程挂载的镜像层，例如stargz格式的镜像内每一层都是remote layer remote snapshotter：支持挂载remote layer的snapshotter 2.2 CRFS和stargz CRFS是一种FUSE文件系统，它允许用户直接从镜像仓库挂载镜像到本地而不需要通过拉取。它的挂载和overlay类似，事实上，stargz格式的镜像也可以直接用overlay`文件系统挂载。 CRFS的代码通过Golang的fuseAPI实现类似overlay2的文件系统，可以在这里查看，这里不再展开。 2.2.1 tar文件 目前容器镜像都是由tar.gz文件表示，gzip流是不可搜索，且tar文件也没有索引，因此即使要读取1KB的文件也需要从远程镜像仓库拉取整个镜像，这种文件格式并不支持CRFS的挂载。 同时，符合OCI格式的镜像是以层为单位进行校验，而CRFS需要以文件为单位进行校验。 tar.gz的压缩格式为Gzip(TarF(file1) + TarF(file2) + TarF(file3) + TarFooter)) 2.2.2 stargz文件 stargz的文件格式为Gzip(TarF(file1)) + Gzip(TarF(file2)) + Gzip(TarF(file3_chunk1)) + Gzip(F(file3_chunk2)) + Gzip(F(index of earlier files in magic file), TarFooter)，tar.gz和stargz的对比如下图所示。 stargz文件会将镜像层的骨架抽取为一个TOC(JSON)文件，它作为该层的元数据保存让该层的stargz文件可索引，具体的结构可参考https://github.com/containerd/stargz-snapshotter/blob/master/docs/stargz-estargz.md。 2.2.3 estargz文件 estargz文件通过landmark类型的文件区分需要优先拉取的文件内容，在landmark前的文件作为优先文件会在拉取镜像时直接被拉取，在预拉取和普通拉取之间找到一个平衡点。 2.3 containerd和snapshotter 2.3.1 containerd Containerd 是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性。Containerd 可以在宿主机中管理完整的容器生命周期：容器镜像的传输和存储、容器的执行和管理、存储和网络等。 目前docker创建镜像的流程之一就是调用containerd的grpc接口，它通过调用cri创建容器，目前k8s支持直接对接containerd。它的架构如下图所示。 2.3.2 snapshotter 其中snapshotter负责镜像的挂载和状态转换（类似docker的graphdriver），它允许符合OCI标准的镜像能在不通的操作系统上运行，不同于graphdriver，它是一种更加灵活的模型，除提供基本的挂载和快照的功能外，它和镜像结构的耦合没有那么紧密。它的API定义如下，可以在containerd的源码内查看API的详细定义。 snapshotter提供了分配、快照和挂载抽象且基于层结构的文件系统的API，我们可以认为每种snapshotter都对应一种分层的文件系统，一个snapshot的状态流转可用下图表示。 snapshot反映了一个文件系统的状态，每个snapshot都有一个父节点，父子节点间的差别可以转化为一个layer（类似于docker的graphdriver）。Committed状态的snapshot可以通过Prepare调用转化为Active，反之用Commit调用。Active状态的snapshot就是我们正在用的容器，容器内做的所有修改操作都可以通过Commit转化为一个新的layer，从而获得一个子镜像。 在containerd的源码中，每种分层文件系统都对应一种snapshotter（例如overlay,zfs等），在镜像延迟加载中，抽象化的snapshotter允许开发者针对CRFS文件系统提供一个插件，在镜像拉取时通过指定snapshotter实现拉取的具体操作，并把延迟加载拉取需要的信息通过label打入镜像，在挂载时通过fuse的远程挂载stargz格式的镜像到本地，这类snapshotter也被成为remote snapshotter（remote snapshotter也需要对containerd的源码进行更改，目前只有1.4.2后的版本支持remote snapshotter）。 2.4 stargz-snapshotter 在了解stargz和snapshotter后，stargz-snapshotter便是两者的结合。 stargz格式的镜像层在拉取时会校验是否为remote snapshotter挂载的镜像，如果不是则拉取其内容，如果是则不拉取内容；随后镜像在运行时会转交给stargz snapshotter处理，对于一般的镜像直接解压缩，对stargz格式的镜像用镜像元数据进行远程挂载，最终拉起容器提供给用户。 由于镜像拉取和挂载在containerd中已经解耦，remote snapshotter在接手镜像前需要保证镜像内容没有被拉取，因此在拉取镜像时需要将使用的snapshotter传入并校验一个镜像层是否可以远程挂载，并将可以远程挂载的层过滤掉，具体改动在这个commit。 整体的流程如上图所示，一般镜像的拉起需要经过下载、解压、挂载的步骤，但是remote snapshotter并不需要下载、解压镜像的内容，在筛选blob的流程内如果检测到某个层是remote layer（通过镜像层的元数据检测），remote snapshotter会直接commit这个层，从而跳过下载和解压的过程。 3. 使用 stargz-snapshotter的作者已经预转化了多种镜像并做了实验，下面是他给出的数据。 当然，在AI的场景下延迟加载的效果也需要重新验证，下面我来验证一下在tensorflow镜像上跑benchmark的效果。 3.1 环境配置 kubelet版本 >= 1.10（为了CRI对接containerd） containerd版本 >= 1.4.2 containerd配置 # /etc/containerd/config.toml # See also: https://github.com/kubernetes-sigs/kind/blob/fd64a56b0c3d5654eb6d22bce812e2a87eac5853/images/base/files/etc/containerd/config.toml # explicitly use v2 config format version = 2 # - Set default runtime handler to v2, which has a per-pod shim # - Enable to use stargz snapshotter [plugins.\"io.containerd.grpc.v1.cri\".containerd] default_runtime_name = \"runc\" snapshotter = \"stargz\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] runtime_type = \"io.containerd.runc.v2\" # Use stargz snapshotter [proxy_plugins] [proxy_plugins.stargz] type = \"snapshot\" address = \"/run/containerd-stargz-grpc/containerd-stargz-grpc.sock\" 通过作者给出的转化工具(教程)将dockerhub上最新的tensorflow镜像转化为esgz格式，推送到registry上 3.2 stargz-snapshotter部署 kubelet对接containerd # /etc/sysconfig/kubelet KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock 部署stargz-snapshotter 拉下repo直接make，编译出来的二进制通过如下service文件在systemctl上部署。 [Unit] Description=containerd stargz snapshotter Documentation=https://github.com/containerd/stargz-snapshotter After=network.target Before=containerd.service [Service] ExecStart=/usr/local/bin/containerd-stargz-grpc --address=/run/containerd-stargz-grpc/containerd-stargz-grpc.sock --config=/etc/containerd-stargz-grpc/config.toml Restart=always RestartSec=1 [Install] WantedBy=multi-user.target 3.3 验证 普通镜像 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m30s default-scheduler Successfully assigned default/tensorflow to k8s-cpu-node-1 Normal Pulling 3m27s kubelet, k8s-cpu-node-1 Pulling image \"elihe/tensorflow:v1.0.5\" Normal Pulled 2s kubelet, k8s-cpu-node-1 Successfully pulled image \"elihe/tensorflow:v1.0.5\" Normal Created 2s kubelet, k8s-cpu-node-1 Created container tensorflow Normal Started 2s kubelet, k8s-cpu-node-1 Started container tensorflow /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)]) WARNING:tensorflow:From /benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:1120: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version. Instructions for updating: Please switch to tf.train.MonitoredTrainingSession TensorFlow: 1.5 Model: trivial Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64.0 per device Devices: ['/cpu:0'] Data format: NHWC Layout optimizer: False Optimizer: sgd Variables: parameter_server ========== Generating model Running warm up Done warm up Step Img/sec loss 1 images/sec: 513.7 +/- 0.0 (jitter = 0.0) 7.055 10 images/sec: 518.4 +/- 4.5 (jitter = 15.4) 7.055 20 images/sec: 517.8 +/- 3.2 (jitter = 17.8) 7.055 30 images/sec: 521.5 +/- 2.4 (jitter = 11.6) 7.055 40 images/sec: 520.1 +/- 2.3 (jitter = 13.1) 7.055 50 images/sec: 519.2 +/- 2.2 (jitter = 12.8) 7.055 60 images/sec: 518.0 +/- 2.0 (jitter = 14.5) 7.055 70 images/sec: 517.2 +/- 1.8 (jitter = 14.8) 7.055 80 images/sec: 515.3 +/- 1.9 (jitter = 15.6) 7.055 90 images/sec: 513.7 +/- 1.9 (jitter = 17.0) 7.055 100 images/sec: 514.2 +/- 1.7 (jitter = 16.7) 7.055 110 images/sec: 513.3 +/- 1.6 (jitter = 17.7) 7.055 120 images/sec: 512.6 +/- 1.6 (jitter = 18.6) 7.055 130 images/sec: 513.1 +/- 1.5 (jitter = 17.5) 7.055 140 images/sec: 512.7 +/- 1.4 (jitter = 19.1) 7.055 150 images/sec: 512.6 +/- 1.4 (jitter = 20.2) 7.055 160 images/sec: 512.6 +/- 1.4 (jitter = 19.9) 7.055 170 images/sec: 513.1 +/- 1.3 (jitter = 19.3) 7.055 180 images/sec: 512.9 +/- 1.3 (jitter = 20.0) 7.055 190 images/sec: 512.9 +/- 1.3 (jitter = 20.0) 7.055 200 images/sec: 513.5 +/- 1.2 (jitter = 19.8) 7.055 210 images/sec: 513.8 +/- 1.2 (jitter = 19.0) 7.055 220 images/sec: 514.0 +/- 1.1 (jitter = 18.6) 7.055 230 images/sec: 513.9 +/- 1.1 (jitter = 18.7) 7.055 240 images/sec: 514.5 +/- 1.1 (jitter = 18.4) 7.055 250 images/sec: 514.4 +/- 1.1 (jitter = 18.6) 7.055 260 images/sec: 514.8 +/- 1.1 (jitter = 18.6) 7.055 270 images/sec: 514.6 +/- 1.1 (jitter = 18.7) 7.055 280 images/sec: 514.8 +/- 1.0 (jitter = 18.6) 7.055 290 images/sec: 515.1 +/- 1.0 (jitter = 18.6) 7.055 300 images/sec: 515.2 +/- 1.0 (jitter = 18.2) 7.055 310 images/sec: 515.2 +/- 1.0 (jitter = 18.2) 7.055 320 images/sec: 515.2 +/- 1.0 (jitter = 17.7) 7.055 330 images/sec: 515.3 +/- 0.9 (jitter = 17.5) 7.055 340 images/sec: 515.4 +/- 0.9 (jitter = 17.9) 7.055 350 images/sec: 515.6 +/- 0.9 (jitter = 17.5) 7.055 360 images/sec: 515.5 +/- 0.9 (jitter = 17.5) 7.055 370 images/sec: 515.6 +/- 0.9 (jitter = 17.5) 7.055 380 images/sec: 515.8 +/- 0.9 (jitter = 17.4) 7.055 390 images/sec: 515.3 +/- 0.9 (jitter = 17.8) 7.055 400 images/sec: 515.1 +/- 0.9 (jitter = 18.4) 7.055 410 images/sec: 514.6 +/- 0.9 (jitter = 18.7) 7.055 420 images/sec: 514.1 +/- 0.9 (jitter = 19.4) 7.055 430 images/sec: 512.5 +/- 1.0 (jitter = 20.2) 7.055 440 images/sec: 512.3 +/- 1.0 (jitter = 20.3) 7.055 450 images/sec: 512.3 +/- 1.0 (jitter = 20.5) 7.055 460 images/sec: 511.9 +/- 1.0 (jitter = 20.7) 7.055 470 images/sec: 511.5 +/- 1.0 (jitter = 21.0) 7.055 480 images/sec: 511.1 +/- 1.0 (jitter = 21.3) 7.055 490 images/sec: 510.8 +/- 1.0 (jitter = 21.3) 7.055 500 images/sec: 510.7 +/- 1.0 (jitter = 21.1) 7.055 ---------------------------------------------------------------- total images/sec: 511.38 ---------------------------------------------------------------- esgz镜像 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 6s default-scheduler Successfully assigned default/tensorflow-esgz to k8s-cpu-node-1 Normal Pulling 5s kubelet, k8s-cpu-node-1 Pulling image \"elihe/tensorflow:v1.0.5-esgz\" Normal Pulled 0s kubelet, k8s-cpu-node-1 Successfully pulled image \"elihe/tensorflow:v1.0.5-esgz\" Normal Created 0s kubelet, k8s-cpu-node-1 Created container tensorflow /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)]) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)]) WARNING:tensorflow:From /benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:1120: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version. Instructions for updating: Please switch to tf.train.MonitoredTrainingSession TensorFlow: 1.5 Model: trivial Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64.0 per device Devices: ['/cpu:0'] Data format: NHWC Layout optimizer: False Optimizer: sgd Variables: parameter_server ========== Generating model Running warm up Done warm up Step Img/sec loss 1 images/sec: 487.6 +/- 0.0 (jitter = 0.0) 7.055 10 images/sec: 505.9 +/- 3.5 (jitter = 14.3) 7.055 20 images/sec: 505.6 +/- 2.6 (jitter = 12.2) 7.055 30 images/sec: 505.5 +/- 2.4 (jitter = 15.7) 7.055 40 images/sec: 501.1 +/- 2.5 (jitter = 17.7) 7.055 50 images/sec: 500.8 +/- 2.1 (jitter = 18.3) 7.055 60 images/sec: 500.2 +/- 1.9 (jitter = 17.6) 7.055 70 images/sec: 500.5 +/- 1.8 (jitter = 16.1) 7.055 80 images/sec: 502.6 +/- 1.8 (jitter = 17.1) 7.055 90 images/sec: 504.3 +/- 1.7 (jitter = 16.5) 7.055 100 images/sec: 505.6 +/- 1.6 (jitter = 16.7) 7.055 110 images/sec: 507.6 +/- 1.6 (jitter = 18.4) 7.055 120 images/sec: 509.1 +/- 1.6 (jitter = 18.5) 7.055 130 images/sec: 509.0 +/- 1.6 (jitter = 18.4) 7.055 140 images/sec: 508.1 +/- 1.5 (jitter = 19.7) 7.055 150 images/sec: 508.9 +/- 1.5 (jitter = 17.5) 7.055 160 images/sec: 510.0 +/- 1.4 (jitter = 18.7) 7.055 170 images/sec: 510.5 +/- 1.4 (jitter = 19.9) 7.055 180 images/sec: 511.4 +/- 1.4 (jitter = 20.2) 7.055 190 images/sec: 511.6 +/- 1.3 (jitter = 20.7) 7.055 200 images/sec: 511.6 +/- 1.3 (jitter = 20.6) 7.055 210 images/sec: 511.8 +/- 1.3 (jitter = 20.7) 7.055 220 images/sec: 512.7 +/- 1.2 (jitter = 20.2) 7.055 230 images/sec: 512.8 +/- 1.2 (jitter = 19.9) 7.055 240 images/sec: 513.4 +/- 1.2 (jitter = 19.4) 7.055 250 images/sec: 513.7 +/- 1.1 (jitter = 18.8) 7.055 260 images/sec: 513.6 +/- 1.1 (jitter = 19.0) 7.055 270 images/sec: 514.0 +/- 1.1 (jitter = 19.0) 7.055 280 images/sec: 514.1 +/- 1.1 (jitter = 18.6) 7.055 290 images/sec: 514.4 +/- 1.1 (jitter = 18.7) 7.055 300 images/sec: 514.5 +/- 1.0 (jitter = 18.4) 7.055 310 images/sec: 514.7 +/- 1.0 (jitter = 18.2) 7.055 320 images/sec: 514.7 +/- 1.0 (jitter = 18.4) 7.055 330 images/sec: 514.8 +/- 1.0 (jitter = 18.8) 7.055 340 images/sec: 515.2 +/- 1.0 (jitter = 18.5) 7.055 350 images/sec: 515.1 +/- 1.0 (jitter = 18.8) 7.055 360 images/sec: 515.6 +/- 0.9 (jitter = 18.7) 7.055 370 images/sec: 515.9 +/- 0.9 (jitter = 18.5) 7.055 380 images/sec: 516.0 +/- 0.9 (jitter = 18.1) 7.055 390 images/sec: 516.2 +/- 0.9 (jitter = 17.7) 7.055 400 images/sec: 516.1 +/- 0.9 (jitter = 17.7) 7.055 410 images/sec: 516.3 +/- 0.9 (jitter = 17.7) 7.055 420 images/sec: 516.3 +/- 0.8 (jitter = 17.8) 7.055 430 images/sec: 516.7 +/- 0.8 (jitter = 17.5) 7.055 440 images/sec: 516.6 +/- 0.8 (jitter = 17.6) 7.055 450 images/sec: 516.8 +/- 0.8 (jitter = 17.6) 7.055 460 images/sec: 517.1 +/- 0.8 (jitter = 17.3) 7.055 470 images/sec: 517.0 +/- 0.8 (jitter = 17.6) 7.055 480 images/sec: 517.0 +/- 0.8 (jitter = 17.6) 7.055 490 images/sec: 517.0 +/- 0.8 (jitter = 17.8) 7.055 500 images/sec: 517.1 +/- 0.8 (jitter = 17.9) 7.055 ---------------------------------------------------------------- total images/sec: 516.24 ---------------------------------------------------------------- 可以观察到esgz格式的镜像在行为一致的情况下拉取镜像的时间比普通快了将近3分钟。 本人也对小镜像做过实验，esgz镜像的拉取速度也在5秒左右，因为拉取行为实则是拉取元数据，因此对于转化后的镜像拉取时延几乎一致。 4. 小结 镜像延迟加载通过远程挂载特殊格式的镜像文件实现了镜像的“懒惰”拉取。对于小镜像来说，它的拉取时延优化并不明显（如之前所述，拉取耗时基本都在5秒左右），但是对于大镜像而言（尤其是AI训练的场景下），这种“懒惰”策略能优化大量的创建耗时。 当然，它也有其缺点，因为延迟加载的前提是“大部分镜像中的文件没有被用户访问”，在特殊场景下延迟加载的镜像会给用户的操作带来大量延迟（因为拉取镜像的时延被平均到了访问文件的网络IO中），目前只能通过estargz格式的文件将镜像内容切分为高低优来实现，但是这并不是一个长久之计。此外，延迟加载对网络的稳定性也有很大的要求（尤其是registry），在网络不稳定时容器的运行也会频繁被网络IO阻塞。 在这里延伸引入一下阿里的解决方案。 阿里将镜像拆分为元数据和数据两层（这种镜像格式被称为Rafs），其中元数据层是一颗自校验的哈希树，数据层切分为固定大小切片，数据可以被不同镜像的不同文件共享，如下图所示。 和stargz-snapshotter类似，Rafs也需要一个负责解析镜像格式的FUSE进程，这一套镜像的服务被称为Nydus。此外，阿里通过引入高可用P2P镜像文件分发系统Dragonfly来解决网络的问题。 基于这个设计架构，Nydus在镜像的生命流程中做了如下的优化。 build环节：块级别镜像去重 ship环节：支持不同镜像存储后端，和Dragonfly的p2p良好集成 run环节：兼容OCI标准 5. reference [1] https://arkingc.github.io/2017/05/05/2017-05-05-docke-filesystem-overlay/ [2] https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-the-overlay2-driver-works [3] https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter [4] https://medium.com/nttlabs/startup-containers-in-lightning-speed-with-lazy-image-distribution-on-containerd-243d94522361 [5] https://www.cnblogs.com/sparkdev/p/9063042.html [6] https://github.com/containerd/stargz-snapshotter/blob/master/docs/stargz-estargz.md [7] https://github.com/containerd/containerd/issues/2943 [8] https://github.com/dragonflyoss Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/集群升级/kubeadm 编译.html":{"url":"blog/kubernetes/集群升级/kubeadm 编译.html","title":"Kubeadm 编译","keywords":"","body":"kubeadm 源码编译 源码修改 // CertificateValidity defines the validity for all the signed certificates generated by kubeadm // CertificateValidity = time.Hour * 24 * 365 CertificateValidity = time.Hour * 24 * 365 * 100 NewSelfSignedCACert 修改KUBE_GIT_TREE_STATE 修改 kubernetes/hack/lib/version.sh 文件，将变量 KUBE_GIT_TREE_STATE=\"dirty\" 修改为 KUBE_GIT_TREE_STATE=\"clean\"，dirty 表示 Git 提交 ID 之后的源代码有更改，clean 表示 Git 提交 ID 之后没有更改，为了确保版本号干净，都修改为 clean。 源码编译 mac 环境 make WHAT=cmd/kubeadm KUBE_BUILD_PLATFORMS=linux/amd64 https://sysin.org/blog/kubernetes-kubeadmin-cert-100y/ https://www.dazhuanlan.com/tangjiefei/topics/1575858 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/集群升级/证书使用10年期.html":{"url":"blog/kubernetes/集群升级/证书使用10年期.html","title":"证书使用10年期","keywords":"","body":"手动更新证书 查看当前证书过期剩余时间 kubeadm alpha certs check-expiration 备份证书 mkdir /etc/kubernetes.bak cp -r /etc/kubernetes/pki/ /etc/kubernetes.bak cp /etc/kubernetes/*.conf /etc/kubernetes.bak 备份etcd 数据 cp -r /var/lib/etcd /var/lib/etcd.bak 修改controller-manage csr 签发默认时长 $ vi /etc/kubernetes/manifests/kube-controller-manager.yaml --experimental-cluster-signing-duration=87600h0m0s --use-api 重新签发所有证书 $ kubeadm alpha certs renew all --use-api --config kubeadm-config.yaml & $ kubectl get csr | grep -v 'NAME' |awk {'print $1'} | xargs kubectl certificate approve $ kubeadm alpha certs check-expiration 批准完成后检查证书的有效期： $ kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Sep 26, 2031 01:18 UTC 9y no apiserver Sep 26, 2031 01:18 UTC 9y ca no apiserver-etcd-client Sep 26, 2031 01:18 UTC 9y etcd-ca no apiserver-kubelet-client Sep 26, 2031 01:18 UTC 9y ca no controller-manager.conf Sep 26, 2031 01:18 UTC 9y no etcd-healthcheck-client Sep 26, 2031 01:18 UTC 9y etcd-ca no etcd-peer Sep 26, 2031 01:18 UTC 9y etcd-ca no etcd-server Sep 26, 2031 01:18 UTC 9y etcd-ca no front-proxy-client Sep 26, 2031 01:18 UTC 9y front-proxy-ca no scheduler.conf Sep 26, 2031 01:18 UTC 9y no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 26, 2031 01:18 UTC 9y no etcd-ca Sep 26, 2031 01:18 UTC 9y no front-proxy-ca Sep 26, 2031 01:18 UTC 9y no 但是现在我们还不能直接重启控制面板的几个组件，这是因为使用 kubeadm 安装的集群对应的 etcd 默认是使用的 /etc/kubernetes/pki/etcd/ca.crt 这个证书进行前面的，而上面我们用命令 kubectl certificate approve 批准过后的证书是使用的默认的 /etc/kubernetes/pki/ca.crt 证书进行签发的，所以我们需要替换 etcd 中的 ca 机构证书: # 先拷贝静态 Pod 资源清单 $ cp -r /etc/kubernetes/manifests/ /etc/kubernetes/manifests.bak $ vi /etc/kubernetes/manifests/etcd.yaml ...... spec: containers: - command: - etcd # 修改为 CA 文件 - --peer-trusted-ca-file=/etc/kubernetes/pki/ca.crt - --trusted-ca-file=/etc/kubernetes/pki/ca.crt ...... volumeMounts: - mountPath: /var/lib/etcd name: etcd-data - mountPath: /etc/kubernetes/pki # 更改证书目录 name: etcd-certs volumes: - hostPath: path: /etc/kubernetes/pki # 将 pki 目录挂载到 etcd 中去 type: DirectoryOrCreate name: etcd-certs - hostPath: path: /var/lib/etcd type: DirectoryOrCreate name: etcd-data ...... 上面的修改命令简化版 sed -i 's/\\/etc\\/kubernetes\\/pki\\/etcd\\/ca.crt/\\/etc\\/kubernetes\\/pki\\/ca.crt/g' /etc/kubernetes/manifests/etcd.yaml 由于 kube-apiserver 要连接 etcd 集群，所以也需要重新修改对应的 etcd ca 文件： $ vi /etc/kubernetes/manifests/kube-apiserver.yaml ...... spec: containers: - command: - kube-apiserver # 将etcd ca文件修改为默认的ca.crt文件 - --etcd-cafile=/etc/kubernetes/pki/ca.crt ...... 上面的修改命令简化版 sed -i 's/\\/etc\\/kubernetes\\/pki\\/etcd\\/ca.crt/\\/etc\\/kubernetes\\/pki\\/ca.crt/g' /etc/kubernetes/manifests/kube-apiserver.yaml 除此之外还需要替换 requestheader-client-ca-file 文件，默认是 /etc/kubernetes/pki/front-proxy-ca.crt 文件，现在也需要替换成默认的 CA 文件，否则使用聚合 API，比如安装了 metrics-server 后执行 kubectl top 命令就会报错： cp -r /etc/kubernetes/pki/ /etc/kubernetes/pki.bak cp /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/front-proxy-ca.key 重启组件： etcd，apiserver，controller-manager，scheduler docker ps |grep -E 'k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler|k8s_etcd_etcd' | awk -F ' ' '{print $1}'|xargs docker restart Quesetion： Q1： CA-重用和冲突 A： Kubernetes apiserver 有两个客户端 CA 选项： --client-ca-file --requestheader-client-ca-file 这些功能中的每个功能都是独立的；如果使用不正确，可能彼此冲突。 --client-ca-file：当请求到达 Kubernetes apiserver 时，如果启用了此选项，则 Kubernetes apiserver 会检查请求的证书。如果它是由 --client-ca-file 引用的文件中的 CA 证书之一签名的，并且用户是公用名CN=的值，而组是组织O= 的取值，则该请求被视为合法请求。请参阅 关于 TLS 身份验证的文档。 --requestheader-client-ca-file：当请求到达 Kubernetes apiserver 时，如果启用此选项，则 Kubernetes apiserver 会检查请求的证书。如果它是由文件引用中的 --requestheader-client-ca-file 所签署的 CA 证书之一签名的，则该请求将被视为潜在的合法请求。然后，Kubernetes apiserver 检查通用名称CN=是否是 --requestheader-allowed-names 提供的列表中的名称之一。如果名称允许，则请求被批准；如果不是，则请求被拒绝。 如果同时提供了 --client-ca-file 和--requestheader-client-ca-file，则首先检查 --requestheader-client-ca-file CA，然后再检查--client-ca-file。通常，这些选项中的每一个都使用不同的 CA（根 CA 或中间 CA）。常规客户端请求与 --client-ca-file 相匹配，而聚合请求与 --requestheader-client-ca-file 相匹配。但是，如果两者都使用同一个 CA，则通常会通过 --client-ca-file 传递的客户端请求将失败，因为 CA 将与 --requestheader-client-ca-file 中的 CA 匹配，但是通用名称 CN= 将不匹配 --requestheader-allowed-names 中可接受的通用名称之一。这可能导致您的 kubelet 和其他控制平面组件以及最终用户无法向 Kubernetes apiserver 认证。 因此，请对用于控制平面组件和最终用户鉴权的 --client-ca-file 选项和用于聚合 apiserver 鉴权的 --requestheader-client-ca-file 选项使用不同的 CA 证书。 Q2:为什么 --use-api 是10年起的，不加这个参数是1年的： A：一个是用apiserver 签发的，一个是通过kubeadm签发的 续签一年的命令: kubeadm alpha certs renew all --config=kubeadm-config.yaml Q3:为什么--user-pai执行后会一直报错 Q4: etcd 使用的证书被k8s 根证书替换了，会对以后的集群升级产生影响吗 参考： 手动更新证书 目前使用阿里云对象存储服务，通过不同目录来区分存放开发环境、测试环境、生产环境的静态配置文件 1. 创建 [When] 1）某些软工程中需要使用到一些固定不变的文件制品，但是又不适合与代码构建放到一起，需要为其提供一个专门存储、下载的地方。 [How] 对象存储需要云平台管理员创建：遵循以下原则： 1）桶名称 使用 项目名称 2）桶下的一级目录为环境 2）桶下的二级目录为大应用 2. 上传制品 [when] 1）在流水线中，构建制品，上传到指定 对象存储桶的目录下 [how] 1) IDP agent 录入 对象存储桶 2）流水线的某个阶段中，选择agent 中录入的桶 3）选择上传目录 3. 使用制品 [when] 1) 网站静态图片或文件 2）代码编译依赖中间制品 3）软件部署依赖中间工具软件、镜像、配置等 [how] 1）配置项中直接引用对象存储的网络地址 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:22:34 "},"blog/kubernetes/集群升级/证书续签1年.html":{"url":"blog/kubernetes/集群升级/证书续签1年.html","title":"证书续签1年","keywords":"","body":"[toc] 手动更新kuberentes证书 1. 查询 当前证书过期时间 kubeadm alpha certs check-expiration 该命令显示 /etc/kubernetes/pki 文件夹中的客户端证书以及 kubeadm 使用的 KUBECONFIG 文件中嵌入的客户端证书的到期时间/剩余时间。 kubeadm 不能管理由外部 CA 签名的证书，如果是外部得证书，需要自己手动去管理证书的更新。 另外需要说明的是上面的列表中没有包含 kubelet.conf，因为 kubeadm 将 kubelet 配置为自动更新证书。 另外 kubeadm 会在控制面板升级的时候自动更新所有证书，所以使用 kubeadm 搭建得集群最佳的做法是经常升级集群，这样可以确保你的集群保持最新状态并保持合理的安全性。但是对于实际的生产环境我们可能并不会去频繁得升级集群，所以这个时候我们就需要去手动更新证书。 要手动更新证书也非常方便，我们只需要通过 kubeadm alpha certs renew 命令即可更新你的证书，这个命令用 CA（或者 front-proxy-CA ）证书和存储在 /etc/kubernetes/pki 中的密钥执行更新。 如果你运行了一个高可用的集群，这个命令需要在所有控制面板节点上执行。 接下来我们来更新我们的集群证书，下面的操作都是在 master 节点上进行，首先备份原有证书： mkdir /etc/kubernetes.bak cp -r /etc/kubernetes/pki/ /etc/kubernetes.bak cp /etc/kubernetes/*.conf /etc/kubernetes.bak 备份etcd 数据 cp -r /var/lib/etcd /var/lib/etcd.bak 2. 接下来执行更新证书的命令： kubeadm alpha certs renew all --config=kubeadm.yaml 3. 更新下 kubeconfig 文件： kubeadm init phase kubeconfig all --config kubeadm.yaml 将新生成的 admin 配置文件覆盖掉原本的 admin 文件: cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 4. 重启容器 完成后重启 kube-apiserver、kube-controller、kube-scheduler、etcd 这4个容器即可，我们可以查看 apiserver 的证书的有效期来验证是否更新成功： docker ps |grep -E 'k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler' | awk -F ' ' '{print $1}'|xargs docker restart 可以看到现在的有效期是一年过后的，证明已经更新成功了。 echo | openssl s_client -showcerts -connect 127.0.0.1:6443 -servername api 2>/dev/null | openssl x509 -noout -enddate notAfter=Aug 26 03:47:23 2021 GMT 5. 批量执行 // 生成kubeadm 配置 kubeadm config view > kubeadm.yaml // 证书续签 kubeadm alpha certs renew all --config=kubeadm.yaml // 重生成组件证书 kubeadm init phase kubeconfig all --config kubeadm.yaml // copy kubeconfig cp -i /etc/kubernetes/admin.conf /root/.kube/config // 重启控制面组件 docker ps |grep -E 'k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler' | awk -F ' ' '{print $1}'|xargs docker restart Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/集群升级/集群升级.html":{"url":"blog/kubernetes/集群升级/集群升级.html","title":"集群升级","keywords":"","body":"Kuberntes 集群升级步骤 kubernetes 可以通过kubeadm工具实现平滑升级，但是每次升级只能按序递增一个大版本，比如1.17.xx 只能升级到1.18.x。本文脚本以为1.17.11 升级到1.18.0为例。 1. 查询可用k8s 版本 yum list --showduplicates kubeadm --disableexcludes=kubernetes 2. 获取集群配置文件 通过kubeadm 安装的集群，会自动将配置文件存放在configmap： kubeadm-config。在1.23 之前的版本， 可以通过kubeadm view 的方式查看。 方式一： # kubeadm version upgrade-k8s.yaml 方式二： kubectl -n kube-system get cm kubeadm-config --template={{.data.ClusterConfiguration}} > upgrade-k8s.yaml 3. 升级master 节点kubeadm/kubelet/kubectl组件 在所有节点（包括 master、worker 节点）执行安装升级命令 yum upgrade -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 4. 创建升级用的配置文件 我们在拿到集群现有的配置后，需要修改版本。 如果你的kuberntes 镜像无法获取可以使用阿里云提供的镜像，地址：registry.aliyuncs.com/google_containers sed -i 's/^kubernetesVersion:.*/kubernetesVersion: 1.18.0/g' upgrade-k8s.yaml 5. 执行kubeadm upgrade命令升级master节点 在 master 节点执行 # 查看配置文件差异 kubeadm upgrade diff --config upgrade-k8s.yaml # 执行升级前试运行 kubeadm upgrade apply --config upgrade-k8s.yaml --dry-run # 执行升级动作 kubeadm upgrade apply --config upgrade-k8s.yaml systemctl daemon-reload systemctl restart kubelet 6. 升级worker节点 yum upgrade -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 systemctl daemon-reload systemctl restart kubelet 7.检查是否升级成功 kubectl get nodes 常见问题 1. 版本1.21 coredns 镜像在二级目录下，如果只修改了镜像地址，会发生pull 镜像失败。 k8s.gcr.io/kube-apiserver:v1.21.11 k8s.gcr.io/kube-controller-manager:v1.21.11 k8s.gcr.io/kube-scheduler:v1.21.11 k8s.gcr.io/kube-proxy:v1.21.11 k8s.gcr.io/pause:3.4.1 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns/coredns:v1.8.0 解决方案： ​ kubernetes/cmd/kubeadm/app/constants/constants.go // CoreDNSImageName specifies the name of the image for CoreDNS add-on CoreDNSImageName = \"coredns/coredns\" 朗澈公司 编译的kubeadm，支持证书100年。下载地址如下 wget -O /usr/bin/kubeadm https://lstack-qa.oss-cn-hangzhou.aliyuncs.com/kubeadm-1.17.2 wget -O /usr/bin/kubeadm https://lstack-qa.oss-cn-hangzhou.aliyuncs.com/kubeadm-1.21.0 wget -O /usr/bin/kubeadm https://lstack-qa.oss-cn-hangzhou.aliyuncs.com/kubeadm-1.22.8 2. Croups 报错, docker 和 kubelet 都配置的是systemd,但是仍然报错 InitBinary:docker-init ContainerdCommit:{ID:3df54a852345ae127d1fa3092b95168e4a88e2f8 Expected:3df54a852345ae127d1fa3092b95168e4a88e2f8} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: DefaultAddressPools:[] Warnings:[]} 3月 26 14:55:24 dev-slave-3 kubelet[31173]: E0326 14:55:24.502947 31173 server.go:294] \"Failed to run kubelet\" err=\"failed to run Kubelet: misconfiguration: kubelet cgroup driver: \\\"cgroupfs\\\" is different from docker cgroup driver: \\\"systemd\\\"\" 3月 26 14:55:24 dev-slave-3 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE 3月 26 14:55:24 dev-slave-3 systemd[1]: Unit kubelet.service entered failed state. 3月 26 14:55:24 dev-slave-3 systemd[1]: kubelet.service failed. ​ 解决方案一： ​ https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/ ​ 更新所有节点的 cgroup 驱动 对于集群中的每一个节点： 执行命令 kubectl drain --ignore-daemonsets，以 腾空节点 执行命令 systemctl stop kubelet，以停止 kubelet 停止容器运行时 修改容器运行时 cgroup 驱动为 systemd 在文件 /var/lib/kubelet/config.yaml 中添加设置 cgroupDriver: systemd 启动容器运行时 执行命令 systemctl start kubelet，以启动 kubelet 执行命令 kubectl uncordon ，以 取消节点隔离 在节点上依次执行上述步骤，确保工作负载有充足的时间被调度到其他节点。 流程完成后，确认所有节点和工作负载均健康如常。 解决方案二： ​ node 节点没有安装kubeadm，笔者在升级的时候先卸载了kubelet，kubeadm，后来只安装了kubelet和kubectl 导致一直报cgroups，后来重新执行了下命令 yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 ,问题解决。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/集群稳定性保障/api-timeout.html":{"url":"blog/kubernetes/集群稳定性保障/api-timeout.html","title":"Api Timeout","keywords":"","body":"https://50.96.207.97:443/apis/custom.metrics.k8s.io/v1beta1: Get https://50.96.207.97:443/apis/custom.metrics.k8s.io/v1beta1: read tcp 50.96.207.97:34496->50.96.207.97:443: read: connection reset by peer I0323 05:58:29.083182 1 controller.go:107] OpenAPI AggregationController: Processing item v1beta1.metrics.k8s.io 现象： node curl svc 通 pods内 curl svc 通 api-server request svc 不通 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/集群稳定性保障/join failed.html":{"url":"blog/kubernetes/集群稳定性保障/join failed.html","title":"Join Failed","keywords":"","body":"k8s nodes is forbidden user cannot list resource nodes in api group at the cluster scope May 7, 2020· 4 min read ·DOCKERK8S 继续将k8s用于模型转换和部署的自动化流程...然后发现之前安装k8s的文档不work了．． 时间是2020年5月7日，当前最新的k8s版本是　v1.18.2 报错如下: 1 2 3 ... SHELL 看起来重点的报错在这一句 1 2nodes \"host-10-198-21-97\" is forbidden: User \"system:bootstrap:0752yx\" cannot get resource \"nodes\" in API group \"\" at the cluster scope 3cannot get Node \"host-10-198-21-97\" 4 SHELL 然后google发现大概是权限相关的原因...Role-Based Access Contro　相关的．　但是似乎都不是在搭建集群的时候遇到的． 然后打算重新看一遍最新的搭建手册，发现troubleshooting里面 Not possible to join a v1.18 Node to a v1.17 cluster due to missing RBAC 原来是v1.18增加了权限控制，1.18的slave机器没办法加入到1.17的master节点上...看来就是这个问题． 然后在控制节点上apply了如下内容: 1 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRole 4metadata: 5 name: kubeadm:get-nodes 6rules: 7- apiGroups: 8 - \"\" 9 resources: 10 - nodes 11 verbs: 12 - get 13--- 14apiVersion: rbac.authorization.k8s.io/v1 15kind: ClusterRoleBinding 16metadata: 17 name: kubeadm:get-nodes 18roleRef: 19 apiGroup: rbac.authorization.k8s.io 20 kind: ClusterRole 21 name: kubeadm:get-nodes 22subjects: 23- apiGroup: rbac.authorization.k8s.io 24 kind: Group 25 name: system:bootstrappers:kubeadm:default-node-token 26 ... YAML 重新尝试加入，已经可以了． Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:22:34 "},"blog/kubernetes/集群稳定性保障/kubelet cgroups.html":{"url":"blog/kubernetes/集群稳定性保障/kubelet cgroups.html","title":"Kubelet Cgroups","keywords":"","body":"[toc] 解决 Kubernetes 中 Kubelet 组件报 failed to get cgroup 错误 摘自： http://www.mydlq.club/article/80/ 作者：超级小豆丁 系统环境： Kubernetes 版本：1.18.1 操作系统版本：CentOS 7.8 一、问题描述 最近查看 Kubelet 日志，发现日志中一堆错误信息，内容如下： -n：指定获取最后指定行数的日志信息。 $ journalctl -u kubelet -n 10 19 02:40:17 k8s-node-2-14 kubelet[1291]: E0419 02:40:17.749145 1291 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service 19 02:40:27 k8s-node-2-14 kubelet[1291]: E0419 02:40:27.772168 1291 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service 19 02:40:32 k8s-node-2-14 kubelet[1291]: E0419 02:40:32.377548 1291 summary_sys_containers.go:82] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service 19 02:40:37 k8s-node-2-14 kubelet[1291]: E0419 02:40:37.800210 1291 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service BASH 可以观察到提示 failed to get cgroup stats for \"/system.slice/docker.service\" 错误，下面是分析与解决该问题的过程。 二、问题分析 首先呢，参考几个 Kubernetes Github 上的 issue： https://github.com/kubernetes/kubernetes/issues/56850 https://github.com/kubermatic/machine-controller/pull/476 https://github.com/kubernetes/kubernetes/issues/56850#issuecomment-406241077 从上面各个 issue 中，本人综合其中的问题探讨猜测，该问题只会发生在 CentOS 系统上，而引起上面的问题的原因是 kubelet 启动时，会执行节点资源统计，需要 systemd 中开启对应的选项，如下： CPUAccounting：是否开启该 unit 的 CPU 使用统计，bool 类型，可配置 true 或者 false。 MemoryAccounting：是否开启该 unit 的 Memory 使用统计，bool 类型，可配置 true 或者 false。 如果不设置这两项，kubelet 是无法执行该统计命令，导致 kubelet 一直报上面的错误信息。 三、解决问题 解决上面问题也很简单，直接编辑 systemd 中的 kubelet 服务配置文件中，添加 CPU 和 Memory 配置，可以按下面操作进行更改。 1、编辑配置文件并添加对应配置项 编辑 /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 文件，并添加下面配置： CPUAccounting=true MemoryAccounting=true BASH 具体操作如下： $ vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf [Service] CPUAccounting=true ## 添加 CPUAccounting=true 选项，开启 systemd CPU 统计功能 MemoryAccounting=true ## 添加 MemoryAccounting=true 选项，开启 systemd Memory 统计功能 Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env EnvironmentFile=-/etc/sysconfig/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS BASH 2、重启 Kubelet 服务 重启 kubelet 服务，让 kubelet 重新加载配置。 $ systemctl daemon-reload $ systemctl restart kubelet BASH 3、观察 kubelet 日志 重启完 kubelet 后等一段时间，再次观察 kubelet 日志信息： $ journalctl -u kubelet -n 10 19 02:48:11 k8s-node-2-14 kubelet[1308]: I0419 02:48:11.875632 1308 clientconn.go:933] ClientConn switching balancer to \"pick_first\" 19 02:48:11 k8s-node-2-14 kubelet[1308]: I0419 02:48:11.875655 1308 clientconn.go:882] blockingPicker: the picked transport is not ready, loop back to repick 19 02:48:12 k8s-node-2-14 kubelet[1308]: I0419 02:48:12.361764 1308 topology_manager.go:219] [topologymanager] RemoveContainer - Container ID: a2a3780a36a823317821f27871dc2572f5236be1ae7244b91c29f4fd0dfd7c25 19 02:48:12 k8s-node-2-14 kubelet[1308]: I0419 02:48:12.365887 1308 kubelet_resources.go:45] allocatable: map[cpu:{{8 0} {} 8 DecimalSI} ephemeral-storage:{{45389555637 0} {} 45389555637 DecimalSI} hugepages-1Gi:{{0 0} {} 8 DecimalSI} ephemeral-storage:{{45389555637 0} {} 45389555637 DecimalSI} hugepages-1Gi:{{0 0} {} 8 DecimalSI} ephemeral-storage:{{45389555637 0} {} 45389555637 DecimalSI} hugepages-1Gi:{{0 0} {} 8 DecimalSI} ephemeral-storage:{{45389555637 0} {} 45389555637 DecimalSI} hugepages-1Gi:{{0 0} { BASH 可以看到系统已经没有之前的错误日志信息了。 ---END--- Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/集群稳定性保障/npd.html":{"url":"blog/kubernetes/集群稳定性保障/npd.html","title":"Npd","keywords":"","body":"https://github.com/kubernetes/node-problem-detector Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/kubernetes/集群稳定性保障/orphaned pod.html":{"url":"blog/kubernetes/集群稳定性保障/orphaned pod.html","title":"Orphaned Pod","keywords":"","body":"kubernetes故障现场一之Orphaned pod 摘自： https://www.cnblogs.com/tylerzhou/p/11075185.html 作者： 周国通 系列目录 问题描述:周五写字楼整体停电,周一再来的时候发现很多pod的状态都是Terminating,经排查是因为测试环境kubernetes集群中的有些节点是PC机,停电后需要手动开机才能起来.起来以后节点恢复正常,但是通过journalctl -fu kubelet查看日志不断有以下错误 [root@k8s-node4 pods]# journalctl -fu kubelet -- Logs begin at 二 2019-05-21 08:52:08 CST. -- 5月 21 14:48:48 k8s-node4 kubelet[2493]: E0521 14:48:48.748460 2493 kubelet_volumes.go:140] Orphaned pod \"d29f26dc-77bb-11e9-971b-0050568417a2\" found, but volume paths are still present on disk : There were a total of 1 errors similar to this. Turn up verbosity to see them. 我们通过cd进入/var/lib/kubelet/pods目录,使用ls查看 [root@k8s-node4 pods]# ls 36e224e2-7b73-11e9-99bc-0050568417a2 42e8cd65-76b1-11e9-971b-0050568417a2 42eaca2d-76b1-11e9-971b-0050568417a2 36e30462-7b73-11e9-99bc-0050568417a2 42e94e29-76b1-11e9-971b-0050568417a2 d29f26dc-77bb-11e9-971b-0050568417a2 可以看到,错误信息里的pod的ID在这里面,我们cd进入它(d29f26dc-77bb-11e9-971b-0050568417a2),可以看到里面有以下文件 [root@k8s-node4 d29f26dc-77bb-11e9-971b-0050568417a2]# ls containers etc-hosts plugins volumes 我们查看etc-hosts文件 [root@k8s-node4 d29f26dc-77bb-11e9-971b-0050568417a2]# cat etc-hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet fe00::0 ip6-mcastprefix fe00::1 ip6-allnodes fe00::2 ip6-allrouters 10.244.7.7 sagent-b4dd8b5b9-zq649 我们在主节点上执行kubectl get pod|grep sagent-b4dd8b5b9-zq649发现这个pod已经不存在了. 问题的讨论查看这里有人在pr里提交了来解决这个问题,截至目前PR仍然是未合并状态. 目前解决办法是先在问题节点上进入/var/lib/kubelet/pods目录,删除报错的pod对应的hash(rm -rf 名称),然后从集群主节点删除此节点(kubectl delete node),然后在问题节点上执行 kubeadm reset systemctl stop kubelet systemctl stop docker systemctl start docker systemctl start kubelet 执行完成以后此节点重新加入集群 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/math/":{"url":"blog/math/","title":"Math","keywords":"","body":"Math Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/math/bloomfilter.html":{"url":"blog/math/bloomfilter.html","title":"Bloomfilter","keywords":"","body":"bloomfilter 布隆过滤器，是一种节省空间的概率数据结构（就是位图+哈希）。主要的功能是查找一个元素是否在给定的集合中。说它是一种概率数据结构是因为每次对它的查询会返回 2 种结果；一种是“可能在集合中”，也就是说它可能会误报，这个误报是有一定概率的。 在正式介绍布隆过滤器之前，我们先介绍一下位图，因为前面我们也说了布隆过滤器=位图+哈希。那么什么是位图呢？位图（bitmap）我们可以理解为是一个 bit 数组，每个元素存储数据的状态（由于每个元素只有 1 bit，所以只能存储 0 或 1 这 2 种状态）适用于数据量超大，但是数据的状态很少的情况。比如判断一个整数是否在给定的超大的整数集中。举个例子我们初始化一个包含1,7,5,20,10,99,96 数据的文件： 我们只需要将给定数字的对应位置改为 1 即可，当需要判断一个数字是不是在该集合中的时候就只需要判断该数字对应的位置是不是 1 即可，是 1 则表示该数字在集合中，为 0 则表示该数字不在集合中。由于位图不需要存储元数据，还需要用一个 bit 存储元数据的状态，所以能极大的减少存储空间。当然位图的缺陷也很明显；只能处理整数。 好现在我们回到布隆过滤器看看它的原理。它先定义一个长度为 m 的位图数组，初始值都为 0 ，然后定义 k 个不同的符合随机分布的哈希函数，添加一个元素的时候通过 k 个哈希函数得到 k 个 hash 值，将它们映射到位图数组中（当然计算出来的 hash 值可能超过了 m，那么就需要扩容，java 中 BitSet 的扩容方案是 Math.max(2 * 当前长度, 计算出来的 hash 值); ） 查询的时候把这个元素作为 K 个哈希函数的输入，得到 K 个数组的位置。如果这些位置中有任意一个是 0，说明元素肯定不在集合中。如果这些位置全部为 1，那么该元素很可能是在集合中，因为也有可能这些位置是被其他元素设置的。 通过计算 4 G 空间就基本能满足需求了。我们总结一下布隆过滤器的优缺点： 优点： 1.查询时间复杂度 O(k) 一个较小的常数 2.极度节约空间 3.不存储数据本身，对源数据有一定保密性 缺点： 1.有一定误差 2.不能删除 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/math/readme.html":{"url":"blog/math/readme.html","title":"Readme","keywords":"","body":"Math Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/math/二项分布.html":{"url":"blog/math/二项分布.html","title":"二项分布","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/math/最大似然估计估计.html":{"url":"blog/math/最大似然估计估计.html","title":"最大似然估计估计","keywords":"","body":"最大似然估计 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/math/泊松分布.html":{"url":"blog/math/泊松分布.html","title":"泊松分布","keywords":"","body":"泊松分布 泊松分布解决的问题看起来非常简单。比如这个：已知某医院平均一天里有 8 名新生儿诞生，那么医院一个月里，每日新生儿出生数量的分布是怎样的？ 把泊松分布发扬光大的 Bortkiewicz 在《小数法则》一书里举了这么一个例子：从1875到1894年的20年间，德国的十四个军团部有士兵被马踢伤致死的人数纪录。这 20×14 = 280个纪录，按死亡人数来分，如下所示。 这280个记录中，共有196人死亡，死亡率是 0.7，根据这一数据，Bortkiewicz 用泊松分布计算得出结果，如下所示，可以看到泊松分布得出的结果和现实出奇地吻合。 泊松分布其实只是二项式分布的极限情况，泊松分布公式如下： 它的推导非常简单，但在推导之前，我们必须了解自然对数 e 的意义。 自然对数 e 的意义 我们就从简单的存钱问题入手。 你向银行存了 100 元，年利率是 100%，到下一年的此时，你就能取出 200 元。 现在银行允许你半年就能取出利息，年利率还是 100%，但半年后，你就能取出年利息的一半，也就是 50%，50 元。半年后，你决定把取出的五十块钱利息立即存进银行里，又过了半年后（第一存款的一年后），你的存款就是 现在银行允许你一个月就能取出利息，年利率还是 100%。你 1 月存了 100 元，1月结束，你取出利息后立即存到银行里，2月结束你取出利息后又存进去... 这样一年后，你的存款是 现在银行允许你一天就能取出利息，一年后你的存款是 现在我们考虑更极限的情况，银行允许你一小时就能取出利息。一年后你的存款是 可以发现，当我们取的时间越来越小时，也就是 n 无限大时，存款逐渐逼近某个数，这个数就是 因此，你可以把 e 理解为极限情况下，100% 年利率下，利滚利能取得的最高收益。 但如果年利率是 10%呢？极限情况（n 是个无限大的数）下，一年后你的存款是 再将公式推广，本钱是1，年利率（增长率）是 a，n 无限大时，t 年后的存款是 泊松分布是二项分布的极限情况，我们现在来看一下二项分布的性质。 二项分布 在一个无限空间口袋里，有无限多的黑球和白球，40%是黑球，60%是白球。分别拿三次球并记录每次拿出球的颜色，其中1个是白球，剩下2个是黑球的概率 P 是多少？ 首先拿三次球一白二黑，有三种排列组合的情况，白黑黑，黑白黑，黑黑白。 拿出白黑黑的概率是，0.6×0.4×0.4=0.096 拿出黑白黑的概率是，0.4×0.6×0.4=0.096 拿出黑黑白的概率是，0.4×0.4×0.6=0.096 因此，P = 0.096 + 0.096 + 0.096 = 0.288 以上就是二项分布了，二项分布的公式很好理解，P = 排列组合的数量 × 该组合的概率 求排列组合的数量可以这么想。事件总数（size）是 n，事件分成2种情况，S 是白方块，数量为 k，F 是黑方块，数量就是 n-k。现在我们把所有白方块和黑方块都看做不同的，也就是 S1, S2, S3… Sk, F1, F2, F3… Fn-k，让这些方块排列组合，组合数量就是 n!。 但如果我们把黑色的方块视为相同的，白色还是不同的，每种排列都有 (n-k)! 个重复情况，如下图所示。 如果我们把白色的方块视为相同的，每种排列会有 k! 个重复情况。 因此当所有黑方块和白方块视为相同时，组合数量为 n! / [(n-k)! × k!)] p 为成功概率，(1-p) 就是失败的概率，事件总数为 n，成功次数为 k，失败次数就是 n-k，不考虑顺序（也就是同时发生）该组合的概率就是 若考虑顺序，概率 P 就是 泊松分布的推导 泊松分布就是二项分布的极限情况。 已知某医院平均一天里有 λ 名新生儿诞生，我们可以把这一天以每秒计算，一天有86400秒，我们把出生瞬间定义为婴儿刚刚离开阴道的时间，假设新生儿不可能在同一秒出生，且每秒婴儿出生的概率相同，均为 λ/86400。 这就是二项分布问题，一天里的事件总数 n = 86400，成功概率 p = λ/86400=，失败概率 1-p = 1-λ/86400，带入公式，每日出生 k 名婴儿的概率就是 但我们是不是也能把这一秒再分割呢？比如毫秒、微秒？这样我们就更能保证在一微秒不会有多名婴儿同时诞生，我们把事件总数 n 看作无限大，每日出生 k 名婴儿的概率就是 因为此时 n 无限大，k 和 λ 相对小，结合自然对数 e 的性质，可以做以下的近似 因此 泊松分布的应用 已知某医院平均一天里有 8 名新生儿诞生，每日出生 k 名婴儿的概率是怎样的？ 用刚刚推出的公式，已知 λ =8，我们可以分别计算出每日诞生 1 名、2 名、3 名... 婴儿的概率。 用 Python 写了个循环，代码如下： import math import numpy as np for k in range(1,30): l = 8. p = l ** k * np.exp(-l) / math.factorial(k) print '每日诞生 %d 名婴儿的概率 p = %f' %(k,p) #部分输出如下 每日诞生 6 名婴儿的概率 p = 0.122138 每日诞生 7 名婴儿的概率 p = 0.139587 每日诞生 8 名婴儿的概率 p = 0.139587 每日诞生 9 名婴儿的概率 p = 0.124077 每日诞生 10 名婴儿的概率 p = 0.099262 每日诞生 11 名婴儿的概率 p = 0.072190 每日诞生 12 名婴儿的概率 p = 0.048127 每日诞生 13 名婴儿的概率 p = 0.029616 每日诞生 14 名婴儿的概率 p = 0.016924 用图像表示概率分布如下： 参考资料 Poisson 分布 数学常数e的含义 - 阮一峰的网络日志 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/stock/invest.html":{"url":"blog/stock/invest.html","title":"Invest","keywords":"","body":"https://github.com/axiaoxin-com/investool Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/stock/record.html":{"url":"blog/stock/record.html","title":"Record","keywords":"","body":"socket record 【银行股】 下跌趋势；止跌32； 【宁波银行】 下跌趋势；32.58； 31 可以抄底 【许继电气】 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/stock/行为心理学.html":{"url":"blog/stock/行为心理学.html","title":"行为心理学","keywords":"","body":"行为心理学 市场特性 不可预测 随机性 多头和空头的博弈 容易收到信息和政策影响产生波动 交易策略 设置止损位 概率思维来思考 把握优势机会 买入先扪心自问 止损位 盈利幅度 达到盈利预期的概率是否 >= 50 思考损失后，我们应该做怎么应对（贼吃肉，贼挨打。挨打比吃肉痛苦的多） 中性看待信息（半仓） 规律 市场中每一刻都是独一无二的 想赚钱，不必知道下一刻的变化 不必追求每把必胜 常见的心里反应 损失厌恶 铆钉心里 不愿被规则束缚 心态 亏损是必要的概率损失 预测可以错，交易不可以错 客观，客观，客观 关注其他交易者的心态 8次失败，2次成功 负面 自大 自恋 自负 不自律 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/stock/铭记.html":{"url":"blog/stock/铭记.html","title":"铭记","keywords":"","body":"交易模型 买入模型 左侧交易， 在市场持续下跌的时候买入。 买入太快，有可能买在半山腰。 控制每天的买入量，假设现在可能只是半山腰 大盘下跌比较猛的时候，可以大仓位入手下跌比较狠的好股，预测肯定会强力反弹。 以概率的角度来讲，应该抓住大跌的机会，如果基本面没有恶化，只是正常回调，这个时候风险投资赢率较大 卖出模型 估值过高的时候开始减仓 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/安全/HTTP X-XSS-Protection缺失.html":{"url":"blog/安全/HTTP X-XSS-Protection缺失.html","title":"HTTP X XSS Protection缺失","keywords":"","body":"HTTP X-XSS-Protection 响应头是Internet Explorer，Chrome和Safari的一个功能，当检测到跨站脚本攻击 (XSS)时，浏览器将停止加载页面。虽然这些保护在现代浏览器中基本上是不必要的，当网站实施一个强大的Content-Security-Policy来禁用内联的JavaScript ('unsafe-inline')时, 他们仍然可以为尚不支持 CSP 的旧版浏览器的用户提供保护。 解决办法 Nginx配置 /usr/local/nginx/conf 里，打开nginx.conf add_header X-Xss-Protection \"1; mode=block\"; • 0：禁用XSS保护； • 1：启用XSS保护； • 1; mode=block：启用XSS保护，并在检查到XSS攻击时，停止渲染页面（例如IE8中，检查到攻击时，整个页面会被一个#替换）； 重启nginx Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/安全/Untitled.html":{"url":"blog/安全/Untitled.html","title":"Untitled","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/安全/csrf.html":{"url":"blog/安全/csrf.html","title":"Csrf","keywords":"","body":"web安全基础篇-跨站点请求伪造（CSRF) DXR嗯嗯呐 2022-05-25 16:00:05 90579 前言 此文章学习于《白帽子讲WEB安全》 CSRF的全名是Cross Site Request Forgery，翻译为中文就是跨站点请求伪造。 它是web攻击中常见的一种，CSRF也是web安全中最容易被忽略的一种攻击方式。但是CSRF在某些时候却能够产生强大的破坏性。 一、CSRF简介 攻击者盗用了你的身份，以你的名义发送恶意请求，对服务器来说这个请求是完全合法的，但是却完成了攻击者所期望的一个操作，比如以你的名义发送邮件、发消息，盗取你的账号，添加系统管理员，甚至于购买商品、虚拟货币转账等。 如下：其中Web A为存在CSRF漏洞的网站，Web B为攻击者构建的恶意网站，User C为Web A网站的合法用户。 CSRF攻击攻击原理及过程如下： 用户C打开浏览器，访问受信任网站A，输入用户名和密码请求登录网站A； 2.在用户信息通过验证后，网站A产生Cookie信息并返回给浏览器，此时用户登录网站A成功，可以正常发送请求到网站A； 用户未退出网站A之前，在同一浏览器中，打开一个网页访问网站B； 网站B接收到用户请求后，返回一些攻击性代码，并发出一个请求要求访问第三方站点A； 浏览器在接收到这些攻击性代码后，根据网站B的请求，在用户不知情的情况下携带Cookie信息，向网站A发出请求。网站A并不知道该请求其实是由B发起的，所以会根据用户C的Cookie信息以C的权限处理该请求，导致来自网站B的恶意代码被执行。 网站B通过user C权限操作网站A，这种伪造的请求，所以叫做跨站点请求伪造攻击。 二、CSRF进阶 2.1 浏览器的Cookie策略 浏览器所支持的Cookie分为两种：一种是“Session Cookie\" 又被称为 临时cookie；另外一种是”Third-party\" ，也被称为本地cookie。 两者的区别在于，Third-party Cookie 是服务器在Set-Cookie时指定了Expire时间，只有到了Expire时间后Cookie才会失效，而Session Cookie保存在浏览器进程的内存空间中，所以浏览器关闭后，session cookie就失效了。 如果浏览器从一个域的页面中，要加载另一个域的资源，由于安全原因，某些浏览器会阻止Third-party Cookie的发送。 下面举个例子 访问这个界面，发现浏览器同时接收两个cookie 这时候，再打开一个新的浏览器界面，访问同一个域的不同界面，因为新的界面在同一个浏览器进程中，因此session cookie将被发送 此时在另一个域中，有一个页面http://192.168.163.128/1.html, 此界面构造了csrf以访问http://192.168.163.131/test/index.html 通过IE浏览器访问，我们会发现，只能发送Session cookie，而Third-party Cookie被禁止了。是因为IE处于安全考虑，默认禁止了浏览器在、、等标签中发送第三方cookie。 在Firefox中，默认策略是允许发送第三方cookie的 在上面案例中，用户使用Firefox浏览器，所以我们成功获取了用于认证的Third-party Cookie，最终导致CSRF成功。 但若csrf攻击的目标并不需要使用cookie，则也不必顾虑浏览器的cookie策略了。 主流浏览器默认拦截Third-party Cookie有：IE6、IE7、IE8、Safari等 2.2 P3P头的副作用 虽然CSRF攻击不需要认证，不需要发送cookie，但是不可否认的是大部分敏感的操作是在认证之后的，因此浏览器拦截第三方cookie的发送，在某种程度上降低了CSRF攻击的威力，可是这一情况在P3P头介入变得复杂起来。 P3P Header是W3C定制的一项关于隐私的标准，全称The Platform for Privacy Preferences 如果网站返回给浏览器的HTTP头中包含有P3P头，则某种程度上来说，将浏览器发送给第三方cookie。在IE下即使是等标签也将不在拦截第三方cookie发送。 在网站业务中，P3P头主要用于类似广告等需要跨域访问的页面。 2.3 GET? POST? CSRF不只是通过GET请求发送，还可以通过POST方法，之前我们发起CSRF攻击主要使用HTML标签、等中的src属性。这类标签只能发送一次GET请求，而不能发起POST请求。而对于很多网站的应用来说，一些重要的操作并未严格的区分get与post，攻击者可以使用get请求表单的提交地址。比如在PHP中，如果使用的是$_REQUEST,而非 $ _POST获取变量，则会出现这个问题。 对于一个表单来说，用户往往也可以使用get方式提交参数，比如以下表单： 我们抓包看到正常填报的请求是post请求 用户也可以尝试构造一个get请求 http://192.168.163.131/1.html?username=test&password=passwd 尝试提交，若服务器未对请求方法进行限制，则这个请求会通过 如果服务器端已经区分了get或者post，攻击者也可以通过其他若干个方法构造post请求。 最简单的方法就是在一个页面中构造好一个form表单，然后使用JavaScript自动提交这个表单，比如攻击者在http://192.168.163.132/1.html中编写如下代码 var f = document.getElementById(\"register\") f.inputs[0].value = \"test\"; f.inputs[1].value = \"passwd\"; f.submit(); 攻击者甚至可以将这个页面隐藏在一个不可以见的iframe窗口中，那么整改自动提交表单的过程对于用户来说也是不可见的。 2.4 Flash CSRF Flash也有很多种方式能够发起网络请求，包括POST，比如下面这段代码 import flash.net.URLRequest; import flash.net.Security var url = net URLRequest(\"http://www.a.com\"); var param = new URLVariables(); param = \"test=123\"; url.method=\"POST\"; url.data = param; sendToURL(url); stop(); 除了URLRequest(),还有getURL,loadVars等方式发起请求。 在IE6、IE7中，Flash发送的网络请求均可以带上本地cookie，但从IE 8 开始，Flash发起的网络请求已经不再发送本地cookie。 三、CSRF防御 CSRF是一种比较奇特的攻击，我们通过什么方式防御呢。 3.1 验证码 验证码被认为是对抗CSRF攻击最简洁而有效的防御方法。 CSRF攻击过程往往是用户不知情下构造了网络请求，而验证码，则强制用户与应用交互，才能完成请求。但是验证码不是万能的，用户不能给全部操作都加验证码，这样系统就没办法用了，所以验证码是能作为防御CSRF防御的辅助手段，而不是作为主要的解决方案。 3.2 Referer Check Referer Check 在互联网中最常见的应用就是防止图片盗链，同理，Referer Check也可以用于检查请求是否来自合法的“源”。 即便咱们能够经过检查 referer 是否合法来判断用户是否被 csrf 攻击，也仅仅是知足了防护的充分条件。 referer check 的缺陷在于，服务器并非何时都能去到 referer。（不少用户出于隐私保护的考虑，限制了 referer 的发送。在某些状况下，浏览器也不会发送referer，好比从 https 跳转到 http ，出于安全的考虑，浏览器也不会发送 referer）。 出于以上原因，我们无法依赖于Referer Check 作为防御CSRF的主要手段，但是通过Referer Check 来监控csrf攻击的发生，倒是可行的方法。 3.3 Anti CSRF Token 针对CSRF的防御，一般都是使用一个Token。在介绍此方法之前，首先了解一下csrf的本质。 3.3.1 CSRF的本质 CSRF为什么能够攻击成功？其本质原因是重要操作的参数都是可以被攻击者猜测的。 攻击者只有预测处URL的所有参数于参数值，才能成功地构造一个伪造的请求；反之，攻击者将无法攻击成功。 出于这个原因，可以想到一个解决方案：把参数加密，或者使用一些随机数，从而让攻击者无法猜测到参数值。这是 不可预测性原则。 因为对参数加密会导致某些网站收藏有问题，所有就衍生出来新增一个Token参数。这个Token的值是随机的，不可预测的： http://www.a.com/delete?username=test&item=123&token=[random(seed)] Token需要足够的随机，必须使用足够安全的随机数生成算法，或者采用真随机数生成器。Token应该作为一个密码，为用户与服务器所共同持有，不能被第三者知晓。实际应用时，Token可以放在用户的session中或者浏览器的cookie中。 由于Token的存在，攻击者无法再构造处一个完整的url实施CSRF攻击。 Token需要同时放在表单和Session中，在提交请求时，服务器只需要验证表单中的Token，与用户session或者cookie的token是否一致，如果一致，则认为是合法请求，如果不一致，或者有一个为空，则认为请求不合法，可能是csrf攻击。 3.3.2 Token的使用原则 Anti CSRF Token使用注意事项： 防御CSRF的Token，是根据不可预测性原则涉及的方案，所以Token的生成一定要足够随机，需要使用安全的随机数生成器。 Token的目的不是为了防止重复提交，所以为了使用方便，可以允许在一个用户的有效生命周期内，在Token消耗掉前都使用同一个Token。但是如果用户已经提交了表单，则这个Token已经消耗掉，应该在次重新生成一个新的Token。 如果Token保存在cookie中，而不是服务端的session中，则会带来一个新的问题。如果一个用户打开几个相同页面同时操作，当某个页面消耗掉Token后，其他页面的表单内保存的还是被消耗掉的那个Token，因此其他页面消耗掉Token后，会出现Token错误，在这种情况下考虑生成多个有效的Token，以解决多页面共存的场景。 使用Toke时应该注意Token的保密性。比如URL中包含Token，就可以导致泄露： http://www.a.com/delete?username=test&item=123&token=[random] 因此使用Token时，应该尽量把Token放在表单中。把敏感操作由get修改为post。以form表单或者AJAX的形式提交，避免导致Token泄露。 还有一些其他途径泄露Token，比如XSS漏洞或者一些跨域漏洞，都可能让Token被攻击者窃取。 Token仅仅用于对抗CSRF攻击，当网站还同时存在xss漏洞时，这个方案就会失效，因为XSS可以模拟客户端浏览器任意执行操作。在xss攻击下，攻击者完全可以请求页面后，读出页面内容中的Token，然后在构造出来一个合法的请求，这个过程称为XSRF，和CSRF以示区别。 四、总结 CSRF攻击时攻击者利用用户的身份操作用户账户的一种攻击方式，设计CSRF的防御必须理解CSRF攻击的原理和本质。 根据不可预测性原则，我们通常使用token来防御CSRF攻击，使用时要注意Token的保密性和随机性。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/安全/vue-golang-csrf.html":{"url":"blog/安全/vue-golang-csrf.html","title":"Vue Golang Csrf","keywords":"","body":"https://blog.csdn.net/huadongqiang/article/details/114819353?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-1-114819353-null-null.pc_agg_new_rank&utm_term=axios%20%E8%AE%BF%E9%97%AEsecurity&spm=1000.2123.3001.4430 https://github.com/gorilla/csrf add_header X-Frame-Options SAMEORIGIN; X-Content-Type-Options: nosniff； Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/":{"url":"blog/效率/","title":"效率","keywords":"","body":"效率 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/UML.html":{"url":"blog/效率/UML.html","title":"UML","keywords":"","body":"[toc] 看懂UML类图和时序图 这里不会将UML的各种元素都提到，我只想讲讲类图中各个类之间的关系； 能看懂类图中各个类之间的线条、箭头代表什么意思后，也就足够应对 日常的工作和交流； 同时，我们应该能将类图所表达的含义和最终的代码对应起来； 有了这些知识，看后面章节的设计模式结构图就没有什么问题了； 本章所有图形使用Enterprise Architect 9.2来画,所有示例详见根目录下的design_patterns.EAP 从一个示例开始 请看以下这个类图，类之间的关系是我们需要关注的： .. image:: /_static/uml_class_struct.jpg 车的类图结构为>，表示车是一个抽象类； 它有两个继承类：小汽车和自行车；它们之间的关系为实现关系，使用带空心箭头的虚线表示； 小汽车为与SUV之间也是继承关系，它们之间的关系为泛化关系，使用带空心箭头的实线表示； 小汽车与发动机之间是组合关系，使用带实心箭头的实线表示； 学生与班级之间是聚合关系，使用带空心箭头的实线表示； 学生与身份证之间为关联关系，使用一根实线表示； 学生上学需要用到自行车，与自行车是一种依赖关系，使用带箭头的虚线表示； 下面我们将介绍这六种关系； 类之间的关系 泛化关系(generalization) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 类的继承结构表现在UML中为：泛化(generalize)与实现(realize)： 继承关系为 is-a的关系；两个对象之间如果可以用 is-a 来表示，就是继承关系：（..是..) eg：自行车是车、猫是动物 泛化关系用一条带空心箭头的直接表示；如下图表示（A继承自B）； .. image:: /_static/uml_generalization.jpg eg：汽车在现实中有实现，可用汽车定义具体的对象；汽车与SUV之间为泛化关系； .. image:: /_static/uml_generalize.jpg 注：最终代码中，泛化关系表现为继承非抽象类； 实现关系(realize) ^^^^^^^^^^^^^^^^^^^^ 实现关系用一条带空心箭头的虚线表示； eg：\"车\"为一个抽象概念，在现实中并无法直接用来定义对象；只有指明具体的子类(汽车还是自行车)，才 可以用来定义对象（\"车\"这个类在C++中用抽象类表示，在JAVA中有接口这个概念，更容易理解） .. image:: /_static/uml_realize.jpg 注：最终代码中，实现关系表现为继承抽象类； 聚合关系(aggregation) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 聚合关系用一条带空心菱形箭头的直线表示，如下图表示A聚合到B上，或者说B由A组成； .. image:: /_static/uml_aggregation.jpg 聚合关系用于表示实体对象之间的关系，表示整体由部分构成的语义；例如一个部门由多个员工组成； 与组合关系不同的是，整体和部分不是强依赖的，即使整体不存在了，部分仍然存在；例如， 部门撤销了，人员不会消失，他们依然存在； 组合关系(composition) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 组合关系用一条带实心菱形箭头直线表示，如下图表示A组成B，或者B由A组成； .. image:: /_static/uml_composition.jpg 与聚合关系一样，组合关系同样表示整体由部分构成的语义；比如公司由多个部门组成； 但组合关系是一种强依赖的特殊聚合关系，如果整体不存在了，则部分也不存在了；例如， 公司不存在了，部门也将不存在了； 关联关系(association) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 关联关系是用一条直线表示的；它描述不同类的对象之间的结构关系；它是一种静态关系， 通常与运行状态无关，一般由常识等因素决定的；它一般用来定义对象之间静态的、天然的结构； 所以，关联关系是一种“强关联”的关系； 比如，乘车人和车票之间就是一种关联关系；学生和学校就是一种关联关系； 关联关系默认不强调方向，表示对象间相互知道；如果特别强调方向，如下图，表示A知道B，但 B不知道A； .. image:: /_static/uml_association.jpg 注：在最终代码中，关联对象通常是以成员变量的形式实现的； 依赖关系(dependency) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 依赖关系是用一套带箭头的虚线表示的；如下图表示A依赖于B；他描述一个对象在运行期间会用到另一个对象的关系； .. image:: /_static/uml_dependency.jpg 与关联关系不同的是，它是一种临时性的关系，通常在运行期间产生，并且随着运行时的变化； 依赖关系也可能发生变化； 显然，依赖也有方向，双向依赖是一种非常糟糕的结构，我们总是应该保持单向依赖，杜绝双向依赖的产生； 注：在最终代码中，依赖关系体现为类构造方法及类方法的传入参数，箭头的指向为调用关系；依赖关系除了临时知道对方外，还是“使用”对方的方法和属性； 时序图 为了展示对象之间的交互细节，后续对设计模式解析的章节，都会用到时序图； 时序图（Sequence Diagram）是显示对象之间交互的图，这些对象是按时间顺序排列的。时序图中显示的是参与交互的对象及其对象之间消息交互的顺序。 时序图包括的建模元素主要有：对象（Actor）、生命线（Lifeline）、控制焦点（Focus of control）、消息（Message）等等。 关于时序图，以下这篇文章将概念介绍的比较详细；更多实例应用，参见后续章节模式中的时序图； http://smartlife.blog.51cto.com/1146871/284874 附录 在EA中定义一个抽象类（其版型为《abstract》) .. image:: /_static/uml_AbatractClass.jpg Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/auth2.html":{"url":"blog/效率/auth2.html","title":"Auth2","keywords":"","body":"GitLab API 授权认证及使用 2019.12.26 2020.6.12 Tech 2527 6 分钟 作者：Zander Hsueh 链接：https://xuezenghui.com/posts/use-gitlab-api/ 许可：[CC BY-NC-SA 4.0] 前言 通常，GitLab 的操作都是通过 Web 界面或命令行来完成的，但 GitLab 也提供了简单而强大的开放 API 来自动执行 GitLab 相关操作，查看项目信息、提交 Issues、合并分支……统统都在 docs.gitlab.com，GitLab API 大多为 REST API，但也同样支持 GraphQL API，并且 v5 版本的 API 将全部基于 GraphQL API，as Facebook does. 授权认证 绝大多数的 GitLab API 都是需要身份验证的，这毋庸置疑，其它公司的内部数据不能随随便便就被获取到，即使是公司内部也会有各个 Group 或 Project 的权限设置。 再以使用 GitLab API 的目的出发——主要是为了在公司 Portal 网站中显示 GitLab 中的项目信息、成员信息、Issues 等，这就要求在用户登录 Portal 网站时获取其 GitLab 内具体权限，以显示其权限对应的 GitLab 数据。问题出来了：Portal 网站如何经过 GitLab 的同意来获取用户数据？ API 的使用无非 CRUD，按照 GitLab API 清晰完整的文档来就可以了，如何进行身份验证才是重头戏，GitLab API 的身份验证有四种方法： 1. OAuth2 tokens OAuth2 即 OAuth 2.0版本，是一个关于授权的开放网络标准，也是目前应用最广泛的授权认证方式。它的运行流程是这样的： [1]◎ AOuth 2.0 Workflow 简单来说就是在第三方应用（客户端 Client）需要资源服务器（Resource Server）的数据时，资源拥有者（Resource Owner）同意后授权给客户端，客户端向认证服务器（Authorization Server）申请令牌（Token），认证服务器确认后发放令牌，客户端就可以拿着令牌去获取资源服务器的数据了。 这些步骤中最重要的又在于客户端如何得到用户的授权（Authorization Grant）从而拿到令牌（Access Token），OAuth 2.0提供了四种授权方式，其中授权码模式（Authorization Code Grant）是最严密完整的，也是绝大多数网站作为资源服务器时采用的授权方式（包括 GitLab）。授权码模式流程图： [2]◎ Authorization Code Workflow 以 GitLab 的 OAuth2 验证方式解释一下此流程（开发者视角）： 第一步、创建应用 在 GitLab Web 界面的 Setting ➡️ Applications 中注册用于提供 OAuth 验证的应用。重定向 URI（Redirect URI）本应设为第三方应用（本例中即为公司 Portal 网站）的线上 URI，处于开发阶段时也可设为本地应用运行后的访问路径，如http://localhost:8080/login，此重定向 URI 的作用下文会详述。页面还会要求选择 Scopes，表示此应用的授权范围，应根据第三方应用的具体需求选择，我选 api，嘿嘿嘿，应用成功创建后会显示其具体信息，包括应用 Id Application Id、应用密钥Secret、回调 URLCallback url 和权限 Scopes。 GitLab 要求 Redirect URI 中不能包含一些特殊字符，如 #。在 Vue 中如果 vue-router 采用了 hash 模式，就与 Redirect URI 的格式要求冲突了，因此 vue-router 应改为采用 history 模式，详参 HTML5 History 模式。 第二步、请求授权码 ◎ Partol 中的授权按钮 点击 Portal 中 「GitLab 授权」按钮时使用 location.href 跳转至授权页面： https://gitlab.zander.com/oauth/authorize?client_id=4e1fe77ba1d43b151428d907574er866a48af8dbc8766ea839a84a88c6dace39&redirect_uri=http://localhost:8080/login&response_type=code&state=zander&scope=api URI 中的参数包括： 参数 是否必须 含义 client_id true 注册 GitLab 应用成功后的 Application Id redirect_uri true 注册应用时设置的重定向 URI response_type true 返回的类型，授权码模式即为code state false 用于确认请求和回调的状态，OAuth 建议以此来防止 CSRF 攻击[3] scope false 权限设置，范围不得超出创建应用时的配置，以空格分隔 第三步、用户授权 ◎ 授权页面 发送请求后网页会跳转到 GitLab 的授权页面，先要求用户登录 GitLab，然后询问用户是否同意授权🥺。用户点击同意后页面便会返回一个包含授权码 code 和参数 state（如果你传了它的话）的重定向 URI 并跳转至对应的网页，即网页的地址栏变成了这样： http://localhost:8080/login?code=90792302acc2a0724d44c74f43d0fd77f005723c9ae5def965b02675f532949a&state=zander 第四步、获取令牌 Token 既然拿到了 code，嘿嘿嘿😎，只需要一个 Post 请求就能拿到可任意调用 GitLab API 大军的虎符🐯—— Access Token。要注意的是，获取 Token 的操作是需要在第三方应用的后台完成的，以保证数据的安全性。 POST https://gitlab.zander.com/oauth/token 参数包括： 参数 是否必须 含义 client_id true 注册应用的 Application Id client_secret true 注册应用的 Secret code true 上面获取到的授权码，但是其有效期很短，一般为10min grant_type true 授权方式，authorization_code redirect_uri true 颁发令牌后的回调网址 GitLab 收到此请求后便会向参数中的redirect_uri网址发送一段 JSON 数据，虎符在此： 1 2 3 4 5 6 7 { \"access_token\": \"a7e514632722f45a9edfe4e8624ec3fcd826ebbcb830055f180efee4533a50dd\", \"token_type\": \"bearer\", \"refresh_token\": \"360c6864b42247fafeaac4715fc524f939ca4545f8400126705144d7e37b5042\", \"scope\": \"api\", \"created_at\": 1577427939 } 2. Personal access tokens ◎ 生成 Personal access token GitLab Web 界面中进入 Seting ➡️ Access Tokens，输入名字和到期日期就可以生成对应的 Access Token，注意生成后需要保存好 Token，因为生成的这条 Token 不会再出现第二次，虽然你可以继续生成新的 Token😑。最简单的一种验证方式，但是此方式要求用户必须登入 GitLab Web 页面进特定操作，不可取，自己玩玩倒是很方便。 3. Session cookie 登录 GitLab 应用程序时会生成 Session cookie，之后 GitLab 中的 API 都通过此 cookie 进行身份验证，也就是人家官网使用的验证方式，不能通过特定 API 生成这个 cookie，排除。 4. GitLab CI job token 在 GitLab 内置的持续集成工具 GitLab CI 的 Job 中使用，每个 Job 可配置一个 CI job token，使用方式类似用户名和密码。 使用 有了 Token，就有了使用 GitLab API 的🔑，但是不同 Token 的使用方式也不同： AOuth 2.0获取的 Token 类型是 bearer-tokens，需要在 GitLab API 请求中加入 Key 为Authorization，Value 为 Bearer 的 Header。 Personal access tokens 获取的是 Private-Token，需要加入 Key 为Private-Token、Value 为 Token 值的请求 Header。 所有的 GitLab API 前缀均为https://gitlab.example.com/api/v4/，使用方式与常见 API 无异，例： 1 GET https://gitlab.example.com/api/v4/prjects 此 API 可获取到用户在 GitLab 上所有的可见项目列表： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 [ { \"id\": 1548, \"description\": \"demo\", \"name\": \"Hello\", \"name_with_namespace\": \"Zander Xue / Hello\", \"path\": \"hello\", \"path_with_namespace\": \"zander/hello\", \"created_at\": \"2019-12-26T06:01:59.746Z\", \"default_branch\": \"master\", \"tag_list\": [], \"ssh_url_to_repo\": \"git@gitlab.example.com:zander/hello\", \"http_url_to_repo\": \"https://gitlab.example.com/zander/hello.git\", \"web_url\": \"https://gitlab.example.com/zander/hello\", \"avatar_url\": null, \"star_count\": 999, \"forks_count\": 888, \"last_activity_at\": \"2019-12-26T06:01:59.746Z\", \"_links\": { \"self\": \"http://gitlab.example.com/api/v4/projects/1548\", \"issues\": \"http://gitlab.example.com/api/v4/projects/1548/issues\", \"merge_requests\": \"http://gitlab.example.com/api/v4/projects/1548/merge_requests\", \"repo_branches\": \"http://gitlab.example.com/api/v4/projects/1548/repository/branches\", \"labels\": \"http://gitlab.example.com/api/v4/projects/1548/labels\", \"events\": \"http://gitlab.example.com/api/v4/projects/1548/events\", \"members\": \"http://gitlab.example.com/api/v4/projects/1548/members\" }, \"archived\": false, \"visibility\": \"internal\", \"owner\": { \"id\": 268, \"name\": \"Zander Hsueh\", \"username\": \"zander\", \"state\": \"active\", \"avatar_url\": \"null\", \"web_url\": \"https://gitlab.example.com/zander\" }, \"resolve_outdated_diff_discussions\": false, \"container_registry_enabled\": true, \"issues_enabled\": true, \"merge_requests_enabled\": true, \"wiki_enabled\": true, \"jobs_enabled\": true, \"snippets_enabled\": true, \"shared_runners_enabled\": true, \"lfs_enabled\": true, \"creator_id\": 268, \"namespace\": { \"id\": 666, \"name\": \"zander\", \"path\": \"zander\", \"kind\": \"user\", \"full_path\": \"zander\", \"parent_id\": null }, \"import_status\": \"none\", \"open_issues_count\": 0, \"public_jobs\": true, \"ci_config_path\": null, \"shared_with_groups\": [], \"only_allow_merge_if_pipeline_succeeds\": false, \"request_access_enabled\": false, \"only_allow_merge_if_all_discussions_are_resolved\": false, \"printing_merge_request_link_enabled\": true, \"permissions\": { \"project_access\": null, \"group_access\": null } }, \"...\" ] References & Resources 理解 OAuth 2.0 | 阮一峰 OAuth 2.0的四种方式 | 阮一峰 OAuth 2.0协议入门 | 掘金 来源：https://tools.ietf.org/html/rfc6749#section-1.2 来源：https://tools.ietf.org/html/rfc6749#section-4.1 详见 https://tools.ietf.org/html/rfc6749#section-10.12 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/cat.html":{"url":"blog/效率/cat.html","title":"Cat","keywords":"","body":"cat 1. 通过cat 的输出管道给命令提供参数 cat Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/docker volume clean.html":{"url":"blog/效率/docker volume clean.html","title":"Docker Volume Clean","keywords":"","body":"docker 磁盘清理 自动清理 docker system prune 已使用的镜像：指所有已被容器（包括stop的）关联的镜像，也就是docker ps -a所看到的所有容器对应的image。 未引用镜像：没有被分配或使用在容器中的镜像 悬空镜像（dangling image）：未配置任何Tag（也就是无法被引用）的镜像。通常是由于镜像编译过程中未指定-t参数配置Tag导致的。 删除悬空的镜像docker image prune docker container prune：删除无用的容器。 --默认情况下docker container prune命令会清理掉所有处于stopped状态的容器 --如果不想那么残忍统统都删掉，也可以使用--filter标志来筛选出不希望被清理掉的容器。例子：清除掉所有停掉的容器，但24内创建的除外： --$ docker container prune --filter \"until=24h\" docker volume prune：删除无用的卷。 docker network prune：删除无用的网络 ``` 手动清除 对于悬空镜像和未使用镜像可以使用手动进行个别删除： 1、删除所有悬空镜像，不删除未使用镜像： docker rmi $(docker images -f \"dangling=true\" -q) 2、删除所有未使用镜像和悬空镜像 docker rmi $(docker images -q) 3、清理卷 如果卷占用空间过高，可以清除一些不使用的卷，包括一些未被任何容器调用的卷（-v 详细信息中若显示 LINKS = 0，则是未被调用）： 删除所有未被容器引用的卷： docker volume rm $(docker volume ls -qf dangling=true) 4、容器清理 如果发现是容器占用过高的空间，可以手动删除一些： Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/git book.html":{"url":"blog/效率/git book.html","title":"Git Book","keywords":"","body":"gitbook 笔记 Gitbook配置目录折叠 在执行gitbook init主目录下增加book.json文件做定制化配置 配置目录折叠功能如下： { 　　\"plugins\":[ 　　　　\"expandable-chapters\" 　　] } 然后在主目录下用命令行执行gitbook install，会生成node_modules文件夹，配置的插件也会自动下载到该目录下。 在SUMMARY.md文件中配置目录时直接配置目录名称即可，不用配置连接地址，如下: 　　[目录名称]() 启动后查看即可达到预期。 除此之外，如果目录内容比较多，左边菜单栏显示不下，也可以使用插件来达到放大菜单栏宽度的目的 插件：在book.json中配置splitter，后续步骤与以上一致 　　 除了在book.json中配置外，还可以直接使用命令进行安装，如：npm install gitbook-plugin-splitter。为维护方便，推荐使用book.json文件进行配置。 gitbook插件官网地址：https://plugins.gitbook.com/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/git.html":{"url":"blog/效率/git.html","title":"Git","keywords":"","body":"[toc] Git Tips push git push origin refs/heads/master:refs/heads/master 简写 git push origin master:master Pull rebase git add ./ git commit -m \"update some\" git pull --rebase origin/dev if has conflict if conflict can handle git add ./ git rebase --continue else git rebase --abort else git push origin ${your_branch} end history id git log show current commit's content git show commit 内容填写不规范，重写编辑 git commit --amend 完成撤销,同时将代码恢复到前一commit_id 对应的版本 git reset --hard commit_id 完成Commit命令的撤销，但是不对代码修改进行撤销 git reset commit_id git瘦身 git gc --prune=now 完全复制其他分支 git reset --hard origin/${other_branch} delete remote tag 只需将\"空\"引用推送到远程标记名： git push origin :tagname 或者，更具体地说，使用--delete选项(如果您的git版本早于1.8.0，则使用-d选项)： git push --delete origin tagname 注意，Git有标记名称空间和分支名称空间，因此可以对分支和标记使用相同的名称。如果要确保不会意外地删除分支而不是标记，可以指定完全引用，它将永远不会删除分支： git push origin :refs/tags/tagname delete local tag git push -d tagName Pull Request For Open Source Project You only have one commit incorrectly signed off! To fix, first ensure you have a local copy of your branch by checking out the pull request locally via command line. Next, head to your local branch and run: git commit --amend --no-edit --signoff Now your commits will have your sign off. Next run git push --force-with-lease origin master Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/linux command.html":{"url":"blog/效率/linux command.html","title":"Linux Command","keywords":"","body":"Linux command copy cp -r dir /target/ # 带目录拷贝 cp -r dir/ target/ # 不带目录拷贝 统计代码行数 find ./e2e -name \"*.go\" |xargs cat|grep -v ^$|wc -l Macos 查看端口命令 a. `netstat -nat | grep ` , 如命令 `netstat -nat | grep 3306` b. `netstat -nat |grep LISTEN` Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/linux 内核.html":{"url":"blog/效率/linux 内核.html","title":"Linux 内核","keywords":"","body":" 此篇文章是基于高通 ODM BSP 开发做的一个简单总结，起初是用来对新人进行培训的。 我觉得学习内核驱动时，最开始只需要 ‘Know what, not know how ’。 不用去探究细节，只需要知道整体的框架，知道有哪些需要我们重视的内容即可。 何为 Linux 内核开发？ 首先，初步认识下 Linux kernel Linux 内核的框架如上图。 设备子系统负责和硬件打交道。 大部分工作集中在设备子系统部分。 内核开发是什么？ 广义上讲，新增或修改上图中内核部分的所有子系统。 非 Linux 源码贡献者，一般来说只修改设备子系统部分。 接下来，简单聊聊初学者需要重点关注的三个部分：设备树，字符设备，平台设备驱动。 设备树（DTS） 设备树相当于一份软件中描述硬件结构的配置框图。假设下图为硬件框图： 那么其软件描述的代码片段如下： / { // root node model = \"Qualcomm Technologies, Inc. SDM xxx\"; compatible = \"qcom,sdmxxx\"; cpus { ... cpu@0 { ... }; cpu@1 { ... }; }; usb@ { ... }; serial@ { ... }; gpio@ { ... }; intc: interrupt-controller@ { ... }; external-bus { ... i2c@0,0 { ... xxx@ { // I2C Dev .... }; }; flash@1,0 { ... }; }; }; 字符设备驱动 字符设备驱动是理解设备驱动的基础。 大多数设备都可以归于字符设备。 平台设备驱动模型 设备（或驱动）注册时，会通过总线去匹配对应的驱动（或设备）。 设备和驱动通常需要挂在一种实际总线上，除带有 I2C、SPI、USB 等的设备外，内核为没有实际总线的外设实现了虚拟的平台总线 。 平台设备驱动是独立于字符设备、网络设备等的一种抽象概念 。 kernel 开发需要什么样的知识储备？ C 语言 良好的 C 语言能力， Linux 官方推荐了如下书籍。 The C Programming Language Practical C Programming C: A Reference Manual GNU 内核由 GNU C 和 GNU toolchain 实现，所以如下两方面的知识是需要的。 GNU C 的编码规则 GNU 工具链的使用 Linux 基本命令 鸟哥的 Linux 私房菜 设备驱动相关知识 Linux 设备驱动程序 内核原理 Linux 内核完全注释 深入理解 Linux 内核 在我们的工作中，kernel 开发一般怎么做？ Android 设备通常的开发周期 在我们的工作中，kernel 开发主要集中在 Bringup、Integrate、Verify 三个阶段 。 源码获取 高通的代码分两部分：一部分是开源的，可以从 codeaurora.org 下载，还有一部分是高通产权的，需要从高通的网站上下载。 高通产权的代码存放路径：vendor/qcom/proprietary 。 实际工作中，SCM 一般会帮忙准备好 Base 代码。 可以通过 repo init -u https://android.googlesource.com/platform/manifest -b android-4.0.1_r1 在 Source.android 下载 Google 官方源码。 Bringup 根据需求实现各种外设模块的基本功能。 LCD、TP 、Sensor 、Charger 等功能正常，手机能进入 Launcher 界面，能正常使用，USB 连接正常。 这样 Bringup 工作就基本完成了。 Porting 和编写各种外设的驱动（需求的具体实现） Porting 硬件相关配置，即实现 DTS 。 Porting 相关驱动 。 Sensor 和其他外设有一点差异 。 其分为 AP 侧驱动（厂商提供）和 ADSP 侧驱动（高通和厂商协同）两种方式 。 主要配置总线、 GPIO 及 Sensor 的属性 。 系统维护（解 BUG） 对比机 阅读源码 善用调试工具 Createpoint + QCOM Case （高通文档工具下载，及向高通在线寻求帮助。） 搜索引擎 GTD （主动性） 文档（Read + Write） kernel 调试的常用方式有哪些？ 硬件调试 示波器 程控电源 万用表 Power monitor Logs 串口日志 Logging System logcat/kmsg… Enhanced log pstore ramdump Tools adb dumpsys gdb QPST Get ramdump/adsp log/… - systrace trace CPU/GPU/Function/Activity/… - powerTop power consumption kmemleak vmstat + top/ps + pmap in android out/soong/host/linux-x86/bin/ - simg2img/lpdump… objdump 文件系统或节点 sys power/irq/gpio … proc 内核信息 打印级别 dynamic debug echo “file xxx.c +p” > /sys/kernel/dynamic_debug/control Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/linux 常用命令.html":{"url":"blog/效率/linux 常用命令.html","title":"Linux 常用命令","keywords":"","body":"系统信息 arch #显示机器的处理器架构(1) uname -m #显示机器的处理器架构(2) uname -r #显示正在使用的内核版本 dmidecode -q #显示硬件系统部件 - (SMBIOS / DMI) hdparm -i /dev/hda #罗列一个磁盘的架构特性 hdparm -tT /dev/sda #在磁盘上执行测试性读取操作 cat /proc/cpuinfo #显示CPU info的信息 cat /proc/interrupts #显示中断 cat /proc/meminfo #校验内存使用 cat /proc/swaps #显示哪些swap被使用 cat /proc/version #显示内核的版本 cat /proc/net/dev #显示网络适配器及统计 cat /proc/mounts #显示已加载的文件系统 lspci -tv #罗列PCI设备 lsusb -tv #显示USB设备 date 显示系统日期 cal 2007 #显示2007年的日历表 date 041217002007.00 #设置日期和时间 - 月日时分年.秒 clock -w #将时间修改保存到 BIOS 关机 (系统的关机、重启以及登出 ) shutdown -h now #关闭系统(1) init 0 #关闭系统(2) telinit 0 #关闭系统(3) shutdown -h hours:minutes & #按预定时间关闭系统 shutdown -c #取消按预定时间关闭系统 shutdown -r now #重启(1) reboot #重启(2) logout #注销 文件和目录 cd /home #进入 '/ home' 目录' cd .. #返回上一级目录 cd ../.. #返回上两级目录 cd #进入个人的主目录 cd ~user1 #进入个人的主目录 cd - #返回上次所在的目录 pwd #显示工作路径 ls #查看目录中的文件 ls -F #查看目录中的文件 ls -l #显示文件和目录的详细资料 ls -a #显示隐藏文件 ls *[0-9]* #显示包含数字的文件名和目录名 tree #显示文件和目录由根目录开始的树形结构(1) lstree #显示文件和目录由根目录开始的树形结构(2) mkdir dir1 #创建一个叫做 'dir1' 的目录' mkdir dir1 dir2 #同时创建两个目录 mkdir -p /tmp/dir1/dir2 #创建一个目录树 rm -f file1 #删除一个叫做 'file1' 的文件' rmdir dir1 #删除一个叫做 'dir1' 的目录' rm -rf dir1 #删除一个叫做 'dir1' 的目录并同时删除其内容 rm -rf dir1 dir2 #同时删除两个目录及它们的内容 mv dir1 new_dir #重命名/移动 一个目录 cp file1 file2 #复制一个文件 cp dir/* . #复制一个目录下的所有文件到当前工作目录 cp -a /tmp/dir1 . #复制一个目录到当前工作目录 cp -a dir1 dir2 #复制一个目录 ln -s file1 lnk1 #创建一个指向文件或目录的软链接 ln file1 lnk1 #创建一个指向文件或目录的物理链接 touch -t 0712250000 file1 #修改一个文件或目录的时间戳 - (YYMMDDhhmm) file file1 outputs the mime type of the file as text iconv -l #列出已知的编码 iconv -f fromEncoding -t toEncoding inputFile > outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding. find . -maxdepth 1 -name *.jpg -print -exec convert \"{}\" -resize 80x60 \"thumbs/{}\" \\; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick) 文件搜索 find / -name file1 #从 '/' 开始进入根文件系统搜索文件和目录 find / -user user1 #搜索属于用户 'user1' 的文件和目录 find /home/user1 -name \\*.bin #在目录 '/ home/user1' 中搜索带有'.bin' 结尾的文件 find /usr/bin -type f -atime +100 #搜索在过去100天内未被使用过的执行文件 find /usr/bin -type f -mtime -10 #搜索在10天内被创建或者修改过的文件 find / -name \\*.rpm -exec chmod 755 '{}' \\; #搜索以 '.rpm' 结尾的文件并定义其权限 find / -xdev -name \\*.rpm #搜索以 '.rpm' 结尾的文件，忽略光驱、捷盘等可移动设备 locate \\*.ps #寻找以 '.ps' 结尾的文件 - 先运行 'updatedb' 命令 whereis halt #显示一个二进制文件、源码或man的位置 which halt #显示一个二进制文件或可执行文件的完整路径 挂载一个文件系统 mount /dev/hda2 /mnt/hda2 #挂载一个叫做hda2的盘 - 确定目录 '/ mnt/hda2' 已经存在 umount /dev/hda2 #卸载一个叫做hda2的盘 - 先从挂载点 '/ mnt/hda2' 退出 fuser -km /mnt/hda2 #当设备繁忙时强制卸载 umount -n /mnt/hda2 #运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用 mount /dev/fd0 /mnt/floppy #挂载一个软盘 mount /dev/cdrom /mnt/cdrom #挂载一个cdrom或dvdrom mount /dev/hdc /mnt/cdrecorder #挂载一个cdrw或dvdrom mount /dev/hdb /mnt/cdrecorder #挂载一个cdrw或dvdrom mount -o loop file.iso /mnt/cdrom #挂载一个文件或ISO镜像文件 mount -t vfat /dev/hda5 /mnt/hda5 #挂载一个Windows FAT32文件系统 mount /dev/sda1 /mnt/usbdisk #挂载一个usb 捷盘或闪存设备 mount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share #挂载一个windows网络共享 磁盘空间 df -h #显示已经挂载的分区列表 ls -lSr |more #以尺寸大小排列文件和目录 du -sh dir1 #估算目录 'dir1' 已经使用的磁盘空间' du -sk * | sort -rn #以容量大小为依据依次显示文件和目录的大小 rpm -q -a --qf '%10{SIZE}t%{NAME}n' | sort -k1,1n #以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统) dpkg-query -W -f='${Installed-Size;10}t${Package}n' | sort -k1,1n #以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统) 用户和群组 groupadd group_name #创建一个新用户组 groupdel group_name #删除一个用户组 groupmod -n new_group_name old_group_name #重命名一个用户组 useradd -c \"Name Surname \" -g admin -d /home/user1 -s /bin/bash user1 #创建一个属于 \"admin\" 用户组的用户 useradd user1 #创建一个新用户 userdel -r user1 #删除一个用户 ( '-r' 排除主目录) usermod -c \"User FTP\" -g system -d /ftp/user1 -s /bin/nologin user1 #修改用户属性 passwd #修改口令 passwd user1 #修改一个用户的口令 (只允许root执行) chage -E 2005-12-31 user1 #设置用户口令的失效期限 pwck #检查 '/etc/passwd' 的文件格式和语法修正以及存在的用户 grpck #检查 '/etc/passwd' 的文件格式和语法修正以及存在的群组 newgrp group_name #登陆进一个新的群组以改变新创建文件的预设群组 文件的权限 使用 \"+\" 设置权限，使用 \"-\" 用于取消 ls -lh #显示权限 ls /tmp | pr -T5 -W$COLUMNS #将终端划分成5栏显示 chmod ugo+rwx directory1 #设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限 chmod go-rwx directory1 #删除群组(g)与其他人(o)对目录的读写执行权限 chown user1 file1 #改变一个文件的所有人属性 chown -R user1 directory1 #改变一个目录的所有人属性并同时改变改目录下所有文件的属性 chgrp group1 file1 #改变文件的群组 chown user1:group1 file1 #改变一个文件的所有人和群组属性 find / -perm -u+s #罗列一个系统中所有使用了SUID控制的文件 chmod u+s /bin/file1 #设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限 chmod u-s /bin/file1 #禁用一个二进制文件的 SUID位 chmod g+s /home/public #设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的 chmod g-s /home/public #禁用一个目录的 SGID 位 chmod o+t /home/public #设置一个文件的 STIKY 位 - 只允许合法所有人删除文件 chmod o-t /home/public #禁用一个目录的 STIKY 位 文件的特殊属性 - 使用 \"+\" 设置权限，使用 \"-\" 用于取消 chattr +a file1 #只允许以追加方式读写文件 chattr +c file1 #允许这个文件能被内核自动压缩/解压 chattr +d file1 #在进行文件系统备份时，dump程序将忽略这个文件 chattr +i file1 #设置成不可变的文件，不能被删除、修改、重命名或者链接 chattr +s file1 #允许一个文件被安全地删除 chattr +S file1 #一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘 chattr +u file1 #若文件被删除，系统会允许你在以后恢复这个被删除的文件 lsattr #显示特殊的属性 打包和压缩文件 bunzip2 file1.bz2 #解压一个叫做 'file1.bz2'的文件 bzip2 file1 #压缩一个叫做 'file1' 的文件 gunzip file1.gz #解压一个叫做 'file1.gz'的文件 gzip file1 #压缩一个叫做 'file1'的文件 gzip -9 file1 #最大程度压缩 rar a file1.rar test_file #创建一个叫做 'file1.rar' 的包 rar a file1.rar file1 file2 dir1 #同时压缩 'file1', 'file2' 以及目录 'dir1' rar x file1.rar #解压rar包 unrar x file1.rar #解压rar包 tar -cvf archive.tar file1 #创建一个非压缩的 tarball tar -cvf archive.tar file1 file2 dir1 #创建一个包含了 'file1', 'file2' 以及 'dir1'的档案文件 tar -tf archive.tar #显示一个包中的内容 tar -xvf archive.tar #释放一个包 tar -xvf archive.tar -C /tmp #将压缩包释放到 /tmp目录下 tar -cvfj archive.tar.bz2 dir1 #创建一个bzip2格式的压缩包 tar -jxvf archive.tar.bz2 #解压一个bzip2格式的压缩包 tar -cvfz archive.tar.gz dir1 #创建一个gzip格式的压缩包 tar -zxvf archive.tar.gz #解压一个gzip格式的压缩包 zip file1.zip file1 #创建一个zip格式的压缩包 zip -r file1.zip file1 file2 dir1 #将几个文件和目录同时压缩成一个zip格式的压缩包 unzip file1.zip #解压一个zip格式压缩包 RPM 包 - （Fedora, Redhat及类似系统） rpm -ivh package.rpm #安装一个rpm包 rpm -ivh --nodeeps package.rpm #安装一个rpm包而忽略依赖关系警告 rpm -U package.rpm #更新一个rpm包但不改变其配置文件 rpm -F package.rpm #更新一个确定已经安装的rpm包 rpm -e package_name.rpm #删除一个rpm包 rpm -qa #显示系统中所有已经安装的rpm包 rpm -qa | grep httpd #显示所有名称中包含 \"httpd\" 字样的rpm包 rpm -qi package_name #获取一个已安装包的特殊信息 rpm -qg \"System Environment/Daemons\" #显示一个组件的rpm包 rpm -ql package_name #显示一个已经安装的rpm包提供的文件列表 rpm -qc package_name #显示一个已经安装的rpm包提供的配置文件列表 rpm -q package_name --whatrequires #显示与一个rpm包存在依赖关系的列表 rpm -q package_name --whatprovides #显示一个rpm包所占的体积 rpm -q package_name --scripts #显示在安装/删除期间所执行的脚本l rpm -q package_name --changelog #显示一个rpm包的修改历史 rpm -qf /etc/httpd/conf/httpd.conf #确认所给的文件由哪个rpm包所提供 rpm -qp package.rpm -l #显示由一个尚未安装的rpm包提供的文件列表 rpm --import /media/cdrom/RPM-GPG-KEY #导入公钥数字证书 rpm --checksig package.rpm #确认一个rpm包的完整性 rpm -qa gpg-pubkey #确认已安装的所有rpm包的完整性 rpm -V package_name #检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间 rpm -Va #检查系统中所有已安装的rpm包- 小心使用 rpm -Vp package.rpm #确认一个rpm包还未安装 rpm2cpio package.rpm | cpio --extract --make-directories *bin* #从一个rpm包运行可执行文件 rpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm #从一个rpm源码安装一个构建好的包 rpmbuild --rebuild package_name.src.rpm #从一个rpm源码构建一个 rpm 包 YUM 软件包升级器 - （Fedora, RedHat及类似系统） yum install package_name #下载并安装一个rpm包 yum localinstall package_name.rpm #将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系 yum update package_name.rpm #更新当前系统中所有安装的rpm包 yum update package_name #更新一个rpm包 yum remove package_name #删除一个rpm包 yum list #列出当前系统中安装的所有包 yum search package_name #在rpm仓库中搜寻软件包 yum clean packages #清理rpm缓存删除下载的包 yum clean headers #删除所有头文件 yum clean all #删除所有缓存的包和头文件 DEB 包 (Debian, Ubuntu 以及类似系统) dpkg -i package.deb #安装/更新一个 deb 包 dpkg -r package_name #从系统删除一个 deb 包 dpkg -l #显示系统中所有已经安装的 deb 包 dpkg -l | grep httpd #显示所有名称中包含 \"httpd\" 字样的deb包 dpkg -s package_name #获得已经安装在系统中一个特殊包的信息 dpkg -L package_name #显示系统中已经安装的一个deb包所提供的文件列表 dpkg --contents package.deb #显示尚未安装的一个包所提供的文件列表 dpkg -S /bin/ping #确认所给的文件由哪个deb包提供 APT 软件工具 (Debian, Ubuntu 以及类似系统) apt-get install package_name #安装/更新一个 deb 包 apt-cdrom install package_name #从光盘安装/更新一个 deb 包 apt-get update #升级列表中的软件包 apt-get upgrade #升级所有已安装的软件 apt-get remove package_name #从系统删除一个deb包 apt-get check #确认依赖的软件仓库正确 apt-get clean #从下载的软件包中清理缓存 apt-cache search searched-package #返回包含所要搜索字符串的软件包名称 查看文件内容 cat file1 #从第一个字节开始正向查看文件的内容 tac file1 #从最后一行开始反向查看一个文件的内容 more file1 #查看一个长文件的内容 less file1 #类似于 'more' 命令，但是它允许在文件中和正向操作一样的反向操作 head -2 file1 #查看一个文件的前两行 tail -2 file1 #查看一个文件的最后两行 tail -f /var/log/messages #实时查看被添加到一个文件中的内容 文本处理 cat file1 file2 ... | command <> file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUT cat file1 | command( sed, grep, awk, grep, etc...) > result.txt #合并一个文件的详细说明文本，并将简介写入一个新文件中 cat file1 | command( sed, grep, awk, grep, etc...) >> result.txt #合并一个文件的详细说明文本，并将简介写入一个已有的文件中 grep Aug /var/log/messages #在文件 '/var/log/messages'中查找关键词\"Aug\" grep ^Aug /var/log/messages #在文件 '/var/log/messages'中查找以\"Aug\"开始的词汇 grep [0-9] /var/log/messages #选择 '/var/log/messages' 文件中所有包含数字的行 grep Aug -R /var/log/* #在目录 '/var/log' 及随后的目录中搜索字符串\"Aug\" sed 's/stringa1/stringa2/g' example.txt #将example.txt文件中的 \"string1\" 替换成 \"string2\" sed '/^$/d' example.txt #从example.txt文件中删除所有空白行 sed '/ *#/d; /^$/d' example.txt #从example.txt文件中删除所有注释和空白行 echo 'esempio' | tr '[:lower:]' '[:upper:]' #合并上下单元格内容 sed -e '1d' result.txt #从文件example.txt 中排除第一行 sed -n '/stringa1/p' #查看只包含词汇 \"string1\"的行 sed -e 's/ *$//' example.txt #删除每一行最后的空白字符 sed -e 's/stringa1//g' example.txt #从文档中只删除词汇 \"string1\" 并保留剩余全部 sed -n '1,5p;5q' example.txt #查看从第一行到第5行内容 sed -n '5p;5q' example.txt #查看第5行 sed -e 's/00*/0/g' example.txt #用单个零替换多个零 cat -n file1 #标示文件的行数 cat example.txt | awk 'NR%2==1' #删除example.txt文件中的所有偶数行 echo a b c | awk '{print $1}' #查看一行第一栏 echo a b c | awk '{print $1,$3}' #查看一行的第一和第三栏 paste file1 file2 #合并两个文件或两栏的内容 paste -d '+' file1 file2 #合并两个文件或两栏的内容，中间用\"+\"区分 sort file1 file2 #排序两个文件的内容 sort file1 file2 | uniq #取出两个文件的并集(重复的行只保留一份) sort file1 file2 | uniq -u #删除交集，留下其他的行 sort file1 file2 | uniq -d #取出两个文件的交集(只留下同时存在于两个文件中的文件) comm -1 file1 file2 #比较两个文件的内容只删除 'file1' 所包含的内容 comm -2 file1 file2 #比较两个文件的内容只删除 'file2' 所包含的内容 comm -3 file1 file2 #比较两个文件的内容只删除两个文件共有的部分 字符设置和文件格式转换 dos2unix filedos.txt fileunix.txt #将一个文本文件的格式从MSDOS转换成UNIX unix2dos fileunix.txt filedos.txt #将一个文本文件的格式从UNIX转换成MSDOS recode ..HTML page.html #将一个文本文件转换成html recode -l | more #显示所有允许的转换格式 文件系统分析 badblocks -v /dev/hda1 #检查磁盘hda1上的坏磁块 fsck /dev/hda1 #修复/检查hda1磁盘上linux文件系统的完整性 fsck.ext2 /dev/hda1 #修复/检查hda1磁盘上ext2文件系统的完整性 e2fsck /dev/hda1 #修复/检查hda1磁盘上ext2文件系统的完整性 e2fsck -j /dev/hda1 #修复/检查hda1磁盘上ext3文件系统的完整性 fsck.ext3 /dev/hda1 #修复/检查hda1磁盘上ext3文件系统的完整性 fsck.vfat /dev/hda1 #修复/检查hda1磁盘上fat文件系统的完整性 fsck.msdos /dev/hda1 #修复/检查hda1磁盘上dos文件系统的完整性 dosfsck /dev/hda1 #修复/检查hda1磁盘上dos文件系统的完整性 初始化一个文件系统 mkfs /dev/hda1 #在hda1分区创建一个文件系统 mke2fs /dev/hda1 #在hda1分区创建一个linux ext2的文件系统 mke2fs -j /dev/hda1 #在hda1分区创建一个linux ext3(日志型)的文件系统 mkfs -t vfat 32 -F /dev/hda1 #创建一个 FAT32 文件系统 fdformat -n /dev/fd0 #格式化一个软盘 mkswap /dev/hda3 #创建一个swap文件系统 SWAP文件系统 mkswap /dev/hda3 #创建一个swap文件系统 swapon /dev/hda3 #启用一个新的swap文件系统 swapon /dev/hda2 /dev/hdb3 #启用两个swap分区 备份 dump -0aj -f /tmp/home0.bak /home #制作一个 '/home' 目录的完整备份 dump -1aj -f /tmp/home0.bak /home #制作一个 '/home' 目录的交互式备份 restore -if /tmp/home0.bak #还原一个交互式备份 rsync -rogpav --delete /home /tmp #同步两边的目录 rsync -rogpav -e ssh --delete /home ip_address:/tmp #通过SSH通道rsync rsync -az -e ssh --delete ip_addr:/home/public /home/local #通过ssh和压缩将一个远程目录同步到本地目录 rsync -az -e ssh --delete /home/local ip_addr:/home/public #通过ssh和压缩将本地目录同步到远程目录 dd bs=1M if=/dev/hda | gzip | ssh user@ip_addr 'dd of=hda.gz' #通过ssh在远程主机上执行一次备份本地磁盘的操作 dd if=/dev/sda of=/tmp/file1 #备份磁盘内容到一个文件 tar -Puf backup.tar /home/user 执行一次对 '/home/user' #目录的交互式备份操作 ( cd /tmp/local/ && tar c . ) | ssh -C user@ip_addr 'cd /home/share/ && tar x -p' #通过ssh在远程目录中复制一个目录内容 ( tar c /home ) | ssh -C user@ip_addr 'cd /home/backup-home && tar x -p' #通过ssh在远程目录中复制一个本地目录 tar cf - . | (cd /tmp/backup ; tar xf - ) #本地将一个目录复制到另一个地方，保留原有权限及链接 find /home/user1 -name '*.txt' | xargs cp -av --target-directory=/home/backup/ --parents #从一个目录查找并复制所有以 '.txt' 结尾的文件到另一个目录 find /var/log -name '*.log' | tar cv --files-from=- | bzip2 > log.tar.bz2 #查找所有以 '.log' 结尾的文件并做成一个bzip包 dd if=/dev/hda of=/dev/fd0 bs=512 count=1 #做一个将 MBR (Master Boot Record)内容复制到软盘的动作 dd if=/dev/fd0 of=/dev/hda bs=512 count=1 #从已经保存到软盘的备份中恢复MBR内容 光盘 cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force #清空一个可复写的光盘内容 mkisofs /dev/cdrom > cd.iso #在磁盘上创建一个光盘的iso镜像文件 mkisofs /dev/cdrom | gzip > cd_iso.gz #在磁盘上创建一个压缩了的光盘iso镜像文件 mkisofs -J -allow-leading-dots -R -V \"Label CD\" -iso-level 4 -o ./cd.iso data_cd #创建一个目录的iso镜像文件 cdrecord -v dev=/dev/cdrom cd.iso #刻录一个ISO镜像文件 gzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - #刻录一个压缩了的ISO镜像文件 mount -o loop cd.iso /mnt/iso #挂载一个ISO镜像文件 cd-paranoia -B #从一个CD光盘转录音轨到 wav 文件中 cd-paranoia -- \"-3\" #从一个CD光盘转录音轨到 wav 文件中（参数-3） cdrecord --scanbus #扫描总线以识别scsi通道 dd if=/dev/hdc | md5sum #校验一个设备的md5sum编码，例如一张 CD 网络 - （以太网和WIFI无线） ifconfig eth0 #显示一个以太网卡的配置 ifup eth0 #启用一个 'eth0' 网络设备 ifdown eth0 #禁用一个 'eth0' 网络设备 ifconfig eth0 192.168.1.1 netmask 255.255.255.0 #控制IP地址 ifconfig eth0 promisc #设置 'eth0' 成混杂模式以嗅探数据包 (sniffing) dhclient eth0 #以dhcp模式启用 'eth0' route -n #查看路由表 route add -net 0/0 gw IP_Gateway #配置默认网关 route add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 #配置静态路由到达网络'192.168.0.0/16' route del 0/0 gw IP_gateway #删除静态路由 hostname #查看机器名 host www.example.com #把一个主机名解析到一个网际地址或把一个网际地址解析到一个主机名。 nslookup www.example.com #用于查询DNS的记录，查看域名解析是否正常，在网络故障的时候用来诊断网络问题。 ip link show #查看网卡信息 mii-tool #用于查看、管理介质的网络接口的状态 ethtool #用于查询和设置网卡配置 netstat -tupl #用于显示TCP/UDP的状态信息 tcpdump tcp port 80 #显示所有http协议的流量 JPS工具 jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。 我想很多人都是用过unix系统里的ps命令，这个命令主要是用来显示当前系统的进程情况，有哪些进程，及其 id。jps 也是一样，它的作用是显示当前系统的java进程情况，及其id号。我们可以通过它来查看我们到底启动了几个java进程（因为每一个java程序都会独占一个java虚拟机实例），和他们的进程号（为下面几个程序做准备），并可通过opt来查看这些进程的详细启动参数。 使用方法：在当前命令行下打 jps(需要JAVA_HOME，没有的话，到改程序的目录下打) 。 jps存放在JAVA_HOME/bin/jps，使用时为了方便请将JAVA_HOME/bin/加入到Path. $> jps 23991 Jps 23789 BossMain 23651 Resin 比较常用的参数： #-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数 $> jps -q 28680 23789 23651 #-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null $> jps -m 28715 Jps -m 23789 BossMain 23651 Resin -socketwait 32768 -stdout /data/aoxj/resin/log/stdout.log -stderr /data/aoxj/resin/log/stderr.log #-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名 $> jps -l 28729 sun.tools.jps.Jps 23789 com.asiainfo.aimc.bossbi.BossMain 23651 com.caucho.server.resin.Resin #-v 输出传递给JVM的参数 $> jps -v 23789 BossMain 28802 Jps -Denv.class.path=/data/aoxj/bossbi/twsecurity/java/trustwork140.jar:/data/aoxj/bossbi/twsecurity/java/:/data/aoxj/bossbi/twsecurity/java/twcmcc.jar:/data/aoxj/jdk15/lib/rt.jar:/data/aoxj/jd k15/lib/tools.jar -Dapplication.home=/data/aoxj/jdk15 -Xms8m 23651 Resin -Xss1m -Dresin.home=/data/aoxj/resin -Dserver.root=/data/aoxj/resin -Djava.util.logging.manager=com.caucho.log.LogManagerImpl - Djavax.management.builder.initial=com.caucho.jmx.MBeanServerBuilderImpl jps 192.168.0.77 #列出远程服务器192.168.0.77机器所有的jvm实例，采用rmi协议，默认连接端口为1099（前提是远程服务器提供jstatd服务） #注：jps命令有个地方很不好，似乎只能显示当前用户的java进程，要显示其他用户的还是只能用unix 原文链接：https://mp.weixin.qq.com/s/n9sHr6lMVcV-ZiUFcd84Lw Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/macos path.html":{"url":"blog/效率/macos path.html","title":"Macos Path","keywords":"","body":"macos Path Mac系统的环境变量，加载顺序为： /etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc /etc/profile和/etc/paths是系统级别的，系统启动就会加载，后面几个是当前用户级的环境变量。后面3个按照从前往后的顺序读取，如果/.bash_profile文件存在，则后面的几个文件就会被忽略不读了，如果/.bash_profile文件不存在，才会以此类推读取后面的文件。~/.bashrc没有上述规则，它是bash shell打开的时候载入的。 PATH的语法为如下 #中间用冒号隔开 export PATH=$PATH::::------: 上述文件的科普 /etc/paths （全局建议修改这个文件 ） 编辑 paths，将环境变量添加到 paths文件中 ，一行一个路径 Hint：输入环境变量时，不用一个一个地输入，只要拖动文件夹到 Terminal 里就可以了。 /etc/profile （建议不修改这个文件 ） 全局（公有）配置，不管是哪个用户，登录时都会读取该文件。 /etc/bashrc （一般在这个文件中添加系统级环境变量） 全局（公有）配置，bash shell执行时，不管是何种方式，都会读取此文件 .profile 文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置 使用注意：如果你有对/etc/profile有修改的话必须得重启你的修改才会生效，此修改对每个用户都生效。 ./bashrc 每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取. 使用注意 对所有的使用bash的用户修改某个配置并在以后打开的bash都生效的话可以修改这个文件，修改这个文件不用重启，重新打开一个bash即可生效。 ./bash_profile 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取.（每个用户都有一个.bashrc文件，在用户目录下） 使用注意 需要需要重启才会生效，/etc/profile对所有用户生效，~/.bash_profile只对当前用户生效。 source ./.bash_profile 或者 ./.profile 环境信息生效 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/mongodb-init.html":{"url":"blog/效率/mongodb-init.html","title":"Mongodb Init","keywords":"","body":"mongodb init Initiate rs.initiate({ _id: \"rs\", version: 1, members: [ { _id: 0, host : \"mongo-rs-0:27017\" }, { _id: 1, host : \"mongo-rs-1:27017\" }, { _id: 2, host : \"mongo-rs-2:27017\" } ]}); reconfig rs.reconfig({_id: \"rs\", version: 2, protocolVersion: 1, members: [ { _id: 0, host : \"mongo-rs-0.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\" }, { _id: 1, host : \"mongo-rs-1.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\" }, { _id: 2, host : \"mongo-rs-2.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\" } ]}, {\"force\":true}); add new node rs.add({_id: 2, host: \"mongo-rs-2.mongo-rs-svc.test-345627352593600512.svc.cluster.local:27017\", priority: 0, hidden: true}) 查询 rs.status() nslookup 查询mongo apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28.4 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always XishengdeMacBook-Pro:~ xishengcai$ kubectl exec -it busybox sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. / # nslookup BusyBox v1.28.4 (2018-05-22 17:00:17 UTC) multi-call binary. Usage: nslookup [HOST] [SERVER] Query the nameserver for the IP address of the given HOST optionally using a specified DNS server / # nslookup mongo-rs-svc Server: 50.96.0.10 Address 1: 50.96.0.10 kube-dns.kube-system.svc.cluster.local nslookup: can't resolve 'mongo-rs-svc' / # nslookup mongo-rs-svc.322488871377965056 Server: 50.96.0.10 Address 1: 50.96.0.10 kube-dns.kube-system.svc.cluster.local Name: mongo-rs-svc.322488871377965056 Address 1: 50.96.107.246 mongo-rs-svc.322488871377965056.svc.cluster.local / # nslookup mongo-rs-0.mongo-rs-svc.322488871377965056 Server: 50.96.0.10 Address 1: 50.96.0.10 kube-dns.kube-system.svc.cluster.local Name: mongo-rs-0.mongo-rs-svc.322488871377965056 Address 1: 10.0.1.45 10-0-1-45.lsh-mcp-mongo.322488871377965056.svc.cluster.local / # connect mongo localhost:27017/admin -u user -p password Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 00:22:34 "},"blog/效率/nfs.html":{"url":"blog/效率/nfs.html","title":"Nfs","keywords":"","body":"搭建NFS server 1.安装软件包 yum install -y nfs-utils 2.编辑exports文件，添加从机 vim /etc/exports /data/nfs 8.210.84.124(rw,sync,fsid=0) 192.168.222.202(rw,sync,fsid=0) 配置说明 第一部分： /home/nfs, 这个是本地主要共享出去的目录 第二部分： 192.168.222.0/24, 允许访问的主机，可以是一个IP：192.168.222.201, 也可以是一个IP段：192.168.222.0/24，也可用，拼接多个ip地址段 第三部分： 括号中的部分 rw ： 表示可读写， ro只读； sync ： 同步模式， 内存中数据实时写入磁盘； async ：不同步，把内存中数据定期写入磁盘中； no_root_squash : 加上这个选项后， root用户就会对共享目录拥有最高的控制权，就像是对本机的目录操作一样。不安全， 不建议使用； root_squash : 和上面的选项对应， root用户对共享目录的权限不高，只用普通用户权限； all_squash : 不管使用NFS的用户是谁，他的身份都会被限定成为一个指定的普通用户。 anonuid/anongid: 要和root_squash 以及 all_squash 一同使用， 用于指定使用NFS的用户限定后的uid和gid， 前提是本机的/etc/passwd中存在这个uid和gid。 fsid=0 表示将/home/nfs 真个目录包装成根目录 /opt/test/ 192.168.222.0/24(rw, no_roo_squash, no_all_squash, anonuid=501,anongid=501) 3.启动A机上的nfs服务 3.1 先为rpcbind和nfs做开机启动： systemctl enable rpcbind.service systemctl enable nfs-server.service 3.2 分别启动rpcbind和nfs服务 systemctl start rpcbind.service systemctl start nfs-server.service 3.3 确认NFS服务器启动成功 #通过查看service列中是否有nfs服务来确认NFS是否启动 rpcinfo -p #查看可挂载目录及可连接的IP showmount -e 192.168.222.200 4. 关闭A机上的防火墙或者给防火墙配置NFS的通过规则 5. 在其他机器上配置client端 1.安装nfs，并启动服务 yum install -y nfs-utils systemctl enable rpcbind.service systemctl start rpcbind.service 客户端不需要启动nfs服务，只需要启动rpcbind服务 2.检查NFS服务器端是否有目录共享 showmount -e 192.168.222.200 3.使用mount挂载A服务器端的目录/home/nfs到客户端B目录的/home/nfs下 mkdir /home/nfs mount -t nfs 192.168.222.200:/home/nfs /home/nfs df -h ... 192.168.222.200:/home/nfs 11G 1.3G 13% /home/nfs 4.挂载完成，可以正常访问本机下的/home/nfs, 如果在服务端A的共享目录/home/nfs中写入文件，B、C机上可以看到， 但是不能在这个目录中写入文件 6. 在多个服务器中建立一个共享目录，并且可以允许A、B、C写入共享目录 6.1 在B、C机上取得root用户ID号 id root uid=0(root) gid=0(root) group=0(root) 6.2 在A服务器上再建立一个共享目录 mkdir /home/nfs1 vim /etc/nfs1 /home/nfs 192.168.222.201(rw, sync,fsid=0) 192.168.222.202(rw, sync, fsid=0) /home/nfs1 192.168.222.0/24(rw, sync,all_squash, anonuid=0, anongid=0) 加入第二行， anonuid=0， anongid=0， 即为root用户id 6.3 让修改过的配置文件生效 exportfs -arv 使用exportfs命令， 当改变/etc/exports 配置文件后， 不用重启nfs服务器直接用这个exportfs即可，它的常用选项为[-aruv] -a: 全部挂载或者卸载 -r: 重新挂载 -u: 卸载某一个目录 -v: 显示共享的目录 6.4 查看新的可挂载目录以及可连接的IP showmount -e 192.168.222.200 6.5 在B、C client端新挂载一个目录 showmount -e 192.168.222.200 查看新的共享目录是否有了 mkdir nfs1 mount -t nfs 192.168.222.200:/home/nfs1/ /home/nfs1/ ll / > /home/nfs1/ll.txt 卸载： umount /home/nfs1 7. 想在客户端上实现开机挂载， 则需要编辑/etc/fstab: vim /etc/fstab 加入以下内容： 192.168.222.200：/home/nfs /home/nfs nfs nolock 0 0 192.168.222.200 : /home/nfs1 /home/nfs1 nfs1 nolock 0 0 保存后， 重新挂载 mount -a Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/nginx/nginx.html":{"url":"blog/效率/nginx/nginx.html","title":"Nginx","keywords":"","body":"nginx 实践总结 install yum installrpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm yum install nginx apt-get install source code install build your image deny statistics tls create CA openssl genrsa -out rootCA.key 2048 openssl req -x509 -new -nodes -key rootCA.key -days 1024 -out rootCA.pem create crt openssl genrsa -out server.key 2048 openssl req -new -key server.key -out server.csr openssl x509 -req -in server.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server.crt -days 500 Nginx 是开源、高性能、高可靠的 Web 和反向代理服务器，而且支持热部署，几乎可以做到 7 * 24 小时不间断运行，即使运行几个月也不需要重新启动，还能在不间断服务的情况下对软件版本进行热更新。性能是 Nginx 最重要的考量，其占用内存少、并发能力强、能支持高达 5w 个并发连接数，最重要的是， Nginx 是免费的并可以商业化，配置使用也比较简单。 Nginx 特点 高并发、高性能； 模块化架构使得它的扩展性非常好； 异步非阻塞的事件驱动模型这点和 Node.js 相似； 相对于其它服务器来说它可以连续几个月甚至更长而不需要重启服务器使得它具有高可靠性； 热部署、平滑升级； 完全开源，生态繁荣； Nginx 作用 Nginx 的最重要的几个使用场景： 静态资源服务，通过本地文件系统提供服务； 反向代理服务，延伸出包括缓存、负载均衡等； API 服务， OpenResty ； 对于前端来说 Node.js 并不陌生， Nginx 和 Node.js 的很多理念类似， HTTP 服务器、事件驱动、异步非阻塞等，且 Nginx 的大部分功能使用 Node.js 也可以实现，但 Nginx 和 Node.js 并不冲突，都有自己擅长的领域。 Nginx 擅长于底层服务器端资源的处理（静态资源处理转发、反向代理，负载均衡等）， Node.js 更擅长上层具体业务逻辑的处理，两者可以完美组合。 用一张图表示： command nginx -s reload # 向主进程发送信号，重新加载配置文件，热重启 nginx -s reopen # 重启 Nginx nginx -s stop # 快速关闭 nginx -s quit # 等待工作进程处理完成后关闭 nginx -T # 查看当前 Nginx 最终的配置 nginx -t # 检查配置是否有问题 典型配置 # main段配置信息 user nginx; # 运行用户，默认即是nginx，可以不进行设置 worker_processes auto; # Nginx 进程数，一般设置为和 CPU 核数一样 error_log /var/log/nginx/error.log warn; # Nginx 的错误日志存放目录 pid /var/run/nginx.pid; # Nginx 服务启动时的 pid 存放位置 # events段配置信息 events { use epoll; # 使用epoll的I/O模型(如果你不知道Nginx该使用哪种轮询方法，会自动选择一个最适合你操作系统的) worker_connections 1024; # 每个进程允许最大并发数 } # http段配置信息 # 配置使用最频繁的部分，代理、缓存、日志定义等绝大多数功能和第三方模块的配置都在这里设置 http { # 设置日志模式 log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; # Nginx访问日志存放位置 sendfile on; # 开启高效传输模式 tcp_nopush on; # 减少网络报文段的数量 tcp_nodelay on; keepalive_timeout 65; # 保持连接的时间，也叫超时时间，单位秒 types_hash_max_size 2048; include /etc/nginx/mime.types; # 文件扩展名与类型映射表 default_type application/octet-stream; # 默认文件类型 include /etc/nginx/conf.d/*.conf; # 加载子配置项 # server段配置信息 server { listen 80; # 配置监听的端口 server_name localhost; # 配置的域名 # location段配置信息 location / { root /usr/share/nginx/html; # 网站根目录 index index.html index.htm; # 默认首页文件 deny 172.168.22.11; # 禁止访问的ip地址，可以为all allow 172.168.33.44；# 允许访问的ip地址，可以为all } error_page 500 502 503 504 /50x.html; # 默认50x对应的访问页面 error_page 400 404 error.html; # 同上 } } main 全局配置，对全局生效； events 配置影响 Nginx 服务器与用户的网络连接； http 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置； server 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块； location 用于配置匹配的 uri ； upstream 配置后端服务器具体地址，负载均衡配置不可或缺的部分； 用一张图清晰的展示它的层级结构： 配置文件 main 段核心参数 real ip process server { listen 80; server_name xxxx; root /usr/share/nginx/html/xxx; client_max_body_size 10m; location / { root /usr/share/nginx/html/xxx; index index.html; try_files $uri $uri/ /index.html last; } location /api/lsh/ { proxy_buffer_size 64k; proxy_buffers 32 32k; proxy_busy_buffers_size 128k; proxy_connect_timeout 600; proxy_read_timeout 600; proxy_send_timeout 600; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://xxxxx; } } 配置文件 main 段核心参数 user 指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid 指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number 指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core 指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制 working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number 指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量 worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity 将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority 指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout 指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolution worker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon 指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 配置文件 events 段核心参数 use Nginx 使用何种事件驱动模型。 use method; # 不推荐配置它，让nginx自己选择 method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport worker_connections worker 子进程能够处理的最大并发连接数。 worker_connections 1024 # 每个子进程的最大连接数为1024 accept_mutex 是否打开负载均衡互斥锁。 accept_mutex on # 默认是off关闭的，这里推荐打开 server_name 指令 指定虚拟主机域名。 server_name name1 name2 name3 # 示例： server_name www.nginx.com; 域名匹配的四种写法： 精确匹配： server_name www.nginx.com ; 左侧通配： server_name *.nginx.com ; 右侧统配： server_name www.nginx.* ; 正则匹配： server_name ~^www\\.nginx\\.*$ ; 匹配优先级：精确匹配 > 左侧通配符匹配 > 右侧通配符匹配 > 正则表达式匹配 server_name 配置实例： 1、配置本地 DNS 解析 vim /etc/hosts （ macOS 系统） # 添加如下内容，其中 121.42.11.34 是阿里云服务器IP地址 121.42.11.34 www.nginx-test.com 121.42.11.34 mail.nginx-test.com 121.42.11.34 www.nginx-test.org 121.42.11.34 doc.nginx-test.com 121.42.11.34 www.nginx-test.cn 121.42.11.34 fe.nginx-test.club [注意] 这里使用的是虚拟域名进行测试，因此需要配置本地 DNS 解析，如果使用阿里云上购买的域名，则需要在阿里云上设置好域名解析。 2、配置阿里云 Nginx ，vim /etc/nginx/nginx.conf # 这里只列举了http端中的sever端配置 # 左匹配 server { listen 80; server_name *.nginx-test.com; root /usr/share/nginx/html/nginx-test/left-match/; location / { index index.html; } } # 正则匹配 server { listen 80; server_name ~^.*\\.nginx-test\\..*$; root /usr/share/nginx/html/nginx-test/reg-match/; location / { index index.html; } } # 右匹配 server { listen 80; server_name www.nginx-test.*; root /usr/share/nginx/html/nginx-test/right-match/; location / { index index.html; } } # 完全匹配 server { listen 80; server_name www.nginx-test.com; root /usr/share/nginx/html/nginx-test/all-match/; location / { index index.html; } } 3、访问分析 当访问 www.nginx-test.com 时，都可以被匹配上，因此选择优先级最高的“完全匹配”； 当访问 mail.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.org 时，会进行“右匹配”； 当访问 doc.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.cn 时，会进行“右匹配”； 当访问 fe.nginx-test.club 时，会进行“正则匹配”； root 指定静态资源目录位置，它可以写在 http 、 server 、 location 等配置中。 root path 例如： location /image { root /opt/nginx/static; } 当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png [注意] root 会将定义路径与 URI 叠加， alias 则只取定义路径。 alias 它也是指定静态资源目录位置，它只能写在 location 中。 location /image { alias /opt/nginx/static/image/; } 当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png [注意] 使用 alias 末尾一定要添加 / ，并且它只能位于 location 中。 location 配置路径。 location [ = | ~ | ~* | ^~ ] uri { ... } 匹配规则： = 精确匹配； ~ 正则匹配，区分大小写； ~* 正则匹配，不区分大小写； ^~ 匹配到即停止搜索； 匹配优先级： = > ^~ > ~ > ~* > 不带任何字符。 实例： server { listen 80; server_name www.nginx-test.com; # 只有当访问 www.nginx-test.com/match_all/ 时才会匹配到/usr/share/nginx/html/match_all/index.html location = /match_all/ { root /usr/share/nginx/html index index.html } # 当访问 www.nginx-test.com/1.jpg 等路径时会去 /usr/share/nginx/images/1.jpg 找对应的资源 location ~ \\.(jpeg|jpg|png|svg)$ { root /usr/share/nginx/images; } # 当访问 www.nginx-test.com/bbs/ 时会匹配上 /usr/share/nginx/html/bbs/index.html location ^~ /bbs/ { root /usr/share/nginx/html; index index.html index.htm; } } location 中的反斜线 location /test { ... } location /test/ { ... } 不带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ；如果没有 test 目录， nginx 则会找是否有 test 文件。 带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ，如果没有它也不会去找是否存在 test 文件。 return 停止处理请求，直接返回响应码或重定向到其他 URL ；执行 return 指令后， location 中后续指令将不会被执行。 return code [text]; return code URL; return URL; 例如： location / { return 404; # 直接返回状态码 } location / { return 404 \"pages not found\"; # 返回状态码 + 一段文本 } location / { return 302 /bbs ; # 返回状态码 + 重定向地址 } location / { return https://www.baidu.com ; # 返回重定向地址 } rewrite 根据指定正则表达式匹配规则，重写 URL 。 语法：rewrite 正则表达式 要替换的内容 [flag]; 上下文：server、location、if 示例：rewirte /images/(.*\\.jpg)$ /pic/$1; # $1是前面括号(.*\\.jpg)的反向引用 flag 可选值的含义： last 重写后的 URL 发起新请求，再次进入 server 段，重试 location 的中的匹配； break 直接使用重写后的 URL ，不再匹配其它 location 中语句； redirect 返回302临时重定向； permanent 返回301永久重定向； server{ listen 80; server_name fe.lion.club; # 要在本地hosts文件进行配置 root html; location /search { rewrite ^/(.*) https://www.baidu.com redirect; } location /images { rewrite /images/(.*) /pics/$1; } location /pics { rewrite /pics/(.*) /photos/$1; } location /photos { } } 按照这个配置我们来分析： 当访问 fe.lion.club/search 时，会自动帮我们重定向到 https://www.baidu.com。 当访问 fe.lion.club/images/1.jpg 时，第一步重写 URL 为 fe.lion.club/pics/1.jpg ，找到 pics 的 location ，继续重写 URL 为 fe.lion.club/photos/1.jpg ，找到 /photos 的 location 后，去 html/photos 目录下寻找 1.jpg 静态资源。 if 指令 语法：if (condition) {...} 上下文：server、location 示例： if($http_user_agent ~ Chrome){ rewrite /(.*)/browser/$1 break; } condition 判断条件： $variable 仅为变量时，值为空或以0开头字符串都会被当做 false 处理； = 或 != 相等或不等； ~ 正则匹配； ! ~ 非正则匹配； ~* 正则匹配，不区分大小写； -f 或 ! -f 检测文件存在或不存在； -d 或 ! -d 检测目录存在或不存在； -e 或 ! -e 检测文件、目录、符号链接等存在或不存在； -x 或 ! -x 检测文件可以执行或不可执行； 实例： server { listen 8080; server_name localhost; root html; location / { if ( $uri = \"/images/\" ){ rewrite (.*) /pics/ break; } } } 当访问 localhost:8080/images/ 时，会进入 if 判断里面执行 rewrite 命令。 autoindex 用户请求以 / 结尾时，列出目录结构，可以用于快速搭建静态资源下载网站。 autoindex.conf 配置信息： server { listen 80; server_name fe.lion-test.club; location /download/ { root /opt/source; autoindex on; # 打开 autoindex，，可选参数有 on | off autoindex_exact_size on; # 修改为off，以KB、MB、GB显示文件大小，默认为on，以bytes显示出⽂件的确切⼤⼩ autoindex_format html; # 以html的方式进行格式化，可选参数有 html | json | xml autoindex_localtime off; # 显示的⽂件时间为⽂件的服务器时间。默认为off，显示的⽂件时间为GMT时间 } } 当访问 fe.lion.com/download/ 时，会把服务器 /opt/source/download/ 路径下的文件展示出来，如下图所示： 变量 Nginx 提供给使用者的变量非常多，但是终究是一个完整的请求过程所产生数据， Nginx 将这些数据以变量的形式提供给使用者。 下面列举些项目中常用的变量： 变量名 含义 remote_addr 客户端 IP 地址 remote_port 客户端端口 server_addr 服务端 IP 地址 server_port 服务端端口 server_protocol 服务端协议 binary_remote_addr 二进制格式的客户端 IP 地址 connection TCP 连接的序号，递增 connection_request TCP 连接当前的请求数量 uri 请求的URL，不包含参数 request_uri 请求的URL，包含参数 scheme 协议名， http 或 https request_method 请求方法 request_length 全部请求的长度，包含请求行、请求头、请求体 args 全部参数字符串 arg_参数名 获取特定参数值 is_args URL 中是否有参数，有的话返回 ? ，否则返回空 query_string 与 args 相同 host 请求信息中的 Host ，如果请求中没有 Host 行，则在请求头中找，最后使用 nginx 中设置的 server_name 。 http_user_agent 用户浏览器 http_referer 从哪些链接过来的请求 http_via 每经过一层代理服务器，都会添加相应的信息 http_cookie 获取用户 cookie request_time 处理请求已消耗的时间 https 是否开启了 https ，是则返回 on ，否则返回空 request_filename 磁盘文件系统待访问文件的完整路径 document_root 由 URI 和 root/alias 规则生成的文件夹路径 limit_rate 返回响应时的速度上限值 实例演示 var.conf ： server{ listen 8081; server_name var.lion-test.club; root /usr/share/nginx/html; location / { return 200 \" remote_addr: $remote_addr remote_port: $remote_port server_addr: $server_addr server_port: $server_port server_protocol: $server_protocol binary_remote_addr: $binary_remote_addr connection: $connection uri: $uri request_uri: $request_uri scheme: $scheme request_method: $request_method request_length: $request_length args: $args arg_pid: $arg_pid is_args: $is_args query_string: $query_string host: $host http_user_agent: $http_user_agent http_referer: $http_referer http_via: $http_via request_time: $request_time https: $https request_filename: $request_filename document_root: $document_root \"; } } 当我们访问 http://var.lion-test.club:8081/test?pid=121414&cid=sadasd 时，由于 Nginx 中写了 return 方法，因此 chrome 浏览器会默认为我们下载一个文件，下面展示的就是下载的文件内容： remote_addr: 27.16.220.84 remote_port: 56838 server_addr: 172.17.0.2 server_port: 8081 server_protocol: HTTP/1.1 binary_remote_addr: 茉 connection: 126 uri: /test/ request_uri: /test/?pid=121414&cid=sadasd scheme: http request_method: GET request_length: 518 args: pid=121414&cid=sadasd arg_pid: 121414 is_args: ? query_string: pid=121414&cid=sadasd host: var.lion-test.club http_user_agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36 http_referer: http_via: request_time: 0.000 https: request_filename: /usr/share/nginx/html/test/ document_root: /usr/share/nginx/html Nginx 的配置还有非常多，以上只是罗列了一些常用的配置，在实际项目中还是要学会查阅文档。 Nginx 应用核心概念 代理是在服务器和客户端之间假设的一层服务器，代理将接收客户端的请求并将它转发给服务器，然后将服务端的响应转发给客户端。 不管是正向代理还是反向代理，实现的都是上面的功能。 正向代理 正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。 正向代理是为我们服务的，即为客户端服务的，客户端可以根据正向代理访问到它本身无法访问到的服务器资源。 正向代理对我们是透明的，对服务端是非透明的，即服务端并不知道自己收到的是来自代理的访问还是来自真实客户端的访问。 反向代理 反向代理*（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 反向代理是为服务端服务的，反向代理可以帮助服务器接收来自客户端的请求，帮助服务器做请求转发，负载均衡等。 反向代理对服务端是透明的，对我们是非透明的，即我们并不知道自己访问的是代理服务器，而服务器知道反向代理在为他服务。 反向代理的优势： 隐藏真实服务器； 负载均衡便于横向扩充后端动态服务； 动静分离，提升系统健壮性； 那么“动静分离”是什么？负载均衡又是什么？ 动静分离 动静分离是指在 web 服务器架构中，将静态页面与动态页面或者静态内容接口和动态内容接口分开不同系统访问的架构设计方法，进而提示整个服务的访问性和可维护性。 一般来说，都需要将动态资源和静态资源分开，由于 Nginx 的高并发和静态资源缓存等特性，经常将静态资源部署在 Nginx 上。如果请求的是静态资源，直接到静态资源目录获取资源，如果是动态资源的请求，则利用反向代理的原理，把请求转发给对应后台应用去处理，从而实现动静分离。 使用前后端分离后，可以很大程度提升静态资源的访问速度，即使动态服务不可用，静态资源的访问也不会受到影响。 负载均衡 一般情况下，客户端发送多个请求到服务器，服务器处理请求，其中一部分可能要操作一些资源比如数据库、静态资源等，服务器处理完毕后，再将结果返回给客户端。 这种模式对于早期的系统来说，功能要求不复杂，且并发请求相对较少的情况下还能胜任，成本也低。随着信息数量不断增长，访问量和数据量飞速增长，以及系统业务复杂度持续增加，这种做法已无法满足要求，并发量特别大时，服务器容易崩。 很明显这是由于服务器性能的瓶颈造成的问题，除了堆机器之外，最重要的做法就是负载均衡。 请求爆发式增长的情况下，单个机器性能再强劲也无法满足要求了，这个时候集群的概念产生了，单个服务器解决不了的问题，可以使用多个服务器，然后将请求分发到各个服务器上，将负载分发到不同的服务器，这就是负载均衡，核心是「分摊压力」。 Nginx 实现负载均衡，一般来说指的是将请求转发给服务器集群。 举个具体的例子，晚高峰乘坐地铁的时候，入站口经常会有地铁工作人员大喇叭“请走 B 口， B 口人少车空....”，这个工作人员的作用就是负载均衡。 Nginx 实现负载均衡的策略： 轮询策略：默认情况下采用的策略，将所有客户端请求轮询分配给服务端。这种策略是可以正常工作的，但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。 最小连接数策略：将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。 最快响应时间策略：优先分配给响应时间最短的服务器。 客户端 ip 绑定策略：来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。 Nginx 实战配置 在配置反向代理和负载均衡等等功能之前，有两个核心模块是我们必须要掌握的，这两个模块应该说是 Nginx 应用配置中的核心，它们分别是： upstream 、proxy_pass 。 upstream 用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ... } 上下文：http 示例： upstream back_end_server{ server 192.168.100.33:8081 } 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server 定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive 限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests 单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout 空闲长连接的最长保持时间。 语法：keepalive_timeout time; 默认值：keepalive_timeout 60s; 上下文：upstream 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } proxy_pass 用于配置代理服务器。 语法：proxy_pass URL; 上下文：location、if、limit_except 示例： proxy_pass http://127.0.0.1:8081 proxy_pass http://127.0.0.1:8081/proxy URL 参数原则 URL 必须以 http 或 https 开头； URL 中可以携带变量； URL 中是否带 URI ，会直接影响发往上游请求的 URL ； 接下来让我们来看看两种常见的 URL 用法： proxy_pass http://192.168.100.33:8081 proxy_pass http://192.168.100.33:8081/ 这两种用法的区别就是带 / 和不带 / ，在配置代理时它们的区别可大了： 不带 / 意味着 Nginx 不会修改用户 URL ，而是直接透传给上游的应用服务器； 带 / 意味着 Nginx 会修改用户 URL ，修改方法是将 location 后的 URL 从用户 URL 中删除； 不带 / 的用法： location /bbs/{ proxy_pass http://127.0.0.1:8080; } 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /bbs/abc/test.html 带 / 的用法： location /bbs/{ proxy_pass http://127.0.0.1:8080/; } 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /abc/test.html 并没有拼接上 /bbs ，这点和 root 与 alias 之间的区别是保持一致的。 配置反向代理 这里为了演示更加接近实际，作者准备了两台云服务器，它们的公网 IP 分别是： 121.42.11.34 与 121.5.180.193 。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置： # /etc/nginx/conf.d/proxy.conf server{ listen 8080; server_name localhost; location /proxy/ { root /usr/share/nginx/html/proxy; index index.html; } } # /usr/share/nginx/html/proxy/index.html 121.42.11.34 proxy html 配置完成后重启 Nginx 服务器 nginx \\-s reload 。 把 121.5.180.193 服务器作为代理服务器，做如下配置： # /etc/nginx/conf.d/proxy.conf upstream back_end { server 121.42.11.34:8080 weight=2 max_conns=1000 fail_timeout=10s max_fails=3; keepalive 32; keepalive_requests 80; keepalive_timeout 20s; } server { listen 80; server_name proxy.lion.club; location /proxy { proxy_pass http://back_end/proxy; } } 本地机器要访问 proxy.lion.club 域名，因此需要配置本地 hosts ，通过命令：vim /etc/hosts 进入配置文件，添加如下内容： 121.5.180.193 proxy.lion.club 分析： 当访问 proxy.lion.club/proxy 时通过 upstream 的配置找到 121.42.11.34:8080 ； 因此访问地址变为 http://121.42.11.34:8080/proxy ； 连接到 121.42.11.34 服务器，找到 8080 端口提供的 server ； 通过 server 找到 /usr/share/nginx/html/proxy/index.html 资源，最终展示出来。 配置负载均衡 配置负载均衡主要是要使用 upstream 指令。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： server{ listen 8020; location / { return 200 'return 8020 \\n'; } } server{ listen 8030; location / { return 200 'return 8030 \\n'; } } server{ listen 8040; location / { return 200 'return 8040 \\n'; } } 配置完成后： nginx -t 检测配置是否正确； nginx -s reload 重启 Nginx 服务器； 执行 ss -nlt 命令查看端口是否被占用，从而判断 Nginx 服务是否正确启动。 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： upstream demo_server { server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 配置完成后重启 Nginx 服务器。并且在需要访问的客户端配置好 ip 和域名的映射关系。 # /etc/hosts 121.5.180.193 balance.lion.club 在客户端机器执行 curl http://balance.lion.club/balance/ 命令： 不难看出，负载均衡的配置已经生效了，每次给我们分发的上游服务器都不一样。就是通过简单的轮询策略进行上游服务器分发。 接下来，我们再来了解下 Nginx 的其它分发策略。 hash 算法 通过制定关键字作为 hash key ，基于 hash 算法映射到特定的上游服务器中。关键字可以包含有变量、字符串。 upstream demo_server { hash $request_uri; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } hash $request_uri 表示使用 request_uri 变量作为 hash 的 key 值，只要访问的 URI 保持不变，就会一直分发给同一台服务器。 ip_hash 根据客户端的请求 ip 进行判断，只要 ip 地址不变就永远分配到同一台主机。它可以有效解决后台服务器 session 保持的问题。 upstream demo_server { ip_hash; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 最少连接数算法 各个 worker 子进程通过读取共享内存的数据，来获取后端服务器的信息。来挑选一台当前已建立连接数最少的服务器进行分配请求。 语法：least_conn; 上下文：upstream; 示例： upstream demo_server { zone test 10M; # zone可以设置共享内存空间的名字和大小 least_conn; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 最后你会发现，负载均衡的配置其实一点都不复杂。 配置缓存 缓存可以非常有效的提升性能，因此不论是客户端（浏览器），还是代理服务器（ Nginx ），乃至上游服务器都多少会涉及到缓存。可见缓存在每个环节都是非常重要的。下面让我们来学习 Nginx 中如何设置缓存策略。 proxy_cache 存储一些之前被访问过、而且可能将要被再次访问的资源，使用户可以直接从代理服务器获得，从而减少上游服务器的压力，加快整个访问速度。 语法：proxy_cache zone | off ; # zone 是共享内存的名称 默认值：proxy_cache off; 上下文：http、server、location proxy_cache_path 设置缓存文件的存放路径。 语法：proxy_cache_path path [level=levels] ...可选参数省略，下面会详细列举 默认值：proxy_cache_path off 上下文：http 参数含义： path 缓存文件的存放路径； level path 的目录层级； keys_zone 设置共享内存； inactive 在指定时间内没有被访问，缓存会被清理，默认10分钟； proxy_cache_key 设置缓存文件的 key 。 语法：proxy_cache_key 默认值：proxy_cache_key $scheme$proxy_host$request_uri; 上下文：http、server、location proxy_cache_valid 配置什么状态码可以被缓存，以及缓存时长。 语法：proxy_cache_valid [code...] time; 上下文：http、server、location 配置示例：proxy_cache_valid 200 304 2m;; # 说明对于状态为200和304的缓存文件的缓存时间是2分钟 proxy_no_cache 定义相应保存到缓存的条件，如果字符串参数的至少一个值不为空且不等于“ 0”，则将不保存该响应到缓存。 语法：proxy_no_cache string; 上下文：http、server、location 示例：proxy_no_cache $http_pragma $http_authorization; proxy_cache_bypass 定义条件，在该条件下将不会从缓存中获取响应。 语法：proxy_cache_bypass string; 上下文：http、server、location 示例：proxy_cache_bypass $http_pragma $http_authorization; upstream_cache_status 变量 它存储了缓存是否命中的信息，会设置在响应头信息中，在调试中非常有用。 MISS: 未命中缓存 HIT： 命中缓存 EXPIRED: 缓存过期 STALE: 命中了陈旧缓存 REVALIDDATED: Nginx验证陈旧缓存依然有效 UPDATING: 内容陈旧，但正在更新 BYPASS: X响应从原始服务器获取 配置实例 我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： server { listen 1010; root /usr/share/nginx/html/1010; location / { index index.html; } } server { listen 1020; root /usr/share/nginx/html/1020; location / { index index.html; } } 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： proxy_cache_path /etc/nginx/cache_temp levels=2:2 keys_zone=cache_zone:30m max_size=2g inactive=60m use_temp_path=off; upstream cache_server{ server 121.42.11.34:1010; server 121.42.11.34:1020; } server { listen 80; server_name cache.lion.club; location / { proxy_cache cache_zone; # 设置缓存内存，上面配置中已经定义好的 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 } } 缓存就是这样配置，我们可以在 /etc/nginx/cache_temp 路径下找到相应的缓存文件。 对于一些实时性要求非常高的页面或数据来说，就不应该去设置缓存，下面来看看如何配置不缓存的内容。 ... server { listen 80; server_name cache.lion.club; # URI 中后缀为 .txt 或 .text 的设置变量值为 \"no cache\" if ($request_uri ~ \\.(txt|text)$) { set $cache_name \"no cache\" } location / { proxy_no_cache $cache_name; # 判断该变量是否有值，如果有值则不进行缓存，如果没有值则进行缓存 proxy_cache cache_zone; # 设置缓存内存 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 } } HTTPS 在学习如何配置 HTTPS 之前，我们先来简单回顾下 HTTPS 的工作流程是怎么样的？它是如何进行加密保证安全的？ HTTPS 工作流程 客户端（浏览器）访问 https://www.baidu.com 百度网站； 百度服务器返回 HTTPS 使用的 CA 证书； 浏览器验证 CA 证书是否为合法证书； 验证通过，证书合法，生成一串随机数并使用公钥（证书中提供的）进行加密； 发送公钥加密后的随机数给百度服务器； 百度服务器拿到密文，通过私钥进行解密，获取到随机数（公钥加密，私钥解密，反之也可以）； 百度服务器把要发送给浏览器的内容，使用随机数进行加密后传输给浏览器； 此时浏览器可以使用随机数进行解密，获取到服务器的真实传输内容； 这就是 HTTPS 的基本运作原理，使用对称加密和非对称机密配合使用，保证传输内容的安全性。 关于HTTPS更多知识，可以查看作者的另外一篇文章《学习 HTTP 协议》。 配置证书 下载证书的压缩文件，里面有个 Nginx 文件夹，把 xxx.crt 和 xxx.key 文件拷贝到服务器目录，再进行如下配置： server { listen 443 ssl http2 default_server; # SSL 访问端口号为 443 server_name lion.club; # 填写绑定证书的域名(我这里是随便写的) ssl_certificate /etc/nginx/https/lion.club_bundle.crt; # 证书地址 ssl_certificate_key /etc/nginx/https/lion.club.key; # 私钥地址 ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 支持ssl协议版本，默认为后三个，主流版本是[TLSv1.2] location / { root /usr/share/nginx/html; index index.html index.htm; } } 如此配置后就能正常访问 HTTPS 版的网站了。 配置跨域 CORS 先简单回顾下跨域究竟是怎么回事。 跨域的定义 同源策略限制了从同一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。通常不允许不同源间的读操作。 同源的定义 如果两个页面的协议，端口（如果有指定）和域名都相同，则两个页面具有相同的源。 下面给出了与 URL http://store.company.com/dir/page.html 的源进行对比的示例: http://store.company.com/dir2/other.html 同源 https://store.company.com/secure.html 不同源，协议不同 http://store.company.com:81/dir/etc.html 不同源，端口不同 http://news.company.com/dir/other.html 不同源，主机不同 不同源会有如下限制： Web 数据层面，同源策略限制了不同源的站点读取当前站点的 Cookie 、 IndexDB 、 LocalStorage 等数据。 DOM 层面，同源策略限制了来自不同源的 JavaScript 脚本对当前 DOM 对象读和写的操作。 网络层面，同源策略限制了通过 XMLHttpRequest 等方式将站点的数据发送给不同源的站点。 Nginx 解决跨域的原理 例如： 前端 server 的域名为： fe.server.com 后端服务的域名为： dev.server.com 现在我在 fe.server.com 对 dev.server.com 发起请求一定会出现跨域。 现在我们只需要启动一个 Nginx 服务器，将 server_name 设置为 fe.server.com 然后设置相应的 location 以拦截前端需要跨域的请求，最后将请求代理回 dev.server.com 。如下面的配置： server { listen 80; server_name fe.server.com; location / { proxy_pass dev.server.com; } } 这样可以完美绕过浏览器的同源策略： fe.server.com 访问 Nginx 的 fe.server.com 属于同源访问，而 Nginx 对服务端转发的请求不会触发浏览器的同源策略。 配置开启 gzip 压缩 GZIP 是规定的三种标准 HTTP 压缩格式之一。目前绝大多数的网站都在使用 GZIP 传输 HTML 、CSS 、 JavaScript 等资源文件。 对于文本文件， GZiP 的效果非常明显，开启后传输所需流量大约会降至 1/4~1/3 。 并不是每个浏览器都支持 gzip 的，如何知道客户端是否支持 gzip 呢，请求头中的 Accept-Encoding 来标识对压缩的支持。 启用 gzip 同时需要客户端和服务端的支持，如果客户端支持 gzip 的解析，那么只要服务端能够返回 gzip 的文件就可以启用 gzip 了,我们可以通过 Nginx 的配置来让服务端支持 gzip 。下面的 respone 中 content-encoding:gzip ，指服务端开启了 gzip 的压缩方式。 在 /etc/nginx/conf.d/ 文件夹中新建配置文件 gzip.conf ： # # 默认off，是否开启gzip gzip on; # 要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用； gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; # ---- 以上两个参数开启就可以支持Gzip压缩了 ---- # # 默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容； gzip_static on; # 默认 off，nginx做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩； gzip_proxied any; # 用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩； gzip_vary on; # gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6； gzip_comp_level 6; # 获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得； gzip_buffers 16 8k; # 允许压缩的页面最小字节数，页面字节数从header头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大； # gzip_min_length 1k; # 默认 1.1，启用 gzip 所需的 HTTP 最低版本； gzip_http_version 1.1; 其实也可以通过前端构建工具例如 webpack 、rollup 等在打生产包时就做好 Gzip 压缩，然后放到 Nginx 服务器中，这样可以减少服务器的开销，加快访问速度。 关于 Nginx 的实际应用就学习到这里，相信通过掌握了 Nginx 核心配置以及实战配置，之后再遇到什么需求，我们也能轻松应对。接下来，让我们再深入一点学习下 Nginx 的架构。 Nginx 架构 进程结构 多进程结构 Nginx 的进程模型图： 多进程中的 Nginx 进程架构如下图所示，会有一个父进程（ Master Process ），它会有很多子进程（ Child Processes ）。 Master Process 用来管理子进程的，其本身并不真正处理用户请求。 某个子进程 down 掉的话，它会向 Master 进程发送一条消息，表明自己不可用了，此时 Master 进程会去新起一个子进程。 某个配置文件被修改了 Master 进程会去通知 work 进程获取新的配置信息，这也就是我们所说的热部署。 子进程间是通过共享内存的方式进行通信的。 配置文件重载原理 reload 重载配置文件的流程： 向 master 进程发送 HUP 信号（ reload 命令）； master 进程检查配置语法是否正确； master 进程打开监听端口； master 进程使用新的配置文件启动新的 worker 子进程； master 进程向老的 worker 子进程发送 QUIT 信号； 老的 worker 进程关闭监听句柄，处理完当前连接后关闭进程； 整个过程 Nginx 始终处于平稳运行中，实现了平滑升级，用户无感知； Nginx 模块化管理机制 Nginx 的内部结构是由核心部分和一系列的功能模块所组成。这样划分是为了使得每个模块的功能相对简单，便于开发，同时也便于对系统进行功能扩展。Nginx 的模块是互相独立的,低耦合高内聚。 总结 相信通过本文的学习，你应该会对 Nginx 有一个更加全面的认识。 都看到这里了，就点个👍 👍 👍 吧。 本文转载自：「掘金」，原文：http://t.cn/A6c25eYS，版权归原作者所有。欢迎投稿，投稿邮箱: editor@hi-linux.com。nginx blog Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/nginx/nginx2.html":{"url":"blog/效率/nginx/nginx2.html","title":"Nginx2","keywords":"","body":"本文演示的是 Linux centOS 7.x 的操作系统上安装 Nginx ，至于在其它操作系统上进行安装可以网上自行搜索，都非常简单的。 使用 yum 安装 Nginx ： yum install nginx -y 安装完成后，通过 rpm -ql nginx 命令查看 Nginx 的安装信息： # Nginx配置文件 /etc/nginx/nginx.conf # nginx 主配置文件 /etc/nginx/nginx.conf.default # 可执行程序文件 /usr/bin/nginx-upgrade /usr/sbin/nginx # nginx库文件 /usr/lib/systemd/system/nginx.service # 用于配置系统守护进程 /usr/lib64/nginx/modules # Nginx模块目录 # 帮助文档 /usr/share/doc/nginx-1.16.1 /usr/share/doc/nginx-1.16.1/CHANGES /usr/share/doc/nginx-1.16.1/README /usr/share/doc/nginx-1.16.1/README.dynamic /usr/share/doc/nginx-1.16.1/UPGRADE-NOTES-1.6-to-1.10 # 静态资源目录 /usr/share/nginx/html/404.html /usr/share/nginx/html/50x.html /usr/share/nginx/html/index.html # 存放Nginx日志文件 /var/log/nginx 主要关注的文件夹有两个： /etc/nginx/conf.d/ 是子配置项存放处， /etc/nginx/nginx.conf 主配置文件会默认把这个文件夹中所有子配置项都引入； /usr/share/nginx/html/ 静态文件都放在这个文件夹，也可以根据你自己的习惯放在其他地方； Nginx 常用命令 systemctl 系统命令： # 开机配置 systemctl enable nginx # 开机自动启动 systemctl disable nginx # 关闭开机自动启动 # 启动Nginx systemctl start nginx # 启动Nginx成功后，可以直接访问主机IP，此时会展示Nginx默认页面 # 停止Nginx systemctl stop nginx # 重启Nginx systemctl restart nginx # 重新加载Nginx systemctl reload nginx # 查看 Nginx 运行状态 systemctl status nginx # 查看Nginx进程 ps -ef | grep nginx # 杀死Nginx进程 kill -9 pid # 根据上面查看到的Nginx进程号，杀死Nginx进程，-9 表示强制结束进程 Nginx 应用程序命令： nginx -s reload # 向主进程发送信号，重新加载配置文件，热重启 nginx -s reopen # 重启 Nginx nginx -s stop # 快速关闭 nginx -s quit # 等待工作进程处理完成后关闭 nginx -T # 查看当前 Nginx 最终的配置 nginx -t # 检查配置是否有问题 Nginx 核心配置 配置文件结构 Nginx 的典型配置示例： # main段配置信息 user nginx; # 运行用户，默认即是nginx，可以不进行设置 worker_processes auto; # Nginx 进程数，一般设置为和 CPU 核数一样 error_log /var/log/nginx/error.log warn; # Nginx 的错误日志存放目录 pid /var/run/nginx.pid; # Nginx 服务启动时的 pid 存放位置 # events段配置信息 events { use epoll; # 使用epoll的I/O模型(如果你不知道Nginx该使用哪种轮询方法，会自动选择一个最适合你操作系统的) worker_connections 1024; # 每个进程允许最大并发数 } # http段配置信息 # 配置使用最频繁的部分，代理、缓存、日志定义等绝大多数功能和第三方模块的配置都在这里设置 http { # 设置日志模式 log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; # Nginx访问日志存放位置 sendfile on; # 开启高效传输模式 tcp_nopush on; # 减少网络报文段的数量 tcp_nodelay on; keepalive_timeout 65; # 保持连接的时间，也叫超时时间，单位秒 types_hash_max_size 2048; include /etc/nginx/mime.types; # 文件扩展名与类型映射表 default_type application/octet-stream; # 默认文件类型 include /etc/nginx/conf.d/*.conf; # 加载子配置项 # server段配置信息 server { listen 80; # 配置监听的端口 server_name localhost; # 配置的域名 # location段配置信息 location / { root /usr/share/nginx/html; # 网站根目录 index index.html index.htm; # 默认首页文件 deny 172.168.22.11; # 禁止访问的ip地址，可以为all allow 172.168.33.44；# 允许访问的ip地址，可以为all } error_page 500 502 503 504 /50x.html; # 默认50x对应的访问页面 error_page 400 404 error.html; # 同上 } } main 全局配置，对全局生效； events 配置影响 Nginx 服务器与用户的网络连接； http 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置； server 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块； location 用于配置匹配的 uri ； upstream 配置后端服务器具体地址，负载均衡配置不可或缺的部分； 用一张图清晰的展示它的层级结构： 配置文件 main 段核心参数 user 指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 user USERNAME [GROUP] user nginx lion; # 用户是nginx;组是lion pid 指定运行 Nginx master 主进程的 pid 文件存放路径。 pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number 指定 worker 子进程可以打开的最大文件句柄数。 worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core 指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 worker_rlimit_core 50M; # 存放大小限制working_directory /opt/nginx/tmp; # 存放目录复制代码 worker_processes_number 指定 Nginx 启动的 worker 子进程数量。 worker_processes 4; # 指定具体子进程数量worker_processes auto; # 与当前cpu物理核心数一致复制代码 worker_cpu_affinity 将每个 worker 子进程与我们的 cpu 物理核心绑定。 worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程复制代码 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority 指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout 指定 worker 子进程优雅退出时的超时时间。 worker_shutdown_timeout 5s; timer_resolution worker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon 指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 daemon off; # 默认是on，后台运行模式 配置文件 events 段核心参数 use Nginx 使用何种事件驱动模型。 use method; # 不推荐配置它，让nginx自己选择method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport复制代码 worker_connections worker 子进程能够处理的最大并发连接数。 worker_connections 1024 # 每个子进程的最大连接数为1024复制代码 accept_mutex 是否打开负载均衡互斥锁。 accept_mutex on # 默认是off关闭的，这里推荐打开复制代码 server_name 指令 指定虚拟主机域名。 server_name name1 name2 name3# 示例：server_name www.nginx.com;复制代码 域名匹配的四种写法： 精确匹配： server_name www.nginx.com ; 左侧通配： server_name *.nginx.com ; 右侧统配： server_name www.nginx.* ; 正则匹配： server_name ~^www\\.nginx\\.*$ ; 匹配优先级：精确匹配 > 左侧通配符匹配 > 右侧通配符匹配 > 正则表达式匹配 server_name 配置实例： 1、配置本地 DNS 解析 vim /etc/hosts （ macOS 系统） # 添加如下内容，其中 121.42.11.34 是阿里云服务器IP地址 121.42.11.34 www.nginx-test.com 121.42.11.34 mail.nginx-test.com 121.42.11.34 www.nginx-test.org 121.42.11.34 doc.nginx-test.com 121.42.11.34 www.nginx-test.cn 121.42.11.34 fe.nginx-test.club [注意] 这里使用的是虚拟域名进行测试，因此需要配置本地 DNS 解析，如果使用阿里云上购买的域名，则需要在阿里云上设置好域名解析。 2、配置阿里云 Nginx ，vim /etc/nginx/nginx.conf # 这里只列举了http端中的sever端配置 # 左匹配 server { listen 80; server_name *.nginx-test.com; root /usr/share/nginx/html/nginx-test/left-match/; location / { index index.html; } } # 正则匹配 server { listen 80; server_name ~^.*\\.nginx-test\\..*$; root /usr/share/nginx/html/nginx-test/reg-match/; location / { index index.html; } } # 右匹配 server { listen 80; server_name www.nginx-test.*; root /usr/share/nginx/html/nginx-test/right-match/; location / { index index.html; } } # 完全匹配 server { listen 80; server_name www.nginx-test.com; root /usr/share/nginx/html/nginx-test/all-match/; location / { index index.html; } } 3、访问分析 当访问 www.nginx-test.com 时，都可以被匹配上，因此选择优先级最高的“完全匹配”； 当访问 mail.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.org 时，会进行“右匹配”； 当访问 doc.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.cn 时，会进行“右匹配”； 当访问 fe.nginx-test.club 时，会进行“正则匹配”； root 指定静态资源目录位置，它可以写在 http 、 server 、 location 等配置中。 root path例如：location /image { root /opt/nginx/static;}当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png复制代码 [注意] root 会将定义路径与 URI 叠加， alias 则只取定义路径。 alias 它也是指定静态资源目录位置，它只能写在 location 中。 location /image { alias /opt/nginx/static/image/;}当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png复制代码 [注意] 使用 alias 末尾一定要添加 / ，并且它只能位于 location 中。 location 配置路径。 location [ = | ~ | ~* | ^~ ] uri { ...}复制代码 匹配规则： = 精确匹配； ~ 正则匹配，区分大小写； ~* 正则匹配，不区分大小写； ^~ 匹配到即停止搜索； 匹配优先级： = > ^~ > ~ > ~* > 不带任何字符。 实例： server { listen 80; server_name www.nginx-test.com; # 只有当访问 www.nginx-test.com/match_all/ 时才会匹配到/usr/share/nginx/html/match_all/index.html location = /match_all/ { root /usr/share/nginx/html index index.html } # 当访问 www.nginx-test.com/1.jpg 等路径时会去 /usr/share/nginx/images/1.jpg 找对应的资源 location ~ \\.(jpeg|jpg|png|svg)$ { root /usr/share/nginx/images; } # 当访问 www.nginx-test.com/bbs/ 时会匹配上 /usr/share/nginx/html/bbs/index.html location ^~ /bbs/ { root /usr/share/nginx/html; index index.html index.htm; } } location 中的反斜线 location /test { ... } location /test/ { ... } 不带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ；如果没有 test 目录， nginx 则会找是否有 test 文件。 带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ，如果没有它也不会去找是否存在 test 文件。 return 停止处理请求，直接返回响应码或重定向到其他 URL ；执行 return 指令后， location 中后续指令将不会被执行。 return code [text]; return code URL; return URL; 例如： location / { return 404; # 直接返回状态码 } location / { return 404 \"pages not found\"; # 返回状态码 + 一段文本 } location / { return 302 /bbs ; # 返回状态码 + 重定向地址 } location / { return https://www.baidu.com ; # 返回重定向地址 } rewrite 根据指定正则表达式匹配规则，重写 URL 。 语法：rewrite 正则表达式 要替换的内容 [flag]; 上下文：server、location、if 示例：rewirte /images/(.*\\.jpg)$ /pic/$1; # $1是前面括号(.*\\.jpg)的反向引用 flag 可选值的含义： last 重写后的 URL 发起新请求，再次进入 server 段，重试 location 的中的匹配； break 直接使用重写后的 URL ，不再匹配其它 location 中语句； redirect 返回302临时重定向； permanent 返回301永久重定向； server{ listen 80; server_name fe.lion.club; # 要在本地hosts文件进行配置 root html; location /search { rewrite ^/(.*) https://www.baidu.com redirect; } location /images { rewrite /images/(.*) /pics/$1; } location /pics { rewrite /pics/(.*) /photos/$1; } location /photos { } } 按照这个配置我们来分析： 当访问 fe.lion.club/search 时，会自动帮我们重定向到 https://www.baidu.com。 当访问 fe.lion.club/images/1.jpg 时，第一步重写 URL 为 fe.lion.club/pics/1.jpg ，找到 pics 的 location ，继续重写 URL 为 fe.lion.club/photos/1.jpg ，找到 /photos 的 location 后，去 html/photos 目录下寻找 1.jpg 静态资源。 if 指令 语法：if (condition) {...} 上下文：server、location 示例： if($http_user_agent ~ Chrome){ rewrite /(.*)/browser/$1 break; } condition 判断条件： $variable 仅为变量时，值为空或以0开头字符串都会被当做 false 处理； = 或 != 相等或不等； ~ 正则匹配； ! ~ 非正则匹配； ~* 正则匹配，不区分大小写； -f 或 ! -f 检测文件存在或不存在； -d 或 ! -d 检测目录存在或不存在； -e 或 ! -e 检测文件、目录、符号链接等存在或不存在； -x 或 ! -x 检测文件可以执行或不可执行； 实例： server { listen 8080; server_name localhost; root html; location / { if ( $uri = \"/images/\" ){ rewrite (.*) /pics/ break; } } } 当访问 localhost:8080/images/ 时，会进入 if 判断里面执行 rewrite 命令。 autoindex 用户请求以 / 结尾时，列出目录结构，可以用于快速搭建静态资源下载网站。 autoindex.conf 配置信息： server { listen 80; server_name fe.lion-test.club; location /download/ { root /opt/source; autoindex on; # 打开 autoindex，，可选参数有 on | off autoindex_exact_size on; # 修改为off，以KB、MB、GB显示文件大小，默认为on，以bytes显示出⽂件的确切⼤⼩ autoindex_format html; # 以html的方式进行格式化，可选参数有 html | json | xml autoindex_localtime off; # 显示的⽂件时间为⽂件的服务器时间。默认为off，显示的⽂件时间为GMT时间 } } 当访问 fe.lion.com/download/ 时，会把服务器 /opt/source/download/ 路径下的文件展示出来，如下图所示： 变量 Nginx 提供给使用者的变量非常多，但是终究是一个完整的请求过程所产生数据， Nginx 将这些数据以变量的形式提供给使用者。 下面列举些项目中常用的变量： 变量名 含义 remote_addr 客户端 IP 地址 remote_port 客户端端口 server_addr 服务端 IP 地址 server_port 服务端端口 server_protocol 服务端协议 binary_remote_addr 二进制格式的客户端 IP 地址 connection TCP 连接的序号，递增 connection_request TCP 连接当前的请求数量 uri 请求的URL，不包含参数 request_uri 请求的URL，包含参数 scheme 协议名， http 或 https request_method 请求方法 request_length 全部请求的长度，包含请求行、请求头、请求体 args 全部参数字符串 arg_参数名 获取特定参数值 is_args URL 中是否有参数，有的话返回 ? ，否则返回空 query_string 与 args 相同 host 请求信息中的 Host ，如果请求中没有 Host 行，则在请求头中找，最后使用 nginx 中设置的 server_name 。 http_user_agent 用户浏览器 http_referer 从哪些链接过来的请求 http_via 每经过一层代理服务器，都会添加相应的信息 http_cookie 获取用户 cookie request_time 处理请求已消耗的时间 https 是否开启了 https ，是则返回 on ，否则返回空 request_filename 磁盘文件系统待访问文件的完整路径 document_root 由 URI 和 root/alias 规则生成的文件夹路径 limit_rate 返回响应时的速度上限值 实例演示 var.conf ： server{ listen 8081; server_name var.lion-test.club; root /usr/share/nginx/html; location / { return 200 \" remote_addr: $remote_addr remote_port: $remote_port server_addr: $server_addr server_port: $server_port server_protocol: $server_protocol binary_remote_addr: $binary_remote_addr connection: $connection uri: $uri request_uri: $request_uri scheme: $scheme request_method: $request_method request_length: $request_length args: $args arg_pid: $arg_pid is_args: $is_args query_string: $query_string host: $host http_user_agent: $http_user_agent http_referer: $http_referer http_via: $http_via request_time: $request_time https: $https request_filename: $request_filename document_root: $document_root \"; } } 当我们访问 http://var.lion-test.club:8081/test?pid=121414&cid=sadasd 时，由于 Nginx 中写了 return 方法，因此 chrome 浏览器会默认为我们下载一个文件，下面展示的就是下载的文件内容： remote_addr: 27.16.220.84 remote_port: 56838 server_addr: 172.17.0.2 server_port: 8081 server_protocol: HTTP/1.1 binary_remote_addr: 茉 connection: 126 uri: /test/ request_uri: /test/?pid=121414&cid=sadasd scheme: http request_method: GET request_length: 518 args: pid=121414&cid=sadasd arg_pid: 121414 is_args: ? query_string: pid=121414&cid=sadasd host: var.lion-test.club http_user_agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36 http_referer: http_via: request_time: 0.000 https: request_filename: /usr/share/nginx/html/test/ document_root: /usr/share/nginx/html Nginx 的配置还有非常多，以上只是罗列了一些常用的配置，在实际项目中还是要学会查阅文档。 Nginx 应用核心概念 代理是在服务器和客户端之间假设的一层服务器，代理将接收客户端的请求并将它转发给服务器，然后将服务端的响应转发给客户端。 不管是正向代理还是反向代理，实现的都是上面的功能。 正向代理 正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。 正向代理是为我们服务的，即为客户端服务的，客户端可以根据正向代理访问到它本身无法访问到的服务器资源。 正向代理对我们是透明的，对服务端是非透明的，即服务端并不知道自己收到的是来自代理的访问还是来自真实客户端的访问。 反向代理 反向代理*（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 反向代理是为服务端服务的，反向代理可以帮助服务器接收来自客户端的请求，帮助服务器做请求转发，负载均衡等。 反向代理对服务端是透明的，对我们是非透明的，即我们并不知道自己访问的是代理服务器，而服务器知道反向代理在为他服务。 反向代理的优势： 隐藏真实服务器； 负载均衡便于横向扩充后端动态服务； 动静分离，提升系统健壮性； 那么“动静分离”是什么？负载均衡又是什么？ 动静分离 动静分离是指在 web 服务器架构中，将静态页面与动态页面或者静态内容接口和动态内容接口分开不同系统访问的架构设计方法，进而提示整个服务的访问性和可维护性。 一般来说，都需要将动态资源和静态资源分开，由于 Nginx 的高并发和静态资源缓存等特性，经常将静态资源部署在 Nginx 上。如果请求的是静态资源，直接到静态资源目录获取资源，如果是动态资源的请求，则利用反向代理的原理，把请求转发给对应后台应用去处理，从而实现动静分离。 使用前后端分离后，可以很大程度提升静态资源的访问速度，即使动态服务不可用，静态资源的访问也不会受到影响。 负载均衡 一般情况下，客户端发送多个请求到服务器，服务器处理请求，其中一部分可能要操作一些资源比如数据库、静态资源等，服务器处理完毕后，再将结果返回给客户端。 这种模式对于早期的系统来说，功能要求不复杂，且并发请求相对较少的情况下还能胜任，成本也低。随着信息数量不断增长，访问量和数据量飞速增长，以及系统业务复杂度持续增加，这种做法已无法满足要求，并发量特别大时，服务器容易崩。 很明显这是由于服务器性能的瓶颈造成的问题，除了堆机器之外，最重要的做法就是负载均衡。 请求爆发式增长的情况下，单个机器性能再强劲也无法满足要求了，这个时候集群的概念产生了，单个服务器解决不了的问题，可以使用多个服务器，然后将请求分发到各个服务器上，将负载分发到不同的服务器，这就是负载均衡，核心是「分摊压力」。 Nginx 实现负载均衡，一般来说指的是将请求转发给服务器集群。 举个具体的例子，晚高峰乘坐地铁的时候，入站口经常会有地铁工作人员大喇叭“请走 B 口， B 口人少车空....”，这个工作人员的作用就是负载均衡。 Nginx 实现负载均衡的策略： 轮询策略：默认情况下采用的策略，将所有客户端请求轮询分配给服务端。这种策略是可以正常工作的，但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。 最小连接数策略：将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。 最快响应时间策略：优先分配给响应时间最短的服务器。 客户端 ip 绑定策略：来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。 Nginx 实战配置 在配置反向代理和负载均衡等等功能之前，有两个核心模块是我们必须要掌握的，这两个模块应该说是 Nginx 应用配置中的核心，它们分别是： upstream 、proxy_pass 。 upstream 用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 语法：upstream name { ...}上下文：http示例：upstream back_end_server{ server 192.168.100.33:8081}复制代码 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server 定义上游服务器地址。 语法：server address [parameters] 上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive 限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 keepalive connections; 上下文：upstream 示例：keepalive 16; keepalive_requests 单个长连接可以处理的最多 HTTP 请求个数。 语法：keepalive_requests number; 默认值：keepalive_requests 100; 上下文：upstream keepalive_timeout 空闲长连接的最长保持时间。 语法：keepalive_timeout time;默认值：keepalive_timeout 60s;上下文：upstream复制代码 配置实例 upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s; } proxy_pass 用于配置代理服务器。 语法：proxy_pass URL; 上下文：location、if、limit_except 示例： proxy_pass http://127.0.0.1:8081 proxy_pass http://127.0.0.1:8081/proxy URL 参数原则 URL 必须以 http 或 https 开头； URL 中可以携带变量； URL 中是否带 URI ，会直接影响发往上游请求的 URL ； 接下来让我们来看看两种常见的 URL 用法： proxy_pass http://192.168.100.33:8081 proxy_pass http://192.168.100.33:8081/ 这两种用法的区别就是带 / 和不带 / ，在配置代理时它们的区别可大了： 不带 / 意味着 Nginx 不会修改用户 URL ，而是直接透传给上游的应用服务器； 带 / 意味着 Nginx 会修改用户 URL ，修改方法是将 location 后的 URL 从用户 URL 中删除； 不带 / 的用法： location /bbs/{ proxy_pass http://127.0.0.1:8080; } 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /bbs/abc/test.html 带 / 的用法： location /bbs/{ proxy_pass http://127.0.0.1:8080/; } 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /abc/test.html 并没有拼接上 /bbs ，这点和 root 与 alias 之间的区别是保持一致的。 配置反向代理 这里为了演示更加接近实际，作者准备了两台云服务器，它们的公网 IP 分别是： 121.42.11.34 与 121.5.180.193 。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置： # /etc/nginx/conf.d/proxy.conf server{ listen 8080; server_name localhost; location /proxy/ { root /usr/share/nginx/html/proxy; index index.html; } } # /usr/share/nginx/html/proxy/index.html 121.42.11.34 proxy html 配置完成后重启 Nginx 服务器 nginx -s reload 。 把 121.5.180.193 服务器作为代理服务器，做如下配置： # /etc/nginx/conf.d/proxy.conf upstream back_end { server 121.42.11.34:8080 weight=2 max_conns=1000 fail_timeout=10s max_fails=3; keepalive 32; keepalive_requests 80; keepalive_timeout 20s; } server { listen 80; server_name proxy.lion.club; location /proxy { proxy_pass http://back_end/proxy; } } 本地机器要访问 proxy.lion.club 域名，因此需要配置本地 hosts ，通过命令：vim /etc/hosts 进入配置文件，添加如下内容： 121.5.180.193 proxy.lion.club 分析： 当访问 proxy.lion.club/proxy 时通过 upstream 的配置找到 121.42.11.34:8080 ； 因此访问地址变为 http://121.42.11.34:8080/proxy ； 连接到 121.42.11.34 服务器，找到 8080 端口提供的 server ； 通过 server 找到 /usr/share/nginx/html/proxy/index.html 资源，最终展示出来。 配置负载均衡 配置负载均衡主要是要使用 upstream 指令。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： server{ listen 8020; location / { return 200 'return 8020 \\n'; } } server{ listen 8030; location / { return 200 'return 8030 \\n'; } } server{ listen 8040; location / { return 200 'return 8040 \\n'; } } 配置完成后： nginx -t 检测配置是否正确； nginx -s reload 重启 Nginx 服务器； 执行 ss -nlt 命令查看端口是否被占用，从而判断 Nginx 服务是否正确启动。 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： upstream demo_server { server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 配置完成后重启 Nginx 服务器。并且在需要访问的客户端配置好 ip 和域名的映射关系。 # /etc/hosts 121.5.180.193 balance.lion.club 在客户端机器执行 curl http://balance.lion.club/balance/ 命令： 不难看出，负载均衡的配置已经生效了，每次给我们分发的上游服务器都不一样。就是通过简单的轮询策略进行上游服务器分发。 接下来，我们再来了解下 Nginx 的其它分发策略。 hash 算法 通过制定关键字作为 hash key ，基于 hash 算法映射到特定的上游服务器中。关键字可以包含有变量、字符串。 upstream demo_server { hash $request_uri; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } hash $request_uri 表示使用 request_uri 变量作为 hash 的 key 值，只要访问的 URI 保持不变，就会一直分发给同一台服务器。 ip_hash 根据客户端的请求 ip 进行判断，只要 ip 地址不变就永远分配到同一台主机。它可以有效解决后台服务器 session 保持的问题。 upstream demo_server { ip_hash; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 最少连接数算法 各个 worker 子进程通过读取共享内存的数据，来获取后端服务器的信息。来挑选一台当前已建立连接数最少的服务器进行分配请求。 语法：least_conn; 上下文：upstream; 示例： upstream demo_server { zone test 10M; # zone可以设置共享内存空间的名字和大小 least_conn; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040; } server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; } } 最后你会发现，负载均衡的配置其实一点都不复杂。 配置缓存 缓存可以非常有效的提升性能，因此不论是客户端（浏览器），还是代理服务器（ Nginx ），乃至上游服务器都多少会涉及到缓存。可见缓存在每个环节都是非常重要的。下面让我们来学习 Nginx 中如何设置缓存策略。 proxy_cache 存储一些之前被访问过、而且可能将要被再次访问的资源，使用户可以直接从代理服务器获得，从而减少上游服务器的压力，加快整个访问速度。 语法：proxy_cache zone | off ; # zone 是共享内存的名称 默认值：proxy_cache off; 上下文：http、server、location proxy_cache_path 设置缓存文件的存放路径。 语法：proxy_cache_path path [level=levels] ...可选参数省略，下面会详细列举 默认值：proxy_cache_path off 上下文：http 参数含义： path 缓存文件的存放路径； level path 的目录层级； keys_zone 设置共享内存； inactive 在指定时间内没有被访问，缓存会被清理，默认10分钟； proxy_cache_key 设置缓存文件的 key 。 语法：proxy_cache_key 默认值：proxy_cache_key $scheme$proxy_host$request_uri; 上下文：http、server、location proxy_cache_valid 配置什么状态码可以被缓存，以及缓存时长。 语法：proxy_cache_valid [code...] time; 上下文：http、server、location 配置示例：proxy_cache_valid 200 304 2m;; # 说明对于状态为200和304的缓存文件的缓存时间是2分钟 proxy_no_cache 定义相应保存到缓存的条件，如果字符串参数的至少一个值不为空且不等于“ 0”，则将不保存该响应到缓存。 语法：proxy_no_cache string; 上下文：http、server、location 示例：proxy_no_cache $http_pragma $http_authorization; proxy_cache_bypass 定义条件，在该条件下将不会从缓存中获取响应。 语法：proxy_cache_bypass string; 上下文：http、server、location 示例：proxy_cache_bypass $http_pragma $http_authorization; upstream_cache_status 变量 它存储了缓存是否命中的信息，会设置在响应头信息中，在调试中非常有用。 MISS: 未命中缓存 HIT： 命中缓存 EXPIRED: 缓存过期 STALE: 命中了陈旧缓存 REVALIDDATED: Nginx验证陈旧缓存依然有效 UPDATING: 内容陈旧，但正在更新 BYPASS: X响应从原始服务器获取 配置实例 我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： server { listen 1010; root /usr/share/nginx/html/1010; location / { index index.html; } } server { listen 1020; root /usr/share/nginx/html/1020; location / { index index.html; } } 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： proxy_cache_path /etc/nginx/cache_temp levels=2:2 keys_zone=cache_zone:30m max_size=2g inactive=60m use_temp_path=off; upstream cache_server{ server 121.42.11.34:1010; server 121.42.11.34:1020; } server { listen 80; server_name cache.lion.club; location / { proxy_cache cache_zone; # 设置缓存内存，上面配置中已经定义好的 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 } } 缓存就是这样配置，我们可以在 /etc/nginx/cache_temp 路径下找到相应的缓存文件。 对于一些实时性要求非常高的页面或数据来说，就不应该去设置缓存，下面来看看如何配置不缓存的内容。 ... server { listen 80; server_name cache.lion.club; # URI 中后缀为 .txt 或 .text 的设置变量值为 \"no cache\" if ($request_uri ~ \\.(txt|text)$) { set $cache_name \"no cache\" } location / { proxy_no_cache $cache_name; # 判断该变量是否有值，如果有值则不进行缓存，如果没有值则进行缓存 proxy_cache cache_zone; # 设置缓存内存 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 } } HTTPS 在学习如何配置 HTTPS 之前，我们先来简单回顾下 HTTPS 的工作流程是怎么样的？它是如何进行加密保证安全的？ HTTPS 工作流程 客户端（浏览器）访问 https://www.baidu.com 百度网站； 百度服务器返回 HTTPS 使用的 CA 证书； 浏览器验证 CA 证书是否为合法证书； 验证通过，证书合法，生成一串随机数并使用公钥（证书中提供的）进行加密； 发送公钥加密后的随机数给百度服务器； 百度服务器拿到密文，通过私钥进行解密，获取到随机数（公钥加密，私钥解密，反之也可以）； 百度服务器把要发送给浏览器的内容，使用随机数进行加密后传输给浏览器； 此时浏览器可以使用随机数进行解密，获取到服务器的真实传输内容； 这就是 HTTPS 的基本运作原理，使用对称加密和非对称机密配合使用，保证传输内容的安全性。 关于HTTPS更多知识，可以查看作者的另外一篇文章《学习 HTTP 协议》。 配置证书 下载证书的压缩文件，里面有个 Nginx 文件夹，把 xxx.crt 和 xxx.key 文件拷贝到服务器目录，再进行如下配置： server { listen 443 ssl http2 default_server; # SSL 访问端口号为 443 server_name lion.club; # 填写绑定证书的域名(我这里是随便写的) ssl_certificate /etc/nginx/https/lion.club_bundle.crt; # 证书地址 ssl_certificate_key /etc/nginx/https/lion.club.key; # 私钥地址 ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 支持ssl协议版本，默认为后三个，主流版本是[TLSv1.2] location / { root /usr/share/nginx/html; index index.html index.htm; } } 如此配置后就能正常访问 HTTPS 版的网站了。 配置跨域 CORS 先简单回顾下跨域究竟是怎么回事。 跨域的定义 同源策略限制了从同一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。通常不允许不同源间的读操作。 同源的定义 如果两个页面的协议，端口（如果有指定）和域名都相同，则两个页面具有相同的源。 下面给出了与 URL http://store.company.com/dir/page.html 的源进行对比的示例: http://store.company.com/dir2/other.html 同源 https://store.company.com/secure.html 不同源，协议不同 http://store.company.com:81/dir/etc.html 不同源，端口不同 http://news.company.com/dir/other.html 不同源，主机不同 不同源会有如下限制： Web 数据层面，同源策略限制了不同源的站点读取当前站点的 Cookie 、 IndexDB 、 LocalStorage 等数据。 DOM 层面，同源策略限制了来自不同源的 JavaScript 脚本对当前 DOM 对象读和写的操作。 网络层面，同源策略限制了通过 XMLHttpRequest 等方式将站点的数据发送给不同源的站点。 Nginx 解决跨域的原理 例如： 前端 server 的域名为： fe.server.com 后端服务的域名为： dev.server.com 现在我在 fe.server.com 对 dev.server.com 发起请求一定会出现跨域。 现在我们只需要启动一个 Nginx 服务器，将 server_name 设置为 fe.server.com 然后设置相应的 location 以拦截前端需要跨域的请求，最后将请求代理回 dev.server.com 。如下面的配置： server { listen 80; server_name fe.server.com; location / { proxy_pass dev.server.com; }}复制代码 这样可以完美绕过浏览器的同源策略： fe.server.com 访问 Nginx 的 fe.server.com 属于同源访问，而 Nginx 对服务端转发的请求不会触发浏览器的同源策略。 配置开启 gzip 压缩 GZIP 是规定的三种标准 HTTP 压缩格式之一。目前绝大多数的网站都在使用 GZIP 传输 HTML 、CSS 、 JavaScript 等资源文件。 对于文本文件， GZiP 的效果非常明显，开启后传输所需流量大约会降至 1/4~1/3 。 并不是每个浏览器都支持 gzip 的，如何知道客户端是否支持 gzip 呢，请求头中的 Accept-Encoding 来标识对压缩的支持。启用 gzip 同时需要客户端和服务端的支持，如果客户端支持 gzip 的解析，那么只要服务端能够返回 gzip 的文件就可以启用 gzip 了,我们可以通过 Nginx 的配置来让服务端支持 gzip 。下面的 respone 中 content-encoding:gzip ，指服务端开启了 gzip 的压缩方式。在 /etc/nginx/conf.d/ 文件夹中新建配置文件 gzip.conf ： # # 默认off，是否开启gzip gzip on; # 要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用； gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; # ---- 以上两个参数开启就可以支持Gzip压缩了 ---- # # 默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容； gzip_static on; # 默认 off，nginx做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩； gzip_proxied any; # 用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩； gzip_vary on; # gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6； gzip_comp_level 6; # 获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得； gzip_buffers 16 8k; # 允许压缩的页面最小字节数，页面字节数从header头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大； # gzip_min_length 1k; # 默认 1.1，启用 gzip 所需的 HTTP 最低版本； gzip_http_version 1.1; 其实也可以通过前端构建工具例如 webpack 、rollup 等在打生产包时就做好 Gzip 压缩，然后放到 Nginx 服务器中，这样可以减少服务器的开销，加快访问速度。 关于 Nginx 的实际应用就学习到这里，相信通过掌握了 Nginx 核心配置以及实战配置，之后再遇到什么需求，我们也能轻松应对。接下来，让我们再深入一点学习下 Nginx 的架构。 Nginx 架构 进程结构 多进程结构 Nginx 的进程模型图： 多进程中的 Nginx 进程架构如下图所示，会有一个父进程（ Master Process ），它会有很多子进程（ Child Processes ）。 Master Process 用来管理子进程的，其本身并不真正处理用户请求。 某个子进程 down 掉的话，它会向 Master 进程发送一条消息，表明自己不可用了，此时 Master 进程会去新起一个子进程。 某个配置文件被修改了 Master 进程会去通知 work 进程获取新的配置信息，这也就是我们所说的热部署。 子进程间是通过共享内存的方式进行通信的。 配置文件重载原理 reload 重载配置文件的流程： 向 master 进程发送 HUP 信号（ reload 命令）； master 进程检查配置语法是否正确； master 进程打开监听端口； master 进程使用新的配置文件启动新的 worker 子进程； master 进程向老的 worker 子进程发送 QUIT 信号； 老的 worker 进程关闭监听句柄，处理完当前连接后关闭进程； 整个过程 Nginx 始终处于平稳运行中，实现了平滑升级，用户无感知； Nginx 模块化管理机制 Nginx 的内部结构是由核心部分和一系列的功能模块所组成。这样划分是为了使得每个模块的功能相对简单，便于开发，同时也便于对系统进行功能扩展。Nginx 的模块是互相独立的,低耦合高内聚。 总结 相信通过本文的学习，你应该会对 Nginx 有一个更加全面的认识。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/nginx/常见错误.html":{"url":"blog/效率/nginx/常见错误.html","title":"常见错误","keywords":"","body":"nginx 常见错误 SSL_CTX_use_certificate:ca md too weak nginx: [emerg] SSL_CTX_use_certificate(\"/data/certs/management.lstack-dev.cn.pem\") failed (SSL: error:140AB18E:SSL routines:SSL_CTX_use_certificate:ca md too weak) Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/nginx/自动签发证书.html":{"url":"blog/效率/nginx/自动签发证书.html","title":"自动签发证书","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/oauth2.html":{"url":"blog/效率/oauth2.html","title":"Oauth2","keywords":"","body":"原文出处 引言 OAuth 2.0 协议 OAuth 2.0 是目前比较流行的做法，它率先被Google, Yahoo, Microsoft, Facebook等使用。之所以标注为 2.0，是因为最初有一个1.0协议，但这个1.0协议被弄得太复杂，易用性差，所以没有得到普及。2.0是一个新的设计，协议简单清晰，但它并不兼容1.0，可以说与1.0没什么关系。所以，我就只介绍2.0。 协议参与者 从引言部分的描述我们可以看出，OAuth的参与实体至少有如下三个： · RO (resource owner): 资源所有者，对资源具有授权能力的人。如上文中的用户Alice。 · RS (resource server): 资源服务器，它存储资源，并处理对资源的访问请求。如Google资源服务器，它所保管的资源就是用户Alice的照片。 · Client: 第三方应用，它获得RO的授权后便可以去访问RO的资源。如网易印像服务。 此外，为了支持开放授权功能以及更好地描述开放授权协议，OAuth引入了第四个参与实体： · AS (authorization server): 授权服务器，它认证RO的身份，为RO提供授权审批流程，并最终颁发授权令牌(Access Token)。读者请注意，为了便于协议的描述，这里只是在逻辑上把AS与RS区分开来；在物理上，AS与RS的功能可以由同一个服务器来提供服务。 授权类型 在开放授权中，第三方应用(Client)可能是一个Web站点，也可能是在浏览器中运行的一段JavaScript代码，还可能是安装在本地的一个应用程序。这些第三方应用都有各自的安全特性。对于Web站点来说，它与RO浏览器是分离的，它可以自己保存协议中的敏感数据，这些密钥可以不暴露给RO；对于JavaScript代码和本地安全的应用程序来说，它本来就运行在RO的浏览器中，RO是可以访问到Client在协议中的敏感数据。 OAuth为了支持这些不同类型的第三方应用，提出了多种授权类型，如授权码 (Authorization Code Grant)、隐式授权 (Implicit Grant)、RO凭证授权 (Resource Owner Password Credentials Grant)、Client凭证授权 (Client Credentials Grant)。由于本文旨在帮助用户理解OAuth协议，所以我将先介绍这些授权类型的基本思路，然后选择其中最核心、最难理解、也是最广泛使用的一种授权类型——“授权码”，进行深入的介绍。 基本思路 (1) Client请求RO的授权，请求中一般包含：要访问的资源路径，操作类型，Client的身份等信息。 (2) RO批准授权，并将“授权证据”发送给Client。至于RO如何批准，这个是协议之外的事情。典型的做法是，AS提供授权审批界面，让RO显式批准。这个可以参考下一节实例化分析中的描述。 (3) Client向AS请求“访问令牌(Access Token)”。此时，Client需向AS提供RO的“授权证据”，以及Client自己身份的凭证。 (4) AS验证通过后，向Client返回“访问令牌”。访问令牌也有多种类型，若为bearer类型，那么谁持有访问令牌，谁就能访问资源。 (5) Client携带“访问令牌”访问RS上的资源。在令牌的有效期内，Client可以多次携带令牌去访问资源。 (6) RS验证令牌的有效性，比如是否伪造、是否越权、是否过期，验证通过后，才能提供服务。 授权模式 授权码模式（authorization code）：这是功能最完整，流程最严密的模式。现在主流的使用OAuth2.0协议授权的服务提供商都采用了这种模式，我在下面举例也将采取这种模式。 简化模式（implicit）：跳过了请求授权码（Authorization Code）的步骤，直接通过浏览器向授权服务端请求令牌（Access Token）。这种模式的特点是所有步骤都在浏览器中完成，Token对用户可见，且请求令牌的时候不需要传递client_secret进行客户端认证。 密码模式（resource owner password credentials）：用户向第三方客户端提供自己在授权服务端的用户名和密码，客户端通过用户提供的用户名和密码向授权服务端请求令牌（Access Token）。 授权码模式（authorization code）授权的流程 采用Authorization Code获取Access Token的授权验证流程又被称为Web Server Flow，适用于所有有Server端的应用，如Web/Wap站点、有Server端的手机/桌面客户端应用等。一般来说总体流程包含以下几个步骤： 通过client_id请求授权服务端，获取Authorization Code。 通过Authorization Code、client_id、client_secret请求授权服务端，在验证完Authorization Code是否失效以及接入的客户端信息是否有效（通过传递的client_id和client_secret信息和服务端已经保存的客户端信息进行匹配）之后，授权服务端生成Access Token和Refresh Token并返回给客户端。 客户端通过得到的Access Token请求资源服务应用，获取需要的且在申请的Access Token权限范围内的资源信息。 get authorization code client_id：必须参数，注册应用时获得的API Key。 response_type：必须参数，此值固定为“code”。 redirect_uri：必须参数，授权后要回调的URI，即接收Authorization Code的URI。 scope：非必须参数，以空格分隔的权限列表，若不传递此参数，代表请求用户的默认权限。 state：非必须参数，用于保持请求和回调的状态，授权服务器在回调时（重定向用户浏览器到“redirect_uri”时），会在Query Parameter中原样回传该参数。- - OAuth2.0标准协议建议，利用state参数来防止CSRF攻击。 display：非必须参数，登录和授权页面的展现样式，默认为“page”，具体参数定义请参考“自定义授权页面”一节。 force_login：非必须参数，如传递“force_login=1”，则加载登录页时强制用户输入用户名和口令，不会从cookie中读取百度用户的登陆状态。 confirm_login：非必须参数，如传递“confirm_login=1”且百度用户已处于登陆状态，会提示是否使用已当前登陆用户对应用授权。 login_type：非必须参数，如传递“login_type=sms”，授权页面会默认使用短信动态密码注册登陆方式。 by authorization code get access token 通过上面获得的Authorization Code，接下来便可以用其换取一个Access Token。获取方式是：应用在其服务端程序中发送请求（推荐使用POST）到 百度OAuth2.0授权服务的https://openapi.baidu.com/oauth/2.0/token地址，并带上以下5个必须参数： grant_type：必须参数，此值固定为authorization_code。 code：必须参数，通过上面第一步所获得的Authorization Code。 client_id：必须参数，应用的API Key。 client_secret：必须参数，应用的Secret Key。 redirect_uri：必须参数，该值必须与获取Authorization Code时传递的redirect_uri保持一致。 响应数据包格式： 若参数无误，服务器将返回一段JSON文本，包含以下参数： access_token：要获取的Access Token。 expires_in：Access Token的有效期，以秒为单位（30天的有效期）。 refresh_token：用于刷新Access Token 的 Refresh Token,所有应用都会返回该参数（10年的有效期）。 scope：Access Token最终的访问范围，即用户实际授予的权限列表（用户在授权页面时，有可能会取消掉某些请求的权限）。 session_key：基于http调用Open API时所需要的Session Key，其有效期与Access Token一致。 session_secret：基于http调用Open API时计算参数签名用的签名密钥。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/ovftool.html":{"url":"blog/效率/ovftool.html","title":"Ovftool","keywords":"","body":"vmware 虚拟机克隆工具 ovftool ovftool 可以用来创建vmware虚拟机的模板 ovftool vi://root@8.16.0.119:443/template ./ 目的：可以实现跨越物理机克隆esxi虚拟机 利用VMware workstation（本人使用的pro版）的 OVF Tool导出。 假设你的ESXi的服务器ip是172.28.1.1，要备份的虚拟机的名字叫做ubuntu，workstation装在windows上。 首先进入VMware workstation安装目录，找到\\OVFTool\\ovftool.exe，执行命令 .\\ovftool.exe vi://root:@172.28.1.1/ubuntu C: 输入ESXi root用户的密码后，备份开始，保存在windows的C盘中，至少要包含一个ovf文件和一个vmdk文件。 OVF的全称是Open Virtualization Format。 直接拷贝虚拟机的vmdk文件也是可以的，但是如果虚拟机的硬盘是厚制备的话，vmdk文件太大了。举个例子，我的ubuntu虚拟机vmdk文件大小500GB， 导出为OVF后只需要大约10GB。另外OVFTool应该是开源软件，如果你不想安装VMware家的workstation，可以找一找开源的OVFTool。 参考链接 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/readme.html":{"url":"blog/效率/readme.html","title":"Readme","keywords":"","body":"效率 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/shell_judge.html":{"url":"blog/效率/shell_judge.html","title":"Shell Judge","keywords":"","body":"shell 判断 shell判断文件,目录是否存在或者具有权限 #!/bin/sh myPath=\"/var/log/httpd/\" myFile=\"/var /log/httpd/access.log\" 这里的-x 参数判断$myPath是否存在并且是否具有可执行权限 if [ ! -x \"$myPath\"]; then mkdir \"$myPath\" fi 这里的-d 参数判断$myPath是否存在 if [ ! -d \"$myPath\"]; then mkdir \"$myPath\" fi 这里的-f参数判断$myFile是否存在 if [ ! -f \"$myFile\" ]; then touch \"$myFile\" fi 其他参数还有-n,-n是判断一个变量是否是否有值 if [ ! -n \"$myVar\" ]; then echo \"$myVar is empty\" exit 0 fi 两个变量判断是否相等 if [ \"$var1\" = \"$var2\" ]; then echo '$var1 eq $var2' else echo '$var1 not eq $var2' fi -a file exists. -b file exists and is a block special file. -c file exists and is a character special file. -d file exists and is a directory. -e file exists (just the same as -a). -f file exists and is a regular file. -g file exists and has its setgid(2) bit set. -G file exists and has the same group ID as this process. -k file exists and has its sticky bit set. -L file exists and is a symbolic link. -n string length is not zero. -o Named option is set on. -O file exists and is owned by the user ID of this process. -p file exists and is a first in, first out (FIFO) special file or named pipe. -r file exists and is readable by the current process. -s file exists and has a size greater than zero. -S file exists and is a socket. -t file descriptor number fildes is open and associated with a terminal device. -u file exists and has its setuid(2) bit set. -w file exists and is writable by the current process. -x file exists and is executable by the current process. -z string length is zero. shell 参数名解析 while [ $# -gt 0 ] do key=\"$1\" case $key in --image-name) export IMAGE_NAME=$2 shift ;; --version) export VERSION=$2 shift ;; --update-latest) export UPDATE_LATEST=$2 shift ;; *) echo \"unknown option [$key]\" exit 1 ;; esac shift done = Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/sshpass.html":{"url":"blog/效率/sshpass.html","title":"Sshpass","keywords":"","body":"sshpass 在ssh 中使用密码 sshpass -p [password] ssh -o \"StrictHostKeyChecking no\" root@172.16.11.134 for i in host1 host2 ; do sshpass -p password ssh -tt root@$i 免密钥登陆 # 生成公私钥， id_rsa.pub，id_rsa ssh-keygen # copy pub to remote host ~/.ssh/authorized_keys ssh-copy-id -i ~/.ssh/id_rsa.pub root@xxx.xxx.xxx.xxx iptables -t nat -A PREROUTING -p tcp --dport 5443 -j REDIRECT --to-port 5443 iptables -t nat -A PREROUTING -p tcp --dport 5443 -j DNAT --to-destination 10.168.12.195:5443 iptables -t nat -A POSTROUTING -p tcp --dport 5443 -j SNAT --to-source 10.168.12.195 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/tcpdump.html":{"url":"blog/效率/tcpdump.html","title":"Tcpdump","keywords":"","body":"tcpdump https://blog.csdn.net/weixin_36338224/article/details/107035214 今天要给大家介绍的一个 Unix 下的一个 网络数据采集分析工具，也就是我们常说的抓包工具。 与它功能类似的工具有 wireshark ，不同的是，wireshark 有图形化界面，而 tcpdump 则只有命令行。 由于我本人更习惯使用命令行的方式进行抓包，因此今天先跳过 wireshark，直接给大家介绍这个 tcpdump 神器。 这篇文章，我肝了好几天，借助于Linux 的 man 帮助命令，我把 tcpdump 的用法全部研究了个遍，才形成了本文，不夸张的说，应该可以算是中文里把 tcpdump 讲得最清楚明白，并且还最全的文章了（至少我从百度、谷歌的情况来看），所以本文值得你收藏分享，就怕你错过了，就再也找不到像这样把 tcpdump 讲得直白而且特全的文章了。 在讲解之前，有两点需要声明： 第三节到第六节里的 tcpdump 命令示例，只为了说明参数的使用，并不一定就能抓到包，如果要精准抓到你所需要的包，需要配合第五节的逻辑逻辑运算符进行组合搭配。 不同 Linux 发行版下、不同版本的 tcpdump 可能有小许差异， 本文是基于 CentOS 7.2 的 4.5.1 版本的tcpdump 进行学习的，若在你的环境中无法使用，请参考 man tcpdump 进行针对性学习。 tcpdump 核心参数图解 大家都知道，网络上的流量、数据包，非常的多，因此要想抓到我们所需要的数据包，就需要我们定义一个精准的过滤器，把这些目标数据包，从巨大的数据包网络中抓取出来。 所以学习抓包工具，其实就是学习如何定义过滤器的过程。 而在 tcpdump 的世界里，过滤器的实现，都是通过一个又一个的参数组合起来，一个参数不够精准，那就再加一个，直到我们能过滤掉无用的数据包，只留下我们感兴趣的数据包。 tcpdump 的参数非常的多，初学者在没有掌握 tcpdump 时，会对这个命令的众多参数产生很多的疑惑。 就比如下面这个命令，我们要通过 host 参数指定 host ip 进行过滤 $ tcpdump host 192.168.10.100 主程序 + 参数名+ 参数值 这样的组合才是我们正常认知里面命令行该有的样子。 可 tcpdump 却不走寻常路，我们居然还可以在 host 前再加一个限定词，来缩小过滤的范围？ $ tcpdump src host 192.168.10.100 从字面上理解，确实很容易理解，但是这不符合编写命令行程序的正常逻辑，导致我们会有所疑虑： 除了 src ，dst，可还有其它可以用的限定词？ src，host 应该如何理解它们，叫参数名？不合适，因为 src 明显不合适。 如果你在网上看到有关 tcpdump 的博客、教程，无一不是给你一个参数组合，告诉你这是实现了怎样的一个过滤器？这样的教学方式，很容易让你依赖别人的文章来使用 tcpdump，而不能将 tcpdump 这样神器消化，达到灵活应用，灵活搭配过滤器的效果。 上面加了 src 本身就颠覆了我们的认知，你可知道在 src 之前还可以加更多的条件，比如 tcp, udp, icmp 等词，在你之前的基础上再过滤一层。 tcpdump tcp src host 192.168.10.100 这种参数的不确定性，让大多数人对 tcpdump 的学习始终无法得其精髓。 因此，在学习 tcpdump 之前，我觉得有必要要先让你知道：tcpdump 的参数是如何组成的？这非常重要。 为此，我画了一张图，方便你直观的理解 tcpdump 的各种参数： option 可选参数：将在后边一一解释。 proto 类过滤器：根据协议进行过滤，可识别的关键词有： tcp, udp, icmp, ip, ip6, arp, rarp,ether,wlan, fddi, tr, decnet type 类过滤器：可识别的关键词有：host, net, port, portrange，这些词后边需要再接参数。 direction 类过滤器：根据数据流向进行过滤，可识别的关键字有：src, dst，同时你可以使用逻辑运算符进行组合，比如 src or dst proto、type、direction 这三类过滤器的内容比较简单，也最常用，因此我将其放在最前面，也就是 第三节：常规过滤规则一起介绍。 而 option 可选的参数非常多，有的甚至也不经常用到，因此我将其放到后面一点，也就是 第四节：可选参数解析 当你看完前面六节，你对 tcpdump 的认识会上了一个台阶，至少能够满足你 80% 的使用需求。 你一定会问了，还有 20% 呢？ 其实 tcpdump 还有一些过滤关键词，它不符合以上四种过滤规则，可能需要你单独记忆。关于这部分我会在 第六节：特殊过滤规则 里进行介绍。 理解 tcpdump 的输出 2.1 输出内容结构 tcpdump 输出的内容虽然多，却很规律。 这里以我随便抓取的一个 tcp 包为例来看一下 21:26:49.013621 IP 172.20.20.1.15605 > 172.20.20.2.5920: Flags [P.], seq 49:97, ack 106048, win 4723, length 48 1 从上面的输出来看，可以总结出： 第一列：时分秒毫秒 21:26:49.013621 第二列：网络协议 IP 第三列：发送方的ip地址+端口号，其中172.20.20.1是 ip，而15605 是端口号 第四列：箭头 >， 表示数据流向 第五列：接收方的ip地址+端口号，其中 172.20.20.2 是 ip，而5920 是端口号 第六列：冒号 第七列：数据包内容，包括Flags 标识符，seq 号，ack 号，win 窗口，数据长度 length，其中 [P.] 表示 PUSH 标志位为 1，更多标识符见下面 2.2 Flags 标识符 使用 tcpdump 抓包后，会遇到的 TCP 报文 Flags，有以下几种： [S] : SYN（开始连接） [P] : PSH（推送数据） [F] : FIN （结束连接） [R] : RST（重置连接） [.] : 没有 Flag （意思是除上面四种类型外的其他情况，有可能是 ACK 也有可能是 URG） 常规过滤规则 3.1 基于IP地址过滤：host 使用 host 就可以指定 host ip 进行过滤 $ tcpdump host 192.168.10.100 1 数据包的 ip 可以再细分为源ip和目标ip两种 根据源ip进行过滤 $ tcpdump -i eth2 src 192.168.10.100 根据目标ip进行过滤 $ tcpdump -i eth2 dst 192.168.10.200 1 2 3 4 5 3.2 基于网段进行过滤：net 若你的ip范围是一个网段，可以直接这样指定 $ tcpdump net 192.168.10.0/24 1 网段同样可以再细分为源网段和目标网段 根据源网段进行过滤 $ tcpdump src net 192.168 根据目标网段进行过滤 $ tcpdump dst net 192.168 1 2 3 4 5 3.3 基于端口进行过滤：port 使用 port 就可以指定特定端口进行过滤 $ tcpdump port 8088 1 端口同样可以再细分为源端口，目标端口 根据源端口进行过滤 $ tcpdump src port 8088 根据目标端口进行过滤 $ tcpdump dst port 8088 1 2 3 4 5 如果你想要同时指定两个端口你可以这样写 $ tcpdump port 80 or port 8088 1 但也可以简写成这样 $ tcpdump port 80 or 8088 1 如果你的想抓取的不再是一两个端口，而是一个范围，一个一个指定就非常麻烦了，此时你可以这样指定一个端口段。 $ tcpdump portrange 8000-8080 $ tcpdump src portrange 8000-8080 $ tcpdump dst portrange 8000-8080 1 2 3 对于一些常见协议的默认端口，我们还可以直接使用协议名，而不用具体的端口号 比如 http == 80，https == 443 等 $ tcpdump tcp port http 1 3.4 基于协议进行过滤：proto 常见的网络协议有：tcp, udp, icmp, http, ip,ipv6 等 若你只想查看 icmp 的包，可以直接这样写 $ tcpdump icmp 1 protocol 可选值：ip, ip6, arp, rarp, atalk, aarp, decnet, sca, lat, mopdl, moprc, iso, stp, ipx, or netbeui 3.5 基本IP协议的版本进行过滤 当你想查看 tcp 的包，你也许会这样子写 $ tcpdump tcp 1 这样子写也没问题，就是不够精准，为什么这么说呢？ ip 根据版本的不同，可以再细分为 IPv4 和 IPv6 两种，如果你只指定了 tcp，这两种其实都会包含在内。 那有什么办法，能够将 IPv4 和 IPv6 区分开来呢？ 很简单，如果是 IPv4 的 tcp 包 ，就这样写（友情提示：数字 6 表示的是 tcp 在ip报文中的编号。） $ tcpdump 'ip proto tcp' or $ tcpdump ip proto 6 or $ tcpdump 'ip protochain tcp' or $ tcpdump ip protochain 6 1 2 3 4 5 6 7 8 9 10 11 12 13 而如果是 IPv6 的 tcp 包 ，就这样写 $ tcpdump 'ip6 proto tcp' or $ tcpdump ip6 proto 6 or $ tcpdump 'ip6 protochain tcp' or $ tcpdump ip6 protochain 6 1 2 3 4 5 6 7 8 9 10 11 12 13 关于上面这几个命令示例，有两点需要注意： 跟在 proto 和 protochain 后面的如果是 tcp, udp, icmp ，那么过滤器需要用引号包含，这是因为 tcp,udp, icmp 是 tcpdump 的关键字。 跟在ip 和 ip6 关键字后面的 proto 和 protochain 是两个新面孔，看起来用法类似，它们是否等价，又有什么区别呢？ 关于第二点，网络上没有找到很具体的答案，我只能通过 man tcpdump 的提示， 给出自己的个人猜测，但不保证正确。 proto 后面跟的 的关键词是固定的，只能是 ip, ip6, arp, rarp, atalk, aarp, decnet, sca, lat, mopdl, moprc, iso, stp, ipx, or netbeui 这里面的其中一个。 而 protochain 后面跟的 protocol 要求就没有那么严格，它可以是任意词，只要 tcpdump 的 IP 报文头部里的 protocol 字段为 就能匹配上。 理论上来讲，下面两种写法效果是一样的 $ tcpdump 'ip && tcp' $ tcpdump 'ip proto tcp' 1 2 同样的，这两种写法也是一样的 $ tcpdump 'ip6 && tcp' $ tcpdump 'ip6 proto tcp' 1 2 可选参数解析 4.1 设置不解析域名提升速度 -n：不把ip转化成域名，直接显示 ip，避免执行 DNS lookups 的过程，速度会快很多 -nn：不把协议和端口号转化成名字，速度也会快很多。 -N：不打印出host 的域名部分.。比如,，如果设置了此选现，tcpdump 将会打印’nic’ 而不是 ‘nic.ddn.mil’. 4.2 过滤结果输出到文件 使用 tcpdump 工具抓到包后，往往需要再借助其他的工具进行分析，比如常见的 wireshark 。 而要使用wireshark ，我们得将 tcpdump 抓到的包数据生成到文件中，最后再使用 wireshark 打开它即可。 使用 -w 参数后接一个以 .pcap 后缀命令的文件名，就可以将 tcpdump 抓到的数据保存到文件中。 $ tcpdump icmp -w icmp.pcap 1 4.3 从文件中读取包数据 使用 -w 是写入数据到文件，而使用 -r 是从文件中读取数据。 读取后，我们照样可以使用上述的过滤器语法进行过滤分析。 $ tcpdump icmp -r all.pcap 1 4.4 控制详细内容的输出 -v：产生详细的输出. 比如包的TTL，id标识，数据包长度，以及IP包的一些选项。同时它还会打开一些附加的包完整性检测，比如对IP或ICMP包头部的校验和。 -vv：产生比-v更详细的输出. 比如NFS回应包中的附加域将会被打印, SMB数据包也会被完全解码。（摘自网络，目前我还未使用过） -vvv：产生比-vv更详细的输出。比如 telent 时所使用的SB, SE 选项将会被打印, 如果telnet同时使用的是图形界面，其相应的图形选项将会以16进制的方式打印出来（摘自网络，目前我还未使用过） 4.5 控制时间的显示 -t：在每行的输出中不输出时间 -tt：在每行的输出中会输出时间戳 -ttt：输出每两行打印的时间间隔(以毫秒为单位) -tttt：在每行打印的时间戳之前添加日期的打印（此种选项，输出的时间最直观） 4.6 显示数据包的头部 -x：以16进制的形式打印每个包的头部数据（但不包括数据链路层的头部） -xx：以16进制的形式打印每个包的头部数据（包括数据链路层的头部） -X：以16进制和 ASCII码形式打印出每个包的数据(但不包括连接层的头部)，这在分析一些新协议的数据包很方便。 -XX：以16进制和 ASCII码形式打印出每个包的数据(包括连接层的头部)，这在分析一些新协议的数据包很方便。 4.7 过滤指定网卡的数据包 -i：指定要过滤的网卡接口，如果要查看所有网卡，可以 -i any 4.8 过滤特定流向的数据包 -Q： 选择是入方向还是出方向的数据包，可选项有：in, out, inout，也可以使用 --direction=[direction] 这种写法 4.9 其他常用的一些参数 -A：以ASCII码方式显示每一个数据包(不显示链路层头部信息). 在抓取包含网页数据的数据包时, 可方便查看数据 -l : 基于行的输出，便于你保存查看，或者交给其它工具分析 -q : 简洁地打印输出。即打印很少的协议相关信息, 从而输出行都比较简短. -c : 捕获 count 个包 tcpdump 就退出 -s : tcpdump 默认只会截取前 96 字节的内容，要想截取所有的报文内容，可以使用 -s number， number 就是你要截取的报文字节数，如果是 0 的话，表示截取报文全部内容。 -S : 使用绝对序列号，而不是相对序列号 -C：file-size，tcpdump 在把原始数据包直接保存到文件中之前, 检查此文件大小是否超过file-size. 如果超过了, 将关闭此文件,另创一个文件继续用于原始数据包的记录. 新创建的文件名与-w 选项指定的文件名一致, 但文件名后多了一个数字.该数字会从1开始随着新创建文件的增多而增加. file-size的单位是百万字节(nt: 这里指1,000,000个字节,并非1,048,576个字节, 后者是以1024字节为1k, 1024k字节为1M计算所得, 即1M=1024 ＊ 1024 ＝ 1,048,576) -F：使用file 文件作为过滤条件表达式的输入, 此时命令行上的输入将被忽略. 4.10 对输出内容进行控制的参数 -D : 显示所有可用网络接口的列表 -e : 每行的打印输出中将包括数据包的数据链路层头部信息 -E : 揭秘IPSEC数据 -L ：列出指定网络接口所支持的数据链路层的类型后退出 -Z：后接用户名，在抓包时会受到权限的限制。如果以root用户启动tcpdump，tcpdump将会有超级用户权限。 -d：打印出易读的包匹配码 -dd：以C语言的形式打印出包匹配码. -ddd：以十进制数的形式打印出包匹配码 过滤规则组合 有编程基础的同学，对于下面三个逻辑运算符应该不陌生了吧 and：所有的条件都需要满足，也可以表示为 && or：只要有一个条件满足就可以，也可以表示为 || not：取反，也可以使用 ! 举个例子，我想需要抓一个来自10.5.2.3，发往任意主机的3389端口的包 $ tcpdump src 10.5.2.3 and dst port 3389 1 当你在使用多个过滤器进行组合时，有可能需要用到括号，而括号在 shell 中是特殊符号，因为你需要使用引号将其包含。例子如下： $ tcpdump 'src 10.0.2.4 and (dst port 3389 or 22)' 1 而在单个过滤器里，常常会判断一条件是否成立，这时候，就要使用下面两个符号 =：判断二者相等 ==：判断二者相等 !=：判断二者不相等 当你使用这两个符号时，tcpdump 还提供了一些关键字的接口来方便我们进行判断，比如 if：表示网卡接口名、 proc：表示进程名 pid：表示进程 id svc：表示 service class dir：表示方向，in 和 out eproc：表示 effective process name epid：表示 effective process ID 比如我现在要过滤来自进程名为 nc 发出的流经 en0 网卡的数据包，或者不流经 en0 的入方向数据包，可以这样子写 $ tcpdump \"( if=en0 and proc =nc ) || (if != en0 and dir=in)\" 1 特殊过滤规则 5.1 根据 tcpflags 进行过滤 通过上一篇文章，我们知道了 tcp 的首部有一个标志位。 tcpdump 支持我们根据数据包的标志位进行过滤 proto [ expr:size ] 1 proto：可以是熟知的协议之一（如ip，arp，tcp，udp，icmp，ipv6） expr：可以是数值，也可以是一个表达式，表示与指定的协议头开始处的字节偏移量。 size：是可选的，表示从字节偏移量开始取的字节数量。 接下来，我将举几个例子，让人明白它的写法，不过在那之前，有几个点需要你明白，这在后面的例子中会用到： 1、tcpflags 可以理解为是一个别名常量，相当于 13，它代表着与指定的协议头开头相关的字节偏移量，也就是标志位，所以 tcp[tcpflags] 等价于 tcp[13] ，对应下图中的报文位置。 2、tcp-fin, tcp-syn, tcp-rst, tcp-push, tcp-ack, tcp-urg 这些同样可以理解为别名常量，分别代表 1，2，4，8，16，32，64。这些数字是如何计算出来的呢？ 以 tcp-syn 为例，你可以参照下面这张图，计算出来的值 是就是 2 由于数字不好记忆，所以一般使用这样的“别名常量”表示。 因此当下面这个表达式成立时，就代表这个包是一个 syn 包。 tcp[tcpflags] == tcp-syn 1 要抓取特定数据包，方法有很多种。 下面以最常见的 syn包为例，演示一下如何用 tcpdump 抓取到 syn 包，而其他的类型的包也是同样的道理。 据我总结，主要有三种写法： 1、第一种写法：使用数字表示偏移量 $ tcpdump -i eth0 \"tcp[13] & 2 != 0\" 1 2、第二种写法：使用别名常量表示偏移量 $ tcpdump -i eth0 \"tcp[tcpflags] & tcp-syn != 0\" 1 3、第三种写法：使用混合写法 $ tcpdump -i eth0 \"tcp[tcpflags] & 2 != 0\" or $ tcpdump -i eth0 \"tcp[13] & tcp-syn != 0\" 1 2 3 4 5 如果我想同时捕获多种类型的包呢，比如 syn + ack 包 1、第一种写法 $ tcpdump -i eth0 'tcp[13] == 2 or tcp[13] == 16' 1 2、第二种写法 $ tcpdump -i eth0 'tcp[tcpflags] == tcp-syn or tcp[tcpflags] == tcp-ack' 1 3、第三种写法 $ tcpdump -i eth0 \"tcp[tcpflags] & (tcp-syn|tcp-ack) != 0\" 1 4、第四种写法：注意这里是 单个等号，而不是像上面一样两个等号，18（syn+ack） = 2（syn） + 16（ack） $ tcpdump -i eth0 'tcp[13] = 18' or $ tcpdump -i eth0 'tcp[tcpflags] = 18' 1 2 3 4 5 tcp 中有 类似 tcp-syn 的别名常量，其他协议也是有的，比如 icmp 协议，可以使用的别名常量有 icmp-echoreply, icmp-unreach, icmp-sourcequench, icmp-redirect, icmp-echo, icmp-routeradvert, icmp-routersolicit, icmp-timx-ceed, icmp-paramprob, icmp-tstamp, icmp-tstampreply,icmp-ireq, icmp-ireqreply, icmp-maskreq, icmp-maskreply 1 2 3 4 5 5.2 基于包大小进行过滤 若你想查看指定大小的数据包，也是可以的 $ tcpdump less 32 $ tcpdump greater 64 $ tcpdump $ tcpdump ether host [ehost] $ tcpdump ether dst [ehost] $ tcpdump ether src [ehost] 1 2 3 5.4 过滤通过指定网关的数据包 $ tcpdump gateway [host] 1 5.5 过滤广播/多播数据包 $ tcpdump ether broadcast $ tcpdump ether multicast $ tcpdump ip broadcast $ tcpdump ip multicast $ tcpdump ip6 multicast 1 2 3 4 5 6 7 如何抓取到更精准的包？ 先给你抛出一个问题：如果我只想抓取 HTTP 的 POST 请求该如何写呢？ 如果只学习了上面的内容，恐怕你还是无法写法满足这个抓取需求的过滤器。 在学习之前，我先给出答案，然后再剖析一下，这个过滤器是如何生效的，居然能让我们对包内的内容进行判断。 $ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4]' 1 命令里的可选参数，在前面的内容里已经详细讲过了。这里不再细讲。 本节的重点是引号里的内容，看起来很复杂的样子。 将它逐一分解，我们只要先理解了下面几种用法，就能明白 tcp[n]：表示 tcp 报文里 第 n 个字节 tcp[n:c]：表示 tcp 报文里从第n个字节开始取 c 个字节，tcp[12:1] 表示从报文的第12个字节（因为有第0个字节，所以这里的12其实表示的是13）开始算起取一个字节，也就是 8 个bit。查看 tcp 的报文首部结构，可以得知这 8 个bit 其实就是下图中的红框圈起来的位置，而在这里我们只要前面 4个bit，也就是实际数据在整个报文首部中的偏移量。 &：是位运算里的 and 操作符，比如 0011 & 0010 = 0010 ：是位运算里的右移操作，比如 0111 >> 2 = 0001 0xf0：是 10 进制的 240 的 16 进制表示，但对于位操作来说，10进制和16进制都将毫无意义，我们需要的是二进制，将其转换成二进制后是：11110000，这个数有什么特点呢？前面个 4bit 全部是 1，后面4个bit全部是0，往后看你就知道这个特点有什么用了。 分解完后，再慢慢合并起来看 1、tcp[12:1] & 0xf0 其实并不直观，但是我们将它换一种写法，就好看多了，假设 tcp 报文中的 第12 个字节是这样组成的 10110000，那么这个表达式就可以变成 10110110 && 11110000 = 10110000，得到了 10110000 后，再进入下一步。 2、tcp[12:1] & 0xf0) >> 2 ：如果你不理解 tcp 报文首部里的数据偏移，请先点击这个前往我的上一篇文章，搞懂数据偏移的意义，否则我保证你这里会绝对会听懵了。 tcp[12:1] & 0xf0) >> 2 这个表达式实际是 (tcp[12:1] & 0xf0) >> 4 ) > 2 只要理解了(tcp[12:1] & 0xf0) >> 4 ) 从上一步我们算出了 tcp[12:1] & 0xf0 的值其实是一个字节，也就是 8 个bit，但是你再回去看下上面的 tcp 报文首部结构图，表示数据偏移量的只有 4个bit，也就是说 上面得到的值 10110000，前面 4 位（1011）才是正确的偏移量，那么为了得到 1011，只需要将 10110000 右移4位即可，也就是 tcp[12:1] & 0xf0) >> 4，至此我们是不是已经得出了实际数据的正确位置呢，很遗憾还没有，前一篇文章里我们讲到 Data Offset 的单位是 4个字节，因为要将 1011 乘以 4才可以，除以4在位运算中相当于左移2位，也就是 >4 结合起来一起算的话，最终的运算可以简化为 >>2。 至此，我们终于得出了实际数据开始的位置是 tcp[12:1] & 0xf0) >> 2 （单位是字节）。 找到了数据的起点后，可别忘了我们的目的是从数据中打到 HTTP 请求的方法，是 GET 呢 还是 POST ，或者是其他的？ 有了上面的经验，我们自然懂得使用 tcp[((tcp[12:1] & 0xf0) >> 2):4] 从数据开始的位置再取出四个字节，然后将结果与 GET （注意 GET最后还有个空格）的 16进制写法（也就是 0x47455420）进行比对。 0x47 --> 71 --> G 0x45 --> 69 --> E 0x54 --> 84 --> T 0x20 --> 32 --> 空格 1 2 3 4 如果相等，则该表达式为True，tcpdump 认为这就是我们所需要抓的数据包，将其输出到我们的终端屏幕上。 抓包实战应用例子 8.1 提取 HTTP 的 User-Agent 从 HTTP 请求头中提取 HTTP 的 User-Agent： $ tcpdump -nn -A -s1500 -l | grep \"User-Agent:\" 1 通过 egrep 可以同时提取User-Agent 和主机名（或其他头文件）： $ tcpdump -nn -A -s1500 -l | egrep -i 'User-Agent:|Host:' 1 8.2 抓取 HTTP GET 和 POST 请求 抓取 HTTP GET 请求包： $ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420' or $ tcpdump -vvAls0 | grep 'GET' 1 2 3 4 5 可以抓取 HTTP POST 请求包： $ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x504f5354' or $ tcpdump -vvAls0 | grep 'POST' 1 2 3 4 5 注意：该方法不能保证抓取到 HTTP POST 有效数据流量，因为一个 POST 请求会被分割为多个 TCP 数据包。 8.3 找出发包数最多的 IP 找出一段时间内发包最多的 IP，或者从一堆报文中找出发包最多的 IP，可以使用下面的命令： $ tcpdump -nnn -t -c 200 | cut -f 1,2,3,4 -d '.' | sort | uniq -c | sort -nr | head -n 20 1 cut -f 1,2,3,4 -d ‘.’ : 以 . 为分隔符，打印出每行的前四列。即 IP 地址。 sort | uniq -c : 排序并计数 sort -nr : 按照数值大小逆向排序 8.4 抓取 DNS 请求和响应 DNS 的默认端口是 53，因此可以通过端口进行过滤 $ tcpdump -i any -s0 port 53 1 8.5 切割 pcap 文件 当抓取大量数据并写入文件时，可以自动切割为多个大小相同的文件。例如，下面的命令表示每 3600 秒创建一个新文件 capture-(hour).pcap，每个文件大小不超过 200*1000000 字节： $ tcpdump -w /tmp/capture-%H.pcap -G 3600 -C 200 1 这些文件的命名为 capture-{1-24}.pcap，24 小时之后，之前的文件就会被覆盖。 8.6 提取 HTTP POST 请求中的密码 从 HTTP POST 请求中提取密码和主机名： $ tcpdump -s 0 -A -n -l | egrep -i \"POST /|pwd=|passwd=|password=|Host:\" 1 8.7 提取 HTTP 请求的 URL 提取 HTTP 请求的主机名和路径： $ tcpdump -s 0 -v -n -l | egrep -i \"POST /|GET /|Host:\" 1 8.8 抓取 HTTP 有效数据包 抓取 80 端口的 HTTP 有效数据包，排除 TCP 连接建立过程的数据包（SYN / FIN / ACK）： $ tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)>2)) != 0)' 1 8.9 结合 Wireshark 进行分析 通常 Wireshark（或 tshark）比 tcpdump 更容易分析应用层协议。一般的做法是在远程服务器上先使用 tcpdump 抓取数据并写入文件，然后再将文件拷贝到本地工作站上用 Wireshark 分析。 还有一种更高效的方法，可以通过 ssh 连接将抓取到的数据实时发送给 Wireshark 进行分析。以 MacOS 系统为例，可以通过 brew cask install wireshark 来安装，然后通过下面的命令来分析： $ ssh root@remotesystem 'tcpdump -s0 -c 1000 -nn -w - not port 22' | /Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i - 1 例如，如果想分析 DNS 协议，可以使用下面的命令： $ ssh root@remotesystem 'tcpdump -s0 -c 1000 -nn -w - port 53' | /Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i - 1 抓取到的数据： -c 选项用来限制抓取数据的大小。如果不限制大小，就只能通过 ctrl-c 来停止抓取，这样一来不仅关闭了 tcpdump，也关闭了 wireshark。 到这里，我已经将我所知道的 tcpdump 的用法全部说了一遍，如果你有认真地看完本文，相信会有不小的收获，掌握一个上手的抓包工具，对于以后我们学习网络、分析网络协议、以及定位网络问题，会很有帮助，而 tcpdump 是我推荐的一个抓包工具。 参考文章 FreeBSD Manual Pages About tcpdump Linux tcpdump命令详解 一份快速实用的 tcpdump 命令参考手册 超详细的网络抓包神器 tcpdump 使用指南 [译]tcpdump 示例教程 [英]tcpdump 示例教程 ———————————————— 版权声明：本文为CSDN博主「写代码的明哥」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/weixin_36338224/article/details/107035214 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/telepresence.html":{"url":"blog/效率/telepresence.html","title":"Telepresence","keywords":"","body":"telepresence Debug a Kubernetes service locally Install Telepresence with Homebrew/apt/dnf Debugging a service locally with Telepresence Imagine you have a service running in a staging cluster, and someone reports a bug against it. In order to figure out the problem you want to run the service locally... but the service depends on other services in the cluster, and perhaps on cloud resources like a database. In this tutorial you'll see how Telepresence allows you to debug your service locally. We'll use the telepresence command line tool to swap out the version running in the staging cluster for a debug version under your control running on your local machine. Telepresence will then forward traffic from Kubernetes to the local process. You should start a Deployment and publicly exposed Service like this: Terminal kubectl create deployment hello-world --image=datawire/hello-world kubectl expose deployment hello-world --type=LoadBalancer --port=8000 If your cluster is in the cloud you can find the address of the resulting Service like this: $ kubectl get service hello-world NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-world 10.3.242.226 104.197.103.123 8000:30022/TCP 5d If you see under EXTERNAL-IP wait a few seconds and try again. In this case the Service is exposed at http://104.197.103.123:8000/. On minikube you should instead do this to find the URL: $ minikube service --url hello-world http://192.168.99.100:12345/ Once you know the address you can store its value (don't forget to replace this with the real address!): Terminal $ export HELLOWORLD=http://104.197.103.13:8000 And you send it a query and it will be served by the code running in your cluster: Terminal $ curl $HELLOWORLD/ Hello, world! Swapping your deployment with Telepresence Important: Starting telepresence the first time may take a little while, since Kubernetes needs to download the server-side image. At this point you want to switch to developing the service locally, replace the version running on your cluster with a custom version running on your laptop. To simplify the example we'll just use a simple HTTP server that will run locally on your laptop: Terminal $ mkdir /tmp/telepresence-test $ cd /tmp/telepresence-test $ echo \"hello from your laptop\" > file.txt $ python3 -m http.server 8001 & [1] 2324 $ curl http://localhost:8001/file.txt hello from your laptop $ kill %1 We want to expose this local process so that it gets traffic from Kubernetes, replacing the existing hello-world deployment. Important: you're about to expose a web server on your laptop to the Internet. This is pretty cool, but also pretty dangerous! Make sure there are no files in the current directory that you don't want shared with the whole world. Here's how you should run telepresence (you should make sure you're still in the /tmp/telepresence-test directory you created above): Terminal $ cd /tmp/telepresence-test $ telepresence --swap-deployment hello-world --expose 8000 --run python3 -m http.server 8000 & This does three things: Starts a VPN-like process that sends queries to the appropriate DNS and IP ranges to the cluster. --swap-deployment tells Telepresence to replace the existing hello-world pod with one running the Telepresence proxy. On exit, the old pod will be restored. --run tells Telepresence to run the local web server and hook it up to the networking proxy. As long as you leave the HTTP server running inside telepresence it will be accessible from inside the Kubernetes cluster. You've gone from this... graph RL subgraph Kubernetes in Cloud server[\"datawire/hello-world server on port 8000\"] end ...to this: graph RL subgraph Laptop code[\"python HTTP server on port 8000\"]---client[Telepresence client] end subgraph Kubernetes in Cloud client-.-proxy[\"Telepresence proxy, listening on port 8000\"] end We can now send queries via the public address of the Service we created, and they'll hit the web server running on your laptop instead of the original code that was running there before. Wait a few seconds for the Telepresence proxy to startup; you can check its status by doing: Terminal $ kubectl get pod | grep hello-world hello-world-2169952455-874dd 1/1 Running 0 1m hello-world-3842688117-0bzzv 1/1 Terminating 0 4m Once you see that the new pod is in Running state you can use the new proxy to connect to the web server on your laptop: Terminal $ curl $HELLOWORLD/file.txt hello from your laptop Finally, let's kill Telepresence locally so you don't have to worry about other people accessing your local web server by bringing it to the foreground and hitting Ctrl-C: Terminal $ fg telepresence --swap-deployment hello-world --expose 8000 --run python3 -m http.server 8000 Keyboard interrupt received, exiting. Now if we wait a few seconds the old code will be swapped back in. Again, you can check status of swap back by running: Terminal $ kubectl get pod | grep hello-world When the new pod is back to Running state you can see that everything is back to normal: Terminal $ curl $HELLOWORLD/file.txt Hello, world! What you've learned: Telepresence lets you replace an existing deployment with a proxy that reroutes traffic to a local process on your machine. This allows you to easily debug issues by running your code locally, while still giving your local process full access to your staging or testing cluster. Now it's time to clean up the service: Terminal $ kubectl delete deployment,service hello-world Telepresence can do much more than this: see the reference section of the documentation, on the top-left, for details. Install Telepresence with Homebrew/apt/dnf Still have questions? Ask in our Slack chatroom or file an issue on GitHub. 调试服务 B - 集群内服务与本地联调 服务 B 与刚才的不同之处在于，它是被别人访问的，要调试它，首先得要有真实的访问流量。我们如何才能做到将别人对它的访问路由到本地来，从而实现在本地捕捉到集群中的流量呢？ Telepresence 提供这样一个参数，--swap-deployment ，用来将集群中的一个Deployment替换为本地的服务。对于上面的service-b，我们可以这样替换： XishengdeMacBook-Pro:telepresence-test xishengcai$ telepresence --swap-deployment hello-world --expose 8000 --run python3 -m http.server 8000 Legacy Telepresence command used Command roughly translates to the following in Telepresence: telepresence intercept hello-world --port 8000 -- python3 -m http.server 8000 running... Launching Telepresence Daemon v2.3.1 (api v3) Need root privileges to run \"/usr/local/bin/telepresence daemon-foreground /Users/xishengcai/Library/Logs/telepresence '/Users/xishengcai/Library/Application Support/telepresence' ''\" Password: Connecting to traffic manager... Connected to context kubernetes-admin@kubernetes (https://115.238.145.60:6443) Using Deployment hello-world intercepted Intercept name : hello-world State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8000 Volume Mount Point: /var/folders/h7/dnzyb1d520v05x5m647khhnr0000gn/T/telfs-436589704 Intercepting : all TCP connections Serving HTTP on :: port 8000 (http://[::]:8000/) ... ::ffff:127.0.0.1 - - [17/Jun/2021 20:26:28] \"GET / HTTP/1.1\" 200 - ::ffff:127.0.0.1 - - [17/Jun/2021 20:26:34] \"GET /file.txt HTTP/1.1\" 200 - ::ffff:127.0.0.1 - - [17/Jun/2021 20:26:36] \"GET /file.txt HTTP/1.1\" 200 - # XishengdeMacBook-Pro:~ xishengcai$ kubectl describe pods hello-world-6b8bbfd9c5-8kflp Name: hello-world-6b8bbfd9c5-8kflp Namespace: default Priority: 0 Node: ningbo/115.238.145.60 Start Time: Thu, 17 Jun 2021 20:25:29 +0800 Labels: app=hello-world pod-template-hash=6b8bbfd9c5 Annotations: cni.projectcalico.org/podIP: 10.244.210.19/32 Status: Running IP: 10.244.210.19 IPs: IP: 10.244.210.19 Controlled By: ReplicaSet/hello-world-6b8bbfd9c5 Containers: hello-world: Container ID: docker://acfe4a72ce5f98c50b59e5d886bd2dc4c9d13849b22c76a9107d3cc76701447d Image: datawire/hello-world Image ID: docker-pullable://datawire/hello-world@sha256:bf1110a41ec2e672d3beb56b382802255f1958bb28b97bdb9d62066e37bda83b Port: Host Port: State: Running Started: Thu, 17 Jun 2021 20:26:19 +0800 Ready: True Restart Count: 0 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfh72 (ro) traffic-agent: Container ID: docker://22c0cdeb9be8d9d674788911e288e5d76bebfa0cba11d369649aa87e773af42c Image: docker.io/datawire/tel2:2.3.1 Image ID: docker-pullable://datawire/tel2@sha256:342f3ffb0c49b45e7f398a9eafa3558c1761504b7a9fbccdd04085ad85b2bba3 Port: 9900/TCP Host Port: 0/TCP Args: agent State: Running Started: Thu, 17 Jun 2021 20:25:32 +0800 Ready: True Restart Count: 0 Readiness: exec [/bin/stat /tmp/agent/ready] delay=0s timeout=1s period=10s #success=1 #failure=3 Environment: TELEPRESENCE_CONTAINER: hello-world LOG_LEVEL: debug AGENT_NAME: hello-world AGENT_NAMESPACE: default (v1:metadata.namespace) AGENT_POD_IP: (v1:status.podIP) APP_PORT: 8000 MANAGER_HOST: traffic-manager.ambassador Mounts: /tel_pod_info from traffic-annotations (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfh72 (ro) link https://kubernetes.io/zh/docs/tasks/debug-application-cluster/local-debugging/ https://www.telepresence.io/docs/v1/tutorials/kubernetes/ https://cloud.tencent.com/developer/article/1548539 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/typora.html":{"url":"blog/效率/typora.html","title":"Typora","keywords":"","body":"Typora 完全使用详解 liquid617 2019年05月24日 Matrix 精选 Matrix 是少数派的写作社区，我们主张分享真实的产品体验，有实用价值的经验与思考。我们会不定期挑选 Matrix 最优质的文章，展示来自用户的最真实的体验和观点。 文章代表作者个人观点，少数派仅对标题和排版略作修改。 Typora 一直是我认为桌面端笔记应用应有的终极形态。用我之前 一篇文章 中的话来说就是，「它的功能之强大、设计之冷静、体验之美妙、理念之先进，我认为值得所有笔记应用厂商学习。」 但一件很尴尬的事情是，由于它极简的设计理念，有许多使用者并没能完全地了解到 Typora 的全部强大功能。我想在这篇文章中由浅入深地介绍 Typora 的功能亮点。无论你从未用过 Typora，还是已经体验了很久，我相信你都能在这篇文章中发现 Typora 新的惊喜。 Typora 是什么？ Typora 是一款支持实时预览的 Markdown 文本编辑器。它有 OS X、Windows、Linux 三个平台的版本，并且由于仍在测试中，是完全免费的。 在这篇文章中，我希望以「Typora 是什么」这个问题为线索，向大家全面介绍这款令人爱不释手的笔记应用。 一个 Markdown 文本编辑器 Typora 首先是一个 Markdown 文本编辑器，它支持且仅支持 Markdown 语法的文本编辑。在 Typora 官网 上他们将 Typora 描述为 「A truly minimal markdown editor. 」。 官网首页 关于 Markdown Markdown 是用来编写结构化文档的一种纯文本格式，它使我们在双手不离开键盘的情况下，可以对文本进行一定程度的格式排版。你可以在 这篇文章 中快速入门 Markdown。 由于目前还没有一个权威机构对 Markdown 的语法进行规范，各应用厂商制作时遵循的 Markdown 语法也是不尽相同的。其中比较受到认可的是 GFM 标准，它是由著名代码托管网站 GitHub 所制定的。Typora 主要使用的也是 GFM 标准。同时，你还可以在 文件 - 偏好设置 - Markdown 语法偏好 - 严格模式 中将标准设置为「更严格地遵循 GFM 标准」。具体内容你可以在官方的 这篇文档 中查看。 严格模式 写得舒服 一个文本编辑器，写得舒服是关键。我曾说过，「更有趣的是，一个笔记应用不会因为它支持 Markdown 语法而高级或易用很多。」，细枝末节处的人性化考虑才是最重要的。而 Typora 的编辑体验显然是经过深思熟虑设计的产物。 我认为：一个优秀的笔记应用应该给用户选择 Markdown 语法风格的权利。而 Typora 在这一点上是我目前见过所有 Markdown 笔记应用中做得最好的。 文本编辑设置 通过打开 文件 - 偏好设置 你会发现 Typora 为编辑体验的考虑细致到了令人叹为观止的程度。Typora 中提供了大量有关 Markdown 偏好的设置，据此，你可以构建一个几乎完全适合自己的 Markdown 编辑器。下面我将依次介绍一些与文本编辑体验有关的功能亮点。 智能标点 我认为「智能标点」是比较有趣的一点。它可以自动帮你将不是很美观的直引号 \" ' 转化为更美观的弯引号 “ ‘ ’ ”。具体内容你可以在官方的 这篇文档 中查看。关于直弯引号在 macOS 上如何输入你也可以看 这篇文章。 图片插入 Typora 的图片插入功能是广受好评的。要知道，Markdown 原生不太注重图片插入的功能，但你可以在 Typora 中： 直接使用 右键 - 复制 Ctrl + V 将网络图片、剪贴板图片复制到文档中 拖动本地图片到文档中 Typora 会自动帮你插入符合 Markdown 语法的图片语句，并给它加上标题。 复制图片 你也完全可以使用图床来保证文档在分享后图片仍能正常显示。 更强大的是，Typora 支持在拖动或 Ctrl + V 网络图片后自动将其保存到本地。你可以在 文件 - 偏好设置 - 编辑器 - 图片插入 中选择复制到哪个路径，什么情况下需要复制。 图片插入 这一功能保证了即使网络图片源失效了，你还有本地的备份可用。同时也能使你的文档文件夹更合理、完整。 打字机模式和专注模式 「打字机模式」使得你所编辑的那一行永远处于屏幕正中。 「专注模式」使你正在编辑的那一行保留颜色，而其他行的字体呈灰色。 你可以在 视图 - 专注模式 / 打字机模式 中勾选使用这两个模式。 两种模式 实时预览 我想很果断地下这个结论：到现在还不支持编辑界面实时预览的 Markdown 编辑器基本可以退出市场了。Typora 在这一方面显然已经领先了一大步——他们连 Markdown 语法的标记都在实时预览中消去了。当你离开正在编辑的有格式的文本段后，Typora 会自动隐藏 Markdown 标记，只留下「所见即所得」的美妙。他们把这称为 Hybrid View。 所见即所得 为了防止一些程序 bug 的发生（虽然在我使用下来感到是很少的）导致格式问题无法修改，Typora 保留了一个「源代码模式」。你可以通过 视图 - 源代码模式 或左下角的 按钮进入。 源代码模式 大纲 / 文件侧边栏 侧边栏 Typora 会根据你 Markdown 标记的 H1、H2、H3…… 各级标题为你呈现一个大纲。 你也可以选择查看文件夹中的文件，但由于目前 Typora 只支持查看 md 文件，因此我认为文件侧边栏这个功能还是很鸡肋的。 空格与换行 Typora 在空格与换行部分主要是使用 CommonMark 作为标注规范。与前文提到的 GFM 一样，CommonMark 也是比较流行的 Markdown 语言规范（解析器）之一。 空格：在输入连续的空格后，Typora 会在编辑器视图里为你保留这些空格，但当你打印或导出时，这些空格会被省略成一个。 你可以在源代码模式下，为每个空格前加一个 \\ 转义符，或者直接使用 HTML 风格的 &nbps; 来保持连续的空格。 软换行：需要说明的是，在 Markdown 语法中，换行（line break）与换段是不同的。且换行分为软换行和硬换行。在 Typora 中，你可以通过 Shift + Enter 完成一次软换行。软换行只在编辑界面可见，当文档被导出时换行会被省略。 硬换行：你可以通过 空格 + 空格 + Shift + Enter 完成一次硬换行，而这也是许多 Markdown 编辑器所原生支持的。硬换行在文档被导出时将被保留，且没有换段的段后距。 换段：你可以通过 Enter 完成一次换段。Typora 会自动帮你完成两次 Shift + Enter 的软换行，从而完成一次换段。这也意味着在 Markdown 语法下，换段是通过在段与段之间加入空行来实现的。 Windows 风格（CR+LF）与 Unix 风格（CR）的换行符：CR 表示回车 \\r ，即回到一行的开头，而 LF 表示换行 \\n ，即另起一行。 所以 Windows 风格的换行符本质是「回车 + 换行」，而 Unix 风格的换行符是「换行」。这也是为什么 Unix / Mac 系统下的文件，如果在 Windows 系统直接打开会全部在同一行内。 你可以在 文件 - 偏好设置 - 编辑器 - 默认换行符 中对此进行切换。 下附以上各空格、换行、换段的测试结果图。具体内容你可以在官网的 这篇文档 中查阅。 空格、换行、换段 emoji 表情 如今 emoji 表情越来越多地出现在一些网站文章中，但在桌面端（特别是 Windows 系统）文本编辑器上插入 emoji 是一件十分麻烦的事情。在使用 Typora 之前，我打出 emoji 表情的办法基本有两个： 输入法联想：优点是比较方便，但会插入一张图片而不是一个字符，在许多情景下都不是很合适。 复制 emoji 符号：优点是能保证符号的形式，但显然每次用都需要去复制，比较麻烦。 输入法联想 在 Typora 中，你可以用 :emoji: 的形式来打出 emoji，软件会自动给出图形的提示，还是比较好用的。 Typora 中输入 emoji 一个学术文档编辑器 除了基本的文本编辑体验极佳之外，Typora 还是一个非常优秀的学术文档编辑器。当然作为一个轻量级的、基于 Markdown 的编辑器，它不能与那些 LaTeX 编辑器相提并论，但它仍支持了许多可用于学术写作的功能。 LaTeX LaTeX 是一种基于 TeX 的排版系统，由于它易于快速生成复杂表格和数学公式，非常适用于生成高印刷质量的科技和数学类文档。如果你常阅读数学、计算机等领域的学术论文，你一定对 LaTeX 不陌生。 Typora 原生支持 LaTeX 语法，你有两种方式输入 LaTeX 风格的数学公式： 行内公式（inline）：用 $...$ 括起公式，公式会出现在行内。 块间公式（display）：用 ......... 括起公式（注意 $$ 后需要换行），公式会默认显示在行中间。 具体的 LaTeX 语法在此不赘述了，你可以在 这篇文章 中查看。 代码高亮 Typora 中代码的插入也可以分为行内和块间两种： 行内代码：用 ... 或 ... 括起代码，代码会以主题中设置的样式出现在行内，但不会实现代码高亮。 代码块：输入 ````` 后并输入语言名，换行，开始写代码，Typora 就会自动帮你实现代码高亮。Typora 原生支持许多编程语言代码块的语法高亮，基本日常常用的编程语言它都能很好地支持。 除此以外，你也可以直接换行开始写，而后再选择语言。 代码块 表格 在 Markdown 中插入表格一直是一件比较头疼的事情。在一般的 Markdown 编辑器中，你可以通过以下的格式插入表格： | 左对齐 | 右对齐 | 居中对齐 | | :-----| ----: | :----: | | 单元格 | 单元格 | 单元格 | | 单元格 | 单元格 | 单元格 | 乍一看还挺直观好用的是吧？但想想，一旦表格内容层次不齐，又或是表格长得难以下手，直接用键盘输入表格就显得十分麻烦和痛苦了。 好在 Typora 为我们提供了图形界面的插入表格的功能，你只需要在行内 鼠标右键 - 插入 - 表格 ，并输入行数和列数，Typora 就会自动生成一张样式不错的空表格。 Typora 表格 链接引用与脚注 链接引用类似于我们常在论文末尾看到的「参考文献」的写法，你可以通过 []: 的语法来为你的文档加上链接引用。 脚注在少数派的文章中也很常见，即某段话结尾右上角标有数字标记，页面底部进行注释的写法。你可以在需要插入脚注标号的位置写 [^ number ] ，再在下方通过 [^ number ]: 在文档中插入脚注。注意不要遗漏了脚注编号 number 前后的空格。 链接引用和脚注 文件系统 除了前文提到的文件侧边栏，Typora 还提供了一些耦合度不高的文件系统。 快速打开：你可以通过 文件 - 快速打开... 或 Ctrl + P 快捷键快速打开最近的文档。 保存：Typora 支持自动保存，一般很少有写好的文档丢失的情况。同时它也提供了诸如「保存」、「另存为」、「保存全部打开的文件...」之类的功能。 导入：Typora 支持非常多的文件格式：.docx, .latex, .tex, .ltx, .rst, .rest, .org, .wiki, .dokuwiki, .textile, .opml, .epub。 导出：Typora 原生支持导出 PDF，HTML等格式。你可以根据软件内提示安装 Pandoc 插件来导出更多例如 docx，LaTeX 等格式。 导出 一个伪装成文本编辑器的浏览器 当我的一个朋友问我「Typora 有什么好写的？」时，我回答「Typora 是一个伪装成文本编辑器的浏览器」。是的，事实上如果你有一定的计算机基础，你可以找到许多有关于「Typora 其实是一个浏览器」的蛛丝马迹。 图片插入 在图片插入的选项中，Typora 用了「复制图片到 ./${filename}.assets 文件夹」的说法，而这其实是网页前端常用的 Javascript 字符串模板语法的风格。 再比如，Typora 将更遵循 GFM 标准的 Markdown 语法模式称为「严格模式 Strict Mode」，这一说法常见于 HTML 和 JavaScript 编程中。类似「源代码模式」的说法也是同理。 当然，最明显的一点是当你按下 Shift + F12 快捷键时，页面会弹出一个基于 Chrome 的开发者工具栏，也就是我们在浏览器中常说的「审查元素」。 审查元素 伪装从何而来？ 当我们把视角放在「Typora 是一个支持 Markdown 语言的文本编辑器」的出发点来考虑这个问题，一切就都显得很明白了。 John Gruber 在 2004 年用 Perl 创造了 Markdown 语言，这个语言的目的是希望大家使用「易于阅读、易于撰写的纯文字格式，并选择性的转换成有效的 XHTML（或是 HTML）」。也就是说，在 Markdown 诞生之初，它就是为了被浏览器阅读而设计的。 我们在用 Markdown 语言撰写文稿的时候，其实本质上是在借助某种编程语言的转化（解析器）来编写一个 HTML 网页。Markdown 从它诞生之初就与 Web 技术有着及其紧密的联系。 如果我说，我们每一篇文稿都是一个网页，那就很好理解了。Typora 利用解析器先将我们写的 Markdown 文档解析成为 HTML 文档，再为它嵌入一个 CSS 样式，最后再加上可能需要的脚本等。 HTML HTML 是一种标记语言，主要负责构成网页的骨架，它包含所有不加装饰的网页元素。在 Typora 的使用场景下则是所有的文本、段落、标题等的记号。 你可以把一张网页想象成一幅数字油画，HTML 就是那个黑白线条的底，上面写满了数字标记，示意你哪一块区域要涂什么颜色。而 CSS 则负责在对应的区域涂上颜色，甚至加上一些装饰等。 数字油画 HTML 标签 Typora 支持许多常用的 HTML 标签，如果你了解 HTML 语法的话，你可以写出十分美观丰富的文档页面。 HTML 标签 事实上你可以在 Typora 中完成许多基本的 HTML 风格的文本输入，例如 HTML 字符、HTML 块、HTML 风格的注释，甚至是视频和音频。具体支持的功能和限制请在 官方文档 中查阅。 有了这一功能，我们就可以在 Typora 中创造出远超普通 Markdown 文档的页面效果。 导出为 HTML Typora 原生支持将文档导出为 HTML 格式的文件，并选择是否要嵌入 style（也就是后文我将提到的 CSS 的部分）。 除此之外，由于其本身「浏览器」的属性，你可以直接在实时预览界面用 Ctrl +C 复制到 HTML 代码。一个实用的用处是将这些 HTML 代码直接 Ctrl + V 黏贴到微信公众号后台，基本可以保证两边显示效果相同。这一点不仅使公众号推送可以有更自由、美观的样式，也让编辑、排版更轻松了。（由于微信自带浏览器的一些特性，可能有少部分 CSS style 不能生效，建议多多校对。） 公众号 具体如何用 Typora 完成公众号写作，你可以在 这篇文章 中进一步了解。 CSS 为了让文档更美观，我们可以为其加上 CSS style。我认为 Typora 对 CSS 的支持让它成为一众桌面笔记应用中最与众不同的一个。在 Typora 中 CSS 被称为「主题」，但其本质仍是 CSS 文件。你可以在 文件 - 偏好设置 - 主题 - 打开主题文件夹 看到这些 CSS 文件。 主题 主题文件夹 选择不同的主题可以使文档拥有不同的外观，但不会影响内容。Typora 自带了若干主题，你也可以在 官网 下载更多的主题。 主题商店 除此以外，如果你有一定的 Web 编程基础，你当然也可以自己修改、新建适合你使用需求的 CSS 文件。我自己就写了一份名为 WeChat 的 CSS 文件，来符合我公众号特定的排版需求，例如正文是 15px，页边距是 8，小标题是 18px 等等。 CSS 文件 使用 Typora 的「主题」功能写公众号的一个好处是，只需要每次都套用同样的主题，我们就可以在保证每次排版规范都相同的同时，节省许多重复的工作。 YAML front-matter Typora 支持在文档头部加上基于 YAML 的 front-matter 信息，这一特性适用于把 Markdown 文档分类归档上传到用 Hexo 框架搭建的博客中。我对于这一点不太了解，这里就不误导大家了。有所了解的朋友可以在评论区谈谈！ 写在最后 如果你看过 Typora 的 更新日志 你会发现它早在 2015 年 12 月 19 日就发布了 0.7.0 (alpha) 版本，但到目前为止它仍处于 beta 阶段。截止到笔者写作这篇文章的初稿时，Typora 的版本号为 0.9.9.24.6 (beta)。 我想，能够潜心三年多打磨、测试一个产品，开发者们一定是倾注了极大的心血到这款笔记应用中。而在使用中，我也能体会到开发者的良苦用心。他们不想做知识的储备中心，不想做快速的草稿、笔记应用，也不想做任何和生产力无关的功能和特性。对于 Typora 的设计和构想，只有唯一的宗旨——「生产效率」。 因为移动端生产效率低，所以只有桌面端应用。因为左顾右盼的两栏式预览影响思路，所以他们甚至去掉了 Markdown 语法标记，只留下最终呈现的样式。因为 Markdown 是为了 HTML 呈现而设计的，所以他们提供了从入门到高级的一系列 Web 辅助功能…… 这一切都只是为了纯粹的生产效率。如果我们回头想想，Markdown 语言本身，当初不也是为了高效地生成简单网页而诞生的吗？ 最后，让我们一起期待这款顶级桌面文本生产力工具的正式版的到来吧！😆 参考 Front-matter - Hexo 让 Markdown 写作更简单，免费极简编辑器：Typora 使用 Typora 一次性搞定公众号写作与排版 简中求效：Markdown 遇上 LaTeX 关于Typora + pandoc导出文件功能的介绍 我的 LaTeX 入门 选择正确的 Markdown Parser Typora —— 能用 ⌘C⌘V 插图的 Markdown 编辑器 HTML Support in Typora - Typora Markdown - Wikipedia Windows、Unix、Mac不同操作系统的换行问题-剖析回车符\\r和换行符\\n 简中求效：Markdown 遇上 LaTeX 通用标注 (CommonMark) Whitespace and Line Breaks - Typora 评论区朋友们的补充 @Unee Wang：补充一个 Ubuntu 无法源安装的问题，是因为使用ssl的源： 1 安装依赖包 sudo apt-get install libapt-pkg-dev 2 安装、更新 sudo apt-get install apt-transport-https sudo apt-get update 3 安装Typora源 wget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add - sudo add-apt-repository ‘deb https://typora.io/linux ./‘ sudo apt-get update 4 安装typora sudo apt-get install typora 参考：http://blog.chromebook.space/2019/05/linuxmarkdowntypora.html @Vian：Typora 是用 Electron 开发的，Electron 把 Chromium 浏览器和 Node.js 打包起来，并且向开发者提供了一些 native API，从而允许人们用 Web 技术开发桌面端应用。 @I_AM_JOKER：electron有几个固有的缺点。一个，体积大，60M起步，浏览器内核非常大，真正有用的js脚本却很小，及其不经济。第二，慢、卡顿，JavaScript固有缺点。唯一的优点就是可以把md转成html渲染。 原生应用启动往往几秒就可以了，内存占用也不会太高，electron启动就要好几秒（看机器），内存占用也要几百兆起步。 @oneselfly：electron 的出现，好处是易于创建跨平台的应用，坏处是很多人啥都用 electron 套个壳，体积大性能差，小小的功能硬是要带个浏览器，大家都不好好写原生了。 @JasonZone：Typora 集成 iPic 后，在 Markdown 中插图很方便。（仅 Mac 平台） https://sspai.com/post/36275 感谢大家踊跃的讨论，共同进步！ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/url encode.html":{"url":"blog/效率/url encode.html","title":"Url Encode","keywords":"","body":"urldecdoe && urlencode Urlencode and decode from the command line with bash If you want a native bash solution to urlencode and urldecode, put this in your .bashrc urlencode() { # urlencode local length=\"${#1}\" for (( i = 0; i local url_encoded=\"${1//+/ }\" printf '%b' \"${url_encoded//%/\\\\x}\" } If you want to use this from xargs, you'll need to export the function via: export -f urlencode unix Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/vim.html":{"url":"blog/效率/vim.html","title":"Vim","keywords":"","body":"linux下删除空行的几种方法 1. grep grep -v '^$' file 2. sed sed '/^$/d' file 或 sed -n '/./p' file sed -i 's/\\[toc\\]/ - [4.tr](#4tr) /g' *.md 3.awk awk '/./ {print}' file 或 awk '{if($0!=\" \") print}' 4.tr tr -s \"n\" Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/yum.html":{"url":"blog/效率/yum.html","title":"Yum","keywords":"","body":"yum usage download rpm package yum install --downloadonly RPM_Name download rpm package to target dir yum install --downloadonly --downloaddir=/usr/package rpm_name yum install remote file server rpm yum localinstall http://{{ nginx_file_server }}/rpm Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/代码统计.html":{"url":"blog/效率/代码统计.html","title":"代码统计","keywords":"","body":"代码统计 command usage: git diff [] [] [--] [...] or: git diff [] --cached [] [--] [...] or: git diff [] [...] [--] [...] or: git diff [] ...] [--] [...] or: git diff [] ] or: git diff [] --no-index [--] ] 目标： 这里我们要比较出 tag：v100r001c02b013 和当前master分支的代码 操作步骤 查询出tag v100r001c02b013 的 commit $ XishengdeMacBook-Pro:oam xishengcai$ git tag v0.0.1 v0.0.2 v0.0.3 v0.1.8 v0.2.1 v0.3.0 v0.3.1 v0.3.3 v100r001c01b051 v100r001c01b053 v100r001c02b011 v100r001c02b013 $ XishengdeMacBook-Pro:oam xishengcai$ git checkout v100r001c02b013 注意：正在切换到 'v100r001c02b013'。 您正处于分离头指针状态。您可以查看、做试验性的修改及提交，并且您可以在切换 回一个分支时，丢弃在此状态下所做的提交而不对分支造成影响。 如果您想要通过创建分支来保留在此状态下所做的提交，您可以通过在 switch 命令 中添加参数 -c 来实现（现在或稍后）。例如： git switch -c 或者撤销此操作： git switch - 通过将配置变量 advice.detachedHead 设置为 false 来关闭此建议 HEAD 目前位于 cd8c1f4 Merge branch 'dev' into 'master' $ XishengdeMacBook-Pro:oam xishengcai$ git log commit cd8c1f45ba518d13c878cdc706100cac30c23ae4 (HEAD, tag: v100r001c02b013, lsh/mkk-donot-delete) Merge: b9541de 2974361 Author: 蔡锡生 Date: Tue Mar 9 09:04:17 2021 +0800 Merge branch 'dev' into 'master' 代码优化，svc 支持多容器端口暴露 See merge request lstack-hybrid/app-center/lsh-cluster-oam-kubernetes-runtime!31 commit 2974361b9b6dfc149dbd157afc37ebbce9abb02e 同理查询出master 的commit git diff 比较， 注意commit 参数的顺序， 一定要吧 历史分支放前面，master 放后面 $ XishengdeMacBook-Pro:oam xishengcai$ git diff cd8c1f45ba518d13c878cdc706100cac30c23ae4 e4a17ac253c9b6134e13cf46e7a1c0419296dc69 \\ > --author=\"$(git config --get user.name)\" \\ > --pretty=tformat: --numstat 10 1 .gitignore 122 173 .golangci.yml 2 1 apis/core/v1alpha2/canary_trait.go 1 1 apis/core/v1alpha2/core_trait_horizontalpodautoscalertrait_types.go 5 12 apis/core/v1alpha2/core_workload_types.go 1 0 charts/oam-kubernetes-runtime/crds/core.oam.dev_canarytraits.yaml 1 0 charts/oam-kubernetes-runtime/crds/core.oam.dev_containerizedworkloads.yaml 1 1 charts/oam-kubernetes-runtime/crds/core.oam.dev_horizontalpodautoscalertraits.yaml 17 1 charts/oam-kubernetes-runtime/templates/webhook.yaml 8 0 charts/oam-kubernetes-runtime/templates/workloaddefinitions.yaml 18 0 examples/helm-charts/README.md 9 0 examples/helm-charts/appconfig.yaml 17 0 examples/helm-charts/component.yaml 2 2 test/e2e-test/testdata/revision/workload-def.yaml => examples/helm-charts/workload-definition.yaml 1 1 examples/two-container/sample_application_config.yaml 3 0 examples/two-container/sample_component.yaml 3 14 go.mod 17 116 go.sum ... 上面的数据第一列是 addline， 第二列是 remove grep -v 去除 不统计的文件 git diff cd8c1f45ba518d13c878cdc706100cac30c23ae4 e4a17ac253c9b6134e13cf46e7a1c0419296dc69 \\ --author=\"$(git config --get user.name)\" \\ --pretty=tformat: --numstat \\ | grep -v 'vendor' \\ | grep -v 'swaggerui' \\ | grep -v 'go.mod' \\ | grep -v 'go.sum' \\ | grep -v 'crd' \\ | grep -v 'charts' \\ awk 聚合计算 git diff 15a93bbef79556abbc6028745ac7b80cf1548c83 3cf61f29bbc6dc79b1c8fff5e71249a6d6c41abd \\ --author=\"$(git config --get user.name)\" \\ --pretty=tformat: --numstat \\ | grep -v 'vendor' \\ | grep -v 'swaggerui' \\ | grep -v 'go.mod' \\ | grep -v 'go.sum' \\ | grep -v 'crd' \\ | grep -v 'charts' \\ | awk '{ add += $1 ; subs += $2 ; loc += $1 + $2 } END { printf \"added lines: %s removed lines : %s total lines: %s\\n\",add,subs,loc }' Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/内网穿透.html":{"url":"blog/效率/内网穿透.html","title":"内网穿透","keywords":"","body":"内网穿透 作者：古强 frp 是一个专注于内网穿透的高性能的反向代理应用，支持 TCP、UDP、HTTP、HTTPS 等多种协议。可以将内网服务以安全、便捷的方式通过具有公网 IP 节点的中转暴露到公网。 frp提供的特性包括： 支持TCP、KCP以及WebSocket等多种通信协议 TCP连接流式复用 负载均衡 端口复用 插件支持 WebUI支持 安装 frp在Github发布版本，可以从GitHub下载最新版本。frp支持Linux、Windows和BSD系统，请根据操作系统类型和处理器架构选择正确的版本。 frp分为服务端和客户端，服务端的可执行程序名是frps，客户端可执行程序名是frpc，下面将使用frps和frpc作为代称。 服务端 下载 wget https://github.com/fatedier/frp/releases/download/v0.36.2/frp_0.36.2_linux_amd64.tar.gz 解压 tar -zxf frp_0.36.2_linux_amd64.tar.gz 拷贝文件 cd frp_0.36.2_linux_amd64 cp frps /usr/bin/frps mkdir -p /etc/frp cp frps.ini /etc/frp/frps.ini cp systemd/frps.service /etc/systemd/system 如果不需要systemd集成，最后一行可以不执行。 如果没有root权限，也可以使用nohup运行 nohup path/to/frps -c path/to/frps.ini & 编辑配置文件 完整的服务端配置文件可以参考frps_full.ini，这里只作基本配置。frps.ini文件的默认内容如下： [common] bind_port = 7000 frps默认只需要申明服务端口即可运行，所有端口穿透的配置在客户端frpc实现。 为了防止恶意连接，我们需要把token认证，在frps.ini中添加： [common] bind_port = 7000 token = some_token_str 这样配置后，没有正确token的客户端便无法连接。 如果想要使用WebUI，在frps.ini加入以下内容： [common] bind_port = 7000 token = some_token_str dashboard_port = 7500 dashboard_user = admin dashboard_pwd = admin 运行 systemctl enable frps systemctl start frps 客户端 下载 wget https://github.com/fatedier/frp/releases/download/v0.36.2/frp_0.36.2_linux_amd64.tar.gz 解压 tar -zxf frp_0.36.2_linux_amd64.tar.gz 拷贝文件 cd frp_0.36.2_linux_amd64 cp frpc /usr/bin/frpc mkdir -p /etc/frp cp frpc.ini /etc/frp/frpc.ini cp systemd/frpc.service /etc/systemd/system 如果不需要systemd集成，最后一行可以不执行。 如果没有root权限，也可以使用nohup运行 nohup path/to/frpc -c path/to/frpc.ini & 编辑配置文件 完整的服务端配置文件可以参考frpc_full.ini，这里只作基本配置。frps.ini文件的默认内容如下： [common] server_addr = 127.0.0.1 server_port = 7000 [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 remote_port = 6000 [common]段是基本配置，根据我们服务端的配置需要加入token [common] server_addr = x.x.x.x server_port = 7000 token = some_token_str frpc中，每一个端口代理，都以[name]的形式声明。例如下面这个： # 唯一性的名称 [ssh] # 代理类型 type = tcp # 本地地址，可以不是本机 local_ip = 127.0.0.1 # 本地端口 local_port = 22 # 远程端口 remote_port = 6000 运行 systemctl enable frpc systemctl start frpc FAQ Q：我想把本地443暴露到远程的443，为什么不生效？ A：在Linux上绑定低于1024的端口需要特权，有两种方式解决这个问题： 使用root权限运行，如果是systemd方式运行的，需要在/etc/systemd/system/frps.service（frps.service文件所在位置）中加入User=root 为frps添加特权 sudo setcap cap_net_bind_service=ep /usr/bin/frps，注意frps路径是否正确 Q：我有多个客户端，如何负载均衡？ A：在声明端口代理时，使用group。例如： [web01] type = http local_ip = 127.0.0.1 local_port = 80 group = web group_key = web health_check_type = http health_check_timeout_s = 3 health_check_max_failed = 3 health_check_interval_s = 10 [web02] type = http local_ip = 127.0.0.1 local_port = 80 group = web group_key = web health_check_type = http health_check_timeout_s = 3 health_check_max_failed = 3 health_check_interval_s = 10 group和group_key是必要的，并且每个组的group和group_key必须一致。健康检查是可选的，不过强烈建议加上:) Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/科学上网.html":{"url":"blog/效率/科学上网.html","title":"科学上网","keywords":"","body":"科学上网 平台支持 V2Ray 在以下平台中可用： Windows 7 及之后版本（x86 / amd64）； Mac OS X 10.10 Yosemite 及之后版本（amd64）； Linux 2.6.23 及之后版本（x86 / amd64 / arm / arm64 / mips64 / mips）； 包括但不限于 Debian 7 / 8、Ubuntu 12.04 / 14.04 及后续版本、CentOS 6 / 7、Arch Linux； FreeBSD (x86 / amd64)； OpenBSD (x86 / amd64)； Dragonfly BSD (amd64)； 下载 V2Ray 预编译的压缩包可以在如下几个站点找到： Github Release: github.com/v2ray/v2ray-core Github 分流: github.com/v2ray/dist Homebrew: github.com/v2ray/homebrew-v2ray Arch Linux: packages/community/x86_64/v2ray/ Snapcraft: snapcraft.io/v2ray-core 压缩包均为 zip 格式，找到对应平台的压缩包，下载解压即可使用。 验证安装包 V2Ray 提供两种验证方式： 安装包 zip 文件的 SHA1 / SHA256 摘要，在每个安装包对应的.dgst文件中可以找到。 可运行程序（v2ray 或 v2ray.exe）的 gpg 签名，文件位于安装包中的 v2ray.sig 或 v2ray.exe.sig。签名公钥可以在代码库中找到。 Windows 和 Mac OS 安装方式 通过上述方式下载的压缩包，解压之后可看到 v2ray 或 v2ray.exe。直接运行即可。 Linux 发行版仓库 部分发行版可能已收录 V2Ray 到其官方维护和支持的软件仓库/软件源中。出于兼容性、适配性考虑，您可以考虑选用由您发行版开发团队维护的软件包或下文的安装脚本亦或基于已发布的二进制文件或源代码安装。 Linux 安装脚本 V2Ray 提供了一个在 Linux 中的自动化安装脚本。这个脚本会自动检测有没有安装过 V2Ray，如果没有，则进行完整的安装和配置；如果之前安装过 V2Ray，则只更新 V2Ray 二进制程序而不更新配置。 以下指令假设已在 su 环境下，如果不是，请先运行 sudo su。 运行下面的指令下载并安装 V2Ray。当 yum 或 apt-get 可用的情况下，此脚本会自动安装 unzip 和 daemon。这两个组件是安装 V2Ray 的必要组件。如果你使用的系统不支持 yum 或 apt-get，请自行安装 unzip 和 daemon bash 此脚本会自动安装以下文件： /usr/bin/v2ray/v2ray：V2Ray 程序； /usr/bin/v2ray/v2ctl：V2Ray 工具； /etc/v2ray/config.json：配置文件； /usr/bin/v2ray/geoip.dat：IP 数据文件 /usr/bin/v2ray/geosite.dat：域名数据文件 此脚本会配置自动运行脚本。自动运行脚本会在系统重启之后，自动运行 V2Ray。目前自动运行脚本只支持带有 Systemd 的系统，以及 Debian / Ubuntu 全系列。 运行脚本位于系统的以下位置： /etc/systemd/system/v2ray.service: Systemd /etc/init.d/v2ray: SysV 脚本运行完成后，你需要： 编辑 /etc/v2ray/config.json 文件来配置你需要的代理方式； 运行 service v2ray start 来启动 V2Ray 进程； 之后可以使用 service v2ray start|stop|status|reload|restart|force-reload 控制 V2Ray 的运行。 go.sh 参数 go.sh 支持如下参数，可在手动安装时根据实际情况调整： -p 或 --proxy: 使用代理服务器来下载 V2Ray 的文件，格式与 curl 接受的参数一致，比如 \"socks5://127.0.0.1:1080\" 或 \"http://127.0.0.1:3128\"。 -f 或 --force: 强制安装。在默认情况下，如果当前系统中已有最新版本的 V2Ray，go.sh 会在检测之后就退出。如果需要强制重装一遍，则需要指定该参数。 --version: 指定需要安装的版本，比如 \"v1.13\"。默认值为最新版本。 --local: 使用一个本地文件进行安装。如果你已经下载了某个版本的 V2Ray，则可通过这个参数指定一个文件路径来进行安装。 示例： 使用地址为 127.0.0.1:1080 的 SOCKS 代理下载并安装最新版本：./go.sh -p socks5://127.0.0.1:1080 安装本地的 v1.13 版本：./go.sh --version v1.13 --local /path/to/v2ray.zip Docker V2Ray 提供了两个预编译的 Docker image： v2ray/official: 包含最新发布的版本，每周跟随新版本更新； v2ray/dev: 包含由最新的代码编译而成的程序文件，随代码库更新； 两个 image 的文件结构相同： /etc/v2ray/config.json: 配置文件 /usr/bin/v2ray/v2ray: V2Ray 主程序 /usr/bin/v2ray/v2ctl: V2Ray 辅助工具 /usr/bin/v2ray/geoip.dat: IP 数据文件 /usr/bin/v2ray/geosite.dat: 域名数据文件 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/效率/证书制作.html":{"url":"blog/效率/证书制作.html","title":"证书制作","keywords":"","body":"generate certificate #!/bin/bash usage() { cat ${dir}/extfile.cnf openssl x509 -req -in ${dir}/server.csr -CA ${dir}/ca.crt -CAkey ${dir}/ca.key -CAcreateserial -extfile ${dir}/extfile.cnf -out ${dir}/server.crt -days 3650 else openssl x509 -req -days 36500 -in ${dir}/server.csr -CA ${dir}/ca.crt -CAkey ${dir}/ca.key -CAcreateserial -out ${dir}/server.crt fi } # 生成客户端密钥，证书并使用CA证书签名 genClientCert(){ openssl req -new -newkey rsa:${rsa} -keyout ${dir}/client.key -subj \"/CN=${CN}\" -out ${dir}/client.csr -nodes openssl x509 -req -sha256 -days 36500 -in ${dir}/client.csr -CA ${dir}/ca.crt -CAkey ${dir}/ca.key -set_serial 02 -out ${dir}/client.crt } # 生成p12 creteP12(){ openssl pkcs12 -export -clcerts -inkey ${dir}/client.key -in ${dir}/client.crt -out ${dir}/client.p12 } createCert() { kubectl delete secret tls-secret -n test kubectl -n test \\ create secret generic tls-secret \\ --from-file=tls.crt=server.crt \\ --from-file=tls.key=server.key \\ --from-file=ca.crt=ca.crt } createTls() { set +e # 忽略删除错误 kubectl delete secret tls-secret -n ${ns} set -e kubectl -n ${ns} \\ create secret tls tls-secret \\ --cert=server.crt \\ --key=server.key } testCurl() { curl -v https://xxx:6443/apis/launchercontroller.k8s.io/v1/rsoverviews/lau-crd-resource-overview?timeout=5s \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key \\ --cacert /etc/kubernetes/pki/ca.crt curl https://xx \\ --cert ./cert/client.crt \\ --key ./cert/client.key \\ --cacert ./cert/ca.crt } clean(){ rm -rf ./extfile.cnf rm -rf ./client.* rm -rf ./ca.* rm -rf ./server.* } main() { mkdir -p $dir genServerCrt genClientCert if [ \"$pkcs12\" ]; then creteP12 fi if [ \"$ns\" ]; then createTls fi } main Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/杂谈/PDCA.html":{"url":"blog/杂谈/PDCA.html","title":"PDCA","keywords":"","body":"Plan：对于目标，非常讲究，执着于第一 Do：想尽一切办法，让”所有能用的方法“同时进行 Check：必须要用数字来检验结果 Action： 有没有更好的办法 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/杂谈/劝学.html":{"url":"blog/杂谈/劝学.html","title":"劝学","keywords":"","body":"文言文《为学》: 天下事有难易乎?为之，则难者亦易矣；不为，则易者亦难矣。人之为学有难易乎?学之，则难者亦易矣；不学，则易者亦难矣。 吾资之昏，不逮人也；吾材之庸，不逮人也。旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也；吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉? 蜀之鄙有二僧，其一贫，其一富。贫者语于富者曰：“吾欲之南海，何如?”富者曰：“子何恃而往?”曰：“吾一瓶一钵足矣。”富者曰：“吾数年来欲买舟而下，犹未能也。子何恃而往?”越明年，贫者自南海还，以告富者。富者有惭色。 西蜀之去南海，不知几千里也，僧富者不能至而贫者至焉。人之立志，顾不如蜀鄙之僧哉?是故聪与敏，可恃而不可恃也；自恃其聪与敏而不学者，自败者也。昏与庸，可限而不可限也；不自限其昏与庸而力学不倦者，自力者也 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/杂谈/富翁给儿子的38封信.html":{"url":"blog/杂谈/富翁给儿子的38封信.html","title":"富翁给儿子的38封信","keywords":"","body":"1. 2. Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/杂谈/论好文章和烂文章.html":{"url":"blog/杂谈/论好文章和烂文章.html","title":"论好文章和烂文章","keywords":"","body":"论好文章和烂文章 作者 | 许晓斌 来源 | 阿里巴巴云原生公众号 写作动机 我们为何写作？对于许多技术同学来说，写作是一件比写代码困难许多的事情，和电脑相顾无言数小时，发现自己写不出什么像样的东西来，着实不是一种很好的体验。即便对于有些经验的人来说，写四千字质量尚可的文章，我估计也要花 6 小时以上的时间，这还不算平时素材积累的时间消耗。 这么费事费力的事情，为什么要去做呢？我认为此事有着极大的价值，这个价值分两层，我暂且称之为表层价值和深层价值。 表层价值是极其功利的，例如有同学想晋升，而晋升的一项指标是个人影响力，那么写文章就能提升个人影响力；例如有团队 Team Leader 想招聘，那怎么让别人了解你及你的团队呢，写文章也是个不错的方法；再有一些就是冲着上级或者利益相关方写的文章，以类似项目汇报的方式写的文章。表层价值的核心关注点其实并不在文章本身，而在于文章背后的人，作者对读者的期望往往不在于读者认可文章内容，也不期望读者参与对内容的讨论，而仅仅是期望读者快速地认可作者这个人。 只关注表层价值去写文章，非常的本末倒置。就好比写一篇讨论 PM 2.5 的科普文章，如果你一上来就冲着推销自己的空气净化器这个动机去写，其恶臭很快会从字里行间流露出来。 与表层价值相对的，我认为任何一篇文章都应该从深层价值出发。这个所谓深层价值就是文章的内容，文章的观点，文章需要尽可能地客观，要向学术真理的态度逼近。写一篇文章是因为对一个问题有着自己的思考，并且去详细了解了很多人的思考，发现了一些观点的冲突，并且不会为了政治正确去迎合别人的观点；尽我所能把那些我认为有价值的想法，总结下来，用清晰、有趣的方法传播给他人；我能体会到写作的热情，这种热情来自于思维的乐趣，来自于观点的碰撞。这个写作的过程，自己的思维有成长，通过大量的逻辑思考，我的思维得到提升；其次写出来的文章，对读者有着很高的价值，因为有价值的知识得到了传播。 还有一种写作动机就是想要传播有价值的技术，例如 Pivatal 公司的布道师 Josh Long 就写了大量的技术介绍文章，也有很多精彩的演讲。我曾问他为什么能够做的如此出色，以致于多年被评为全球 Java 领域最有影响力的 20 人之一。他的回答是这样的： I think that people don’t trust technology, they trust people. So, while it is possible that the spring team could just publish good documentation and leave it for the world to find, it’s far more compelling when u feel u can ask questions of someone. And u can see that they’re having fun.I love Spring because it has made millions of lives easier. It makes me happy to think about its application, to see people happy with its possibilities. 一篇文章写出来，是因为作者喜爱一项技术，从内心认可技术的价值，相信技术的潜力；还是因为作者想兜售什么东西。这两者动机的差异，稍微细心的读者很快就能发现。当然，上述动机经常混合在一起，但是如果写作的主要动机都在表层，那么基本上这样的文章也就没什么价值了。 影响力 Josh 是 Java 领域在全世界有影响力的人，他通过演讲，写书，博客，影响着全世界数百万的 Java 程序员，帮助了 Spring 等优秀的技术在全世界的推广。我写了一本关于 Maven 的书，这本书也得以卖书了几万册，再加上盗版 PDF 的数量，那是影响了全国十多万的 Java 程序员了，帮助了 Maven 这一技术在中国的推广。我加入阿里 8 年多，在内部技术社区 ATA 发表了 60 多篇文章，累计的阅读数大概也有五万以上，我相信这些文章也给阿里的技术带来了一些微小的正面改变。从这些角度看，写优质的文章虽然耗费很大的精力，但是由于其非常便于分享传播，因此能够快速地影响许多人，而且这种影响能够持续的存在。 当然时代在改变，以前由于网络技术的限制，文字的传播比较方便，视频的制作及传播成本比较高，因此演讲的实际影响力相比文章和书籍会弱很多。而今天的网络和视频技术已经非常成熟，制作高质量的视频也许更容易传播了。 文章也好，视频也好，影响力应该是被用来做正确的事情。今天控制影响力的很多流量入口，已然形成了一种新的权力，他们能控制什么声音容易被大家听见，什么画面容易被大家看见，什么文字容易被大家阅读，什么态度容易被大家感受。权力的形成进而演变成了权力寻租，然后这个事情就往往和所谓“正确的事情”无关了。 通过影响力做正确的事情，我觉得最好的例子就是张瓅玶（花名：谷朴）的这两篇文章了，第一篇是「API 设计最佳实践的思考」，第二篇是「警惕复杂度困局：关于软件复杂度的思考」。文章的内容都是工程师喜闻乐见的关于软件设计的深度分析和思考，但我觉得在这个例子中，比内容更重要的是这两篇文章中传递了一个非常重要的信息，那就是，“在阿里巴巴，即便是研究员级别，也是有人在仔仔细细认认真真看技术的，那些层级更低的却早已脱离一线的，张口闭口都说技术只是细节的管理者，见鬼去吧。”（当然，这个信息只是我个人观点，不能代表谷朴） 写作方法 我们的基础教育似乎在写作的培养方面很有问题，我印象中市面上常见的那些优秀作文选，往往都是注重形式和套路，往往缺乏逻辑和观点，用一个成语总结就是“无病呻吟”，什么事情都要拔高抒情，以小见大，最终的结果就是假大空成灾。大家整体的基础已经薄弱了，而从事技术工作的同学，往往在读书的时候语文还是个弱项，那么结果就可想而知了。写出来的东西，要逻辑逻辑不严密，要形式形式乱糟糟，基本的分段、标点、用词更是问题颇多。好在写作这个能力不是只在学校才能训练的，工作中也可以训练，而且明显有章可循。 1. 阅读量 郑子颖（花名：南门）在他的文章中强调了“多看”的重要性，他用豆瓣记录了年均 50 本以上的阅读量，这相当于平均每周至少一本书，这是一个比较非常惊人的数字。我回顾了一下自己的豆瓣记录，阅读量大概是他的一半，也就是年均 25 本的样子。阅读的益处自不必说，我这里想强调的一点是，不断阅读优质图书能提升自己的文字鉴赏力，书读多了，拿起一本翻翻目录，其中找几个段落读一下，对于此书的质量，心里大概就有个谱了。自己写作的时候，其实大多数时间也是在参考现有例子的结构和方法，如果你照着大量优秀的案例学习，那么自然也不会差到哪里去。 我在写《Maven 实战》之前，阅读了大量的 O‘Reilly，Pragmatic Bookshelf，和 Manning 的计算机图书，这些公司出版的图书大都有非常高的质量保证，在介绍技术的时候，有清晰的由浅入深的结构，辅以大小适中的案例，以及不枯燥的理论分析。可以说，我就是参考着那些优秀的书籍的写作结构和写作方法，写了自己的书，最终我的书也得到了整体不错的评价。 除了写作方法上的受益，阅读更重要的是从内容受益。再举例来说，我在「如何做好技术 TL」这篇文章中，提到了自己在做 TL 的这些年阅读了好多本讲管理的书籍，包括《赢》、《驱动力》、《门后的秘密》、《非暴力沟通》等等，这些书籍极大程度上帮助我补充了自己的思考脉络。写作即思考，而思考不是凭空出现的，思考需要原料，常见的原料就是我们实际的工作经验，然而一个人的体验毕竟是非常有限的，而阅读他人的体验及思考，以谦卑的态度为自己的思考提供更多原料，显然是明智的做法。 「如何做好技术 TL」收到过一条很有意思的评论，评论是这样的： 个人认为管理相关的书籍都是鸡汤，不必看，如果需要靠着“教你怎么做管理”的书才能做管理，那么说明这个人不适合做管理。 这句话隐含的意思是，有些知识，例如管理的知识，是无法传承的，只能靠自己天性领悟。对于此评论，我只能说无知者无畏了，我们写文章必不可持有这种心态，即便自己的想法有多么独特、多么原创，也一定不能自建藩篱，坐井观天。 2. 素材 我最近一年一直在做云原生相关的工作，期间我一直在思考，到底什么才是云原生架构，目前行业里对这个词有非常多的解释，但是所有这些解释都没法给让我满意，它们都过于偏重技术和云厂商的视角，而缺乏应用架构的视角。对于这个状况，我想基于过去一年，以及未来的相关工作，对云原生架构这个概念写一些东西，对其概念做一些补充阐述，帮助大家更好的使用技术，文章还没写出来，但是动机就这样形成了。 有了这个动机，再结合我自己个人的一些兴趣，过去一年我一直在积累素材，这些素材非常有意思。例如，我直接深入参与了国际化中台和考拉的项目，他们的架构基于云原生的技术，以及许多阿里云的产品，做了非常大的升级；我也详细了解了菜鸟如何在云上构建平台服务他的合作方公司；学习了解钉钉、IoT 这些本身在公有云上售卖服务的部门，自身是如何在云上架构。我通过 IM，电话或者面对面的方式和相关的同学沟通，他们都非常友好地知无不言，然后我整理相关的材料，再从中分析相同的模式。人脑非常喜欢模式识别的刺激，我在做这些事情的过程中也是乐在其中。除了实际的案例，行业资深人员的观点也是我平时收集素材的来源，例如林昊（花名：毕玄）最近写了一篇名为「云原生的进一步具象化」的文章，他们的文章通常不会人云亦云，仔细阅读会发现一些比较原创的观点。 除了上述和目标主题关联度非常大的素材外，我还发现跨学科的阅读也往往会给自己带来意向不到的收获。还是以云原生为例，虽然我在这个领域工作，但是最近一两年我一直在断断续续的阅读一些经济学的书籍，包括曼昆的《经济学原理》等（我估计好多人买了这套书了，但是真正认真读完的非专业人士还是比较少的）。在学习经济学的思维方式时，我就想从经济学的视角去解释云原生这个事情，原本自建的技术基础设施，在云的时代在逐渐演变的基于云的基础设施，这背后的选择对于业务来说，从经济学的角度应该怎样去分析？自建的成本和效用是什么？购买的成本和效用是什么？将复杂的业务技术架构拆分成一层一层之后，或许可以逐层分析，在每一层做一个从经济学角度最优的选择。 3. 思维导图 我最早了解思维导图是因为读了「Pragmatic Thinking and Learning」这本书，书中介绍思维导图的核心用途是把一个主题相关的所有内容，无论重要次要，都在一个图中写出来，那些不断拓展延伸的线，不是为了构成一个清晰的结构，而是为了给大脑显示什么地方思维可以进一步拓展。因此画思维导图的方式应该是尽量开放自由，目标是把主题相关的内容全部展现出来，思维导图的目的不是建立清晰的逻辑结构。因此，当我看到很多画的思维导图实际上是一个目录的时候，我觉得他们基本都用错了。 写文章（做演讲也是类似的），都是一个先开放后收敛的过程，在前期不断积累素材，使用思维导图拓展思维，建立材料的关联，就是开放的阶段。这个阶段中可以适当让逻辑思维退到幕后，把目光打开，不要过多评价（建立结构，分主次其实就是一种评价），用一种类似空杯的心态去倾听和观察现实世界及他人想法。我在做演讲或者做文章之前，总会先找个安静的地方，准备好咖啡，打开 MindNode，或者直接找拿出 A4 白纸和笔，给自己半小时到一小时的时间，把主题相关的思维导图画一画。安静免打扰的环境、没有时间的压力、富有精力的大脑，这些条件放在一起，才能够让自己从手头事务的聚焦中移开，让大脑中平时得不到关注的意识浮现出来，然后落到纸上。 4. 结构 将大量的材料胡乱堆砌在一起作成文章自然是不行的，作品必然要有一个清晰的结果。在建筑中，我们耳熟能详的有古希腊建筑的柱式结构，也有哥特式建筑以尖拱券、肋拱、飞扶壁等要素为核心的结构；在程序中，常见的 MVC、分层、微内核等模式也是清晰的结构。文章的结构能帮助作者把自己的观点清晰地表达，引导读者在清晰的路径中阅读。在有了思维导图之后，我通常会从那些纷繁复杂的材料中提取出一个最合适的结构，然后再基于这个结构组织材料。 写技术文章还是有一些常见的结构的，以下是几种范式： 解决问题型结构。这种范式通常围绕解决一个具体问题出发，常见逻辑是：背景介绍 -> 提出问题 -> 讨论解决问题的思路 -> 解决问题 -> 价值总结。这种结构可能是工程师最熟悉的了，因为大家都擅于解决具体问题。 知识介绍型结构。这种范式通常用于新技术的介绍，常见逻辑是：行业背景 -> 技术提出 -> 简单 Demo -> Core Conecpts -> 概念深入及 Demo （1-N次）-> 前景分析。大家平时看到的很多技术介绍文章通常使用的是这样的结构，这样的结构的好处是可以由浅入深地介绍新技术和新概念。 观点输出型结构。这种范式相较于前面的两种会更有挑战，常见的逻辑是：总体观点 -> 子观点 1 -> 子观点 1 阐述 -> 子观点 2 -> 子观点 2 阐述 … -> 总结。这种结构的文章写好了力度是非常强的，但是写起来非常有挑战，因为观点的阐述需要丰富的素材以及严密的逻辑推导，稍有不慎就有胡扯之嫌。 当然，我们在写作的时候不必拘泥于这些范式，也可以琢磨自己的范式，但不论何种范式，背后总是存在一个有逻辑关系的结构的。有一本书叫「金字塔原理」，我听到很多人推崇（尤其是在绩效季的时候），我看了下介绍和评价，讲的就是如何用高效地方式让对方理解你，我没读过这本书，但应该就是讲行文的结构和逻辑的，有兴趣的同学可以买来看看。 5. 观点 不是所有的文章都有观点的，例如你写文章总结自己如何解决一个性能问题，不一定需要有观点；介绍一项技术，如 Rust，不是非要表达观点。但是能够表达自己观点的文章通常会更有吸引力，例如解决性能问题的时候强调理解排队论的重要性，这就是一个观点，容易让人印象深刻；介绍 Rust 的时候，断言其在性能敏感的场景未来会占据统治地位，也更容易让这门技术吸引人的目光。当然，观点是需要论证的，其坚固性和你论证的投入度成正比，逻辑推导、数据支撑、案例分析都是非常好的论证手段。 我们也看到有一些文章满篇观点，但基本都是引用，一会是乔布斯，一会是张小龙，一会是马云等等；当然，更常见的是引用公司高管的话，谁谁谁在什么时候说过什么等等，然后用这些内容来支持自己的材料。我觉得偶有引用无伤大雅，总是引用就只能说明自己没有观点，或者观点无力，需要强力给自己支撑。 更有勇气，能体现自己素材丰富，思考深度的观点，反而是那些敢于说出皇帝的新衣的文字。技术文章应该是具备科学精神的，科学是基于对过去不断的 say no 发展出来的，技术文章也应该敢于表达 say no 的观点，不用怕得罪人，我们应该明白，正确的技术/架构/方案，应该是经得起质疑的，因为如果技术是错误的，即便当下环境没人敢质疑，时间长了，现实会让错误需要付出的代价呈指数倍上升。因此，相比于通篇正确的废话，敢于对现状 say no 并提出自己反面观点的文章，更值得推崇。 6. 故事 严格来说，给自己的内容添油加醋的整故事，对于逻辑论证没有任何帮助。然而要让自己的文章/演讲有吸引力，故事的元素是必不可少的。人类进化到今天，大脑对逻辑的反应是非常慢的，而且需要经过训练后才能理解逻辑，但是对于故事的反应，三四岁的小孩就有，人类早期流传下来的精神，例如希腊神话和圣经，都充满了精彩的故事。直至今天，无论公司内网，还是微博，大家对吃瓜的热情之高，岂是逻辑论证能点燃的。故事很容易引起人的共情，让人设身处地联想，然后大脑皮层就容易嗨了，神经网络被激活，各种激素开始分泌…… 这是人类生理的现实，所以我们写文章的时候要尊重（利用）这个现实。要讲一下程序性能不好导致身边的同事半夜 3:20 分电话惊醒了；出了故障导致超市收银机被砸了，介绍新编程语言的时候要秀一段代码，顺便搞个对比说 Java 不行；介绍 Mesh 的时候就要说你原来升级中间件得这么干这么干，以后不需要了，等等…… 我介绍自己「Maven 实战」的时候喜欢讲一个真实的故事： 在我书出版的前些年，因为虚荣心作祟，我特别喜欢去各大售书的网站刷评论，什么亚马逊，豆瓣，京东啊等等，一条条的去看，看到 5 星评价就喜滋滋，看到低1星或者 2 星的就很生气，当然，大部分评价都是很正面的。直到有一天，我在京东刷到一条 1 星的评论，我就一边难过一边打开评论，但读罢我就乐了，那个读者打 1 星的原因是这样的“让你们老板泡奶茶，差评！”原来是因为那阵子刘强东奶茶爆出了恋爱的新闻，伤了这位读者的心了，然后我就躺枪了。 这个故事其实和我在书中介绍的技术没有任何关系，但是我却喜欢讲这个故事，因为他会让听众会心一笑，然后记住我写过一本关于 Maven 的书。 烂文章是怎样的 讲了那么多怎么写好文章的方法，我也想说一下烂文章都长什么样。所谓烂文章，就是指那些对于读者来说，几乎没什么任何正面价值的文章，更有甚之，不仅没有正面价值，还存在负面价值。以下我稍作总结： 个人笔记成文章。自己解决了一个技术问题，做了一些记录，然后写成文章发出来了。这类文章充其量只能称之为素材，因为没有总结提炼，而且完全是从一个人的视角出发写的，读者读起来，不仅感觉不到体系，有价值的部分更通常是少的可怜。 PPT 贴成文章。作者在某个地方作了一次演讲，因为想传播给更多的人，因此就用贴图的方式整理成了文章，好心点的，在图片的中间在补充一些解释文字，就这么成文了。我先假设这个演讲本身的质量是不错的，有逻辑，有观点，有案例，但是即便如此，这样的所谓文章，其阅读体验也是非常差的。在演讲中，PPT 是辅助“讲”的，如果只有 PPT，那真正核心的“讲”的部分就丢失了，因此，更负责任的做法，应该是演讲者用丰富的文字把所讲，用清晰的方式写下来，而不仅仅是那几页干瘪的 PPT。 用宣传战功的方式写文章。组织内部此类文章屡见不鲜，标题会带着很多常见的如“总结”，“年度”，“体系”，“反思”，“展望”等词，通常这类文章既没有体系，也罕见反思，其主要的目的是邀功。基本套路就是，写在在一年中做了很多事情，结果非常好，配几个看起来差不多的框框“架构”图，再直白点，就要上合影了。这类文章后面通常会有很多赞，但基本没有讨论，讨论个啥嘛？作者来邀功了，莫非你说他做的不好得罪人？从知识传播，促进思考的角度来看，这类文章价值几乎为零。 各类贴图成文章。相比用 PPT 贴图，还有用各种其他图贴成文章的，思维导图直接贴，设计图直接贴，流程图直接贴，监控图直接贴，一顿看下来就是没见几行字。好的图，的确是一图胜千言，但是那也仅仅应该是点睛之笔采用，如果篇所谓文章全是图，就完全看不出重点，也看不到体系。文字是非常有力量的，文字可以把读者拉入到作者的思考体系中，用逻辑和修辞去引导读者的思考。或归纳，或推导，让原本隐蔽的知识展现；或雄辩，或幽默，让作者的观点闪光。写作应该充分发挥文字的力量，不是只有图才有架构，不是只有图才能体现思考，相反，图往往因为其不精确性，成为很多人掩饰其思考不足的工具。 正确的废话成文章。官话套话，动辄抽象到极高的高度，从公司战略说起，洋洋洒洒一顿分析，满眼就是最近被抨击得厉害的那类字眼，什么“顶层设计”，“底层逻辑”，“赋能”，“抓手”之类；如果找不到清晰的逻辑，就来个 1.0，2.0，3.0，4.0，5.0，6.0，反正主版本号加 1 就是比前面牛逼了，至于为什么小版本号从来没人用，不知道，反正 N.0 就对了，直接 N 是不行的，直接 N.0.0 也是不行的。读罢这类文章你说不清楚哪里是错的，你唯一明白的就是这文章啥价值都没有，你又浪费了几分钟生命。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/":{"url":"blog/测试/","title":"测试","keywords":"","body":"测试 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/ginkgo/":{"url":"blog/测试/ginkgo/","title":"Ginkgo","keywords":"","body":"ginkgo Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/ginkgo/ginkgo.html":{"url":"blog/测试/ginkgo/ginkgo.html","title":"Ginkgo","keywords":"","body":" Ginkgo 实践 install 运行测试 不同场景下的测试命令 ginkgo 常用命令参数详解 不执行部分用例 只执行部分用例 基准测试 Ginkgo与持续集成 编写自定义报告器 生成 JUnit XML 的输出。 项目实践 FQA link Ginkgo 实践 一个 Golang 的BDD(行为驱动开发)测试框架 Ginkgo是一个BDD风格的Go测试框架，旨在帮助你有效地编写富有表现力的全方位测试。它最好与Gomega匹配器库配对使用，但它的设计是与匹配器无关的。 install go get github.com/onsi/ginkgo/ginkgo go get github.com/onsi/gomega/... 运行测试 不同场景下的测试命令 在当前目录下运行该套件，只需： ginkgo #or go test 输出覆盖率 # 运行 e2e文件夹下的测试文件， 只统计 remote 和 service 目录下的代码覆盖率 go test ./e2e -coverpkg=./remote/... -coverpkg=./service/... -covermode=count -coverprofile=coverprofile.cov # 对覆盖率文件转换成txt 报告 go tool cover -func=coverprofile.cov -o coverprofile.txt 在其它目录下运行该套件，只需： ginkgo /path/to/package /path/to/other/package ... 通过正则匹配测试用例的名称，进行专注测试 ginkgo --focus=项目 --v 通过idea 统计覆盖率测试 效果1: 定位包级覆盖率 ​ 效果2: 定位代码行级覆盖率 ginkgo 常用命令参数详解 指定运行哪些测试套件： -r 使用-r递归运行目标文件夹下的所有测试套件。适用于在所有包中运行所有测试。 -skipPackage=PACKAGES,TO,SKIP 当运行带有 -r 的测试，你可以传递一个逗号分隔的条目列表给 -skipPackage 。任何包的路径如果含有逗号分隔的条目列表之一就会被跳过。 并行测试： -p 设置 -p 可以并行运行测试套件并自动经检测节点数。 修改输出： --v 如果设置该参数， Ginkgo 默认报告会在每个 spec 运行前打印文本和位置。同时，GinkgoWriter 会实时刷新输出到标准输出。 --trace 如果设置该参数，Ginkgo 默认报告会为每个失败打印全栈跟踪日志，不仅仅打印失败发现的行号。 控制随机性： --seed=SEED 变换 spec 顺序时使用的随机种子。 --randomizeAllSpecs 如果设置该参数，所有 spec 都会被重新排序。默认 Ginkgo 只会改变顶层容器的顺序。 --randomizeSuites 如果设置该参数并运行多个 spec 套件，specs 运行的顺序会被随机化。 聚焦 spec 和跳过 spec： --skipMeasurements 如果设置该参数，Ginkgo 会跳过任何你定义的 Measure spec 。 --focus=REGEXP 如果设置该参数，Ginkgo 只会运行带有符合正则表达式 REGEXP 的描述的 spec。 --skip=REGEXP 如果设置该参数，Ginkgo 只会运行不有符合正则表达式 REGEXP 的描述的 spec。 运行竞态检测和测试覆盖率工具： -race 设置-race 来让 ginkgo CLI 使用竞态检测来运行测试。 -cover 设置-race 来让 ginkgo CLI 使用代码覆盖率分析工具来运行测试（Go 1.2+ 的功能）。Ginkgo 会在在个测试包的目录下生成名为PACKAGE.coverprofile 的代码覆盖文件。 -coverpkg=, -cover, -coverpkg 运行你的测试并开启代码覆盖率分析。然而， -coverpkg 允许你知道需要分析的包。它允许你获得当前包之外的包的代码覆盖率，这对集成测试很有用。注意，它默认不在当前包运行覆盖率分析，你需要制定所有你想分析的包。包名应该是全写，例如github.com/onsi/ginkgo/reporters/stenographer。 -coverprofile= 使用 FILENAME 重命名代码覆盖率文件的名字。 失败行为： --failOnPending 如果设置该参数，Ginkgo 会在有暂停 spec 的情况下使套件失败。 --failFast 如果设置该参数，Ginkgo 会在第一个 sepc 时候后立即停止套件。 不执行部分用例 您可以将单个Spec或容器标记为待定。这将阻止Spec（或者容器中的Specs）运行。您可以在您的Describe, Context, It 和 Measure前面添加一个P或者一个X来实现这一点：PDescribe(\"some behavior\", func() { ... }) PContext(\"some scenario\", func() { ... }) PIt(\"some assertion\") PMeasure(\"some measurement\") XDescribe(\"some behavior\", func() { ... }) XContext(\"some scenario\", func() { ... }) XIt(\"some assertion\") XMeasure(\"some measurement\") 默认，Ginkgo将会打出每一个处于Pending态的Spec的说明。您可以通过设置--noisyPendings=false标签来关闭它。 在编译时，使用P和X将规格标记为Pending态。如果您需要在运行时（可能是由于只能在运行时才知道约束）跳过一个规格。您可以在您的测试中调用Skip： It(\"should do something, if it can\", func() { if !someCondition { Skip(\"special condition wasn't met\") } // assertions go here }) 只执行部分用例 当开发的时候，运行规格的子集将会非常方便。Ginkgo有两种机制可以让您专注于特定规格： 1，您可以在Descirbe, Context 和 It前面添加F以编程方式专注于单个规格或者整个容器的规格： FDescribe(\"some behavior\", func() { ... }) FContext(\"some scenario\", func() { ... }) FIt(\"some assertion\", func() { ... }) 这样做是为了指示Ginkgo只运行这些规格。要运行所有规格，您需要退回去并删除所有的F。 2，您可以使用--focus = REGEXP和/或--skip = REGEXP标签来传递正则表达式。Ginkgo只运行 focus 正则表达式匹配的规格，不运行skip正则表达式匹配的规格。 3，为了防止规格不能在测试组之间提供足够的等级区分，可以通过--regexScansFilePath选项，将目录加载到focus和skip的匹配中。也就是说，如果测试的初始代码位置是test/a/b/c/my_test.go，可以将--focus=/b/和--regexScansFilePath=true结合起来，专注于包含路径/b/的测试。此功能对于在创建这些测试的原始目录的行中过滤二进制工件中的测试是十分有用的。但理想情况下，您应该遵循最大限度地减少使用此功能的需求来组织您的规格。 当Ginkgo检测到以编程式为测试中心的测试组件时，它将以非零状态码退出。这有助于检测CI系统上错误提交的重点测试。当传入命令行focus/skip标志时，Ginkgo以0状态码退出。如果要将测试集中在CI系统上，则应该显示地传入-focus或-skip标志。 嵌套的以编程方式为重点的规格遵循一个简单的规则：如果叶子节点被标记为重点，那么它的被标记为重点的任何根结点将变为非重点。根据这个规则，标记为重点的兄弟叶子节点（无论相对深度如何），将会运行无论共享的根结点是否是重点；非重点的兄弟节点将不会运行无论共享的根结点或者相对深度的兄弟姐妹是否是重点。更简单地： FDescribe(\"outer describe\", func() { It(\"A\", func() { ... }) It(\"B\", func() { ... }) }) 将会运行所有的It，但是： FDescribe(\"outer describe\", func() { It(\"A\", func() { ... }) FIt(\"B\", func() { ... }) }) 只会运行B，这种行为倾向于更紧密地反应开发人员在测试套件上进行迭代时的实际意图。 程序化方法和--focus=REGEXP/--skip=REGEXP方法是互斥的。使用命令行标志将覆盖程序化的重点。 专注于没有It或者Measure的叶子节点的容器是没有意义的。由于容器中没有任何东西可以运行，因此实际上，Ginkgo忽略了它。 使用命令行标志时，您可以指定--focus和--skip中的一个或两个。如果都指定了，则他们的限制将都会生效。 您可以通过运行ginkgo unfocus来取消以编程为中心的测试的关注。这将从您当前目录中可能具有任何FDescribe，FContext和FIt的测试中删除F。 如果你想跳过整个包（当使用-r标志递归运行ginkgo时），你可以将逗号分隔的列表传递给--skipPackage = PACKAGES, TO, SKIP。包含列表中目录的任何包都将会被忽略。 基准测试 Ginkgo 允许你使用Measure块来测量你的代码的性能。Measure块可以运行在任何It块可以运行的地方--每一个Meature生成一个规格。传递给Measure的闭包函数必须使用Benchmarker参数。Benchmarker用于测量运行时间并记录任意数值。你也必须在该闭包函数之后传递一个整型参数给Measure，它表示Measure将执行的你的代码的样本数。例如： Measure(\"it should do something hard efficiently\", func(b Benchmarker) { runtime := b.Time(\"runtime\", func() { output := SomethingHard() Expect(output).To(Equal(17)) }) Ω(runtime.Seconds()).Should(BeNumerically(\"Ginkgo与持续集成 ginkgo -r --randomizeAllSpecs --randomizeSuites --failOnPending --cover --trace --race --progress 编写自定义报告器 因为 Ginkgo 的默认报告器提供了全面的功能， Ginkgo 很容易同时写和运行多个自定义报告器。这有很多使用案例。你能实现一个自定义报告器使你的持续集成方案支持一个特殊的输出格式，或者你能实现一个自定义报告器从Ginkgo Measure 节点聚合数据 和制造 HTML 或 CSV 报告（或者甚至图表！）。 在 Ginkgo 中，一个报告器必须满足 Reporter 接口： type Reporter interface { SpecSuiteWillBegin(config config.GinkgoConfigType, summary *types.SuiteSummary) BeforeSuiteDidRun(setupSummary *types.SetupSummary) SpecWillRun(specSummary *types.SpecSummary) SpecDidComplete(specSummary *types.SpecSummary) AfterSuiteDidRun(setupSummary *types.SetupSummary) SpecSuiteDidEnd(summary *types.SuiteSummary) } 方法的名字应该能是自解释的。为了使你获得合理可用的数据，确保深入理解 SuiteSummary 和 SpecSummary 。如果你写了一个自定义报告器，用于获取 Measure 节点产生的基准测试数据，你会想看看 ExampleSummary.Measurements 提供的结构体 ExampleMeasurement 。 一旦你创建了自定义报告器，你可能要替换你测试套件中的RunSpecs命令，来传入该实例到 Ginkgo，要么这样： RunSpecsWithDefaultAndCustomReporters(t *testing.T, description string, reporters []Reporter) 要么这样 RunSpecsWithCustomReporters(t *testing.T, description string, reporters []Reporter) RunSpecsWithDefaultAndCustomReporters 会运行你的自定义报告器和 Ginkgo 默认报告器。RunSpecsWithCustomReporters 只会运行你的自定义报告器。 如果你希望运行并行测试，你不应该使用 RunSpecsWithCustomReporters，因为默认报告器是 ginkgo CLI 测试输出流的重要角色。 生成 JUnit XML 的输出。 ​ Ginkgo 提供了一个 自定义报告器 来生成 JUnit 兼容的 XML 输出。这是一个示例引导文件，该文件实例化了JUnit报告程序并将其传递给测试运行器： package foo_test import ( . \"github.com/onsi/ginkgo\" . \"github.com/onsi/gomega\" \"github.com/onsi/ginkgo/reporters\" \"testing\" ) func TestFoo(t *testing.T) { RegisterFailHandler(Fail) junitReporter := reporters.NewJUnitReporter(\"junit.xml\") RunSpecsWithDefaultAndCustomReporters(t, \"Foo Suite\", []Reporter{junitReporter}) } ​ 这会在包含你测试的目录中生成一个名为 “junit.xml” 的文件。这个 xml 文件兼容最新版本的 Jenkins JUnit 插件。 如果你想要并行运行你的测试，你需要让你的 JUnit xml 文件带有并行节点号。你可以这样做： junitReporter := reporters.NewJUnitReporter(fmt.Sprintf(\"junit_%d.xml\", config.GinkgoConfig.ParallelNode)) ​ 注意，你需要导入 fmt 和 github.com/onsi/ginkgo/config ，以使其正常工作。这会为每个并行节点生成一个 xml 文件。 Jenkins JUnit 插件（举例） 会自动聚合所有这些文件的数据。 项目实践 在model 文件夹下构建moke 变量， model 包原则上只能被其他包导入，不可以导入其他包。所以把moke数据放在这里，不管你的单元测试文件放在哪里都不会出现循环导包的问题。 单元测试仍然使用 *testing.T测试（方便Idea 上单个方法测试）， 集成测试使用ginkgo（方便多个用例串行测试）。 注册 package e2e import ( .... ) func TestCluster(t *testing.T) { defer GinkgoRecover() RegisterFailHandler(Fail) //RunSpecs(t, \"Cluster Suite\") junitReporter := reporters.NewJUnitReporter(\"./junit.xml\") RunSpecsWithDefaultAndCustomReporters(t, \"APIServer test\", []Reporter{junitReporter}) } 添加所有测试用例的一次性执行的前置条件 package e2e import ( .... ) var _ = Describe(\"Cluster\", func() { // 在所有测试用例执行前执行 BeforeSuite(func() { SetupOamStatusCheck() gateway.SetUpGatewaysStatusCheck() setClient(moke.TestClusterIdStr) }) // 此Describe 用例测试前 准备环境 It(\"cluster test prepare env\", func() { deleteApp(\"\", \"\") deleteGateway(\"\", moke.TestClusterName) unImportCluster() }) // 此Describe 下每个It 之前执行 BeforeEach(func() { }) // 此Describe 下每个It 之后执行 AfterEach(func() { }) // 一个接口用一个Context测试 Context(\"sync cluster\", func() { It(\"sync cluster\", func() { .... }) It(\"sync cluster: subUser\", func() { .... }) // 鉴权的行为由IAM服务控制，所以这里仍然可以查询到 It(\"sync cluster: subUser no any auth\", func() { .... }) }) Context(\"集群导入, 取消导入\", func() { It(\"未授权的用户导入集群失败\", func() { .... }) It(\"异常1: 正在导入中的集群，取消导入失败\", func() { .... }) It(\"异常2: 集群重复导入\", func() { .... }) }) Context(\"unImport\", func() { // 不要在It外，Contex内有代码，否则会在程序初始化的时候执行，产生意想不到的异常。 It(\"\", func() { .... }) }) }) 不要在It外，Contex内有代码，否则会在程序初始化的时候执行，产生意想不到的异常。 FQA Q: 如何对单个文件测试？单个文件测试会出现自定义函数 undefined。 A: 在Descirbe, Context 和 It前面添加F以编程方式专注于单个规格或者整个容器的规格。 Q :如何将文件按顺序测试？ A: 文件默认是随机测试，不建议顺序测试。可以发现不同的测试文件之间是否有污染。 Q: Context，IT 测试顺序是怎样的？ A: Context ，IT 都是按顺序执行。 Q:有任务发生失败或只执行了部分测试用例，都无法获取覆盖率？ A: ... link gomega ginkgo 中文文档 ginkgo EN doc Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/ginkgo/ginkgo_cmd.html":{"url":"blog/测试/ginkgo/ginkgo_cmd.html","title":"Ginkgo Cmd","keywords":"","body":"Ginkgo cmd 可以通过如下命令来安装Ginkgo命令： $ go install github.com/onsi/ginkgo/ginkgo Ginkgo 比 go test 提供了更多方便的指令。推荐使用 Ginkgo 命令虽然这不是必需的。 运行测试 在当前目录下运行该套件，只需： $ ginkgo #or go test 在其它目录下运行该套件，只需： $ ginkgo /path/to/package /path/to/other/package ... 传递参数和特定的标签到该测试套件： $ ginkgo -- 注意：这个”–“是重要的。只有该双横线后面的参数才会被传递到测试套件。要在你的测试套件中解析参数和特定标签，需要声明一个变量并在包级别初始化它： var myFlag string func init() { flag.StringVar(&myFlag, \"myFlag\", \"defaultvalue\", \"myFlag is used to control my behavior\") } 当然，Ginkgo使用一些标签。在运行指定的包之前必须指定这些标签。以下是调用语法的摘要： $ ginkgo -- 下面是Ginkgo可以接受的一些参数： 指定运行哪些测试套件： -r 使用-r递归运行目标文件夹下的所有测试套件。适用于在所有包中运行所有测试。 -skipPackage=PACKAGES,TO,SKIP 当运行带有 -r 的测试，你可以传递一个逗号分隔的条目列表给 -skipPackage 。任何包的路径如果含有逗号分隔的条目列表之一就会被跳过。 并行测试： -p 设置 -p 可以并行运行测试套件并自动经检测节点数。 --nodes=NODE_TOTAL 使用这个可以并行运行测试套件并使用 NODE_TOTAL 个数的进程。你不需要指定-p （尽管你可以！）。 -stream 默认地，当你并行运行测试套件，测试执行器从每个并行节点聚合数据，在运行测试的时候产生连贯的输出。设置 stream为 true，则会实时以流形式输出所有并行节点日志，每行头都会带有相应节点 id 。 修改输出： --noColor 如果提供该参数，Ginkgo 默认不使用多种颜色打印报告。 --succinct Succinct （简洁）会静默 Ginkgo 的详情输出。成功执行的测试套件基本上只会打印一行！当在一个包中运行测试的时候，Succinct 默认关闭。它在 Ginkgo 运行多个测试包的时候默认打开。 --v 如果设置该参数， Ginkgo 默认报告会在每个 spec 运行前打印文本和位置。同时，GinkgoWriter 会实时刷新输出到标准输出。 --noisyPendings=false 默认情况下，Ginkgo 默认报告会提供暂停 spec 的详情输出。你可以设置--noisyPendings=false 来禁止该行为。 --noisySkippings=false 默认情况下，Ginkgo 默认报告会提供跳过 spec 的详情输出。你可以设置--noisySkippings=false 来禁止该行为。 --reportPassed 如果设置该参数，Ginkgo 默认报告会提供通过 spec 的详情输出。 --reportFile= 在指定路径(相对路径或绝对路径)创建报告输出文件。它会同时覆盖预设的ginkgo.Reporter 路径，并且父目录不存在的话会被创建。 --trace 如果设置该参数，Ginkgo 默认报告会为每个失败打印全栈跟踪日志，不仅仅打印失败发现的行号。 --progress 如果设置该参数，当 Ginkgo 进入并运行每个 BeforeEach, AfterEach, It 节点的时候，Ginkgo 会输出过程到 GinkgoWriter。这在调试被卡主的测试时（例如测试卡在哪里？），或使用测试输出更多易读的日志到GinkgoWriter （例如什么日志在BeforeEach中输出？什么日志在It中输出？）。结合 --v 输出 --progress 日志到标准输出。 控制随机性： --seed=SEED 变换 spec 顺序时使用的随机种子。 --randomizeAllSpecs 如果设置该参数，所有 spec 都会被重新排序。默认 Ginkgo 只会改变顶层容器的顺序。 --randomizeSuites 如果设置该参数并运行多个 spec 套件，specs 运行的顺序会被随机化。 聚焦 spec 和跳过 spec： --skipMeasurements 如果设置该参数，Ginkgo 会跳过任何你定义的 Measure spec 。 --focus=REGEXP 如果设置该参数，Ginkgo 只会运行带有符合正则表达式 REGEXP 的描述的 spec。 --skip=REGEXP 如果设置该参数，Ginkgo 只会运行不有符合正则表达式 REGEXP 的描述的 spec。 运行竞态检测和测试覆盖率工具： -race 设置-race 来让 ginkgo CLI 使用竞态检测来运行测试。 -cover 设置-race 来让 ginkgo CLI 使用代码覆盖率分析工具来运行测试（Go 1.2+ 的功能）。Ginkgo 会在在个测试包的目录下生成名为PACKAGE.coverprofile 的代码覆盖文件。 -coverpkg=, -cover, -coverpkg 运行你的测试并开启代码覆盖率分析。然而， -coverpkg 允许你知道需要分析的包。它允许你获得当前包之外的包的代码覆盖率，这对集成测试很有用。注意，它默认不在当前包运行覆盖率分析，你需要制定所有你想分析的包。包名应该是全写，例如github.com/onsi/ginkgo/reporters/stenographer。 -coverprofile= 使用 FILENAME 重命名代码覆盖率文件的名字。 -outputdir= 将覆盖率输出文件移到到指定目录。 结合-coverprofile 参数也能使用。 构建参数： -tags 设置-tags来传递 标识到编译步骤。 -compilers 当编译多个测试套件（如 ginkgo -r），Ginkgo 会使用 runtime.NumCPU() 绝对启动的编译进程数。在一些环境中这不是个好主意。你可以通过这个参数手动指定编译器进程数。 失败行为： --failOnPending 如果设置该参数，Ginkgo 会在有暂停 spec 的情况下使套件失败。 --failFast 如果设置该参数，Ginkgo 会在第一个 sepc 时候后立即停止套件。 监视参数： --depth=DEPTH 当监视包的时候，Ginkgo 同时监视包依赖的变化。默认的 --depth 为 1 ，意味着只有直接依赖的包被监控。你能调整它到 依赖的依赖（dependencies-of-dependencies），或者设置为零就只监控它自己，不监控依赖。 --watchRegExp=WATCH_REG_EXP 当监视包的时候，Ginkgo只监控符合该正则表达式的文件。默认值是\\.go$ ，意味着只有 go 文件的变化会被监视。 减少随机失败的测试(flaky test): --flakeAttempts=ATTEMPTS 如果一个测试失败了，Ginkgo 能马上返回。设置这个参数大于 1 的话会重试。只要一个重试成功，Ginkgo 就不会认为测试套件失败。单独失败的运行仍会被报告在输出中；举个例子，JUnit 输出中，会声称 0 失败（因为套件通过了），但是仍会包含一个同时失败和成功的测试的所有失败的运行。 这个参数很危险！不要试图使用它来掩盖失败的测试！ 杂项： -dryRun 如果设置该参数，Ginkgo 会遍历你的测试套件并报告输出，但是不会真正运行你的测试。这最好搭配-v来预览你将运行的测试。测试的顺序遵循了 --seed 和 --randomizeAllSpecs 指定的随机策略。 -keepGoing 默认地，当多个测试运行的时候（使用 -r或一列表的包），Ginkgo 在一个测试失败的时候会中断。要让 Ginkgo 时候后继续接下来的测试套件，你可以设置 -keepGoing。 -untilItFails 如果设置为 true，Ginkgo 会持续运行测试直到发送失败。这会有助于弄明白竞态条件或者古怪测试。最好搭配 --randomizeAllSpecs 和 --randomizeSuites 来变换迭代的测试顺序。 -notify 设置 -notify 来接受桌面测试套件完成的通知。结合子命令 watch 特别有用。当前 -notify 只有 OS X 和 Linux 支持。在 OS X 上，你需要运行 brew install terminal-notifier 来接受通知，在 Linux 你需要下载安装 notify-send。 --slowSpecThreshold=TIME_IN_SECONDS 默认地，Ginkgo报告器会表示运行超过 5 秒的测试，这不会使测试失败，它只是通知你该 sepc 运行慢。你可以使用这个参数修改该门槛。 -timeout=DURATION 如果时间超过 DURATION ，Ginkgo 会使测试套件失败。默认值是 24 小时。 --afterSuiteHook=HOOK_COMMAND Ginko 有能力在套件测试结束后运行一个命令（a command hook）。你只需给它需要运行的命令，它就会替换字符串来传给命令数据。举例：–afterSuiteHook=”echo (ginkgo-suite-name) suite tests have [(ginkgo-suite-passed)]”，这个测试沟子会替换 (ginkgo-suite-name) 和 (ginkgo-suite-passed) 为套件名和各自的通过/失败状态，然后输出到终端。 -requireSuite 如果你使用 Ginkgo 测试文件创建包，但是你忘了运行 ginkgo bootstrap 初始化，你的测试不会运行而且该套件会一致通过。Ginkgo 会通知你 Found no test suites, did you forget to run \"ginkgo bootstrap\"? ，但是不会失败。如果有测试文件但没有引用RunSpecs.，这个参数使得 Ginkgo 标识套件为失败。 监视修改 Ginkgo CLI 提供子命令 watch ，监视（几乎）所有的 ginkgo 命令参数。使用ginkgo watch ，Ginkgo 会监控当前目录的包，当有修改的时候就触发测试。 你也可以使用 ginkgo watch -r 递归监控所有包。 对每个被监控的包，Ginkgo 也会监控包的依赖并在依赖产生修改的时候触发测试套件。默认地，ginkgo watch 监控包的直接依赖。你可以使用 -depth 来调整。设置 -depth 为0则不监控依赖，设置 -depth 大于 1 则监控更深依赖路径。 在 Linux 或 OS X 传递 -notify 参数，会在 ginkgo watch 触发和完成测试的时候产生桌面通知。 预编译测试 Ginkgo 对写集成风格的验收测试（integration-style acceptance tests）有强力的支持。比如，这些测试有助于验证一个复杂分布式系统的函数是否正确。它常便于分布这些作为单独二进制文件的验收测试。 Ginkgo 允许你这样构建这些二进制文件： ginkgo build path/to/package 这会产生一个名为 package.test的预编译二进制文件。然后，你能直接调用 package.test 来运行测试套件。原理很简单， ginkgo 只是调用 go test -c -o 来编译 package.test 二进制文件。 直接调用 package.test 会连续运行测试。要并行测试的话，你需要 ginkgo cli 编排并行节点。你可以运行： ginkgo -p path/to/package.test 来这样做。因为 Ginkgo CLI 是一个单独二进制文件，你能直接分布两个二进制文件，来提供一个并行(所以快速)的集成风格验收测试集合。 build子命令接受一系列 ginkgo 和 ginkgo watch 接收的参数。这些参数仅限关注于编译时，就像 --cover 和 --race。通过 ginkgo help build，你能获得更多信息。 使用标准 GOOS 和 GOARCH 环境变量，你能交叉编译并面向不同平台。因此，在 OS X 上运行 GOOS=linux GOARCH=amd64 ginkgo build path/to/package ，会产生一个能在 Linux 上运行的二进制文件。 生成器 在当前目录，为一个包引导 Ginkgo 测试套件，可以运行： $ ginkgo bootstrap 这会生成一个名为 PACKAGE_suite_test.go 的文件，PACKAGE 是当前目录的名称。 如要添加一个测试文件，运行： $ ginkgo generate 这会生成一个名为 SUBJECT_test.go 的文件。如果你不指定 SUBJECT ，它会生成一个名为 PACKAGE_test.go 的文件，PACKAGE 是当前目录的名称。 默认地，这些生成器会点引用（dot-import）Ginkgo 和 Gomega。想避免点导入，你可以传入 --nodot 到两个子命令。详情请看 下一章 注意，你不是必须使用这两个生成器。他们是方便你快速初始化。 避免点导入 Ginkgo 和 Gomega 提供了一个 DSL ，而且，默认地 ginkgo bootstrap 和 ginkgo generate 命令使用点导入导入两个包到顶层命名空间。 有少许确定的情况，你需要避免点导入。例如，你的代码可能定义了与 Ginkgo 或 Gomega 方法冲突的方法名。这中情况下，你可以将你的代码导入到自己的命名空间（换言之，移除导入你的包签名的 .）。或者，你可以移除 Ginkgo 或 Gomega 签名的 .。后者会导致你一直要在 Describe 和 It 前面加 ginkgo. ，并且你的 Expect 和 ContainSubstring前面也都要加 gomega. 。 然而，这是第三个 ginkgo CLI 提供的选项。如果你需要（或想要）避免点导入你可以： ginkgo bootstrap --nodot 和 ginkgo generate --nodot 这会创建一个引导文件，明确地在顶级命名空间，导入所有 Ginkgo 和 Gomega 的导出标识符。这出现在你引导文件的地步，生成的代码就像这样： import ( github.com/onsi/ginkgo ... ) ... // Declarations for Ginkgo DSL var Describe = ginkgo.Describe var Context = ginkgo.Context var It = ginkgo.It // etc... 这允许你使用 Describe, Context, 和 It写测试，而不用添加 ginkgo.前缀。关键地，它同时允许你冲定义任何冲突的标识符（或组织你自己的语意）。例如： var _ = ginkgo.Describe var When = ginkgo.Context var Then = ginkgo.It 这会避免导入Describe，并会将Context 和 It 重命名为 When 和 Then。 当新匹配库被添加到 Gomega ，你需要更新这些导入的标识符。你可以这样，进入包含引导文件的目录并运行： ginkgo nodot 这会更新导入，保留你提供的重命名。 转换已存在的测试 如果你有一个 XUnit 测试套件，而且你想把它转化为 Ginkgo 套件，你可以使用 ginkgo convert 命令： ginkgo convert github.com/your/package 这会生成一个 Ginkgo 引导文件，转化所有 XUnit 风格 TestX...(t *testing.T) 为简单（平坦）的 Ginkgo 测试。它同时将你代码中的 GinkgoT() 替换为 *testing.T 。 ginkgo convert 一般第一次就能正确转换，但事后你可能需要微调一下测试。 同时： ginkgo convert 会覆盖 你的测试文件，因此确保你尝试 ginkgo convert 之前，已经没有未提交的修改了。 ginkgo convert 是Tim Jarratt 的主意。 其它子命令 将当前目录(和子目录)写入代码的重点测试设为普通测试： $ ginkgo unfocus 查看帮助： $ ginkgo help 查看特定子目录的帮助： $ ginkgo help 获取当前 Ginkgo 的版本： $ ginkgo version Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/ginkgo/readme.html":{"url":"blog/测试/ginkgo/readme.html","title":"Readme","keywords":"","body":"ginkgo Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/golang-test.html":{"url":"blog/测试/golang-test.html","title":"Golang Test","keywords":"","body":" 有赞 GO 项目单测、集成、增量覆盖率统计与分析 一、引言 二、单测覆盖率以及静态代码分析 2.1､单测覆盖率分析 2.2､静态代码分析 三、集成测试覆盖率分析 3.1､解决方法 3.2､缺点 3.3､优化 3.4､jenkins 报告 四、集成测试增量覆盖率分析 4.1､diff_cover 4.2､安装 4.3､使用方式 4.4､报告 有赞 GO 项目单测、集成、增量覆盖率统计与分析 一、引言 我是一名中间件 QA，我对应的研发团队是有赞 PaaS，目前我们团队有很多产品是使用 go 语言开发，因此我对 go 语言项目的单测覆盖率、集成以及增量测试覆盖率统计与分析做了探索。 二、单测覆盖率以及静态代码分析 2.1､单测覆盖率分析 测试覆盖率是指，作为被测试对象的代码包中的代码有多少在刚刚执行的测试中被使用到。如果执行的该测试致使当前代码包中的90%的语句都被执行了，那么该测试的测试覆盖率就是90%。 go test 命令可接受的与测试覆盖率有关的标记 标记名称 使用示例 说明 -cover -cover 启用测试覆盖率分析 -covermode -covermode=set 自动添加-cover标记并设置不同的测试覆盖率统计模式，支持的模式共有以下3个。set：只记录语句是否被执行过 count: 记录语句被执行的次数 atomic: 记录语句被执行的次数，并保证在并发执行时也能正确计数，但性能会受到一定的影响 这几个模式不可以被同时使用在,默认情况下，测试覆盖率的统计模式是set -coverpkg -coverpkg bufio,net 自动添加-cover标记并对该标记后罗列的代码包中的程序进行测试覆盖率统计。 在默认情况下，测试运行程序会只对被直接测试的代码包中的程序进行统计。 该标记意味着在测试中被间接使用到的其他代码包中的程序也可以被统计。 另外，代码包需要由它的导入路径指定，且多个导入路径之间以逗号“，”分隔。 -coverprofile -coverprofile cover.out 自动添加-cover标记并把所有通过的测试的覆盖率的概要写入指定的文件中 Go 语言自身提供了单元测试工具 go test，单元测试文件必须以 *_test.go 形式存在，go test 工具同时也提供了分析单测覆盖率的功能。因为需要将单测覆盖率上传到 sonar 平台展示，所以必须将覆盖率文件转换成能被 sonar 识别的格式，因此，还需要另外一个命令行工具 gocov。 首先我们使用 go test 生成覆盖率输出文件 cover.out，并通过 gocov 工具来将生成的覆盖率文件 cover.out 转换成可以被 sonar 识别的 Cobertura 格式的 xml 文件。 如下所示： go test -v ./... -coverprofile=cover.out #生成覆盖率输出 gocov convert cover.out | gocov-xml > coverage.xml #将覆盖率输出转换成xml格式的报告 将生成的单测覆盖率报告发送到 sonar 平台上来展示。 cover工具可接受的标记 标记名称 使用示例 说明 -func -func=cover.out 根据概要文件（即cover.out）中的内容，输出每一个被测试函数的测试覆盖率概要信息 -html -html=cover.out 把概要文件中的内容转换成HTML格式的文件，并使用当前操作系统中的默认网络浏览器查看它 -mode -mode=count 被用于设置测试概要文件的统计模式，详见go test命令的-covermode标记 -o -o=cover.out 把重写后的源代码输出到指定文件中，如果不添加此标记，那么重写后的源代码会输出到标准输出上 -var -var=GoCover 设置被添加到原先的源代码中的额外变量的名称 2.2､静态代码分析 Go 静态代码分析工具有两个，分别是 gometalinter 和 golangci-lint，我们现在使用的是 golangci-lint，因为 gometalinter 已经停止维护，而且作者也推荐去使用 golangci-lint。 2.2.1 golangci-lint 的安装 以下是安装 golangci-lint 推荐的两种方法： 将二进制文件安装在 (go env GOPATH)/bin/golangci-lint 目录下 curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s -- -b $(go env GOPATH)/bin vX.Y.Z 或者将二进制文件安装在 ./bin/ 目录下 curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s vX.Y.Z 安装完成之后可以通过使用golangci-lint --version来查看它的版本。 三、集成测试覆盖率分析 对于 Go 项目没有类似 java jacoco 这样的第三方测试工具，就算是开源的第三方工具，一般单元测试执行以及单测覆盖率分析都是使用 Go 自带的测试工具 go test 来执行的。 阅读了GO的官方博客之后发现其实针对二进制文件是有类似的工具 gcov。在文章中作者也说了，对于在 go 1.2 之前，其实也是使用类似 gcov 的方式对二进制程序在分支上设置断点，在每个分支执行时，将断点清除并将分支的目标语句标记为 “covered” 。 但是通过文章可以知道，在 go 1.2 之后是不支持使用此种方式，而且也不推荐使用 gcov 来统计覆盖率，因为执行二进制分析是很有挑战且很困难的，它还需要一种可靠的方式来执行跟踪绑定到源代码，这也很困难，这些问题包括不准确的调试信息和类似内联函数使分析复杂化，最重要的是，这种方法非常不便携。 3.1､解决方法 通过查找资料，发现了一个并不完美但是可以解决这个问题的方法。go test 中有一个 -c 的 flag，可以将单测的代码和被单测调用的代码编译成二进制包执行，但是这种方式并没有将整个项目的代码包含进去，不过可以通过增加一个测试文件 main_test.go，文件内容如下： func TestMainStart(t *testing.T) { var args []string for _, arg := range os.Args { if !strings.HasPrefix(arg, \"-test\") { args = append(args, arg) } } os.Args = args main() } 12345678910 将主函数放在此测试代码中，由于 Go 的入口函数是 main 函数，所以这样就会将整个 Go 项目都打包成一个已经插桩的二进制文件，如果项目启动的时候需要传入参数，则会将其中程序启动时传入的不是 -test标记的参数放入到os.Args 中传递给main 函数。以上代码也可以自己在测试文件中增加消息通知监听，来退出测试函数。 当集成测试跑完后就可以得到覆盖率代码，整个流程可参考下图： #第一步：执行集成测试，并将此函数编译成二进制文件 go test -coverpkg=\"./...\" -c -o cover.test #第二步：运行二进制文件，指定运行的测试方法是 TestMainStart，并将覆盖率报告输出 ./cover.test -test.run \"TestMainStart\" -test.coverprofile=cover.out #第三步：将输出的覆盖率报告转换成 html 文件（html 文件查看效果比较好） go tool cover -html cover.out -o cover.html #第四步：生成 Cobertura 格式的 xml 文件 gocov convert cover.out | gocov-xml > cover.xml 12345678 3.2､缺点 必须所有 Go 语言项目中新增一个这样的测试代码文件，才可以使用 必须退出进程才可以获得报告，但是如果测试程序是在 k8s 的 pod 中，一旦程序退出，pod 就会自动退出无法获取到文件 想要得到测试覆盖率数据不能像 jacoco 那样直接调用接口可以 dump 到本地，程序必须增加一个接收信号量的参数，保证主函数的退出，不然集成测试代码跑完，覆盖率信息是不会写到磁盘的 由于上面的原因，报告储存在远端，无法下载到当前 Jenkins 上，要去远端 dump 文件下来分析 不能将分布式的应用的数据结合起来之后做全量统计(只能跑单个应用) 以上缺陷在有赞paas团队通过一些不是特别优雅的方式解决,以下是解决方案 3.3､优化 ps：由于当前有赞 PaaS 的 ci 环境是在 k8s 集群中实现的，所以这里就针对 k8s中 的优化方案 3.3.1、针对编译前需要新增一个测试文件，包裹main函数 测试函数也是要求所有项目中增加一个测试文件，或者 Jenkins 编译部署镜像之前在 pipline 中生成一个文件 3.3.2、针对以上必须程序退出才可以或许到测试覆盖率报告的缺点： 假设 k8s 基础镜像中已经装好 python，我在启动 pod 的时候默认启动两个服务，一个是被测试的服务，一个是 python 启动的 http 服务。 然后将项目服务的启动写入脚本中，并在 deployment 中通过 nohup 启动服务，并再启动一个 python 服务 spec: containers: - command: - /bin/bash - -c - (nohup /data/project/start.sh &);(cd python && -m SimpleHTTPServer 12345) image: $imageAddress 1234567 杀死项目服务后，因为还有 python 服务在，pod 不会退出，可以拿到覆盖率测试报告 3.3.3、覆盖率报告在远端，如何在跑完Jenkins任务后来直接获取到报告： 可以在跑集成测试后通过执行 http 请求来获取容器内的 cover.out，比如 wget http://{ip}:{port}/{path}/cover.out，并将此覆盖率报告编译成 Cobertura 格式的 xml，放入到 Jenkins 中统计。 如果是执行了多个服务端，需要合并覆盖率报告，可以使用 gocovmerge 3.3.4、如何在k8s中自动化kill程序让其退出： 对于退出程序可以直接在集成测试代码中使用 kubectl 命令将 pod 中的程序 kill pid=`kubectl exec $podname -c $container -n dts -- ps -ef | grep $process | grep -v grep | awk '{print $2}'` kubectl exec $podname -c $container -n $namespace -- kill $pid 3.4､jenkins 报告 四、集成测试增量覆盖率分析 4.1､diff_cover 增量覆盖率分析我们选择了开源工具 diffover，diffcover 是用 python 开发，通过 git diff 来对比当前分支和需要比对的分支，主要针对新增代码做覆盖率分析。 4.2､安装 安装 diff_cover的机器需要有 python 的环境，有两种安装方式: 1、通过pip 来直接下载安装 pip install diff_cover 2、通过源代码安装 pip install diff_covers 4.3､使用方式 ps：必须在需要对比的项目目录下运行！！！ 4.3.1 生成单元测试覆盖率报告 go test -v ./... -coverprofile=cover.out gocov convert cover.out | gocov-xml > coverage.xml 4.3.2 增量覆盖率分析 diff-cover coverage.xml --compare-branch=xxxx --html-report report.html --compare-branch：是选择需要对比的分支号 --html-report：是将增量测试报告生成 html 的报告模式 除了以上参数，此工具还有很多其他参数，比如 --fail-under：覆盖率低于某个值，返回非零状态代码 --diff-range-notation：设置 diff 的范围,就是 git diff {compare-branch} {diff-range-notation} 的作用等等。 具体可以通过 diff_cover -h 来获得更多详细的信息 4.4､报告 命令行展示 HTML展示 表格中可以看到当前分支覆盖率与选定分支覆盖率的差异。 link -Go测试之性能监控与代码覆盖率 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/http测试.html":{"url":"blog/测试/http测试.html","title":"Http测试","keywords":"","body":"1. httptest - HTTP 测试辅助工具 由于 Go 标准库的强大支持，Go 可以很容易的进行 Web 开发。为此，Go 标准库专门提供了 net/http/httptest 包专门用于进行 http Web 开发测试。 本节我们通过一个社区帖子的增删改查的例子来学习该包。 1.1. 简单的 Web 应用 我们首先构建一个简单的 Web 应用。 为了简单起见，数据保存在内存，并且没有考虑并发问题。 // 保存 Topic，没有考虑并发问题 var TopicCache = make([]*Topic, 0, 16) type Topic struct { Id int `json:\"id\"` Title string `json:\"title\"` Content string `json:\"content\"` CreatedAt time.Time `json:\"created_at\"` } 对于 Topic 的增删改查代码很简单，可以查看完整代码。 接下来，是通过 net/http 包来实现一个 Web 应用。 func main() { http.HandleFunc(\"/topic/\", handleRequest) http.ListenAndServe(\":2017\", nil) } ... /topic/ 开头的请求都交由 handleRequest 处理，它根据不同的 Method 执行相应的增删改查，详细代码可以查看 server.go。 准备好 Web 应用后，我们启动它。 go run server.go data.go 通过 curl 进行简单的测试： 增：curl -i -X POST http://localhost:2017/topic/ -H 'content-type: application/json' -d '{\"title\":\"The Go Standard Library\",\"content\":\"It contains many packages.\"}' 查：curl -i -X GET http://localhost:2017/topic/1 改：curl -i -X PUT http://localhost:2017/topic/1 -H 'content-type: application/json' -d '{\"title\":\"The Go Standard Library By Example\",\"content\":\"It contains many packages, enjoying it.\"}' 删：curl -i -X DELETE http://localhost:2017/topic/1 1.2. 通过 httptest 进行测试 上面，我们通过 curl 对我们的 Web 应用的接口进行了测试。现在，我们通过 net/http/httptest 包进行测试。 我们先测试创建帖子，也就是测试 handlePost 函数。 func TestHandlePost(t *testing.T) { mux := http.NewServeMux() mux.HandleFunc(\"/topic/\", handleRequest) reader := strings.NewReader(`{\"title\":\"The Go Standard Library\",\"content\":\"It contains many packages.\"}`) r, _ := http.NewRequest(http.MethodPost, \"/topic/\", reader) w := httptest.NewRecorder() mux.ServeHTTP(w, r) resp := w.Result() if resp.StatusCode != http.StatusOK { t.Errorf(\"Response code is %v\", resp.StatusCode) } } 首先跟待测试代码一样，配置上路由，对 /topic/ 的请求都交由 handleRequest 处理。 mux := http.NewServeMux() mux.HandleFunc(\"/topic/\", handleRequest) 因为 handlePost 的函数签名是 func handlePost(w http.ResponseWriter, r *http.Request) error，为了测试它，我们必须创建 http.ResponseWriter 和 http.Request 的实例。 接下来的代码就是创建一个 http.Request 实例 和一个 http.ResponseWriter 的实例。这里的关键是，通过 httptest.NewRecorder() 可以获得 httptest.ResponseRecorder 结构，而此结构实现了http.ResponseWriter 接口。 reader := strings.NewReader(`{\"title\":\"The Go Standard Library\",\"content\":\"It contains many packages.\"}`) r, _ := http.NewRequest(http.MethodPost, \"/topic/\", reader) w := httptest.NewRecorder() 准备好之后，可以测试目标函数了。这里，我们没有直接调用 handlePost(w, r)，而是调用 mux.ServeHTTP(w, r)，实际上这里直接调用 handlePost(w, r) 也是可以的，但调用 mux.ServeHTTP(w, r) 会更完整地测试整个流程。mux.ServeHTTP(w, r) 最终也会调用到 handlePost(w, r)。 最后，通过 go test -v 运行测试。 查、改和删帖子的接口测试代码类似，比如，handleGet 的测试代码如下： func TestHandleGet(t *testing.T) { mux := http.NewServeMux() mux.HandleFunc(\"/topic/\", handleRequest) r, _ := http.NewRequest(http.MethodGet, \"/topic/1\", nil) w := httptest.NewRecorder() mux.ServeHTTP(w, r) resp := w.Result() if resp.StatusCode != http.StatusOK { t.Errorf(\"Response code is %v\", resp.StatusCode) } topic := new(Topic) json.Unmarshal(w.Body.Bytes(), topic) if topic.Id != 1 { t.Errorf(\"Cannot get topic\") } } 注意：因为数据没有落地存储，为了保证后面的测试正常，请将 TestHandlePost 放在最前面。 1.3. 测试代码改进 细心的朋友应该会发现，上面的测试代码有重复，比如： mux := http.NewServeMux() mux.HandleFunc(\"/topic/\", handleRequest) 以及： w := httptest.NewRecorder() 这正好是前面学习的 setup 可以做的事情，因此可以使用 TestMain 来做重构。 var w *httptest.ResponseRecorder func TestMain(m *testing.M) { http.DefaultServeMux.HandleFunc(\"/topic/\", handleRequest) w = httptest.NewRecorder() os.Exit(m.Run()) } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/readme.html":{"url":"blog/测试/readme.html","title":"Readme","keywords":"","body":"测试 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/其他功能.html":{"url":"blog/测试/其他功能.html","title":"其他功能","keywords":"","body":"1. testing - 其他功能 1.1. TestMain 在写测试时，有时需要在测试之前或之后进行额外的设置（setup）或拆卸（teardown）；有时，测试还需要控制在主线程上运行的代码。为了支持这些需求，testing 包提供了 TestMain 函数 : func TestMain(m *testing.M) 如果测试文件中包含该函数，那么生成的测试将调用 TestMain(m)，而不是直接运行测试。TestMain 运行在主 goroutine 中 , 可以在调用 m.Run 前后做任何设置和拆卸。注意，在 TestMain 函数的最后，应该使用 m.Run 的返回值作为参数去调用 os.Exit。 另外，在调用 TestMain 时 , flag.Parse 并没有被调用。所以，如果 TestMain 依赖于 command-line 标志（包括 testing 包的标志），则应该显式地调用 flag.Parse。注意，这里的依赖是指，若 TestMain 函数内需要用到 command-line 标志，则必须显式地调用 flag.Parse，否则不需要，因为 m.Run 中调用 flag.Parse。 一个包含 TestMain 的例子如下： package mytestmain import ( \"flag\" \"fmt\" \"os\" \"testing\" ) var db struct { Dns string } func TestMain(m *testing.M) { db.Dns = os.Getenv(\"DATABASE_DNS\") if db.Dns == \"\" { db.Dns = \"root:123456@tcp(localhost:3306)/?charset=utf8&parseTime=True&loc=Local\" } flag.Parse() exitCode := m.Run() db.Dns = \"\" // 退出 os.Exit(exitCode) } func TestDatabase(t *testing.T) { fmt.Println(db.Dns) } 对 m.Run 感兴趣的可以阅读源码，了解其原理。 1.2. Test Coverage 测试覆盖率，这里讨论的是基于代码的测试覆盖率。 Go 从 1.2 开始，引入了对测试覆盖率的支持，使用的是与 cover 相关的工具（go test -cover、go tool cover）。虽然 testing 包提供了 cover 相关函数，不过它们是给 cover 的工具使用的。 关于测试覆盖率的更多信息，可以参考官方的博文：The cover story Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/单元测试.html":{"url":"blog/测试/单元测试.html","title":"单元测试","keywords":"","body":"单元测试 [toc] testing 为 Go 语言 package 提供自动化测试的支持。通过 go test 命令，能够自动执行如下形式的任何函数： func TestXxx(*testing.T) 注意：Xxx 可以是任何字母数字字符串，但是第一个字母不能是小写字母。 在这些函数中，使用 Error、Fail 或相关方法来发出失败信号。 要编写一个新的测试套件，需要创建一个名称以 _test.go 结尾的文件，该文件包含 TestXxx 函数，如上所述。 将该文件放在与被测试文件相同的包中。该文件将被排除在正常的程序包之外，但在运行 go test 命令时将被包含。 有关详细信息，请运行 go help test 和 go help testflag 了解。 如果有需要，可以调用 *T 和 *B 的 Skip 方法，跳过该测试或基准测试： func TestTimeConsuming(t *testing.T) { if testing.Short() { t.Skip(\"skipping test in short mode.\") } ... } 1. 单元测试一般格式 1.1. 第一个单元测试 要测试的代码： func Fib(n int) int { if n 测试代码： func TestFib(t *testing.T) { var ( in = 7 expected = 13 ) actual := Fib(in) if actual != expected { t.Errorf(\"Fib(%d) = %d; expected %d\", in, actual, expected) } } 执行 go test .，输出： $ go test . ok chapter09/testing 0.007s 表示测试通过。 我们将 Sum 函数改为： func Fib(n int) int { if n 再执行 go test .，输出： $ go test . --- FAIL: TestSum (0.00s) t_test.go:16: Fib(10) = 64; expected 13 FAIL FAIL chapter09/testing 0.009s 1.2. Table-Driven Test 测试讲究 case 覆盖，按上面的方式，当我们要覆盖更多 case 时，显然通过修改代码的方式很笨拙。这时我们可以采用 Table-Driven 的方式写测试，标准库中有很多测试是使用这种方式写的。 func TestFib(t *testing.T) { var fibTests = []struct { in int // input expected int // expected result }{ {1, 1}, {2, 1}, {3, 2}, {4, 3}, {5, 5}, {6, 8}, {7, 13}, } for _, tt := range fibTests { actual := Fib(tt.in) if actual != tt.expected { t.Errorf(\"Fib(%d) = %d; expected %d\", tt.in, actual, tt.expected) } } } 由于我们使用的是 t.Errorf，即使其中某个 case 失败，也不会终止测试执行。 1.3. T 类型 单元测试中，传递给测试函数的参数是 *testing.T 类型。它用于管理测试状态并支持格式化测试日志。测试日志会在执行测试的过程中不断累积，并在测试完成时转储至标准输出。 当测试函数返回时，或者当测试函数调用 FailNow、 Fatal、Fatalf、SkipNow、Skip、Skipf 中的任意一个时，则宣告该测试函数结束。跟 Parallel 方法一样，以上提到的这些方法只能在运行测试函数的 goroutine 中调用。 至于其他报告方法，比如 Log 以及 Error 的变种， 则可以在多个 goroutine 中同时进行调用。 1.3.1. 报告方法 上面提到的系列包括方法，带 f 的是格式化的，格式化语法参考 fmt 包。 T 类型内嵌了 common 类型，common 提供这一系列方法，我们经常会用到的（注意，这里说的测试中断，都是指当前测试函数）： 1）当我们遇到一个断言错误的时候，标识这个测试失败，会使用到： Fail : 测试失败，测试继续，也就是之后的代码依然会执行 FailNow : 测试失败，测试中断 在 FailNow 方法实现的内部，是通过调用 runtime.Goexit() 来中断测试的。 2）当我们遇到一个断言错误，只希望跳过这个错误，但是不希望标识测试失败，会使用到： SkipNow : 跳过测试，测试中断 在 SkipNow 方法实现的内部，是通过调用 runtime.Goexit() 来中断测试的。 3）当我们只希望打印信息，会用到 : Log : 输出信息 Logf : 输出格式化的信息 注意：默认情况下，单元测试成功时，它们打印的信息不会输出，可以通过加上 -v 选项，输出这些信息。但对于基准测试，它们总是会被输出。 4）当我们希望跳过这个测试，并且打印出信息，会用到： Skip : 相当于 Log + SkipNow Skipf : 相当于 Logf + SkipNow 5）当我们希望断言失败的时候，标识测试失败，并打印出必要的信息，但是测试继续，会用到： Error : 相当于 Log + Fail Errorf : 相当于 Logf + Fail 6）当我们希望断言失败的时候，标识测试失败，打印出必要的信息，但中断测试，会用到： Fatal : 相当于 Log + FailNow Fatalf : 相当于 Logf + FailNow 1.3.2. Parallel 测试 包中的 Parallel 方法表示当前测试只会与其他带有 Parallel 方法的测试并行进行测试。 下面例子将演示 Parallel 的使用方法： var ( data = make(map[string]string) locker sync.RWMutex ) func WriteToMap(k, v string) { locker.Lock() defer locker.Unlock() data[k] = v } func ReadFromMap(k string) string { locker.RLock() defer locker.RUnlock() return data[k] } 测试代码： var pairs = []struct { k string v string }{ {\"polaris\", \" 徐新华 \"}, {\"studygolang\", \"Go 语言中文网 \"}, {\"stdlib\", \"Go 语言标准库 \"}, {\"polaris1\", \" 徐新华 1\"}, {\"studygolang1\", \"Go 语言中文网 1\"}, {\"stdlib1\", \"Go 语言标准库 1\"}, {\"polaris2\", \" 徐新华 2\"}, {\"studygolang2\", \"Go 语言中文网 2\"}, {\"stdlib2\", \"Go 语言标准库 2\"}, {\"polaris3\", \" 徐新华 3\"}, {\"studygolang3\", \"Go 语言中文网 3\"}, {\"stdlib3\", \"Go 语言标准库 3\"}, {\"polaris4\", \" 徐新华 4\"}, {\"studygolang4\", \"Go 语言中文网 4\"}, {\"stdlib4\", \"Go 语言标准库 4\"}, } // 注意 TestWriteToMap 需要在 TestReadFromMap 之前 func TestWriteToMap(t *testing.T) { t.Parallel() for _, tt := range pairs { WriteToMap(tt.k, tt.v) } } func TestReadFromMap(t *testing.T) { t.Parallel() for _, tt := range pairs { actual := ReadFromMap(tt.k) if actual != tt.v { t.Errorf(\"the value of key(%s) is %s, expected: %s\", tt.k, actual, tt.v) } } } 试验步骤： 注释掉 WriteToMap 和 ReadFromMap 中 locker 保护的代码，同时注释掉测试代码中的 t.Parallel，执行测试，测试通过，即使加上 -race，测试依然通过； 只注释掉 WriteToMap 和 ReadFromMap 中 locker 保护的代码，执行测试，测试失败（如果未失败，加上 -race 一定会失败）； 如果代码能够进行并行测试，在写测试时，尽量加上 Parallel，这样可以测试出一些可能的问题。 关于 Parallel 的更多内容，会在 子测试 中介绍。 当你写完一个函数，结构体，main 之后，你下一步需要的就是测试了。testing 包提供了很简单易用的测试包。 2. 写一个基本的测试用例 测试文件的文件名需要以_test.go 为结尾，测试用例需要以 TestXxxx 的形式存在。 比如我要测试 utils 包的 sql.go 中的函数： func GetOne(db *sql.DB, query string, args ...interface{}) (map[string][]byte, error) { 就需要创建一个 sql_test.go package utils import ( \"database/sql\" _ \"fmt\" _ \"github.com/go-sql-driver/mysql\" \"strconv\" \"testing\" ) func Test_GetOne(t *testing.T) { db, err := sql.Open(\"mysql\", \"root:123.abc@tcp(192.168.33.10:3306)/test\") defer func() { db.Close() }() if err != nil { t.Fatal(err) } // 测试 empty car_brand, err := GetOne(db, \"select * from user where id = 999999\") if (car_brand != nil) || (err != nil) { t.Fatal(\"emtpy 测试错误 \") } } 3. testing 的测试用例形式 测试用例有四种形式： TestXxxx(t *testing.T) // 基本测试用例 BenchmarkXxxx(b *testing.B) // 压力测试的测试用例 Example_Xxx() // 测试控制台输出的例子 TestMain(m *testing.M) // 测试 Main 函数 给个 Example 的例子 :（Example 需要在最后用注释的方式确认控制台输出和预期是不是一致的） func Example_GetScore() { score := getScore(100, 100, 100, 2.1) fmt.Println(score) // Output: // 31.1 } 4. testing 的变量 gotest 的变量有这些： test.short : 一个快速测试的标记，在测试用例中可以使用 testing.Short() 来绕开一些测试 test.outputdir : 输出目录 test.coverprofile : 测试覆盖率参数，指定输出文件 test.run : 指定正则来运行某个 / 某些测试用例 test.memprofile : 内存分析参数，指定输出文件 test.memprofilerate : 内存分析参数，内存分析的抽样率 test.cpuprofile : cpu 分析输出参数，为空则不做 cpu 分析 test.blockprofile : 阻塞事件的分析参数，指定输出文件 test.blockprofilerate : 阻塞事件的分析参数，指定抽样频率 test.timeout : 超时时间 test.cpu : 指定 cpu 数量 test.parallel : 指定运行测试用例的并行数 5. testing 的结构体 B : 压力测试 BenchmarkResult : 压力测试结果 Cover : 代码覆盖率相关结构体 CoverBlock : 代码覆盖率相关结构体 InternalBenchmark : 内部使用的结构体 InternalExample : 内部使用的结构体 InternalTest : 内部使用的结构体 M : main 测试使用的结构体 PB : Parallel benchmarks 并行测试使用的结构体 T : 普通测试用例 TB : 测试用例的接口 6. testing 的通用方法 T 结构内部是继承自 common 结构，common 结构提供集中方法，是我们经常会用到的： 1）当我们遇到一个断言错误的时候，我们就会判断这个测试用例失败，就会使用到： Fail : case 失败，测试用例继续 FailedNow : case 失败，测试用例中断 2）当我们遇到一个断言错误，只希望跳过这个错误，但是不希望标示测试用例失败，会使用到： SkipNow : case 跳过，测试用例不继续 3）当我们只希望在一个地方打印出信息，我们会用到 : Log : 输出信息 Logf : 输出有 format 的信息 4）当我们希望跳过这个用例，并且打印出信息 : Skip : Log + SkipNow Skipf : Logf + SkipNow 5）当我们希望断言失败的时候，测试用例失败，打印出必要的信息，但是测试用例继续： Error : Log + Fail Errorf : Logf + Fail 6）当我们希望断言失败的时候，测试用例失败，打印出必要的信息，测试用例中断： Fatal : Log + FailNow Fatalf : Logf + FailNow 7. 生成测试报告 cd ../src/cover/size/ go test -coverprofile=size_coverage.out cd ../src/cover/size/ go tool cover -func=size_coverage.out https://brantou.github.io/2017/05/24/go-cover-story/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/单元测试2.html":{"url":"blog/测试/单元测试2.html","title":"单元测试2","keywords":"","body":"Golang 单元测试方案 1 什么是单元测试 1.1 定义 单元测试（又称为模块测试）是针对程序模块)(软件设计的最小单位)来进行正确性检验的测试工作。程序单元是应用的最小可测试部件。在过程化编程中，一个单元就是单个程序、函数、过程等；对于面向对象编程，最小单元就是方法，包括基类（超类）、抽象类、或者派生类（子类）中的方法。 1.2 单元测试任务包括： 1 模块接口测试； 2 模块局部数据结构测试； 3 模块边界条件测试； 4 模块中所有独立执行通路测试； 5 模块的各条错误处理通路测试。 1.3 go中的单元测试 单元测试文件名必须以xxx_test.go命名 2.. 方法必须是TestXxx开头 方法参数必须 t *testing.T 测试文件和被测试文件必须在一个包中 2 如何写一个健壮的单元测试 在任何时间，任何地点都可以运行，原则上测试结束，不应该对研发环境产生影响，数据库也不应该有任何修改，这是一个非常困扰程序员的难题，主要涉及如下几点： 1.数据库依赖 2.外部程序或环境依赖： 3.顺序依赖： 2.1 测试数据构造 单元测试的一个重点就是测试数据的构造，在测试数据构造时要考虑这样几个方面： 1.正常输入，整个必不可少，至少验证函数的正常逻辑是否通过 2.边界输入，这个主要验证在极端情况下的输入，函数是否在有相应的容错处理 3.非法输入，对于一些非正常输入，我们要看函数是否处理，会不引起函数的奔溃和数据泄露等问题 4.白盒覆盖，白盒覆盖就要设计了，要设计一些用力，能够覆盖到函数的所有代码，这里主要考虑：语句覆盖、条件覆盖、分支覆盖、分支/条件覆盖、条件组合覆盖。 2.2 编写原则 单元测试是要写额外的代码的，这对开发同学的也是一个不小的工作负担，在一些项目中，我们合理的评估单元测试的编写，我认为我们不能走极端，当然理论上来说全写肯定时好的，但是从成本，效率上来说我们必须做出权衡。所以这里给出一些衡量的原则 1.优先编写核心组件和逻辑模块的测试用例 2.发现Bug时一定先编写测试用例进行Debug 3.关键util工具类要编写测试用例，这些util工具适用的很频繁，所以这个原则也叫做热点原则，和第1点相呼应。 4.测试用户应该独立，一个文件对应一个，而且不同的测试用例之间不要互相依赖。 5.测试用例的保持更新。 2.3 单元测试Demo 2.3.1普通方法测试 2.3.2 HTTP API测试 接口测试由于要使用router包，但是router包引用了service，如果在service包中写单元测试，会导致循环导包的问题，所以这里我的API测试是放在项目根目录 test下。这样会导致service的覆盖检测不足。 2.3.3 数据库依赖测试 本地安装数据库 2.3.4 依赖外部不可达的远程接口测试 使用mock, http://8.14.0.108/lstack-hybrid/lstack-container-registry/lsh-mcp-lcr-cops/-/blob/caixisheng/service/pipelinegroup_service/pipelinegroup_authority_test.go 先对不可达的方法，创建接口 依赖的对象引入接口，比如参数A依赖远程接口调用，在A对象里面加入接口属性 命令生成接口对: mockgen --source pipelinegroup_authority.go -destination pipelinegroup_authority_mock.go -package pipelinegroup_service 写测试用例 测试： go test -v 3. Mock mock测试不但可以支持io类型的测试，比如：数据库，网络API请求，文件访问等。mock测试还可以做为未开发服务的模拟、服务压力测试支持、对未知复杂的服务进行模拟，比如开发阶段我们依赖的服务还没有开发好，那么就可以使用mock方法来模拟一个服务，模拟的这个服务接收的参数和返回的参数和规划设计的服务是一致的，那我们就可以直接使用这个模拟的服务来协助开发测试了；再比如要对服务进行压力测试，这个时候我们就要把服务依赖的网络，数据等服务进行模拟，不然得到的结果不纯粹。总结一下，有以下几种情况下使用mock会比较好： 1.IO类型的，本地文件，数据库，网络API，RPC等 2.依赖的服务还没有开发好，这时候我们自己可以模拟一个服务，加快开发进度提升开发效率 3.压力性能测试的时候屏蔽外部依赖，专注测试本模块 4.依赖的内部函数非常复杂，要构造数据非常不方便，这也是一种 mock测试，简单来说就是通过对服务或者函数发送设计好的参数，并且通过构造注入期望返回的数据来方便以上几种测试开发。 3.1 gomock 介绍 gomock主要包含两部分：gomock库和辅助代码生成工具mockgen 安装 go get github.com/golang/mock/gomockgo install github.com/golang/mock/mockgen 3.2 mock demo spider.go go_version.go 使用mockgen 自动生成接口的实现 go_version_test.go 最终的目录结构 官方例子： https://github.com/golang/mock/tree/master/sample 3.3 mockgen参数介绍 mockgen -destination spider/mock_spider.go -package spider -source spider/spider.go 就是将接口spider/spider.go中的接口做实现并存在 spider/mock_spider.go文件中，文件的包名为\"spider\" -source: 指定接口文件 -destination：生成的文件名 -package：生成的文件包名 -imports：依赖的需要import包 -aux_files: 接口文件不止一个文件时附加文件 -build_flags: 传递给build工具的参数 在我们上面的demo中，并没有使用\"-source\",那是如何实现接口的呢？mockgen还支持通过反射的方式来找到对应的接口。只要在所有选项的最后增加一个包名和里面对应的类型就可以了。其他参数和上面的公用。 通过注释指定mockgen 如上所述，如果有多个文件，并且分散在不同的位置，那么我们要生成mock文件的时候，需要对每个文件执行多次mockgen命令（假设包名不相同）。这样在真正操作起来的时候非常繁琐，mockgen还提供了一种通过注释生成mock文件的方式，此时需要借助go的\"go generate \"工具。 注释： //go:generate mockgen -destination mock_spider.go -package spider github.com/cz-it/blog/blog/Go/testing/gomock/example/spider Spider 在spider下执行命令 go generate 3.4 gomock接口使用 1.创建控制器 2.延时回收 3.调用mock生成的代码，实现接口对象 4.断言EXPECT（） 5.链式调用 4 golang测试命令 4.1 go test command 常用参数： -bench regexp 执行相应的 benchmarks，例如 -bench=.； -cover 开启测试覆盖率； -run regexp 只运行 regexp 匹配的函数，例如 -run=Array 那么就执行包含有 Array 开头的函数； -v 显示测试的详细命令。 4.2 测试覆盖率统计 这里在介绍一下另外一个简单的测试功能，测试覆盖率的测试cover，只要在go test后面加上-cover就可以了，如下面的例子，这里还加了一个参数-coverprofile=cover.out，这个参数是把覆盖率测试数据导出到cover.out这个文件，然后我们可以使用图形化的方式来看具体的测试覆盖情况。 以网页的形式打开统计结果 go tool cover -html=cover.out 参考链接 https://cloud.tencent.com/developer/article/1399249 https://juejin.im/post/6844903853532381198 https://gocn.vip/topics/9796 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/基准测试.html":{"url":"blog/测试/基准测试.html","title":"基准测试","keywords":"","body":"1. testing - 基准测试 在 _test.go 结尾的测试文件中，如下形式的函数： func BenchmarkXxx(*testing.B) 被认为是基准测试，通过 go test 命令，加上 -bench 标志来执行。多个基准测试按照顺序运行。 基准测试函数的形式如下： func BenchmarkHello(b *testing.B) { for i := 0; i 基准函数会运行目标代码 b.N 次。在基准执行期间，程序会自动调整 b.N 直到基准测试函数持续足够长的时间。输出结果形如： BenchmarkHello 10000000 282 ns/op 意味着循环执行了 10000000 次，每次循环花费 282 纳秒 (ns)。 如果基准测试在循环前需要一些耗时的配置，则可以先重置定时器： func BenchmarkBigLen(b *testing.B) { big := NewBig() b.ResetTimer() for i := 0; i 如果基准测试需要在并行设置中测试性能，则可以使用 RunParallel 辅助函数 ; 这样的基准测试一般与 go test -cpu 标志一起使用： func BenchmarkTemplateParallel(b *testing.B) { templ := template.Must(template.New(\"test\").Parse(\"Hello, {{.}}!\")) b.RunParallel(func(pb *testing.PB) { // 每个 goroutine 有属于自己的 bytes.Buffer. var buf bytes.Buffer for pb.Next() { // 循环体在所有 goroutine 中总共执行 b.N 次 buf.Reset() templ.Execute(&buf, \"World\") } }) } 1.1. 基准测试示例 接着上一节的例子，我们对 Fib 进行基准测试： func BenchmarkFib10(b *testing.B) { for n := 0; n 执行 go test -bench=.，输出： $ go test -bench=. BenchmarkFib10-4 3000000 424 ns/op PASS ok chapter09/testing 1.724s 这里测试了 Fib(10) 的情况，我们可能需要测试更多不同的情况，这时可以改写我们的测试代码： func BenchmarkFib1(b *testing.B) { benchmarkFib(1, b) } func BenchmarkFib2(b *testing.B) { benchmarkFib(2, b) } func BenchmarkFib3(b *testing.B) { benchmarkFib(3, b) } func BenchmarkFib10(b *testing.B) { benchmarkFib(10, b) } func BenchmarkFib20(b *testing.B) { benchmarkFib(20, b) } func BenchmarkFib40(b *testing.B) { benchmarkFib(40, b) } func benchmarkFib(i int, b *testing.B) { for n := 0; n 再次执行 go test -bench=.，输出： $ go test -bench=. BenchmarkFib1-4 1000000000 2.58 ns/op BenchmarkFib2-4 200000000 7.38 ns/op BenchmarkFib3-4 100000000 13.0 ns/op BenchmarkFib10-4 3000000 429 ns/op BenchmarkFib20-4 30000 54335 ns/op BenchmarkFib40-4 2 805759850 ns/op PASS ok chapter09/testing 15.361s 默认情况下，每个基准测试最少运行 1 秒。如果基准测试函数返回时，还不到 1 秒钟，b.N 的值会按照序列 1,2,5,10,20,50,... 增加，同时再次运行基准测测试函数。 我们注意到 BenchmarkFib40 一共才运行 2 次。为了更精确的结果，我们可以通过 -benchtime 标志指定运行时间，从而使它运行更多次。 $ go test -bench=Fib40 -benchtime=20s BenchmarkFib40-4 30 838675800 ns/op 1.2. B 类型 B 是传递给基准测试函数的一种类型，它用于管理基准测试的计时行为，并指示应该迭代地运行测试多少次。 当基准测试函数返回时，或者当基准测试函数调用 FailNow、Fatal、Fatalf、SkipNow、Skip、Skipf 中的任意一个方法时，则宣告测试函数结束。至于其他报告方法，比如 Log 和 Error 的变种，则可以在其他 goroutine 中同时进行调用。 跟单元测试一样，基准测试会在执行的过程中积累日志，并在测试完毕时将日志转储到标准错误。但跟单元测试不一样的是，为了避免基准测试的结果受到日志打印操作的影响，基准测试总是会把日志打印出来。 B 类型中的报告方法使用方式和 T 类型是一样的，一般来说，基准测试中也不需要使用，毕竟主要是测性能。这里我们对 B 类型中其他的一些方法进行讲解。 1.2.1. 计时方法 有三个方法用于计时： StartTimer：开始对测试进行计时。该方法会在基准测试开始时自动被调用，我们也可以在调用 StopTimer 之后恢复计时； StopTimer：停止对测试进行计时。当你需要执行一些复杂的初始化操作，并且你不想对这些操作进行测量时，就可以使用这个方法来暂时地停止计时； ResetTimer：对已经逝去的基准测试时间以及内存分配计数器进行清零。对于正在运行中的计时器，这个方法不会产生任何效果。本节开头有使用示例。 1.2.2. 并行执行 通过 RunParallel 方法能够并行地执行给定的基准测试。RunParallel会创建出多个 goroutine，并将 b.N 分配给这些 goroutine 执行，其中 goroutine 数量的默认值为 GOMAXPROCS。用户如果想要增加非 CPU 受限（non-CPU-bound）基准测试的并行性，那么可以在 RunParallel 之前调用 SetParallelism（如 SetParallelism(2)，则 goroutine 数量为 2*GOMAXPROCS）。RunParallel 通常会与 -cpu 标志一同使用。 body 函数将在每个 goroutine 中执行，这个函数需要设置所有 goroutine 本地的状态，并迭代直到 pb.Next 返回 false 值为止。因为 StartTimer、StopTime 和 ResetTimer 这三个方法都带有全局作用，所以 body 函数不应该调用这些方法； 除此之外，body 函数也不应该调用 Run 方法。 具体的使用示例，在本节开头已经提供！ 1.2.3. 内存统计 ReportAllocs 方法用于打开当前基准测试的内存统计功能， 与 go test 使用 -benchmem 标志类似，但 ReportAllocs 只影响那些调用了该函数的基准测试。 测试示例： func BenchmarkTmplExucte(b *testing.B) { b.ReportAllocs() templ := template.Must(template.New(\"test\").Parse(\"Hello, {{.}}!\")) b.RunParallel(func(pb *testing.PB) { // Each goroutine has its own bytes.Buffer. var buf bytes.Buffer for pb.Next() { // The loop body is executed b.N times total across all goroutines. buf.Reset() templ.Execute(&buf, \"World\") } }) } 测试结果类似这样： BenchmarkTmplExucte-4 2000000 898 ns/op 368 B/op 9 allocs/op 1.2.4. 基准测试结果 对上述结果中的每一项，你是否都清楚是什么意思呢？ 2000000 ：基准测试的迭代总次数 b.N 898 ns/op：平均每次迭代所消耗的纳秒数 368 B/op：平均每次迭代内存所分配的字节数 9 allocs/op：平均每次迭代的内存分配次数 testing 包中的 BenchmarkResult 类型能为你提供帮助，它保存了基准测试的结果，定义如下： type BenchmarkResult struct { N int // The number of iterations. 基准测试的迭代总次数，即 b.N T time.Duration // The total time taken. 基准测试的总耗时 Bytes int64 // Bytes processed in one iteration. 一次迭代处理的字节数，通过 b.SetBytes 设置 MemAllocs uint64 // The total number of memory allocations. 内存分配的总次数 MemBytes uint64 // The total number of bytes allocated. 内存分配的总字节数 } 该类型还提供了每次迭代操作所消耗资源的计算方法，示例如下： package main import ( \"bytes\" \"fmt\" \"testing\" \"text/template\" ) func main() { benchmarkResult := testing.Benchmark(func(b *testing.B) { templ := template.Must(template.New(\"test\").Parse(\"Hello, {{.}}!\")) // RunParallel will create GOMAXPROCS goroutines // and distribute work among them. b.RunParallel(func(pb *testing.PB) { // Each goroutine has its own bytes.Buffer. var buf bytes.Buffer for pb.Next() { // The loop body is executed b.N times total across all goroutines. buf.Reset() templ.Execute(&buf, \"World\") } }) }) // fmt.Printf(\"%8d\\t%10d ns/op\\t%10d B/op\\t%10d allocs/op\\n\", benchmarkResult.N, benchmarkResult.NsPerOp(), benchmarkResult.AllocedBytesPerOp(), benchmarkResult.AllocsPerOp()) fmt.Printf(\"%s\\t%s\\n\", benchmarkResult.String(), benchmarkResult.MemString()) } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/子测试与子基准测试.html":{"url":"blog/测试/子测试与子基准测试.html","title":"子测试与子基准测试","keywords":"","body":"1. testing - 子测试与子基准测试 从 Go 1.7 开始，引入了一个新特性：子测试（subtests）与子基准测试（sub-benchmarks），它意味着您现在可以拥有嵌套测试，这对于过滤执行特定测试用例非常有用。 T 和 B 的 Run 方法允许定义子单元测试和子基准测试，而不必为它们单独定义函数。这便于创建基于 Table-Driven 的基准测试和层级测试。它还提供了一种共享通用 setup 和 tear-down 代码的方法： func TestFoo(t *testing.T) { // t.Run(\"A=1\", func(t *testing.T) { ... }) t.Run(\"A=2\", func(t *testing.T) { ... }) t.Run(\"B=1\", func(t *testing.T) { ... }) // } 每个子测试和子基准测试都有一个唯一的名称：由顶层测试的名称与传递给 Run 的名称组成，以斜杠分隔，并具有可选的尾随序列号，用于消除歧义。 命令行标志 -run 和 -bench 的参数是非固定的正则表达式，用于匹配测试名称。对于由斜杠分隔的测试名称，例如子测试的名称，它名称本身即可作为参数，依次匹配由斜杠分隔的每部分名称。因为参数是非固定的，一个空的表达式匹配任何字符串，所以下述例子中的 “匹配” 意味着 “顶层/子测试名称包含有”： go test -run '' # 执行所有测试。 go test -run Foo # 执行匹配 \"Foo\" 的顶层测试，例如 \"TestFooBar\"。 go test -run Foo/A= # 对于匹配 \"Foo\" 的顶层测试，执行其匹配 \"A=\" 的子测试。 go test -run /A=1 # 执行所有匹配 \"A=1\" 的子测试。 子测试也可用于程序并行控制。只有子测试全部执行完毕后，父测试才会完成。在下述例子中，所有子测试之间并行运行，此处的 “并行” 只限于这些子测试之间，并不影响定义在其他顶层测试中的子测试： func TestGroupedParallel(t *testing.T) { for _, tc := range tests { tc := tc // capture range variable t.Run(tc.Name, func(t *testing.T) { t.Parallel() ... }) } } 在所有子测试并行运行完毕之前，Run 方法不会返回。下述例子提供了一种方法，用于在子测试并行运行完毕后清理资源： func TestTeardownParallel(t *testing.T) { // This Run will not return until the parallel tests finish. t.Run(\"group\", func(t *testing.T) { t.Run(\"Test1\", parallelTest1) t.Run(\"Test2\", parallelTest2) t.Run(\"Test3\", parallelTest3) }) // } UnitTest Template import \"testing\" func TestKubeApp(t *testing.T){ testCases := []struct{ name string }{ {}, } for _, item := range testCases{ t.Run(item.name, func(t *testing.T) { }) } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/并行测试.html":{"url":"blog/测试/并行测试.html","title":"并行测试","keywords":"","body":"1. testing - 运行并验证示例 testing 包除了测试，还提供了运行并验证示例的功能。示例，一方面是文档的效果，是关于某个功能的使用例子；另一方面，可以被当做测试运行。 一个示例的例子如下： func ExampleHello() { fmt.Println(\"Hello\") // Output: Hello } 如果 Output: Hello 改为：Output: hello，运行测试会失败，提示： got: Hello want: hello 一个示例函数以 Example 开头，如果示例函数包含以 \"Output:\" 开头的行注释，在运行测试时，go 会将示例函数的输出和 \"Output:\" 注释中的值做比较，就如上面的例子。 有时候，输出顺序可能不确定，比如循环输出 map 的值，那么可以使用 \"Unordered output:\" 开头的注释。 如果示例函数没有上述输出注释，该示例函数只会被编译而不会被运行。 1.1. 命名约定 Go 语言通过大量的命名约定来简化工具的复杂度，规范代码的风格。对示例函数的命名有如下约定： 包级别的示例函数，直接命名为 func Example() { ... } 函数 F 的示例，命名为 func ExampleF() { ... } 类型 T 的示例，命名为 func ExampleT() { ... } 类型 T 上的 方法 M 的示例，命名为 func ExampleT_M() { ... } 有时，我们想要给 包 / 类型 / 函数 / 方法 提供多个示例，可以通过在示例函数名称后附加一个不同的后缀来实现，但这种后缀必须以小写字母开头，如： func Example_suffix() { ... } func ExampleF_suffix() { ... } func ExampleT_suffix() { ... } func ExampleT_M_suffix() { ... } 通常，示例代码会放在单独的示例文件中，命名为 example_test.go。可以查看 io 包中的 example_test.go 了解示例的编写。 1.2. 实现原理 本节开头提到了示例的两个作用，它们分别是由 godoc 和 go test 这两个命令实现的。 在执行 go test 时，会运行示例。具体的实现原理，可以通过阅读 go test 命令源码和 testing 包中 example.go 文件了解。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/质量检查/":{"url":"blog/测试/质量检查/","title":"质量检查","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/质量检查/Sonar.html":{"url":"blog/测试/质量检查/Sonar.html","title":"Sonar","keywords":"","body":"Sonar 为了保证项目代码质量，需要控制每个Pull Request的代码单元测试覆盖率。翻看了Sonar文档，发现Sonar是一款保证代码质量的工具，可以满足此类需求 SonarQube社区版是开源的代码质量管理平台，涵盖了架构设计、注释、编码规范、潜在缺陷、代码复杂度、单元测试、重复代码7个维度。通过强大的插件扩展机制，支持对主流编程语言的指标分析，目前可以支持超过20种以上主流编程语言。 SonarQube 在进行代码质量管理时，会从下图所示的七个纬度来分析项目的质量。 SonarQube 在进行代码质量管理时，会从七个纬度来分析项目的质量。 糟糕的复杂度分析：文件、类、方法等，如果复杂度过高将难以改变，这会使得开发人员难以理解它们，且如果没有自动化的单元测试，对于程序中的任何组件的改变都将可能导致需要全面的回归测试。 重复:显然程序中包含大量复制粘贴的代码是质量低下的，sonar可以展示源码中重复严重的地方。 缺乏单元测试：可以很方便的统计并展示单元测试覆盖率。 没有代码标准:sonar可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具规范代码编写. 没有足够的或者过多的注释：没有注释将使代码可读性变差，特别是当不可避免地出现人员变动时，程序的可读性将大幅下降，而过多的注释又会使得开发人员将精力过多地花费在阅读注释上，亦违背初衷。 潜在的bug：可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具检测出潜在的缺陷。 issue的类型分为五个： [Blocker]阻断:错误，高概率影响程序运行，例如：内存泄漏，未关闭的JDBC连接等等，代码必须修复。 [Critical]严重:低概率影响程序运行活着一个安全漏洞的错误，例如：空catch块，SQL注入等，这样的代码需要立即审查。 [Major]主要:代码缺陷，影响开发人员的生产力，例如：裸露一段代码，复制代码块，未使用的参数，这样的代码需要关注或忽略。 [Minor]次要:可能较少的影响开发人员生产力，这样的代码需要关注或忽略。。 [info]信息:不是错误，也不是质量缺陷，只是发现而已，这样的代码可以忽略。 糟糕的设计：可以找出循环，展示包与包，类与类之间的相互依赖关系，可以检测自定义的架构规则。可以管理第三方的jar包，可以利用LCOM4检测单个任务规则的应用情况，检测耦合。 SonarQube 可以测量的关键指标，包括代码错误、 代码异味(code smells)、安全漏洞和重复的代码。 代码错误 是代码中的一部分不正确或无法正常运行、可能会导致错误的结果，是指那些在代码发布到生产环境之前应该被修复的明显的错误。 代码异味 不同于代码错误，被检测到的代码是可能能正确执行并符合预期。然而，它不容易被修复，也不能被单元测试覆盖，却可能会导致一些未知的错误，或是一些其它的问题。从长期的可维护性来讲，立即修复代码异味是明智之举。通常在编写代码的时候，代码异味并不容易被发现，而 SonarQube 的静态分析是一种发现它们的很好的方式。 安全漏洞 正如听起来的一样：指的是现在的代码中可能存在的安全问题的缺陷。这些缺陷应该立即修复来防止黑客利用它们。 重复的代码 也和听起来的一样：指的是源代码中重复的部分。代码重复在软件设计中是一种很不好的做法。总的来说，如果对一部分代码进行更改而另一部分没有，则会导致一些维护性的问题。例如，识别重复的代码可以很容易的将重复的代码打包成一个库来重复的使用。 为什么它那么重要 SonarQube 为组织提供了一个集中的位置来管理和跟踪多个项目代码中的问题。它还可以把持续的检查与质量门限相结合。一旦项目分析过一次以后，更进一步的分析会参考软件最新的修改来更新原始的统计信息，以反映最新的变化。这些跟踪可以让用户看到问题解决的程度和速度。这与 “尽早发布并经常发布”不谋而合。 另外，SonarQube 可使用 可持续集成流程，比如像 Hudson 和 Jenkins 这样的工具。这个质量门限可以很好的反映代码的整体运行状况，并且通过 Jenkins 等集成工具，在发布代码到生产环境时担任一个重要的角色。 本着 DevOps 的精神， SonarQube 可以量化代码质量，来达到组织内部的要求。为了加快代码生产和发布的周期，组织必须意识到它们自己的技术债务和软件问题。通过发现这些信息， SonarQube 可以帮助组织更快的生成高质量的软件。 组件组成 sonarqube server ： 他有三个程序分别是 webserver（配置和管理sonar） searchserver（搜索结果返回给sonarUI） ComplateEngineserver（计算服务 将分析结果入库）。 sonarqube db : 数据库 存放配置。 sonarqube plugins： 插件增加功能。 sonar-scanner ： 代码扫描工具 可以有多个。 SonarQube各组件的工作流程 开发者在IDE中编码，并使用SonarLint执行本地代码分析； 开发者向软件配置管理平台（Git，SVN，TFVC等）提交代码； 代码提交触发持续集成平台自动构建、使用SonarQube Scanner执行分析； 分析报告被发送到SonarQube Server进行处理； 处理好的报告生成对应可视化的视图，并将数据保持到数据库； 开发者可以在页面通过查看，评论，解决问题来管理和减少技术债； SonarQube中的一些重要概念 指标：SonarQube中的主要指标有可靠性，安全性，可维护性，测试覆盖率，复杂度，重复代码，规模（大小），问题等。 代码规则：在SonarQube中，通过插件提供的规则，在执行代码分析时对代码进行分析并生成问题。由于规则中定义了修复问题话费的成本（时间），解决问题的代价以及技术债可以通过这些问题进行计算。规则一般有三种类型：可靠性（Bug），可维护性（坏味道），安全性（漏洞）。 质量配置：质量配置提供了根据需求配置一组代码规则的能力，这组代码规则将被用于分析某些指定的组件（项目）。例如，项目A对应什么编程语言，适用于那些代码规则等等。 质量阈：质量阈是一系列对项目指标进行度量的条件。项目必须达到所有条件才能算整体上通过了质量阈。例如，配置质量阈为新增Bugs大于10，新代码可靠率低于评级A，新代码可维护率低于评级B，那分析完成后若指标符合这些标准，则代码质量将被认为是不合格的。 SonarQube Server处理分析报告时，根据质量配置中的代码规则进行匹配，从而生成具体的指标数据，然后根据质量阈中的阈值判断出项目的代码是否合格。 安装Sonar server docker run -d --name sonarqube -p 9000:9000 sonarqube 本地安装 cli 添加PATH sudo vim ~/.bashrc 修改配置文件 测试 sonar-scanner -h Sonar create project test your program Copy 上一步中的测试命令 sonar-scanner \\ -Dsonar.projectKey=stream \\ -Dsonar.sources=. \\ -Dsonar.host.url=http://localhost:9000 \\ -Dsonar.login=97bbbbe51ea897682bf8e7fc175aae3e16589385 命令行终端进入到你的程序更目录执行 登陆serve 查看测试结果 link -参考 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/质量检查/go_lint.html":{"url":"blog/测试/质量检查/go_lint.html","title":"Go Lint","keywords":"","body":"golangci-lint 在需要进行静态代码扫描的目录下执行 golangci-lint run，此命令和 golangci-lint run ./… 命令等效，表示扫描整个项目文件代码，并进行监测，也可以通过指定 go 文件或者文件目录名来对特定的代码文件或者目录进行代码扫描，例如 golangci-lint run dir1 dir2/... dir3/file1.go。 ps：扫描指定目录的时候是不支持递归扫描的，如果要进行递归扫描需要在目录路径后面追加/… 默认情况下 golangci-lint 只启用以下的 linters： Enabled by default linters: deadcode: 发现没有使用的代码 errcheck: 用于检查 go 程序中有 error 返回的函数，却没有做判断检查 gosimple: 检测代码是否可以简化 govet (vet, vetshadow): 检查 go 源代码并报告可疑结构，例如 Printf 调用，其参数与格式字符串不一致 ineffassign: 检测是否有未使用的代码、变量、常量、类型、结构体、函数、函数参数等 staticcheck: 提供了巨多的静态检查，检查 bug，分析性能等 structcheck:发现未使用的结构体字段 typecheck: 对 go 代码进行解析和类型检查 unused: 检查未使用的常量，变量，函数和类型 varcheck: 查找未使用的全局变量和常量 Disabled by default linters: bodyclose: 对 HTTP 响应是否 close 成功检测 dupl: 代码克隆监测工具 gochecknoglobals: 检查 go 代码中是否存在全局变量 goimports: 做所有 gofmt 做的事. 此外还检查未使用的导入 golint: 打印出 go 代码的格式错误 gofmt: 检测代码是否都已经格式化, 默认情况下使用 -s 来检查代码是否简化 ………………………….. 未启用的还有很多工具，可以通过使用 golangci-lint help linters 命令查看还有哪些工具可以使用，如果想要启用没有默认开启的工具，可以在执行命令时使用 -E 参数来启用，比如要启用 golint 的话，只需要执行一下命令 golangci-lint run -E=golint。除了用 -E 来启动参数外，还可以指定最长执行时间 —deadline、跳过要扫描的目录 --skip-dirs 等等。如果要了解更多，请使用 golangci-lint run -h 来查看。 特别注意 —-exclude-use-default 参数，golangci-lint 对于上面默认的启用 linters 中做了一些过滤措施，比如对于 errcheck ，它不会扫描 ((os\\.)?std(out|err)\\..*|.*Close|.*Flush|os\\.Remove(All)?|.*printf?|os\\.(Un)?Setenv) 这些函数返回的 error 是否被 checked，所以如果代码中使用到这些函数，并且没有接收 error 的话是不会被扫描到的。类似的还有golint、govet、staticcheck、gosec 需要注意。如果想要不过滤这些就需要使用 --exclude-use-default=false 来启用。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/测试/质量检查/readme.html":{"url":"blog/测试/质量检查/readme.html","title":"Readme","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/网络/http-proxy.html":{"url":"blog/网络/http-proxy.html","title":"Http Proxy","keywords":"","body":"一个简单的Golang实现的HTTP Proxy 2018-08-28阅读 1.9K0 本文为原创文章，转载注明出处，欢迎扫码关注公众号flysnow_org或者网站http://www.flysnow.org/，第一时间看后续精彩文章。觉得好的话，顺手分享到朋友圈吧，感谢支持。 最近因为换了Mac，以前的Linux基本上不再使用了，但是我的SS代理还得用。SS代理大家都了解，一个很NB的Socket代理工具，但是就是因为他是Socket的，想用HTTP代理的时候很不方便。 以前在Linux下的时候，会安装一个Privoxy把Socket代理转换为HTTP代理，开机启动，也比较方便。但是Mac下使用Brew安装的Privoxy就很难用，再加上以前一个有个想法，一个软件搞定Socket和HTTP代理，这样就不用安装一个单独的软件做转换了。 想着就开始做吧，以前基本上没有搞过太多的网络编程，最近也正好在研究Go，正好练练手。 我们这里主要讲使用HTTP／1.1协议中的CONNECT方法建立起来的隧道连接，实现的HTTP Proxy。这种代理的好处就是不用知道客户端请求的数据，只需要原封不动的转发就可以了，对于处理HTTPS的请求就非常方便了，不用解析他的内容，就可以实现代理。 启动代理监听 要想做一个HTTP Proxy，我们需要启动一个服务器，监听一个端口，用于接收客户端的请求。Golang给我们提供了强大的net包供我们使用，我们启动一个代理服务器监听非常方便。 l, err := net.Listen(\"tcp\", \":8080\") if err != nil { log.Panic(err) } 以上代理我们就实现了一个在8080端口上监听的服务器，我们这里没有写ip地址，默认在所有ip地址上进行监听。如果你只想本机适用，可以使用127.0.0.1:8080，这样机器就访问不了你的代理服务器了。 监听接收代理请求 启动了代理服务器，就可以开始接受不了代理请求了，有了请求，我们才能做进一步的处理。 for { client, err := l.Accept() if err != nil { log.Panic(err) } go handleClientRequest(client) } Listener接口的Accept方法，会接受客户端发来的连接数据，这是一个阻塞型的方法，如果客户端没有连接数据发来，他就是阻塞等待。接收来的连接数据，会马上交给handleClientRequest方法进行处理，这里使用一个go关键字开一个goroutine的目的是不阻塞客户端的接收，代理服务器可以马上接收下一个连接请求。 解析请求，获取要访问的IP和端口 有了客户端的代理请求了，我们还得从请求里提取客户端要访问的远程主机的IP和端口，这样我们的代理服务器才可以建立和远程主机的连接，代理转发。 HTTP协议的头信息里就包含有我们需要的主机名(IP)和端口信息，并且是明文的，协议很规范，类似于： CONNECT www.google.com:443 HTTP/1.1 Host: www.google.com:443 Proxy-Connection: keep-alive User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36 可以看到我们需要的在第一行,第一个行的信息以空格分开，第一部分CONNECT是请求方法，这里是CONNECT，除此之外还有GET，POST等，都是HTTP协议的标准方法。 第二部分是URL，https的请求只有host和port，http的请求是一个完成的url，等下会看个样例，就明白了。 第三部是HTTP的协议和版本，这个我们不用太关注。 以上是一个https的请求，我们看下http的： GET http://www.flysnow.org/ HTTP/1.1 Host: www.flysnow.org Proxy-Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36 可以看到htt的，没有端口号（默认是80）；比https多了schame–http://。 有了分析，下面我们就可以从HTTP头信息中获取请求的url和method信息了。 var b [1024]byte n, err := client.Read(b[:]) if err != nil { log.Println(err) return } var method, host, address string fmt.Sscanf(string(b[:bytes.IndexByte(b[:], '\\n')]), \"%s%s\", &method, &host) hostPortURL, err := url.Parse(host) if err != nil { log.Println(err) return } 然后需要进一步对url进行解析，获取我们需要的远程服务器信息 if hostPortURL.Opaque == \"443\" { //https访问 address = hostPortURL.Scheme + \":443\" } else { //http访问 if strings.Index(hostPortURL.Host, \":\") == -1 { //host不带端口， 默认80 address = hostPortURL.Host + \":80\" } else { address = hostPortURL.Host } } 这样就完整了获取了要请求服务器的信息，他们可能是以下几种格式 ip:port hostname:port domainname:port 就是有可能是ip（v4orv6），有可能是主机名（内网），有可能是域名(dns解析) 代理服务器和远程服务器建立连接 有了远程服务器的信息了，就可以进行拨号建立连接了，有了连接，才可以通信。 //获得了请求的host和port，就开始拨号吧 server, err := net.Dial(\"tcp\", address) if err != nil { log.Println(err) return } 数据转发 拨号成功后，就可以进行数据代理传输了 if method == \"CONNECT\" { fmt.Fprint(client, \"HTTP/1.1 200 Connection established\\r\\n\\r\\n\") } else { server.Write(b[:n]) } //进行转发 go io.Copy(server, client) io.Copy(client, server) 其中对CONNECT方法有单独的回应，客户端说要建立连接，代理服务器要回应建立好了，然后才可以像HTTP一样请求访问。 运行外国外VPS上 到这里，我们的代理服务器全部开发完成了，下面是完整的源代码： package main import ( \"bytes\" \"fmt\" \"io\" \"log\" \"net\" \"net/url\" \"strings\" ) func main() { log.SetFlags(log.LstdFlags|log.Lshortfile) l, err := net.Listen(\"tcp\", \":8081\") if err != nil { log.Panic(err) } for { client, err := l.Accept() if err != nil { log.Panic(err) } go handleClientRequest(client) } } func handleClientRequest(client net.Conn) { if client == nil { return } defer client.Close() var b [1024]byte n, err := client.Read(b[:]) if err != nil { log.Println(err) return } var method, host, address string fmt.Sscanf(string(b[:bytes.IndexByte(b[:], '\\n')]), \"%s%s\", &method, &host) hostPortURL, err := url.Parse(host) if err != nil { log.Println(err) return } if hostPortURL.Opaque == \"443\" { //https访问 address = hostPortURL.Scheme + \":443\" } else { //http访问 if strings.Index(hostPortURL.Host, \":\") == -1 { //host不带端口， 默认80 address = hostPortURL.Host + \":80\" } else { address = hostPortURL.Host } } //获得了请求的host和port，就开始拨号吧 server, err := net.Dial(\"tcp\", address) if err != nil { log.Println(err) return } if method == \"CONNECT\" { fmt.Fprint(client, \"HTTP/1.1 200 Connection established\\r\\n\\r\\n\") } else { server.Write(b[:n]) } //进行转发 go io.Copy(server, client) io.Copy(client, server) } 把源代码编译，然后放到你国外的VPS上，在自己机器上配置好HTTP代理，就可以到处访问，自由自在了。 本文为原创文章，转载注明出处，欢迎扫码关注公众号flysnow_org或者网站http://www.flysnow.org/，第一时间看后续精彩文章。觉得好的话，顺手分享到朋友圈吧，感谢支持。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/网络/http-to-socket5.html":{"url":"blog/网络/http-to-socket5.html","title":"Http To Socket5","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/网络/shadow-socket.html":{"url":"blog/网络/shadow-socket.html","title":"Shadow Socket","keywords":"","body":"源码分析之shadow-shocket Socks is an internet protocol that exchanges network packets between a client an server through a proxy server 2.2 socks协议有什么用 对于广大的中国网友来说，一提到代理，肯定会想到翻墙，而socks5作为一种代理协议，肯定也能用来翻墙嘛。不过遗憾的是，虽然它是代理协议，然而并不能用于翻墙。因为它的数据都是明文传输，会被墙轻易阻断。 socks协议历史悠久，它面世时中国的互联网尚未成型，更别说墙，因此它并不是为翻墙而设计的协议。互联网早期，企业内部网络为了保证安全性，都是置于防火墙之后，这样带来的副作用就是访问内部资源会变得很麻烦，socks协议就是为了解决这个问题而诞生的。 socks相当于在防火墙撕了一道口子，让合法的用户可以通过这个口子连接到内部，从而访问内部的一些资源和进行管理。 2.3 什么是socks5协议 socks5顾名思义就是socks协议的第五个版本，作为socks4的一个延伸，在socks4的基础上新增UDP转发和认证功能。唯一遗憾的是socks5并不兼容socks4协议。socks5由IETF在1996年正式发布，经过这么多年的发展，互联网上基本上都以socks5为主，socks4已经退出了历史的舞台。 实际上，你并不需要回头去看socks4协议，因为socks5协议完全可以取代socks4，因此读者对此不必感觉有心理压力。 2.4 工作过程 在开始介绍socks5协议工作工程之前，先来了解一下浏览器不设置代理情况下的请求过程。假设读者通过浏览器访问本博(假设读者使用的是HTTP协议)，流程如下: 建立TCP连接 浏览器向本博所在服务器建立TCP连接，经过3次握手后成功双方建立一条连接，用于数据传输 发起HTTP请求 TCP连接建立成功后，浏览器通过建立的连接发送HTTP请求 GET / Host wiyi.org 服务器响应浏览器一段HTML内容，浏览器收到后对页面进行渲染 上面是正常的请求过程，如果读者给浏览器设置了一个socks5代理，情况会复杂一些。在这里我们假设socks5代理位于读者本地，端口为7582，它的工作流程如下: 浏览器和socks5代理建立TCP连接 和上面不同的时，浏览器和服务器之间多了一个中间人，即socks5，因此浏览器需要跟socks5服务器建立一条连接。 socks5协商阶段 在浏览器正式向socks5服务器发起请求之前，双方需要协商，包括协议版本，支持的认证方式等，双方需要协商成功才能进行下一步。协商的细节将会在下一小节详细描述。 socks5请求阶段 协商成功后，浏览器向socks5代理发起一个请求。请求的内容包括，它要访问的服务器域名或ip，端口等信息。 socks5 relay阶段 scoks5收到浏览器请求后，解析请求内容，然后向目标服务器建立TCP连接。 数据传输阶段 经过上面步骤，我们成功建立了浏览器 –> socks5，socks5–>目标服务器之间的连接。这个阶段浏览器开始把数据传输给scoks5代理，socks5代理把数据转发到目标服务器。 上面的步骤虽然变多，但本质不变，非常容易理解，简单整理为下图 图2.2 2.5 协议细节 在上一个小节介绍了socks5代理简要的工作流程，我们可以把它的的过程总结为3个阶段，分别为:握手阶段、请求阶段，Relay阶段。 2.5.1 握手阶段 握手阶段包含协商和子协商阶段，我们把它拆分为两个分别讨论 2.5.1.1 协商阶段 在这个阶段，客户端向socks5发起请求，内容如下: +----+----------+----------+ |VER | NMETHODS | METHODS | +----+----------+----------+ | 1 | 1 | 1 to 255 | +----+----------+----------+ #上方的数字表示字节数，下面的表格同理，不再赘述 VER: 协议版本，socks5为0x05 NMETHODS: 支持认证的方法数量 METHODS: 对应NMETHODS，NMETHODS的值为多少，METHODS就有多少个字节。RFC预定义了一些值的含义，内容如下: X’00’ NO AUTHENTICATION REQUIRED X’01’ GSSAPI X’02’ USERNAME/PASSWORD X’03’ to X’7F’ IANA ASSIGNED X’80’ to X’FE’ RESERVED FOR PRIVATE METHODS X’FF’ NO ACCEPTABLE METHODS socks5服务器需要选中一个METHOD返回给客户端，格式如下: +----+--------+ |VER | METHOD | +----+--------+ | 1 | 1 | +----+--------+ 当客户端收到0x00时，会跳过认证阶段直接进入请求阶段; 当收到0xFF时，直接断开连接。其他的值进入到对应的认证阶段。 2.5.1.2 认证阶段(也叫子协商) 认证阶段作为协商的一个子流程，它不是必须的。socks5服务器可以决定是否需要认证，如果不需要认证，那么认证阶段会被直接略过。 如果需要认证，客户端向socks5服务器发起一个认证请求，这里以0x02的认证方式举例: +----+------+----------+------+----------+ |VER | ULEN | UNAME | PLEN | PASSWD | +----+------+----------+------+----------+ | 1 | 1 | 1 to 255 | 1 | 1 to 255 | +----+------+----------+------+----------+ VER: 版本，通常为0x01 ULEN: 用户名长度 UNAME: 对应用户名的字节数据 PLEN: 密码长度 PASSWD: 密码对应的数据 socks5服务器收到客户端的认证请求后，解析内容，验证信息是否合法，然后给客户端响应结果。响应格式如下: +----+--------+ |VER | STATUS | +----+--------+ | 1 | 1 | +----+--------+ STATUS字段如果为0x00表示认证成功，其他的值为认证失败。当客户端收到认证失败的响应后，它将会断开连接。 2.5.2 请求阶段 顺利通过协商阶段后，客户端向socks5服务器发起请求细节，格式如下: +----+-----+-------+------+----------+----------+ |VER | CMD | RSV | ATYP | DST.ADDR | DST.PORT | +----+-----+-------+------+----------+----------+ | 1 | 1 | X'00' | 1 | Variable | 2 | +----+-----+-------+------+----------+----------+ VER 版本号，socks5的值为0x05 CMD 0x01表示CONNECT请求 0x02表示BIND请求 0x03表示UDP转发 RSV 保留字段，值为0x00 ATYP 目标地址类型，DST.ADDR的数据对应这个字段的类型。 0x01表示IPv4地址，DST.ADDR为4个字节 0x03表示域名，DST.ADDR是一个可变长度的域名 0x04表示IPv6地址，DST.ADDR为16个字节长度 DST.ADDR 一个可变长度的值 DST.PORT 目标端口，固定2个字节 上面的值中，DST.ADDR是一个变长的数据，它的数据长度根据ATYP的类型决定。我们可以通过掐头去尾解析出这部分数据。 socks5服务器收到客户端的请求后，需要返回一个响应，结构如下 +----+-----+-------+------+----------+----------+ |VER | REP | RSV | ATYP | BND.ADDR | BND.PORT | +----+-----+-------+------+----------+----------+ | 1 | 1 | X'00' | 1 | Variable | 2 | +----+-----+-------+------+----------+----------+ VER socks版本，这里为0x05 REP Relay field,内容取值如下 X’00’ succeeded X’01’ general SOCKS server failure X’02’ connection not allowed by ruleset X’03’ Network unreachable X’04’ Host unreachable X’05’ Connection refused X’06’ TTL expired X’07’ Command not supported X’08’ Address type not supported X’09’ to X’FF’ unassigned RSV 保留字段 ATYPE 同请求的ATYPE BND.ADDR 服务绑定的地址 BND.PORT 服务绑定的端口DST.PORT 针对响应的结构中，BND.ADDR和BND.PORT值得特别关注一下，可能有朋友在这里会产生困惑，返回的地址和端口是用来做什么的呢？ 我们回过头看图2.2，可以发现在图中socks5既充当socks服务器，又充当relay服务器。实际上这两个是可以被拆开的，当我们的socks5 server和relay server不是一体的，就需要告知客户端relay server的地址，这个地址就是BND.ADDR和BND.PORT。 当我们的relay server和socks5 server是同一台服务器时，BND.ADDR和BND.PORT的值全部为0即可。 2.5.3 Relay阶段 socks5服务器收到请求后，解析内容。如果是UDP请求，服务器直接转发; 如果是TCP请求，服务器向目标服务器建立TCP连接，后续负责把客户端的所有数据转发到目标服务。 3.总结 & 下载 本文简单介绍了下socks5协议的作用以及处理过程，下一篇文章，将会手把手用Java实现一个socks5代理服务器，进一步认识socks5协议的处理过程。 读者可以点击socks5.pcapng下载抓包数据，使用wireshark可以查看本文事例的抓包数据。 4. 相关阅读 手把手使用Java实现一个Socks5代理 5.参考资料 https://en.wikipedia.org/wiki/SOCKS https://datatracker.ietf.org/doc/html/rfc1928 https://datatracker.ietf.org/doc/html/rfc1929 https://www.rapidseedbox.com/blog/guide-to-socks5-proxy 本文链接: https://wiyi.org/socks5-protocol-in-deep.html This work is licensed under a Attribution-NonCommercial 4.0 International license. Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/网络/socket5-demo.html":{"url":"blog/网络/socket5-demo.html","title":"Socket5 Demo","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/网络/websocket.html":{"url":"blog/网络/websocket.html","title":"Websocket","keywords":"","body":" RFC协议 golang 开源库 相关概念 message type ControllerMessage https://www.cnblogs.com/chyingp/p/websocket-deep-in.html Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/网络/数据链路层.html":{"url":"blog/网络/数据链路层.html","title":"数据链路层","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/网络/网络层.html":{"url":"blog/网络/网络层.html","title":"网络层","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/":{"url":"blog/规范/","title":"规范","keywords":"","body":"规范 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/API Conventions.html":{"url":"blog/规范/API Conventions.html","title":"API Conventions","keywords":"","body":"API Conventions This document is oriented at users who want a deeper understanding of the Kubernetes API structure, and developers wanting to extend the Kubernetes API. An introduction to using resources with kubectl can be found in the object management overview. Table of Contents Types (Kinds) Resources Objects Metadata Spec and Status Typical status properties References to related objects Lists of named subobjects preferred over maps Primitive types Constants Unions Lists and Simple kinds Differing Representations Verbs on Resources PATCH operations Idempotency Optional vs. Required Defaulting Late Initialization Concurrency Control and Consistency Serialization Format Units Selecting Fields Object references HTTP Status codes Success codes Error codes Response Status Kind Events Naming conventions Label, selector, and annotation conventions WebSockets and SPDY Validation The conventions of the Kubernetes API (and related APIs in the ecosystem) are intended to ease client development and ensure that configuration mechanisms can be implemented that work across a diverse set of use cases consistently. The general style of the Kubernetes API is RESTful - clients create, update, delete, or retrieve a description of an object via the standard HTTP verbs (POST, PUT, DELETE, and GET) - and those APIs preferentially accept and return JSON. Kubernetes also exposes additional endpoints for non-standard verbs and allows alternative content types. All of the JSON accepted and returned by the server has a schema, identified by the \"kind\" and \"apiVersion\" fields. Where relevant HTTP header fields exist, they should mirror the content of JSON fields, but the information should not be represented only in the HTTP header. The following terms are defined: Kind the name of a particular object schema (e.g. the \"Cat\" and \"Dog\" kinds would have different attributes and properties) Resource a representation of a system entity, sent or retrieved as JSON via HTTP to the server. Resources are exposed via: Collections - a list of resources of the same type, which may be queryable Elements - an individual resource, addressable via a URL API Group a set of resources that are exposed together. Along with the version is exposed in the \"apiVersion\" field as \"GROUP/VERSION\", e.g. \"policy.k8s.io/v1\". Each resource typically accepts and returns data of a single kind. A kind may be accepted or returned by multiple resources that reflect specific use cases. For instance, the kind \"Pod\" is exposed as a \"pods\" resource that allows end users to create, update, and delete pods, while a separate \"pod status\" resource (that acts on \"Pod\" kind) allows automated processes to update a subset of the fields in that resource. Resources are bound together in API groups - each group may have one or more versions that evolve independent of other API groups, and each version within the group has one or more resources. Group names are typically in domain name form - the Kubernetes project reserves use of the empty group, all single word names (\"extensions\", \"apps\"), and any group name ending in \"*.k8s.io\" for its sole use. When choosing a group name, we recommend selecting a subdomain your group or organization owns, such as \"widget.mycompany.com\". Version strings should match DNS_LABEL format. Resource collections should be all lowercase and plural, whereas kinds are CamelCase and singular. Group names must be lower case and be valid DNS subdomains. Types (Kinds) Kinds are grouped into three categories: Objects represent a persistent entity in the system. Creating an API object is a record of intent - once created, the system will work to ensure that resource exists. All API objects have common metadata. An object may have multiple resources that clients can use to perform specific actions that create, update, delete, or get. Examples: Pod, ReplicationController, Service, Namespace, Node. Lists are collections of resources of one (usually) or more (occasionally) kinds. The name of a list kind must end with \"List\". Lists have a limited set of common metadata. All lists use the required \"items\" field to contain the array of objects they return. Any kind that has the \"items\" field must be a list kind. Most objects defined in the system should have an endpoint that returns the full set of resources, as well as zero or more endpoints that return subsets of the full list. Some objects may be singletons (the current user, the system defaults) and may not have lists. In addition, all lists that return objects with labels should support label filtering (see the labels documentation), and most lists should support filtering by fields. Examples: PodLists, ServiceLists, NodeLists. TODO: Describe field filtering below or in a separate doc. Simple kinds are used for specific actions on objects and for non-persistent entities. Given their limited scope, they have the same set of limited common metadata as lists. For instance, the \"Status\" kind is returned when errors occur and is not persisted in the system. Many simple resources are \"subresources\", which are rooted at API paths of specific resources. When resources wish to expose alternative actions or views that are closely coupled to a single resource, they should do so using new sub-resources. Common subresources include: /binding: Used to bind a resource representing a user request (e.g., Pod, PersistentVolumeClaim) to a cluster infrastructure resource (e.g., Node, PersistentVolume). /status: Used to write just the status portion of a resource. For example, the /pods endpoint only allows updates to metadata and spec, since those reflect end-user intent. An automated process should be able to modify status for users to see by sending an updated Pod kind to the server to the \"/pods//status\" endpoint - the alternate endpoint allows different rules to be applied to the update, and access to be appropriately restricted. /scale: Used to read and write the count of a resource in a manner that is independent of the specific resource schema. Two additional subresources, proxy and portforward, provide access to cluster resources as described in accessing the cluster. The standard REST verbs (defined below) MUST return singular JSON objects. Some API endpoints may deviate from the strict REST pattern and return resources that are not singular JSON objects, such as streams of JSON objects or unstructured text log data. A common set of \"meta\" API objects are used across all API groups and are thus considered part of the API group named meta.k8s.io. These types may evolve independent of the API group that uses them and API servers may allow them to be addressed in their generic form. Examples are ListOptions, DeleteOptions, List, Status, WatchEvent, and Scale. For historical reasons these types are part of each existing API group. Generic tools like quota, garbage collection, autoscalers, and generic clients like kubectl leverage these types to define consistent behavior across different resource types, like the interfaces in programming languages. The term \"kind\" is reserved for these \"top-level\" API types. The term \"type\" should be used for distinguishing sub-categories within objects or subobjects. Resources All JSON objects returned by an API MUST have the following fields: kind: a string that identifies the schema this object should have apiVersion: a string that identifies the version of the schema the object should have These fields are required for proper decoding of the object. They may be populated by the server by default from the specified URL path, but the client likely needs to know the values in order to construct the URL path. Objects Metadata Every object kind MUST have the following metadata in a nested object field called \"metadata\": namespace: a namespace is a DNS compatible label that objects are subdivided into. The default namespace is 'default'. See the namespace docs for more. name: a string that uniquely identifies this object within the current namespace (see the identifiers docs). This value is used in the path when retrieving an individual object. uid: a unique in time and space value (typically an RFC 4122 generated identifier, see the identifiers docs) used to distinguish between objects with the same name that have been deleted and recreated Every object SHOULD have the following metadata in a nested object field called \"metadata\": resourceVersion: a string that identifies the internal version of this object that can be used by clients to determine when objects have changed. This value MUST be treated as opaque by clients and passed unmodified back to the server. Clients should not assume that the resource version has meaning across namespaces, different kinds of resources, or different servers. (See concurrency control, below, for more details.) generation: a sequence number representing a specific generation of the desired state. Set by the system and monotonically increasing, per-resource. May be compared, such as for RAW and WAW consistency. creationTimestamp: a string representing an RFC 3339 date of the date and time an object was created deletionTimestamp: a string representing an RFC 3339 date of the date and time after which this resource will be deleted. This field is set by the server when a graceful deletion is requested by the user, and is not directly settable by a client. The resource will be deleted (no longer visible from resource lists, and not reachable by name) after the time in this field except when the object has a finalizer set. In case the finalizer is set the deletion of the object is postponed at least until the finalizer is removed. Once the deletionTimestamp is set, this value may not be unset or be set further into the future, although it may be shortened or the resource may be deleted prior to this time. labels: a map of string keys and values that can be used to organize and categorize objects (see the labels docs) annotations: a map of string keys and values that can be used by external tooling to store and retrieve arbitrary metadata about this object (see the annotations docs) Labels are intended for organizational purposes by end users (select the pods that match this label query). Annotations enable third-party automation and tooling to decorate objects with additional metadata for their own use. Spec and Status By convention, the Kubernetes API makes a distinction between the specification of the desired state of an object (a nested object field called \"spec\") and the status of the object at the current time (a nested object field called \"status\"). The specification is a complete description of the desired state, including configuration settings provided by the user, default values expanded by the system, and properties initialized or otherwise changed after creation by other ecosystem components (e.g., schedulers, auto-scalers), and is persisted in stable storage with the API object. If the specification is deleted, the object will be purged from the system. The status summarizes the current state of the object in the system, and is usually persisted with the object by automated processes but may be generated on the fly. At some cost and perhaps some temporary degradation in behavior, the status could be reconstructed by observation if it were lost. When a new version of an object is POSTed or PUT, the \"spec\" is updated and available immediately. Over time the system will work to bring the \"status\" into line with the \"spec\". The system will drive toward the most recent \"spec\" regardless of previous versions of that stanza. In other words, if a value is changed from 2 to 5 in one PUT and then back down to 3 in another PUT the system is not required to 'touch base' at 5 before changing the \"status\" to 3. In other words, the system's behavior is level-based rather than edge-based. This enables robust behavior in the presence of missed intermediate state changes. The Kubernetes API also serves as the foundation for the declarative configuration schema for the system. In order to facilitate level-based operation and expression of declarative configuration, fields in the specification should have declarative rather than imperative names and semantics -- they represent the desired state, not actions intended to yield the desired state. The PUT and POST verbs on objects MUST ignore the \"status\" values, to avoid accidentally overwriting the status in read-modify-write scenarios. A /status subresource MUST be provided to enable system components to update statuses of resources they manage. Otherwise, PUT expects the whole object to be specified. Therefore, if a field is omitted it is assumed that the client wants to clear that field's value. The PUT verb does not accept partial updates. Modification of just part of an object may be achieved by GETting the resource, modifying part of the spec, labels, or annotations, and then PUTting it back. See concurrency control, below, regarding read-modify-write consistency when using this pattern. Some objects may expose alternative resource representations that allow mutation of the status, or performing custom actions on the object. All objects that represent a physical resource whose state may vary from the user's desired intent SHOULD have a \"spec\" and a \"status\". Objects whose state cannot vary from the user's desired intent MAY have only \"spec\", and MAY rename \"spec\" to a more appropriate name. Objects that contain both spec and status should not contain additional top-level fields other than the standard metadata fields. Some objects which are not persisted in the system - such as SubjectAccessReview and other webhook style calls - may choose to add spec and status to encapsulate a \"call and response\" pattern. The spec is the request (often a request for information) and the status is the response. For these RPC like objects the only operation may be POST, but having a consistent schema between submission and response reduces the complexity of these clients. Typical status properties Conditions represent the latest available observations of an object's state. They are an extension mechanism intended to be used when the details of an observation are not a priori known or would not apply to all instances of a given Kind. For observations that are well known and apply to all instances, a regular field is preferred. An example of a Condition that probably should have been a regular field is Pod's \"Ready\" condition - it is managed by core controllers, it is well understood, and it applies to all Pods. Objects may report multiple conditions, and new types of conditions may be added in the future or by 3rd party controllers. Therefore, conditions are represented using a list/slice, where all have similar structure. The FooCondition type for some resource type Foo may include a subset of the following fields, but must contain at least type and status fields: Type FooConditionType `json:\"type\" description:\"type of Foo condition\"` Status ConditionStatus `json:\"status\" description:\"status of the condition, one of True, False, Unknown\"` // +optional Reason *string `json:\"reason,omitempty\" description:\"one-word CamelCase reason for the condition's last transition\"` // +optional Message *string `json:\"message,omitempty\" description:\"human-readable message indicating details about last transition\"` // +optional LastHeartbeatTime *unversioned.Time `json:\"lastHeartbeatTime,omitempty\" description:\"last time we got an update on a given condition\"` // +optional LastTransitionTime *unversioned.Time `json:\"lastTransitionTime,omitempty\" description:\"last time the condition transit from one status to another\"` Additional fields may be added in the future. Do not use fields that you don't need - simpler is better. Use of the Reason field is encouraged. Use the LastHeartbeatTime with great caution - frequent changes to this field can cause a large fan-out effect for some resources. Conditions should be added to explicitly convey properties that users and components care about rather than requiring those properties to be inferred from other observations. Once defined, the meaning of a Condition can not be changed arbitrarily - it becomes part of the API, and has the same backwards- and forwards-compatibility concerns of any other part of the API. Condition status values may be True, False, or Unknown. The absence of a condition should be interpreted the same as Unknown. How controllers handle Unknown depends on the Condition in question. Condition types should indicate state in the \"abnormal-true\" polarity. For example, if the condition indicates when a policy is invalid, the \"is valid\" case is probably the norm, so the condition should be called \"Invalid\". The thinking around conditions has evolved over time, so there are several non-normative examples in wide use. In general, condition values may change back and forth, but some condition transitions may be monotonic, depending on the resource and condition type. However, conditions are observations and not, themselves, state machines, nor do we define comprehensive state machines for objects, nor behaviors associated with state transitions. The system is level-based rather than edge-triggered, and should assume an Open World. An example of an oscillating condition type is Ready (despite it running afoul of current guidance), which indicates the object was believed to be fully operational at the time it was last probed. A possible monotonic condition could be Failed. A True status for Failed would imply failure with no retry. An object that was still active would generally not have a Failed condition. Some resources in the v1 API contain fields called phase, and associated message, reason, and other status fields. The pattern of using phase is deprecated. Newer API types should use conditions instead. Phase was essentially a state-machine enumeration field, that contradicted system-design principles and hampered evolution, since adding new enum values breaks backward compatibility. Rather than encouraging clients to infer implicit properties from phases, we prefer to explicitly expose the individual conditions that clients need to monitor. Conditions also have the benefit that it is possible to create some conditions with uniform meaning across all resource types, while still exposing others that are unique to specific resource types. See #7856 for more details and discussion. In condition types, and everywhere else they appear in the API, Reason is intended to be a one-word, CamelCase representation of the category of cause of the current status, and Message is intended to be a human-readable phrase or sentence, which may contain specific details of the individual occurrence. Reason is intended to be used in concise output, such as one-line kubectl get output, and in summarizing occurrences of causes, whereas Message is intended to be presented to users in detailed status explanations, such as kubectl describe output. Historical information status (e.g., last transition time, failure counts) is only provided with reasonable effort, and is not guaranteed to not be lost. Status information that may be large (especially proportional in size to collections of other resources, such as lists of references to other objects -- see below) and/or rapidly changing, such as resource usage, should be put into separate objects, with possibly a reference from the original object. This helps to ensure that GETs and watch remain reasonably efficient for the majority of clients, which may not need that data. Some resources report the observedGeneration, which is the generation most recently observed by the component responsible for acting upon changes to the desired state of the resource. This can be used, for instance, to ensure that the reported status reflects the most recent desired status. References to related objects References to loosely coupled sets of objects, such as pods overseen by a replication controller, are usually best referred to using a label selector. In order to ensure that GETs of individual objects remain bounded in time and space, these sets may be queried via separate API queries, but will not be expanded in the referring object's status. References to specific objects, especially specific resource versions and/or specific fields of those objects, are specified using the ObjectReference type (or other types representing strict subsets of it). Unlike partial URLs, the ObjectReference type facilitates flexible defaulting of fields from the referring object or other contextual information. References in the status of the referee to the referrer may be permitted, when the references are one-to-one and do not need to be frequently updated, particularly in an edge-based manner. Lists of named subobjects preferred over maps Discussed in #2004 and elsewhere. There are no maps of subobjects in any API objects. Instead, the convention is to use a list of subobjects containing name fields. For example: ports: - name: www containerPort: 80 vs. ports: www: containerPort: 80 This rule maintains the invariant that all JSON/YAML keys are fields in API objects. The only exceptions are pure maps in the API (currently, labels, selectors, annotations, data), as opposed to sets of subobjects. Primitive types Avoid floating-point values as much as possible, and never use them in spec. Floating-point values cannot be reliably round-tripped (encoded and re-decoded) without changing, and have varying precision and representations across languages and architectures. All numbers (e.g., uint32, int64) are converted to float64 by Javascript and some other languages, so any field which is expected to exceed that either in magnitude or in precision (specifically integer values > 53 bits) should be serialized and accepted as strings. Do not use unsigned integers, due to inconsistent support across languages and libraries. Just validate that the integer is non-negative if that's the case. Do not use enums. Use aliases for string instead (e.g., NodeConditionType). Look at similar fields in the API (e.g., ports, durations) and follow the conventions of existing fields. All public integer fields MUST use the Go (u)int32 or Go (u)int64 types, not (u)int (which is ambiguous depending on target platform). Internal types may use (u)int. Think twice about bool fields. Many ideas start as boolean but eventually trend towards a small set of mutually exclusive options. Plan for future expansions by describing the policy options explicitly as a string type alias (e.g. TerminationMessagePolicy). Constants Some fields will have a list of allowed values (enumerations). These values will be strings, and they will be in CamelCase, with an initial uppercase letter. Examples: ClusterFirst, Pending, ClientIP. Unions Sometimes, at most one of a set of fields can be set. For example, the [volumes] field of a PodSpec has 17 different volume type-specific fields, such as nfs and iscsi. All fields in the set should be Optional. Sometimes, when a new type is created, the api designer may anticipate that a union will be needed in the future, even if only one field is allowed initially. In this case, be sure to make the field Optional In the validation, you may still return an error if the sole field is unset. Do not set a default value for that field. Lists and Simple kinds Every list or simple kind SHOULD have the following metadata in a nested object field called \"metadata\": resourceVersion: a string that identifies the common version of the objects returned by in a list. This value MUST be treated as opaque by clients and passed unmodified back to the server. A resource version is only valid within a single namespace on a single kind of resource. Every simple kind returned by the server, and any simple kind sent to the server that must support idempotency or optimistic concurrency should return this value. Since simple resources are often used as input alternate actions that modify objects, the resource version of the simple resource should correspond to the resource version of the object. Differing Representations An API may represent a single entity in different ways for different clients, or transform an object after certain transitions in the system occur. In these cases, one request object may have two representations available as different resources, or different kinds. An example is a Service, which represents the intent of the user to group a set of pods with common behavior on common ports. When Kubernetes detects a pod matches the service selector, the IP address and port of the pod are added to an Endpoints resource for that Service. The Endpoints resource exists only if the Service exists, but exposes only the IPs and ports of the selected pods. The full service is represented by two distinct resources - under the original Service resource the user created, as well as in the Endpoints resource. As another example, a \"pod status\" resource may accept a PUT with the \"pod\" kind, with different rules about what fields may be changed. Future versions of Kubernetes may allow alternative encodings of objects beyond JSON. Verbs on Resources API resources should use the traditional REST pattern: GET / - Retrieve a list of type , e.g. GET /pods returns a list of Pods. POST / - Create a new resource from the JSON object provided by the client. GET // - Retrieves a single resource with the given name, e.g. GET /pods/first returns a Pod named 'first'. Should be constant time, and the resource should be bounded in size. DELETE // - Delete the single resource with the given name. DeleteOptions may specify gracePeriodSeconds, the optional duration in seconds before the object should be deleted. Individual kinds may declare fields which provide a default grace period, and different kinds may have differing kind-wide default grace periods. A user provided grace period overrides a default grace period, including the zero grace period (\"now\"). PUT // - Update or create the resource with the given name with the JSON object provided by the client. PATCH // - Selectively modify the specified fields of the resource. See more information below. GET /&watch=true - Receive a stream of JSON objects corresponding to changes made to any resource of the given kind over time. PATCH operations The API supports three different PATCH operations, determined by their corresponding Content-Type header: JSON Patch, Content-Type: application/json-patch+json As defined in RFC6902, a JSON Patch is a sequence of operations that are executed on the resource, e.g. {\"op\": \"add\", \"path\": \"/a/b/c\", \"value\": [ \"foo\", \"bar\" ]}. For more details on how to use JSON Patch, see the RFC. Merge Patch, Content-Type: application/merge-patch+json As defined in RFC7386, a Merge Patch is essentially a partial representation of the resource. The submitted JSON is \"merged\" with the current resource to create a new one, then the new one is saved. For more details on how to use Merge Patch, see the RFC. Strategic Merge Patch, Content-Type: application/strategic-merge-patch+json Strategic Merge Patch is a custom implementation of Merge Patch. For a detailed explanation of how it works and why it needed to be introduced, see here. Idempotency All compatible Kubernetes APIs MUST support \"name idempotency\" and respond with an HTTP status code 409 when a request is made to POST an object that has the same name as an existing object in the system. See the identifiers docs for details. Names generated by the system may be requested using metadata.generateName. GenerateName indicates that the name should be made unique by the server prior to persisting it. A non-empty value for the field indicates the name will be made unique (and the name returned to the client will be different than the name passed). The value of this field will be combined with a unique suffix on the server if the Name field has not been provided. The provided value must be valid within the rules for Name, and may be truncated by the length of the suffix required to make the value unique on the server. If this field is specified, and Name is not present, the server will NOT return a 409 if the generated name exists - instead, it will either return 201 Created or 504 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header). Optional vs. Required Fields must be either optional or required. Optional fields have the following properties: They have the +optional comment tag in Go. They are a pointer type in the Go definition (e.g. AwesomeFlag *SomeFlag) or have a built-in nil value (e.g. maps and slices). The API server should allow POSTing and PUTing a resource with this field unset. In most cases, optional fields should also have the omitempty struct tag (the omitempty option specifies that the field should be omitted from the json encoding if the field has an empty value). However, If you want to have different logic for an optional field which is not provided vs. provided with empty values, do not use omitempty (e.g. https://github.com/kubernetes/kubernetes/issues/34641). Note that for backward compatibility, any field that has the omitempty struct tag will be considered to be optional, but this may change in the future and having the +optional comment tag is highly recommended. Required fields have the opposite properties, namely: They do not have an +optional comment tag. They do not have an omitempty struct tag. They are not a pointer type in the Go definition (e.g. AnotherFlag SomeFlag). The API server should not allow POSTing or PUTing a resource with this field unset. Using the +optional or the omitempty tag causes OpenAPI documentation to reflect that the field is optional. Using a pointer allows distinguishing unset from the zero value for that type. There are some cases where, in principle, a pointer is not needed for an optional field since the zero value is forbidden, and thus implies unset. There are examples of this in the codebase. However: it can be difficult for implementors to anticipate all cases where an empty value might need to be distinguished from a zero value structs are not omitted from encoder output even where omitempty is specified, which is messy; having a pointer consistently imply optional is clearer for users of the Go language client, and any other clients that use corresponding types Therefore, we ask that pointers always be used with optional fields that do not have a built-in nil value. Defaulting Default resource values are API version-specific, and they are applied during the conversion from API-versioned declarative configuration to internal objects representing the desired state (Spec) of the resource. Subsequent GETs of the resource will include the default values explicitly. Incorporating the default values into the Spec ensures that Spec depicts the full desired state so that it is easier for the system to determine how to achieve the state, and for the user to know what to anticipate. API version-specific default values are set by the API server. Late Initialization Late initialization is when resource fields are set by a system controller after an object is created/updated. For example, the scheduler sets the pod.spec.nodeName field after the pod is created. Late-initializers should only make the following types of modifications: Setting previously unset fields Adding keys to maps Adding values to arrays which have mergeable semantics (patchStrategy:\"merge\" attribute in the type definition). These conventions: allow a user (with sufficient privilege) to override any system-default behaviors by setting the fields that would otherwise have been defaulted. enables updates from users to be merged with changes made during late initialization, using strategic merge patch, as opposed to clobbering the change. allow the component which does the late-initialization to use strategic merge patch, which facilitates composition and concurrency of such components. Although the apiserver Admission Control stage acts prior to object creation, Admission Control plugins should follow the Late Initialization conventions too, to allow their implementation to be later moved to a 'controller', or to client libraries. Concurrency Control and Consistency Kubernetes leverages the concept of resource versions to achieve optimistic concurrency. All Kubernetes resources have a \"resourceVersion\" field as part of their metadata. This resourceVersion is a string that identifies the internal version of an object that can be used by clients to determine when objects have changed. When a record is about to be updated, it's version is checked against a pre-saved value, and if it doesn't match, the update fails with a StatusConflict (HTTP status code 409). The resourceVersion is changed by the server every time an object is modified. If resourceVersion is included with the PUT operation the system will verify that there have not been other successful mutations to the resource during a read/modify/write cycle, by verifying that the current value of resourceVersion matches the specified value. The resourceVersion is currently backed by etcd's modifiedIndex. However, it's important to note that the application should not rely on the implementation details of the versioning system maintained by Kubernetes. We may change the implementation of resourceVersion in the future, such as to change it to a timestamp or per-object counter. The only way for a client to know the expected value of resourceVersion is to have received it from the server in response to a prior operation, typically a GET. This value MUST be treated as opaque by clients and passed unmodified back to the server. Clients should not assume that the resource version has meaning across namespaces, different kinds of resources, or different servers. Currently, the value of resourceVersion is set to match etcd's sequencer. You could think of it as a logical clock the API server can use to order requests. However, we expect the implementation of resourceVersion to change in the future, such as in the case we shard the state by kind and/or namespace, or port to another storage system. In the case of a conflict, the correct client action at this point is to GET the resource again, apply the changes afresh, and try submitting again. This mechanism can be used to prevent races like the following: Client #1 Client #2 GET Foo GET Foo Set Foo.Bar = \"one\" Set Foo.Baz = \"two\" PUT Foo PUT Foo When these sequences occur in parallel, either the change to Foo.Bar or the change to Foo.Baz can be lost. On the other hand, when specifying the resourceVersion, one of the PUTs will fail, since whichever write succeeds changes the resourceVersion for Foo. resourceVersion may be used as a precondition for other operations (e.g., GET, DELETE) in the future, such as for read-after-write consistency in the presence of caching. \"Watch\" operations specify resourceVersion using a query parameter. It is used to specify the point at which to begin watching the specified resources. This may be used to ensure that no mutations are missed between a GET of a resource (or list of resources) and a subsequent Watch, even if the current version of the resource is more recent. This is currently the main reason that list operations (GET on a collection) return resourceVersion. Serialization Format APIs may return alternative representations of any resource in response to an Accept header or under alternative endpoints, but the default serialization for input and output of API responses MUST be JSON. A protobuf encoding is also accepted for built-in resources. As proto is not self-describing, there is an envelope wrapper which describes the type of the contents. All dates should be serialized as RFC3339 strings. Units Units must either be explicit in the field name (e.g., timeoutSeconds), or must be specified as part of the value (e.g., resource.Quantity). Which approach is preferred is TBD, though currently we use the fooSeconds convention for durations. Duration fields must be represented as integer fields with units being part of the field name (e.g. leaseDurationSeconds). We don't use Duration in the API since that would require clients to implement go-compatible parsing. Selecting Fields Some APIs may need to identify which field in a JSON object is invalid, or to reference a value to extract from a separate resource. The current recommendation is to use standard JavaScript syntax for accessing that field, assuming the JSON object was transformed into a JavaScript object, without the leading dot, such as metadata.name. Examples: Find the field \"current\" in the object \"state\" in the second item in the array \"fields\": fields[1].state.current Object references Object references should either be called fooName if referring to an object of kind Foo by just the name (within the current namespace, if a namespaced resource), or should be called fooRef, and should contain a subset of the fields of the ObjectReference type. TODO: Plugins, extensions, nested kinds, headers HTTP Status codes The server will respond with HTTP status codes that match the HTTP spec. See the section below for a breakdown of the types of status codes the server will send. The following HTTP status codes may be returned by the API. Success codes 200 StatusOK Indicates that the request completed successfully. 201 StatusCreated Indicates that the request to create kind completed successfully. 204 StatusNoContent Indicates that the request completed successfully, and the response contains no body. Returned in response to HTTP OPTIONS requests. Error codes 307 StatusTemporaryRedirect Indicates that the address for the requested resource has changed. Suggested client recovery behavior: Follow the redirect. 400 StatusBadRequest Indicates the requested is invalid. Suggested client recovery behavior: Do not retry. Fix the request. 401 StatusUnauthorized Indicates that the server can be reached and understood the request, but refuses to take any further action, because the client must provide authorization. If the client has provided authorization, the server is indicating the provided authorization is unsuitable or invalid. Suggested client recovery behavior: If the user has not supplied authorization information, prompt them for the appropriate credentials. If the user has supplied authorization information, inform them their credentials were rejected and optionally prompt them again. 403 StatusForbidden Indicates that the server can be reached and understood the request, but refuses to take any further action, because it is configured to deny access for some reason to the requested resource by the client. Suggested client recovery behavior: Do not retry. Fix the request. 404 StatusNotFound Indicates that the requested resource does not exist. Suggested client recovery behavior: Do not retry. Fix the request. 405 StatusMethodNotAllowed Indicates that the action the client attempted to perform on the resource was not supported by the code. Suggested client recovery behavior: Do not retry. Fix the request. 409 StatusConflict Indicates that either the resource the client attempted to create already exists or the requested update operation cannot be completed due to a conflict. Suggested client recovery behavior: If creating a new resource: Either change the identifier and try again, or GET and compare the fields in the pre-existing object and issue a PUT/update to modify the existing object. If updating an existing resource: See Conflict from the status response section below on how to retrieve more information about the nature of the conflict. GET and compare the fields in the pre-existing object, merge changes (if still valid according to preconditions), and retry with the updated request (including ResourceVersion). 410 StatusGone Indicates that the item is no longer available at the server and no forwarding address is known. Suggested client recovery behavior: Do not retry. Fix the request. 422 StatusUnprocessableEntity Indicates that the requested create or update operation cannot be completed due to invalid data provided as part of the request. Suggested client recovery behavior: Do not retry. Fix the request. 429 StatusTooManyRequests Indicates that the either the client rate limit has been exceeded or the server has received more requests then it can process. Suggested client recovery behavior: Read the Retry-After HTTP header from the response, and wait at least that long before retrying. 500 StatusInternalServerError Indicates that the server can be reached and understood the request, but either an unexpected internal error occurred and the outcome of the call is unknown, or the server cannot complete the action in a reasonable time (this may be due to temporary server load or a transient communication issue with another server). Suggested client recovery behavior: Retry with exponential backoff. 503 StatusServiceUnavailable Indicates that required service is unavailable. Suggested client recovery behavior: Retry with exponential backoff. 504 StatusServerTimeout Indicates that the request could not be completed within the given time. Clients can get this response ONLY when they specified a timeout param in the request. Suggested client recovery behavior: Increase the value of the timeout param and retry with exponential backoff. Response Status Kind Kubernetes will always return the Status kind from any API endpoint when an error occurs. Clients SHOULD handle these types of objects when appropriate. A Status kind will be returned by the API in two cases: When an operation is not successful (i.e. when the server would return a non 2xx HTTP status code). When a HTTP DELETE call is successful. The status object is encoded as JSON and provided as the body of the response. The status object contains fields for humans and machine consumers of the API to get more detailed information for the cause of the failure. The information in the status object supplements, but does not override, the HTTP status code's meaning. When fields in the status object have the same meaning as generally defined HTTP headers and that header is returned with the response, the header should be considered as having higher priority. Example: $ curl -v -k -H \"Authorization: Bearer WhCDvq4VPpYhrcfmF6ei7V9qlbqTubUc\" https://10.240.122.184:443/api/v1/namespaces/default/pods/grafana > GET /api/v1/namespaces/default/pods/grafana HTTP/1.1 > User-Agent: curl/7.26.0 > Host: 10.240.122.184 > Accept: */* > Authorization: Bearer WhCDvq4VPpYhrcfmF6ei7V9qlbqTubUc > status field contains one of two possible values: Success Failure message may contain human-readable description of the error reason may contain a machine-readable, one-word, CamelCase description of why this operation is in the Failure status. If this value is empty there is no information available. The reason clarifies an HTTP status code but does not override it. details may contain extended data associated with the reason. Each reason may define its own extended details. This field is optional and the data returned is not guaranteed to conform to any schema except that defined by the reason type. Possible values for the reason and details fields: BadRequest Indicates that the request itself was invalid, because the request doesn't make any sense, for example deleting a read-only object. This is different than status reason Invalid above which indicates that the API call could possibly succeed, but the data was invalid. API calls that return BadRequest can never succeed. Http status code: 400 StatusBadRequest Unauthorized Indicates that the server can be reached and understood the request, but refuses to take any further action without the client providing appropriate authorization. If the client has provided authorization, this error indicates the provided credentials are insufficient or invalid. Details (optional): kind string The kind attribute of the unauthorized resource (on some operations may differ from the requested resource). name string The identifier of the unauthorized resource. HTTP status code: 401 StatusUnauthorized Forbidden Indicates that the server can be reached and understood the request, but refuses to take any further action, because it is configured to deny access for some reason to the requested resource by the client. Details (optional): kind string The kind attribute of the forbidden resource (on some operations may differ from the requested resource). name string The identifier of the forbidden resource. HTTP status code: 403 StatusForbidden NotFound Indicates that one or more resources required for this operation could not be found. Details (optional): kind string The kind attribute of the missing resource (on some operations may differ from the requested resource). name string The identifier of the missing resource. HTTP status code: 404 StatusNotFound AlreadyExists Indicates that the resource you are creating already exists. Details (optional): kind string The kind attribute of the conflicting resource. name string The identifier of the conflicting resource. HTTP status code: 409 StatusConflict Conflict Indicates that the requested update operation cannot be completed due to a conflict. The client may need to alter the request. Each resource may define custom details that indicate the nature of the conflict. HTTP status code: 409 StatusConflict Invalid Indicates that the requested create or update operation cannot be completed due to invalid data provided as part of the request. Details (optional): kind string the kind attribute of the invalid resource name string the identifier of the invalid resource causes One or more StatusCause entries indicating the data in the provided resource that was invalid. The reason, message, and field attributes will be set. HTTP status code: 422 StatusUnprocessableEntity Timeout Indicates that the request could not be completed within the given time. Clients may receive this response if the server has decided to rate limit the client, or if the server is overloaded and cannot process the request at this time. Http status code: 429 TooManyRequests The server should set the Retry-After HTTP header and return retryAfterSeconds in the details field of the object. A value of 0 is the default. ServerTimeout Indicates that the server can be reached and understood the request, but cannot complete the action in a reasonable time. This maybe due to temporary server load or a transient communication issue with another server. Details (optional): kind string The kind attribute of the resource being acted on. name string The operation that is being attempted. The server should set the Retry-After HTTP header and return retryAfterSeconds in the details field of the object. A value of 0 is the default. Http status code: 504 StatusServerTimeout MethodNotAllowed Indicates that the action the client attempted to perform on the resource was not supported by the code. For instance, attempting to delete a resource that can only be created. API calls that return MethodNotAllowed can never succeed. Http status code: 405 StatusMethodNotAllowed InternalError Indicates that an internal error occurred, it is unexpected and the outcome of the call is unknown. Details (optional): causes The original error. Http status code: 500 StatusInternalServerError code may contain the suggested HTTP return code for this status. Events Events are complementary to status information, since they can provide some historical information about status and occurrences in addition to current or previous status. Generate events for situations users or administrators should be alerted about. Choose a unique, specific, short, CamelCase reason for each event category. For example, FreeDiskSpaceInvalid is a good event reason because it is likely to refer to just one situation, but Started is not a good reason because it doesn't sufficiently indicate what started, even when combined with other event fields. Error creating foo or Error creating foo %s would be appropriate for an event message, with the latter being preferable, since it is more informational. Accumulate repeated events in the client, especially for frequent events, to reduce data volume, load on the system, and noise exposed to users. Naming conventions Go field names must be CamelCase. JSON field names must be camelCase. Other than capitalization of the initial letter, the two should almost always match. No underscores nor dashes in either. Field and resource names should be declarative, not imperative (DoSomething, SomethingDoer, DoneBy, DoneAt). Use Node where referring to the node resource in the context of the cluster. Use Host where referring to properties of the individual physical/virtual system, such as hostname, hostPath, hostNetwork, etc. FooController is a deprecated kind naming convention. Name the kind after the thing being controlled instead (e.g., Job rather than JobController). The name of a field that specifies the time at which something occurs should be called somethingTime. Do not use stamp (e.g., creationTimestamp). We use the fooSeconds convention for durations, as discussed in the units subsection. fooPeriodSeconds is preferred for periodic intervals and other waiting periods (e.g., over fooIntervalSeconds). fooTimeoutSeconds is preferred for inactivity/unresponsiveness deadlines. fooDeadlineSeconds is preferred for activity completion deadlines. Do not use abbreviations in the API, except where they are extremely commonly used, such as \"id\", \"args\", or \"stdin\". Acronyms should similarly only be used when extremely commonly known. All letters in the acronym should have the same case, using the appropriate case for the situation. For example, at the beginning of a field name, the acronym should be all lowercase, such as \"httpGet\". Where used as a constant, all letters should be uppercase, such as \"TCP\" or \"UDP\". The name of a field referring to another resource of kind Foo by name should be called fooName. The name of a field referring to another resource of kind Foo by ObjectReference (or subset thereof) should be called fooRef. More generally, include the units and/or type in the field name if they could be ambiguous and they are not specified by the value or value type. The name of a field expressing a boolean property called 'fooable' should be called Fooable, not IsFooable. Namespace Names The name of a namespace must be a DNS_LABEL. The kube- prefix is reserved for Kubernetes system namespaces, e.g. kube-system and kube-public. See the namespace docs for more information. Label, selector, and annotation conventions Labels are the domain of users. They are intended to facilitate organization and management of API resources using attributes that are meaningful to users, as opposed to meaningful to the system. Think of them as user-created mp3 or email inbox labels, as opposed to the directory structure used by a program to store its data. The former enables the user to apply an arbitrary ontology, whereas the latter is implementation-centric and inflexible. Users will use labels to select resources to operate on, display label values in CLI/UI columns, etc. Users should always retain full power and flexibility over the label schemas they apply to labels in their namespaces. However, we should support conveniences for common cases by default. For example, what we now do in ReplicationController is automatically set the RC's selector and labels to the labels in the pod template by default, if they are not already set. That ensures that the selector will match the template, and that the RC can be managed using the same labels as the pods it creates. Note that once we generalize selectors, it won't necessarily be possible to unambiguously generate labels that match an arbitrary selector. If the user wants to apply additional labels to the pods that it doesn't select upon, such as to facilitate adoption of pods or in the expectation that some label values will change, they can set the selector to a subset of the pod labels. Similarly, the RC's labels could be initialized to a subset of the pod template's labels, or could include additional/different labels. For disciplined users managing resources within their own namespaces, it's not that hard to consistently apply schemas that ensure uniqueness. One just needs to ensure that at least one value of some label key in common differs compared to all other comparable resources. We could/should provide a verification tool to check that. However, development of conventions similar to the examples in Labels make uniqueness straightforward. Furthermore, relatively narrowly used namespaces (e.g., per environment, per application) can be used to reduce the set of resources that could potentially cause overlap. In cases where users could be running misc. examples with inconsistent schemas, or where tooling or components need to programmatically generate new objects to be selected, there needs to be a straightforward way to generate unique label sets. A simple way to ensure uniqueness of the set is to ensure uniqueness of a single label value, such as by using a resource name, uid, resource hash, or generation number. Problems with uids and hashes, however, include that they have no semantic meaning to the user, are not memorable nor readily recognizable, and are not predictable. Lack of predictability obstructs use cases such as creation of a replication controller from a pod, such as people want to do when exploring the system, bootstrapping a self-hosted cluster, or deletion and re-creation of a new RC that adopts the pods of the previous one, such as to rename it. Generation numbers are more predictable and much clearer, assuming there is a logical sequence. Fortunately, for deployments that's the case. For jobs, use of creation timestamps is common internally. Users should always be able to turn off auto-generation, in order to permit some of the scenarios described above. Note that auto-generated labels will also become one more field that needs to be stripped out when cloning a resource, within a namespace, in a new namespace, in a new cluster, etc., and will need to be ignored around when updating a resource via patch or read-modify-write sequence. Inclusion of a system prefix in a label key is fairly hostile to UX. A prefix is only necessary in the case that the user cannot choose the label key, in order to avoid collisions with user-defined labels. However, I firmly believe that the user should always be allowed to select the label keys to use on their resources, so it should always be possible to override default label keys. Therefore, resources supporting auto-generation of unique labels should have a uniqueLabelKey field, so that the user could specify the key if they wanted to, but if unspecified, it could be set by default, such as to the resource type, like job, deployment, or replicationController. The value would need to be at least spatially unique, and perhaps temporally unique in the case of job. Annotations have very different intended usage from labels. They are primarily generated and consumed by tooling and system extensions, or are used by end-users to engage non-standard behavior of components. For example, an annotation might be used to indicate that an instance of a resource expects additional handling by non-kubernetes controllers. Annotations may carry arbitrary payloads, including JSON documents. Like labels, annotation keys can be prefixed with a governing domain (e.g. example.com/key-name). Unprefixed keys (e.g. key-name) are reserved for end-users. Third-party components must use prefixed keys. Key prefixes under the \"kubernetes.io\" and \"k8s.io\" domains are reserved for use by the kubernetes project and must not be used by third-parties. In early versions of Kubernetes, some in-development features represented new API fields as annotations, generally with the form something.alpha.kubernetes.io/name or something.beta.kubernetes.io/name (depending on our confidence in it). This pattern is deprecated. Some such annotations may still exist, but no new annotations may be defined. New API fields are now developed as regular fields. Other advice regarding use of labels, annotations, taints, and other generic map keys by Kubernetes components and tools: Key names should be all lowercase, with words separated by dashes instead of camelCase For instance, prefer foo.kubernetes.io/foo-bar over foo.kubernetes.io/fooBar, prefer desired-replicas over DesiredReplicas Unprefixed keys are reserved for end-users. All other labels and annotations must be prefixed. Key prefixes under \"kubernetes.io\" and \"k8s.io\" are reserved for the Kubernetes project. Such keys are effectively part of the kubernetes API and may be subject to deprecation and compatibility policies. Key names, including prefixes, should be precise enough that a user could plausibly understand where it came from and what it is for. Key prefixes should carry as much context as possible. For instance, prefer subsystem.kubernetes.io/parameter over kubernetes.io/subsystem-parameter Use annotations to store API extensions that the controller responsible for the resource doesn't need to know about, experimental fields that aren't intended to be generally used API fields, etc. Beware that annotations aren't automatically handled by the API conversion machinery. WebSockets and SPDY Some of the API operations exposed by Kubernetes involve transfer of binary streams between the client and a container, including attach, exec, portforward, and logging. The API therefore exposes certain operations over upgradeable HTTP connections (described in RFC 2817) via the WebSocket and SPDY protocols. These actions are exposed as subresources with their associated verbs (exec, log, attach, and portforward) and are requested via a GET (to support JavaScript in a browser) and POST (semantically accurate). There are two primary protocols in use today: Streamed channels When dealing with multiple independent binary streams of data such as the remote execution of a shell command (writing to STDIN, reading from STDOUT and STDERR) or forwarding multiple ports the streams can be multiplexed onto a single TCP connection. Kubernetes supports a SPDY based framing protocol that leverages SPDY channels and a WebSocket framing protocol that multiplexes multiple channels onto the same stream by prefixing each binary chunk with a byte indicating its channel. The WebSocket protocol supports an optional subprotocol that handles base64-encoded bytes from the client and returns base64-encoded bytes from the server and character based channel prefixes ('0', '1', '2') for ease of use from JavaScript in a browser. Streaming response The default log output for a channel of streaming data is an HTTP Chunked Transfer-Encoding, which can return an arbitrary stream of binary data from the server. Browser-based JavaScript is limited in its ability to access the raw data from a chunked response, especially when very large amounts of logs are returned, and in future API calls it may be desirable to transfer large files. The streaming API endpoints support an optional WebSocket upgrade that provides a unidirectional channel from the server to the client and chunks data as binary WebSocket frames. An optional WebSocket subprotocol is exposed that base64 encodes the stream before returning it to the client. Clients should use the SPDY protocols if their clients have native support, or WebSockets as a fallback. Note that WebSockets is susceptible to Head-of-Line blocking and so clients must read and process each message sequentially. In the future, an HTTP/2 implementation will be exposed that deprecates SPDY. Validation API objects are validated upon receipt by the apiserver. Validation errors are flagged and returned to the caller in a Failure status with reason set to Invalid. In order to facilitate consistent error messages, we ask that validation logic adheres to the following guidelines whenever possible (though exceptional cases will exist). Be as precise as possible. Telling users what they CAN do is more useful than telling them what they CANNOT do. When asserting a requirement in the positive, use \"must\". Examples: \"must be greater than 0\", \"must match regex '[a-z]+'\". Words like \"should\" imply that the assertion is optional, and must be avoided. When asserting a formatting requirement in the negative, use \"must not\". Example: \"must not contain '..'\". Words like \"should not\" imply that the assertion is optional, and must be avoided. When asserting a behavioral requirement in the negative, use \"may not\". Examples: \"may not be specified when otherField is empty\", \"only name may be specified\". When referencing a literal string value, indicate the literal in single-quotes. Example: \"must not contain '..'\". When referencing another field name, indicate the name in back-quotes. Example: \"must be greater than request\". When specifying inequalities, use words rather than symbols. Examples: \"must be less than 256\", \"must be greater than or equal to 0\". Do not use words like \"larger than\", \"bigger than\", \"more than\", \"higher than\", etc. When specifying numeric ranges, use inclusive ranges when possible. https://github.com/kubernetes/community/blob/017dbe2/contributors/devel/sig-architecture/api-conventions.md#primitive-types Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 08:24:44 "},"blog/规范/CodeReviewComments.html":{"url":"blog/规范/CodeReviewComments.html","title":"CodeReviewComments","keywords":"","body":"CodeReviewComments Go Code Review Comments link https://github.com/golang/go/wiki/CodeReviewComments Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/Untitled.html":{"url":"blog/规范/Untitled.html","title":"Untitled","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/git规范.html":{"url":"blog/规范/git规范.html","title":"Git规范","keywords":"","body":"项目使用手册 开发设备安装 node 环境。 将该工程根目录下的 package.json、commitlint.config.js、.npmrc、.commit-template（可选）、.cz-config.js（可选）文件拷贝到需要 commitlint 支持的项目 B 根目录。 在项目 B 根目录的终端运行 npm install。 在项目 B 的.gitignore 增加 node_modules 及 /package-lock.json 两条忽略路径。 执行命令 npm run config:commit-template 设置 commit 提示模板（可选，需要拷贝 .commit-template 文件）。 执行 npm run commit 可进行终端交互提示提交（可选，需要拷贝 .cz-config.js 文件）。 在执行完上面 1~4 的步骤后，每次 git commit 提交将会触发 commit message 规范校验。具体规范如下： LStack 前端代码提交规范 通过前面的介绍，我们了解了规范的代码提交不仅能很好地提升项目的可维护性，还可自动格式化生成版本日志。在了解了 Angular 团队的规范 和 Conventional Commits 之后，LStack 前端在其基础上形成了适用团队的规范并形成规格校验 插件 和 配置 。具体如下： Message 格式 (): 基本结构分为标题行（Header）、主题内容（Body）和页脚（Footer）三个部分，彼此之间使用空行分割。而不管是哪一个部分，他们中任何一行都不得超过 100 个字符，这是为了避免自动换行影响美观。 Header 标题行，是每次提交必填的部分，不要超过一行，用于描述本次主要修改的类别（type）、范围（scope）以及主题（subject）。 type 本次提交的类别，必填，其中 fix 和 feat 是主要的 type，分别代表问题的修复和新功能的增加，我们会根据 fix 和 feat 类别的提交自动生成 changelog 。 类别必须是下面其中一个： import: 限定类别，当源码引入开源库，希望作为 CI 的特使标记时，必须使用该类别，且本次提交必须不涉及其他类别，对应 action 只能为导入。 build: 限定类别，对构建系统或者相关外部依赖项进行了修改（比如: gulp, broccoli, npm），必须使用该类别，且本次提交必须不涉及其他类别。 ci: 限定类别，涉及 CI 配置文件或脚本修改的变动，必须使用该类别，且本次提交必须不涉及其他类别。 feat: 主类别，增加了新的功能特征，对应 action 只能为 添加，除限定类别外，当一次提交涉及主类别与其他多种类别时，取主类别。 fix: 主类别，修复 bug，bug 定义基线为上一个版本（即线上 bug，需要注意的是 ST 及 SIT 等由当前版本新功能引入的缺陷请使用 st 或 sit 类别），否则不要选择该类型，对应 action 只能为 修复。 st: 解决 ST 单子，仅限 release 分支，对应 action 只能为修复，该类别必须单独提交。如果单子是上版本引入的（线上 bug），必须选择 fix 类别。 sit: 解决 SIT 单子，仅限 release 分支，对应 action 只能为修复，该类别必须单独提交，如果单子是上版本引入的，必须选择 fix 类别。 style: 不影响代码含义的风格修改，比如空格、格式化、缺失的分号等，对应 action 只能为调整，该类别只能单独提交。 test: 新增或修改已有测试用例，该类别只能单独提交。 docs: 对文档类文件进行了修改，该类别只能单独提交。 revert: 回滚 commit 操作，对应 scope 为空，subject 为被回滚 commit message 的 完整 header，该类别只能单独提交。 pref: 提高性能的代码更改，对应 action 只能为优化。 refactor: 既不是修复 bug 也不是添加特征的代码重构。 version: 发布版本，一般由脚本自动化发布，对应 action 只能为发布。 scope 本次提交影响的范围，必填。scope 依据项目而定，例如在业务项目中可以依据菜单或者功能模块划分，如果是组件库开发，则可以依据组件划分。 格式为项目名/模块名，例如：node-pc/common rrd-h5/activity，而 we-sdk 不需指定模块名。如果一次 commit 修改多个模块，必须拆分成多次 commit，以便更好追踪和维护，即不允许跨 scope 进行提交。 LStack 前端团队以 monorepo 工程的 package 作为 scope，通过脚本自动获取，无需手动维护。需要注意的是，与工程相关的提交我们约定 scope 为 root。 subject 对于本次提交修改内容的简要描述，必填，以第一人称使用现在时，不以大写字母开头，不以.或。结尾，LStack 在社区的基础上将 subject 分为 action 和 content 两部分，两者间以空格相隔。 action：描述 subject 的具体动作，可选枚举为【'添加', '完善', '修复', '解决', '删除', '禁用', '修改', '调整', '优化', '重构', '发布', '合并', '导入'】。 Body 主题内容，非必须，描述为什么修改, 做了什么样的修改, 以及开发的思路等等，可以由多行组成。 Footer 页脚注释，在正文结束的一个空行之后，可以编写一行或多行脚注。 脚注必须包含关于提交的元信息，例如：关联的合并请求、Reviewer、破坏性变更，每条元信息一行。 破坏性变更必须标示在正文区域最开始处，或脚注区域中某一行的开始。一个破坏性变更必须包含大写的文本 BREAKING CHANGE，后面紧跟冒号和空格。 在 BREAKING CHANGE: 之后必须提供描述，以描述对 API 的变更。 列举本次提交 Closes 对应的 Tapd 缺陷 ID 或 issue 编号（存在则必填，即 st、sit、fix 类别提交必填）当该提交同时关了多个 issues 时，以','加空格进行分隔，如:Closes #33, #34。注意：当关闭的是 tapd 缺陷单子时，要求一次提交仅能 Closes 一个单子，如：Closes #1000001。 提交原则（强制） type、scope 及 subject 皆为必填项。需要注意的是 revert 类别的提交，scope 需且必须必须为空，其 subject 为被 revert commit 的 Header。 不允许跨 scope 提交，如：不允许在一次提交中同时提交 lcs 和 lcr 两个 scope 的文件。 当该次提交的 type 为限定类别（import、build、ci）时，必须原子性单独提交。 当该次提交的 type 同时包含主类别（feat、fix）与其他类别，选取主类别的 type。 break changes 指明是否产生了破坏性修改，涉及 break changes 的改动必须指明该项，类似版本升级、接口参数减少、接口删除、迁移等。 affect issues 指明是否影响了某个问题。当提交类别为 st、sit、fix 时，footer 中必须填写。例如我们使用 TAPD 时，需要填写其影响的 TAPD_ID：Closes #1000001 。 单次提交可以 Closes 多个 issue，但是如果关闭的是 TAPD 的缺陷时，单次提交只能 Closes 一个 TAPD 缺陷 ID。如果是重复的问题单，应该合并问题单。 提交的相关文件必须与 scope 相对应，如：不允许在修改 lcs bug 时提交为 lcr 的 scope。 subject 无需标点结尾。 标题行（Header）、主题内容（Body）和页脚（Footer）三个部分必须空一行（如果存在）。 除 revert 类别的提交外，标题行（Header）长度不能超过 72。 提交标题简要描述 subject 应该以指定动词中的 action 起头（只能为中文现在时），提交标题简要描述 subject 动词 action 后需且仅需添加一个空格。action 取自枚举 ['添加', '完善', '修复', '解决', '删除', '禁用', '修改', '调整', '优化', '重构', '发布', '合并', '导入']，作为 code review 的依据。 如果提交 footer 包含不兼容变更 BREAKING CHANGE，需要在类型/作用域前缀之后，':'之前，附加'!'字符，以进一步提醒注意破坏性变更。反之不需要。 message 文案书写规范建议参考这里 。中英文字符间必须加空格。 action 与 type 的对应规则： 1、当 type 为 'import' 时，action 必须为 '导入'。 2、当 type 为 'feat' 时，action 必须为 '添加'。 3、当 type 为 'version' 时，action 必须为 '发布'，反之亦然。 4、当 type 为 'pref' 时，action 必须为 '优化'。 5、当 type 为 'style' 时，action 必须为 '调整'。 6、当 type 为 [fix, st, sit] 之一时，action 必须为 '修复'，反之亦然。 提交规范（建议） body 用于填写详细描述，主要描述改动之前的情况及修改动机，对于小的修改不作要求，但是重大需求、更新等须添加 body 来作说明。 根据不同的 git 工作流限定问题单修复分支，LStack 前端使用的是 Git Flow，及 si、sit 类别提交仅限于 release 分支，相应的 release 只能进行 fix、st、sit 类型的提交。。 build 涉及的依赖变动理解为更新版本号，但涉及依赖的增删必须跟随相关功能变动，由功能或业务触发，对应 type 不为 build。 commit message 示例： 包含不兼容的变更 BREAKING CHANGE 的提交： refactor(lcs)!: 修改 命名空间结构体，以 yaml 资源形式定义 修改命名空间的结构体，替换原表单形式，用 yaml 资源的形式取代。 BREAKING CHANGE: 本次修改涉及 xxx 文件，具体改动内容为 xxx，影响模块 xxx，需要进行 xxx 处理以兼容本次修改。 只包含 Header 的简单提交： version(stylelint-config-lstack): 发布 v0.1.0 回滚某次提交的提交： revert: version(stylelint-config-lstack): 发布 v0.1.0 被 revert commit 为 #d21a542930e92f12c8ee9da5f378e66958f43875。 修复某个 sit 缺陷的提交 sit(ams): 修复 应用管理 HPA 没有检测 metrics-server 安装 问题分析：粗心导致。 问题解决：当拖拽自动扩缩时检测是否安装 metrics-server 插件。 Closes #1002859 更多社区例子可以参考：这里 常见问题 在规范提交时可能会遇到一些问题，以下是一些相关建议： 问题一：当我功能开发到一半，需要改 bug。 解决建议： 啥也不干，在提交时分 feat 和 fix 两次进行提交。 当前代码先暂存 git stash，改完 bug 提交后在推出来 git stash pop 继续开发新功能。 先临时提交一次半成品功能，改完 bug 提交后，再继续提交后续补充的代码。再本地将两次半成品 feat 类别提交合并 git rebase -i xx。（推荐） 问题二：当我功能开发完本地提交了一次，还没推到库上多人协作分支上，突然想起有些细节要补充。 解决建议： 补充一个 refactor 类别的提交。 补完代码后 git add xx && git commit --amend 对最后一次提交进行编辑修改。（推荐） 在建议 1 的基础上对两次提交进行合并为一次完成的 feat 类别提交 git rebase -i。 直接 git rebase -i xx 到指定 commit 进行修改。改完后 git add xx && git rebase --continue 问题三：刚刚开发新功能接到一个 bug 单子，忘了切分支，直接在开发分支上进行了提交。 解决建议： git cherry-pick xx1 将 commit 捡到 release 分支上，然后 dev 分支上 git reset HEAD~1，删除 reset 后的代码。 git cherry-pick xx1 将 commit 捡到 release 分支上，然后 dev 分支上 git rebase -i xx2 删除被捡 commit。（推荐） 注意：需要强调的是，所有涉及到变基的 git 操作仅限于本地分支或库上个人分支。与他人协作的线上分支严禁使用 git push -f。如果真的必须这么做，请知会相关项目负责人进行操作。 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/golang 代码规范.html":{"url":"blog/规范/golang 代码规范.html","title":"Golang 代码规范","keywords":"","body":" Golang　代码规范 注释语句 包的注释 命名规范 错误处理 Gofmt 方法规范 包管理 单元测试 项目完整性 Goroutine 生命周期 空切片 传值 Contexts Receiver 使用crypto rand生成随机值 代码走读检查点 检查业务逻辑 异常场景是否考虑完全 goroutine 是否存在内存泄漏 是否存在过多的重复代码，可以抽象 并发操作是否安全 数据库操作 Golang　代码规范 注释语句 参考 [https://golang.org/doc/effective_go.html#commentary] 所有导出的名称、函数声明、结构声明等都应该有注释 Go语言提供C风格的/* */块注释和C++风格的//行注释。 包应该有一个包注释，对于多文件包，注释只需要出现在任意一个文件中。包注释应当提供整个包相关的信息。 注释的句子应当具有完整性，这使得它们在提取到文档时能保持良好的格式。使用godoc时包注释将与声明一起提取，作为项目的解释性文本。这些注释的风格和内容决定了文档的质量。 注释应当以描述的事物名称开头，以句点（或者 ! ? ）结束。 参考以下格式： // Request represents a request to run a command. type Request struct { ... // Encode writes the JSON encoding of req to w. func Encode(w io.Writer, req *Request) { ... /* Package regexp implements a simple library for regular expressions. The syntax of the regular expressions accepted is: regexp: concatenation { '|' concatenation } concatenation: { closure } closure: term [ '*' | '+' | '?' ] term: '^' '$' '.' character '[' [ '^' ] character-ranges ']' '(' regexp ')' */ package regexp 包的注释 与godoc呈现的所有注释一样，包的注释必须出现在package子句的旁边，且不带空行： // Package math provides basic constants and mathematical functions. package math /* Package template implements data-driven templates for generating textual output such as HTML. .... */ package template 对于main包的注释，很多种注释格式都是可以接受的，比如在目录seedgen中的 package main包，注释可以这下写: // Binary seedgen ... package main 或 // Command seedgen ... package main or // Program seedgen ... package main 或 // The seedgen command ... package main 或 // The seedgen program ... package main 或 // Seedgen .. package main 请注意，以小写字母开头的句子不在包注释的可接受选项中，因为包是公开可见的，应当用合适的英语格式写成，包括将第一个词首字母大写。 获取更多有关包注释的规范 命名规范 原则：　见名知意，短小精悍，不允许在命名时中使用@、$和%等标点符号 函数命名： 驼峰规则 结构体： 采用驼峰命名法，首字母根据访问控制大写或者小写 包名: 保持package的名字和目录保持一致，尽量采取有意义的包名，简短，有意义，尽量和标准库不要冲突。 包名应该为小写单词，不要使用下划线或者混合大小写。 文件命名: 尽量采取有意义的文件名，简短，有意义，应该为小写单词 使用下划线分隔各个单词。 接口命名: 命名规则基本和上面的结构体类型相同 单个函数的结构名以 “er” 作为后缀，例如 Reader , Writer 常量: 常量均需使用全部大写字母组成，并使用下划线分词 变量: 和结构体类似，变量名称一般遵循驼峰法，首字母根据访问控制原则大写或者小写，但遇到特有名词时，需要遵循以下规则： 如果变量为私有，且特有名词为首个单词，则使用小写，如 apiClient 其它情况都应当使用该名词原有的写法，如 APIClient、repoID、UserID 若变量类型为 bool 类型，则名称应以 Has, Is, Can 或 Allow 开头 组合名 Go语言决定使用MixedCaps或mixedCaps来命名由多个单词组合的名称，而不是使用下划线来连接多个单词。即使它打破了其他语言的惯例。 例如，未导出的常量名为maxLength 而不是MaxLength 或 MAX_LENGTH。 错误处理 错误处理的原则就是不能丢弃任何有返回err的调用，不要使用 _ 丢弃，必须全部处理。接收到错误，要么返回err，或者使用log记录下来 尽早return：一旦有错误发生，马上返回 尽量不要使用panic，除非你知道你在做什么 错误描述如果是英文必须为小写，不需要标点结尾 采用独立的错误流进行处理 Gofmt 你可以在你的代码中运行 Gofmt 以解决大多数代码格式问题。几乎所有的代码都使用 gofmt。如果使用liteide编写Go代码时，使用ctrl+s即可调用gofmt。 另一种方法是使用 goimports，它是 gofmt的超集，可根据需要添加（删除）行。 方法规范 单一职责 注释方功能 注释变量和返回值 对于一些关键位置的代码逻辑，或者局部较为复杂的逻辑，需要有相应的逻辑说明，方便其他开发者阅读该段代码 包管理 尽量避免导入包时的重命名，以避免名称冲突。如果发生名称冲突，尽量重命名本地或项目特定的包。 导入包按名称分组，用空行隔开。 标准库包应始终位于第一组。 package main import ( \"fmt\" \"hash/adler32\" \"os\" \"appengine/foo\" \"appengine/user\" \"github.com/foo/bar\" \"rsc.io/goversion/version\" ) 可以使用 goimports 来规范包的排序。 单元测试 单元测试文件命名规范　example_test.go 测试用例的函数名称必须以Test开头 使用数组存放多个测试条件结构体，然后遍历数组，断言result vs except 测试失败时应当返回有效的错误信息，说明错误在哪，输入是什么，输出时什么，期望输出是什么。 一个典型的测试条件形如： if got != tt.want { t.Errorf(\"Foo(%q) = %d; want %d\", tt.in, got, tt.want) // or Fatalf, if test can't test anything more past this point } 请注意此处的命令是 实际结果 != 预期结果.一些测试框架鼓励程序员编写： 0 != x, \"expected 0, got x\"，go并不推荐。 在任何情况下，您有责任提供有用的错误信息，以便将来调试您的代码。 项目完整性 架构设计文档 项目功能 编译和打包镜像的脚本 必须使用go mod 进行版本控制 Goroutine 生命周期 当你需要使用goruntines时，请确保它们什么时候/什么条件下退出。 Goroutines可能会因阻塞channel的send或者receives而泄露,即使被阻塞的通道无法访问，垃圾收集器也不会终止goroutine。 即使gorountine没有泄露，当不再需要它们时仍在继续运行会导致难以诊断的问题。即使在不需要结果后，修改正在使用的数据也会导致数据竞争。 尽量保证并发代码足够简单，使gorountine的生命周期更明显。如果难以做到，请记录gorountines退出的时间和原因。 空切片 当声明一个空切片时， 使用 var t []string 而不是 t := []string{} 传值 不要仅仅为了节省几个字节而传指针给函数。如果函数仅仅将其参数\"x\"作为*x使用，那么参数就不应该是指针。 常见的传指针的情况有：传递一个string的指针、指向接口值(*io.Reader)的指针;这两种情况下，值本身都是固定的，可以直接传递。 对于大型数据结构，或者是小型的可能增长的结构，请考虑传指针。 goLang slice 和 array区别 更多情况请见 Receiver Type Contexts Go提供了context包来管理gorountine的生命周期。 context.Context类型的值包括了跨API和进程边界的安全凭证，跟踪信息，结束时间和取消信号。 使用context包时请遵循以下规则： 不要将 Contexts 放入结构体，context应该作为第一个参数传入。 不过也有例外，函数签名(method signature)必须与标准库或者第三方库中的接口相匹配func F(ctx context.Context, /* other arguments */) {} 即使函数允许，也不要传入nil的 Context。如果不知道用哪种 Context，可以使用context.TODO() 使用context的Value相关方法只应该用于在程序和接口中传递的和请求相关的元数据，不要用它来传递一些可选的参数 相同的 Context 可以传递给共享结束时间、取消信号、安全凭证和父进程追踪等信息的多个goroutine，Context 是并发安全的 从不做特定请求的函数可以使用 context.Background()，但即使您认为不需要，也可以在传递Context时使用错误(error)。如果您有充分的理由认为替代方案是错误的，那么只能直接使用context.Background() 不要在函数签名中创建Context类型或者使用Context以外的接口。 官方使用context的例子： package main import ( \"context\" \"fmt\" \"time\" ) func main() { d := time.Now().Add(50 * time.Millisecond) ctx, cancel := context.WithDeadline(context.Background(), d) // Even though ctx will be expired, it is good practice to call its // cancelation function in any case. Failure to do so may keep the // context and its parent alive longer than necessary. defer cancel() select { case Receiver 名称 方法receiver的名称应该反映其身份；通常，其类型的一个或两个字母缩写就足够了(例如\"client\"的\"c\"或\"cl\")。请不要使用通用名称如\"me\", \"this\" 或 \"self\"。请始终保持名称不变，如果你在一个地方命名receiver为 \"c\"，那么请不要在另一处叫其\"cl\" 指针还是值 如果您不知道怎么决定，请使用指针。但有时候receiver为值也挺有用，这种通常是出于效率的原因，且值为小的不变结构或基本类型的值。 一些建议： 请不要使用指针，如果满足receiver为map，func或者chan，或receiver为切片并且该方法不会重新分配切片 请不要使用指针，如果receiver是较小数组或结构体，没有可变的字段和指针，或者是一个简单的基本类型，int或者string 请使用指针，如果方法需要改变receiver 请使用指针，以避免被拷贝，如果receiver包含了sync.Mutex 或类似同步字段的结构 请使用指针，以提升效率，如果receiver是较大的数组或结构体 请使用指针，如果需要在方法中改变receiver的值 请使用指针，如果receiver是结构体，数组或切片这种成员是一个指向可变内容的指针 请使用指针，如果不清楚该如何选择 使用crypto rand生成随机值 请不要使用包 math/rand 来生成密钥，即使是一次性的。如果不提供种子，则密钥完全可以被预测到。就算用time.Nanoseconds()作为种子，也仅仅只有几个位上的差别。 使用crypto/rand's Reader作为代替。并且如果你需要生成文本，打印成16进制或者base64类型即可。 import ( \"crypto/rand\" // \"encoding/base64\" // \"encoding/hex\" \"fmt\" ) func Key() string { buf := make([]byte, 16) _, err := rand.Read(buf) if err != nil { panic(err) // out of randomness, should never happen } return fmt.Sprintf(\"%x\", buf) // or hex.EncodeToString(buf) // or base64.StdEncoding.EncodeToString(buf) } 代码走读检查点 检查业务逻辑 - 操作顺序， 如：删除操作，先处理业务，再删除库 异常场景是否考虑完全 - 网络中断异常 - 多使用场景的，是否列举全面 - 依赖的第三方服务暂停或升级 goroutine 是否存在内存泄漏 - 发送到一个没有接受者的channel - 从没有发送者的channel中接受数据 - 传递尚未初始化的channel - goroutine 泄露不仅仅是因为 channel 的错误使用造成的。泄露的原因也可能是 I/O 操作上的堵塞，例如发送请求到 API 服务器，而没有使用超时。另一种原因是，程序可以单纯地陷入死循环中 是否存在过多的重复代码，可以抽象 - log 打印尽量放到最外层 并发操作是否安全 - map 并发写要加锁 - 数据库记录修改需要使用带判断的原子操作修改, update ... set ... where condition - 使用锁的要记得解锁, defer unlock 数据库操作 - 最大连接数设置 - 失败重连接 - 失败后是否需要回滚操作 - 插入前数据库重复条目检查 - 删除前关联应用是否已经删除 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/kubernetes yaml validate.html":{"url":"blog/规范/kubernetes yaml validate.html","title":"Kubernetes Yaml Validate","keywords":"","body":"Validating Kubernetes YAML for best practice and policies https://learnk8s.io/validating-kubernetes-yaml TL;DR: The article compares six static tools to validate and score Kubernetes YAML files for best practices and compliance. Kubernetes workloads are most commonly defined as YAML formatted documents. One of the challenges with YAML is that it's rather hard to express constraints or relationships between manifest files. What if you wish to check that all images deployed into the cluster are pulled from a trusted registry? How can you prevent Deployments that don't have PodDisruptionBudgets from being submitted to the cluster? Integrating static checking allows catching errors and policy violations closer to the development lifecycle. And since the guarantee around the validity and safety of the resource definitions is improved, you can trust that production workloads are following best practices. The ecosystem of static checking of Kubernetes YAML files can be grouped in the following categories: API validators — Tools in this category validate a given YAML manifest against the Kubernetes API server. Built-in checkers — Tools in this category bundle opinionated checks for security, best practices, etc. Custom validators — Tools in this category allow writing custom checks in several languages such as Rego and Javascript. In this article, you will learn and compare six different tools: Kubeval Kube-score Config-lint Copper Conftest Polaris Let's get started! Validating a deployment Before you start comparing tools, you should set a baseline. The following manifest has a few issues and isn't following best practices — how many can you spot? base-valid.yaml apiVersion: apps/v1 kind: Deployment metadata: name: http-echo spec: replicas: 2 selector: matchLabels: app: http-echo template: metadata: labels: app: http-echo spec: containers: - name: http-echo image: hashicorp/http-echo args: [\"-text\", \"hello-world\"] ports: - containerPort: 5678 --- apiVersion: v1 kind: Service metadata: name: http-echo spec: ports: - port: 5678 protocol: TCP targetPort: 5678 selector: app: http-echo You will be using this YAML file to compare the different tools. You can find the above YAML manifest as the file base-valid.yaml along with the other manifests referred to in the article in this git repository. The manifest describes a web application that always replies with a \"Hello World\" message on port 5678. You can deploy the application with: bash kubectl apply -f hello-world.yaml You can test it with: bash kubectl port-forward svc/http-echo 8080:5678 You can visit http://localhost:8080 and confirm that the app works as expected. But does it follow best practices? Let's start. Kubeval The premise of kubeval is that any interaction with Kubernetes goes via its REST API. Hence, you can use the API schema to validate whether a given YAML input conforms to the schema. Let's have a look at an example. You can follow the instructions on the project website to install kubeval. As of this writing, the latest release is 0.15.0. Once installed, let's run it with the manifest discussed earlier: bash kubeval base-valid.yaml PASS - base-valid.yaml contains a valid Deployment (http-echo) PASS - base-valid.yaml contains a valid Service (http-echo) When successful, kubeval exits with an exit code of 0. You can verify the exit code with: bash echo $? 0 Let's now try kubeval with another manifest: kubeval-invalid.yaml apiVersion: apps/v1 kind: Deployment metadata: name: http-echo spec: replicas: 2 template: metadata: labels: app: http-echo spec: containers: - name: http-echo image: hashicorp/http-echo args: [\"-text\", \"hello-world\"] ports: - containerPort: 5678 --- apiVersion: v1 kind: Service metadata: name: http-echo spec: ports: - port: 5678 protocol: TCP targetPort: 5678 selector: app: http-echo Can you spot the issue? Let's run kubeval: bash kubeval kubeval-invalid.yaml WARN - kubeval-invalid.yaml contains an invalid Deployment (http-echo) - selector: selector is required PASS - kubeval-invalid.yaml contains a valid Service (http-echo) # let's check the return value echo $? 1 The resource doesn't pass the validation. Deployments using the app/v1 API version have to include a selector that matches the Pod label. The above manifest doesn't include the selector and running kubeval against the manifest reported an error and a non-zero exit code. You may wonder what happens when you run kubectl apply -f with the above manifest? Let's try: bash kubectl apply -f kubeval-invalid.yaml error: error validating \"kubeval-invalid.yaml\": error validating data: ValidationError(Deployment.spec): missing required field \"selector\" in io.k8s.api.apps.v1.DeploymentSpec; if you choose to ignore these errors, turn validation off with --validate=false Exactly the error that kubeval warned you about. You can fix the resource by adding the selector like this: base-valid.yaml apiVersion: apps/v1 kind: Deployment metadata: name: http-echo spec: replicas: 2 selector: matchLabels: app: http-echo template: metadata: labels: app: http-echo spec: containers: - name: http-echo image: hashicorp/http-echo args: [\"-text\", \"hello-world\"] ports: - containerPort: 5678 --- apiVersion: v1 kind: Service metadata: name: http-echo spec: ports: - port: 5678 protocol: TCP targetPort: 5678 selector: app: http-echo The advantage of a tool like kubeval is that you can catch such errors early in your deployment cycle. Also, you don't need access to a cluster to run the checks — they could run offline. By default, kubeval validates resources against the latest unreleased Kubernetes API schema. In most cases, however, you might want to run validations against a specific Kubernetes release. You can test a specific API version using the flag --kubernetes-version: bash kubeval --kubernetes-version 1.16.1 base-valid.yaml Please notice that the release version should be of the form of Major.Minor.Patch. To see the versions available for validating against, check out the JSON schema on GitHub which kubeval uses to perform its validation. If you need to run kubeval offline, you can download the schemas and then use the --schema-location flag to use a local directory. In addition to individual YAML files, you can run kubeval against directories as well as standard input. You should also know that Kubeval makes it for easy integration with your Continuous Integration pipeline. If you want to include the checks before you submit your manifests to the cluster, you will be pleased to know that kubeval supports three output formats: Plain text JSON and Test Anything Protocol (TAP) And you may be able to use one of the formats to parse the output further to create a custom summary of the results. One limitation of kubeval is that it is currently not able to validate against Custom Resource Definitions (CRDs). However, you can tell kubeval to ignore them. Kubeval is an excellent choice to check and validate resources, but please notice that a resource that passes the test isn't guaranteed to conform to best practices. As an example, using the latest tag in the container images isn't considered a best practice. However, Kubeval doesn't report that as an error, and it will validate the YAML without warnings. What if you want to score the YAML and catch violations such as the latest tag? How can you check your YAML files against best practices? Kube-score Kube-score analyses YAML manifests and scores them against in-built checks. These checks are selected based on security recommendations and best practices, such as: Running containers as a non-root user. Specifying health checks for pods. Defining resource requests and limits. The result of a check can be OK, WARNING, or CRITICAL. You can try out kube-score online or you can install it locally. As of this writing, the latest release is 1.7.0. Let's try and run it with the previous manifest base-valid.yaml: bash kube-score score base-valid.yaml apps/v1/Deployment http-echo [CRITICAL] Container Image Tag · http-echo -> Image with latest tag Using a fixed tag is recommended to avoid accidental upgrades [CRITICAL] Pod NetworkPolicy · The pod does not have a matching network policy Create a NetworkPolicy that targets this pod [CRITICAL] Pod Probes · Container is missing a readinessProbe A readinessProbe should be used to indicate when the service is ready to receive traffic. Without it, the Pod is risking to receive traffic before it has booted. It is also used during rollouts, and can prevent downtime if a new version of the application is failing. More information: https://github.com/zegl/kube-score/blob/master/README_PROBES.md [CRITICAL] Container Security Context · http-echo -> Container has no configured security context Set securityContext to run the container in a more secure context. [CRITICAL] Container Resources · http-echo -> CPU limit is not set Resource limits are recommended to avoid resource DDOS. Set resources.limits.cpu · http-echo -> Memory limit is not set Resource limits are recommended to avoid resource DDOS. Set resources.limits.memory · http-echo -> CPU request is not set Resource requests are recommended to make sure that the application can start and run without crashing. Set resources.requests.cpu · http-echo -> Memory request is not set Resource requests are recommended to make sure that the application can start and run without crashing. Set resources.requests.memory [CRITICAL] Deployment has PodDisruptionBudget · No matching PodDisruptionBudget was found It is recommended to define a PodDisruptionBudget to avoid unexpected downtime during Kubernetes maintenance operations, such as when draining a node. [WARNING] Deployment has host PodAntiAffinity · Deployment does not have a host podAntiAffinity set It is recommended to set a podAntiAffinity that stops multiple pods from a deployment from being scheduled on the same node. This increases availability in case the node becomes unavailable. The YAML file passes the kubeval checks, but kube-score points out several deficiencies: Missing readiness probes. Missing memory and CPU requests and limits. Missing Pod disruption budgets. Missing anti-affinity rules to maximise availability. The container runs as root. Those are all valid points that you should address to make your deployment more robust and reliable. The kube-score command prints a human-friendly output containing all the WARNING and CRITICAL violations, which is great during development. If you plan to use it as part of your Continuous Integration pipeline, you can use a more concise output with the flag --output-format ci which also prints the checks with level OK: bash kube-score score base-valid.yaml --output-format ci [OK] http-echo apps/v1/Deployment [OK] http-echo apps/v1/Deployment [CRITICAL] http-echo apps/v1/Deployment: (http-echo) CPU limit is not set [CRITICAL] http-echo apps/v1/Deployment: (http-echo) Memory limit is not set [CRITICAL] http-echo apps/v1/Deployment: (http-echo) CPU request is not set [CRITICAL] http-echo apps/v1/Deployment: (http-echo) Memory request is not set [CRITICAL] http-echo apps/v1/Deployment: (http-echo) Image with latest tag [OK] http-echo apps/v1/Deployment [CRITICAL] http-echo apps/v1/Deployment: The pod does not have a matching network policy [CRITICAL] http-echo apps/v1/Deployment: Container is missing a readinessProbe [CRITICAL] http-echo apps/v1/Deployment: (http-echo) Container has no configured security context [CRITICAL] http-echo apps/v1/Deployment: No matching PodDisruptionBudget was found [WARNING] http-echo apps/v1/Deployment: Deployment does not have a host podAntiAffinity set [OK] http-echo v1/Service [OK] http-echo v1/Service [OK] http-echo v1/Service [OK] http-echo v1/Service Similar to kubeval, kube-score returns a non-zero exit code when there is a CRITICAL check that failed, but you configured it to fail even on WARNINGs. There is also a built-in check to validate resources against different API versions — similar to kubeval. However, this information is hardcoded in kube-score itself, and you can't select a different Kubernetes version. Hence, if you upgrade your cluster or you have several different clusters running different versions, this can prove to be a severe limitation. Please notice that there is an open issue to implement this feature. You can learn more about kube-score on the official website. Kube-score checks are an excellent tool to enforce best practices, but what if you want to customise one, or add your own rules? You can't. Kube-score isn't designed to be extendable and you can't add or tweak policies. If you want to write custom checks to comply with your organisational policies, you can use one of the next four options - config-lint, copper, conftest or polaris. Config-lint Config-lint is a tool designed to validate configuration files written in YAML, JSON, Terraform, CSV, and Kubernetes manifests. You can install it using the instructions on the project website. The latest release is 1.5.0 at the time of this writing. Config-lint comes with no in-built checks for Kubernetes manifests. You have to write your own rules to perform any validations. The rules are written as YAML files, referred to as rulesets and have the following structure: rule.yaml version: 1 description: Rules for Kubernetes spec files type: Kubernetes files: - \"*.yaml\" rules: # list of rules Let's have a look in more detail: The type field indicates what type of configuration you will be checking with config-lint — it is always Kubernetes for Kubernetes manifests. The files field accepts a directory as input in addition to individual files. The rules field is where you can define custom checks. Let's say you wish to check whether the images in a Deployment are always pulled from a trusted repository such as my-company.com/myapp:1.0. A config-lint rule implementing such a check could look like this: rule-trusted-repo.yaml - id: MY_DEPLOYMENT_IMAGE_TAG severity: FAILURE message: Deployment must use a valid image tag resource: Deployment assertions: - every: key: spec.template.spec.containers expressions: - key: image op: starts-with value: \"my-company.com/\" Each rule must have the following attributes: id — This uniquely identifies the rule. severity — It has to be one of FAILURE, WARNING, and NON_COMPLIANT. message — If a rule is violated, the contents of this string is shown. resource — The kind of resource you want this rule to be applied to. assertions — A list of conditions that will be evaluated against the specified resource. In the above rule, the every assertion checks that each container in a Deployment (key: spec.templates.spec.containers) uses a trusted image (i.e. the image starts with \"my-company.com/\"). The complete ruleset looks like this: ruleset.yaml version: 1 description: Rules for Kubernetes spec files type: Kubernetes files: - \"*.yaml\" rules: - id: DEPLOYMENT_IMAGE_REPOSITORY severity: FAILURE message: Deployment must use a valid image repository resource: Deployment assertions: - every: key: spec.template.spec.containers expressions: - key: image op: starts-with value: \"my-company.com/\" If you want to test the check, you can save the ruleset as check_image_repo.yaml. Let's now run the validation against the base-valid.yaml file: bash config-lint -rules check_image_repo.yaml base-valid.yaml [ { \"AssertionMessage\": \"Every expression fails: And expression fails: image does not start with my-company.com/\", \"Category\": \"\", \"CreatedAt\": \"2020-06-04T01:29:25Z\", \"Filename\": \"test-data/base-valid.yaml\", \"LineNumber\": 0, \"ResourceID\": \"http-echo\", \"ResourceType\": \"Deployment\", \"RuleID\": \"DEPLOYMENT_IMAGE_REPOSITORY\", \"RuleMessage\": \"Deployment must use a valid image repository\", \"Status\": \"FAILURE\" } ] It fails. Now, let's consider the following manifest with a valid image repository: image-valid-mycompany.yaml apiVersion: apps/v1 kind: Deployment metadata: name: http-echo spec: replicas: 2 selector: matchLabels: app: http-echo template: metadata: labels: app: http-echo spec: containers: - name: http-echo image: my-company.com/http-echo:1.0 args: [\"-text\", \"hello-world\"] ports: - containerPort: 5678 Run the same check with the above manifest and there will be no violations reported: bash config-lint -rules check_image_repo.yaml image-valid-mycompany.yaml [] Config-lint is a promising framework that lets you write custom checks for Kubernetes YAML manifests using a YAML DSL. But what if you want to express more complex logic and checks? Isn't YAML too limiting for that? What if you could express those checks with a real programming language? Copper Copper V2 is a framework that validates manifests using custom checks — just like config-lint. However, Copper doesn't use YAML to define the checks. Instead, tests are written in JavaScript and Copper provides a library with a few basic helpers to assist in reading Kubernetes objects and reporting errors. You can follow the official documentation to install Copper. The latest release at the time of this writing is 2.0.1. Similar to config-lint, Copper has no built-in checks. Let's write a check to make sure that deployments can pull container images only from a trusted repository such as my-company.com. Create a new file, check_image_repo.js with the following content: check_image_repo.js {% math %}.forEach(function($){ if ($.kind === 'Deployment') { $.spec.template.spec.containers.forEach(function(container) { var image = new DockerImage(container.image); if (image.registry.lastIndexOf('my-company.com/') != 0) { errors.add_error('no_company_repo',\"Image \" + $.metadata.name + \" is not from my-company.com repo\", 1) } }); } }); Now, to run this check against our base-valid.yaml manifest, you can use the copper validate command: bash copper validate --in=base-valid.yaml --validator=check_image_tag.js Check no_company_repo failed with severity 1 due to Image http-echo is not from my-company.com repo Validation failed As you can imagine, you can write more sophisticated checks such as validating domain names for Ingress manifests or reject any Pod that runs as privileged. Copper has a few built-in helpers: The DockerImage function reads the specified input file and creates an object containing the following attributes: name containing the image name tag containing the image tag registry containing the image registry registry_url containing the protocol and the image registry, and fqin representing the entire fully qualified image location. The findByName function helps find a resource given a kind and name from an input file The findByLabels function helps find a resource provided kind and the labels. You can see all available helpers here. By default, it loads the entire input YAML file into the {% endmath %} variable and makes it available in your scripts (if you used jQuery in the past, you might find this pattern familiar). In addition to not having to learn a custom language, you have access to the entire JavaScript language for writing your checks such as string interpolation, functions, etc. It is worth noting that the current copper release embeds the ES5 version of the JavaScript engine and not ES6. To learn more, you can visit the official project website. If Javascript isn't your preferred language and you prefer a language designed to query and describe policies, you should check out conftest. Conftest Conftest is a testing framework for configuration data that can be used to check and verify Kubernetes manifests. Tests are written using the purpose-built query language, Rego. You can install conftest following the instructions on the project website. At the time of writing, the latest release is 0.18.2. Similar to config-lint and copper, conftest doesn't come with any in-built checks. So let's try it out, by writing a policy. As for the previous example, you will check that the container is coming from a trusted source. Create a new directory, conftest-checks and a file named check_image_registry.rego with the following content: check_image_registry.rego package main deny[msg] { input.kind == \"Deployment\" image := input.spec.template.spec.containers[_].image not startswith(image, \"my-company.com/\") msg := sprintf(\"image '%v' doesn't come from my-company.com repository\", [image]) } Let's now run conftest to validate the manifest base-valid.yaml: bash conftest test --policy ./conftest-checks base-valid.yaml FAIL - base-valid.yaml - image 'hashicorp/http-echo' doesn't come from my-company.com repository 1 tests, 1 passed, 0 warnings, 1 failure Of course, it fails since the image isn't trusted. The above Rego file specifies a deny block which evaluates to a violation when true. When you have more than one deny block, conftest checks them independently, and the overall result is a violation of any of the blocks results in a breach. Other than the default output format, conftest supports JSON, TAP, and a table format via the --output flag, which is excellent if you wish to integrate the reports with your existing Continuous Integration pipeline. To help debug policies, conftest has a convenient --trace flag which prints a trace of how conftest is parsing the specified policy files. Conftest policies can be published and shared as artefacts in OCI (Open Container Initiative) registries. The commands, push and pull allow publishing an artefact and pulling an existing artefact from a remote registry. Let's see a demo of publishing the above policy to a local docker registry using conftest push. Start a local docker registry using: bash docker run -it --rm -p 5000:5000 registry From another terminal, navigate to the conftest-checks directory created above and run the following command: bash conftest push 127.0.0.1:5000/amitsaha/opa-bundle-example:latest The command should complete successfully with the following message: bash 2020/06/10 14:25:43 pushed bundle with digest: sha256:e9765f201364c1a8a182ca637bc88201db3417bacc091e7ef8211f6c2fd2609c Now, create a temporary directory and run the conftest pull command which will download the above bundle to the temporary directory: bash cd $(mktemp -d) conftest pull 127.0.0.1:5000/amitsaha/opa-bundle-example:latest You will see that there is a new sub-directory policy in the temporary directory containing the policy file pushed earlier: bash tree . └── policy └── check_image_registry.rego You can even run the tests directly from the repository: bash conftest test --update 127.0.0.1:5000/amitsaha/opa-bundle-example:latest base-valid.yaml .. FAIL - base-valid.yaml - image 'hashicorp/http-echo' doesn't come from my-company.com repository 2 tests, 1 passed, 0 warnings, 1 failure Unfortunately, DockerHub is not yet one of the supported registries. However, if you are using Azure Container Registry (ACR) or running your container registry, you might be in luck. The artefact format is the same as used by Open Policy Agent (OPA) bundles, which makes it possible to use conftest to run tests from existing OPA bundles. You can find out more about sharing policies and other features of conftest on the official website. Polaris The last tool you will explore in this article is polaris (https://github.com/FairwindsOps/polaris). Polaris can be either installed inside a cluster or as a command-line tool to analyse Kubernetes manifests statically. When running as a command-line tool, it includes several built-in checks covering areas such as security and best practices — similar to kube-score. Also, you can use it to write custom checks similar to config-lint, copper, and conftest. In other words, polaris combines the best of the two categories: built-in and custom checkers. You can install the polaris command-line tool as per the instructions on the project website. The latest release at the time of writing is 1.0.3. Once installed, you can run polaris against the base-valid.yaml manifest with: bash polaris audit --audit-path base-valid.yaml The above command will print a JSON formatted string detailing the checks that were run and the result of each test. The output will have the following structure: output.json { \"PolarisOutputVersion\": \"1.0\", \"AuditTime\": \"0001-01-01T00:00:00Z\", \"SourceType\": \"Path\", \"SourceName\": \"test-data/base-valid.yaml\", \"DisplayName\": \"test-data/base-valid.yaml\", \"ClusterInfo\": { \"Version\": \"unknown\", \"Nodes\": 0, \"Pods\": 2, \"Namespaces\": 0, \"Controllers\": 2 }, \"Results\": [ /* long list */ ] } The complete output is available here. Similar to kube-score, polaris identifies several cases where the manifest falls short of recommended best practices which include: Missing health checks for the pods. Container images don't have a tag specified. The container runs as root. CPU and memory requests and limits are not set. Each check is either classified with a severity level of warning or danger. To learn more about the current in-built checks, refer to the documentation. If you are not interested in the detailed results, passing the flag --format score prints a number in the range 1-100 which polaris refers to as the score: bash polaris audit --audit-path test-data/base-valid.yaml --format score 68 The closer the score is to 100, the higher the degree of conformance. If you inspect the exit code of the polaris audit command, you will see that it was 0. To make polaris audit exit with a non-zero code, you can make use of two other flags. The --set-exit-code-below-score flag accepts a threshold score in the range 1-100 and will exit with an exit code of 4 when the score is below the threshold. This is very useful in cases where your baseline score is 75, and you want to be alerted when it goes lower. The --set-exit-code-on-danger flag will exit with an exit code of 3 when any of the danger checks fail. Let's now see how you can define a custom check for polaris to test whether the container image in a Deployment is from a trusted registry. Custom checks are defined in a YAML format with the test itself described using JSON Schema. The following YAML snippet defines a new check-called checkImageRepo: snippet.yaml checkImageRepo: successMessage: Image registry is valid failureMessage: Image registry is not valid category: Images target: Container schema: '$schema': http://json-schema.org/draft-07/schema type: object properties: image: type: string pattern: ^my-company.com/.+$ Let's have a closer look at it: successMessage is a string which will be displayed when the check succeeds. failureMessage is displayed when the test is unsuccessful. category refers to one of the categories - Images, Health Checks, Security, Networking and Resources. target is a string that determines which spec object the check is applied against - and should be one of Container, Pod, or Controller. The test itself is defined in the schema object using JSON schema. Here the check uses the pattern keyword to match whether the image is from an allowed registry or not. To run the check defined above you will need to create a Polaris configuration file as follows: polaris-conf.yaml checks: checkImageRepo: danger customChecks: checkImageRepo: successMessage: Image registry is valid failureMessage: Image registry is not valid category: Images target: Container schema: '$schema': http://json-schema.org/draft-07/schema type: object properties: image: type: string pattern: ^my-company.com/.+$ Let's break down the file: The checks field specifies the checks and their severity. Since you want to be alerted when the image isn't trusted, checkImageRepo is assigned a danger severity level. The checkImageRepo check itself is then defined in the customChecks object. You can save the above file as custom_check.yaml and run polaris audit with the YAML manifest that you wish to validate. You can test it with the base-valid.yaml manifest: bash polaris audit --config custom_check.yaml --audit-path base-valid.yaml You will see that polaris audit ran only the custom check defined above, which did not succeed. If you amend the container image to my-company.com/http-echo:1.0, polaris will report success. The Github repository contains the amended manifest, so you can test the previous command against the image-valid-mycompany.yaml manifest. But how do you run both the built-in and custom checks? The configuration file above should be updated with all the built-in check identifiers and should look as follows: config_with_custom_check.yaml checks: cpuRequestsMissing: warning cpuLimitsMissing: warning # Other inbuilt checks.. # .. # custom checks checkImageRepo: danger customChecks: checkImageRepo: successMessage: Image registry is valid failureMessage: Image registry is not valid category: Images target: Container schema: '$schema': http://json-schema.org/draft-07/schema type: object properties: image: type: string pattern: ^my-company.com/.+$ You can see an example of a complete configuration file here. You can test the base-valid.yaml manifest with custom and built-in checks with: bash polaris audit --config config_with_custom_check.yaml --audit-path base-valid.yaml Polaris augments the built-in checks with your custom checks, thus combining the best of both worlds. However, not having access to more powerful languages like Rego or JavaScript may be a limitation to write more sophisticated checks. To learn more about polaris, check out the project website. Summary While there are plenty of tools to validate, score and lint Kubernetes YAML files, it's important to have a mental model on how you will design and perform the checks. As an example, if you were to think about Kubernetes manifests going through a pipeline, kubeval could be the first step in such a pipeline as it validates if the object definitions conform to the Kubernetes API schema. Once this check is successful, perhaps you could pass on to more elaborated tests such as standard best practices and custom policies. Kube-score and polaris are to excellent choices here. If you have complex requirements and want to customise the checks down to the details, you should consider copper, config-lint, and conftest. While both conftest and config-lint use more YAML to define custom validation rules, copper gives you access to a real programming language making it quite attractive. But should you use one of these and write all the checks from scratch or should you instead use Polaris and write only the additional custom checks? It depends. The following table presents a summary of the tools: Tool Features Limitations Custom checks kubeval Validate YAML manifests against API Schema of a specific version Doesn't recognise CRDs No kube-score Analyses YAML manifests against standard best practices Deprecated API version check Doesn't validate the definition No support for specific API versions for deprecated resource check No copper A generic framework for writing custom checks for YAML manifests using JavaScript No inbuilt checks Sparse documentation Yes config-lint A generic framework for writing custom checks using DSL embedded in YAML The framework also supports other configuration formats - Terraform, for example. No inbuilt tests The inbuilt assertions and operations may not be sufficient to account for all checks Yes conftest A generic framework for writing custom checks in Rego Rego is a robust policy language Sharing policies via OCI bundles No inbuilt checks Rego has a learning curve Docker hub not supported for sharing of policies Yes polaris Analyses YAML manifest against standard best practices Allows writing custom checks using JSON Schema JSON Schema-based checks may not be sufficient Yes Since these tools don't rely on access to a Kubernetes cluster, they are straightforward to set up and enable you to enforce gating as well as give quick feedback to pull request authors for projects. Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/linux 文件命名规范.html":{"url":"blog/规范/linux 文件命名规范.html","title":"Linux 文件命名规范","keywords":"","body":"精选文章 Linux文件命名规则 Linux文件命名规则 作者：街角守望妳 时间: 2021-02-05 08:53:53 标签：文件命名linux基础 【摘要】文件命名规则(1) 除了/之外，所有字符都合法；(2) 特殊字符如@、#、￥、&、()、-、空格等最好不要使用，当使用空格作为文件名时，执行命令会出错；(3) 避免使用”.”作为文件名的第一个字符，因为在Linux系统中以”.”为开头的文件代表隐藏，系统将自动隐藏以”.”为开头的文件；(4) Linux系统区分大小写，因此文件命名也区分大小写；(5) Linux文件后缀名无意义，但是为方便识... 文件命名规则 (1) 除了/之外，所有字符都合法； (2) 特殊字符如@、#、￥、&、()、-、空格等最好不要使用，当使用空格作为文件名时，执行命令会出错； (3) 避免使用”.”作为文件名的第一个字符，因为在Linux系统中以”.”为开头的文件代表隐藏，系统将自动隐藏以”.”为开头的文件； (4) Linux系统区分大小写，因此文件命名也区分大小写； (5) Linux文件后缀名无意义，但是为方便识别应定义后缀(.txt、.php等)，定义后缀在大多数情况亦能将文件与目录区分； (6) 文件位置最好设置在Linux专用目录下，如配置文件大多时候放置于/etc目录下 (7) 文件夹及文件的命名尽量聚有其特定的含义。 (8) 三个特殊目录，”.”：代表当前目录，”..”：代表上一级目录，”/”：代表根目录。 Linux 文件名合法性检测 ^[^+-./\\t\\b@#$%*()\\[\\]][^/\\t\\b@#$%*()\\[\\]]{1,254}$ 文件夹&&文件名 非打印字符 非打印字符也可以是正则表达式的组成部分。下表列出了表示非打印字符的转义序列： 字符 描述 \\cx 匹配由x指明的控制字符。例如， \\cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 'c' 字符。 \\f 匹配一个换页符。等价于 \\x0c 和 \\cL。 \\n 匹配一个换行符。等价于 \\x0a 和 \\cJ。 \\r 匹配一个回车符。等价于 \\x0d 和 \\cM。 \\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \\f\\n\\r\\t\\v]。注意 Unicode 正则表达式会匹配全角空格符。 \\S 匹配任何非空白字符。等价于 \\f\\n\\r\\t\\v。 \\t 匹配一个制表符。等价于 \\x09 和 \\cI。 \\v 匹配一个垂直制表符。等价于 \\x0b 和 \\cK。 特殊字符 所谓特殊字符，就是一些有特殊含义的字符，如上面说的 runoo*b 中的 *，简单的说就是表示任何字符串的意思。如果要查找字符串中的 * 符号，则需要对 * 进行转义，即在其前加一个 \\，runo*ob 匹配字符串 runo*ob。 许多元字符要求在试图匹配它们时特别对待。若要匹配这些特殊字符，必须首先使字符\"转义\"，即，将反斜杠字符\\ 放在它们前面。下表列出了正则表达式中的特殊字符： 特别字符 描述 $ 匹配输入字符串的结尾位置。如果设置了 RegExp 对象的 Multiline 属性，则 $ 也匹配 '\\n' 或 '\\r'。要匹配 $ 字符本身，请使用 $。 ( ) 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。要匹配这些字符，请使用 ( 和 )。 * 匹配前面的子表达式零次或多次。要匹配 字符，请使用 \\。 + 匹配前面的子表达式一次或多次。要匹配 + 字符，请使用 +。 . 匹配除换行符 \\n 之外的任何单字符。要匹配 . ，请使用 . 。 [ 标记一个中括号表达式的开始。要匹配 [，请使用 [。 ? 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符。要匹配 ? 字符，请使用 \\?。 \\ 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如， 'n' 匹配字符 'n'。'\\n' 匹配换行符。序列 '\\' 匹配 \"\\\"，而 '(' 则匹配 \"(\"。 ^ 匹配输入字符串的开始位置，除非在方括号表达式中使用，当该符号在方括号表达式中使用时，表示不接受该方括号表达式中的字符集合。要匹配 ^ 字符本身，请使用 \\^。 { 标记限定符表达式的开始。要匹配 {，请使用 {。 \\ 指明两项之间的一个选择。要匹配 \\ ，请使用 \\ 。 限定符 限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 * 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。 正则表达式的限定符有： 字符 描述 * 匹配前面的子表达式零次或多次。例如，zo 能匹配 \"z\" 以及 \"zoo\"。 等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，'zo+' 能匹配 \"zo\" 以及 \"zoo\"，但不能匹配 \"z\"。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次。例如，\"do(es)?\" 可以匹配 \"do\" 、 \"does\" 中的 \"does\" 、 \"doxy\" 中的 \"do\" 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。例如，'o{2}' 不能匹配 \"Bob\" 中的 'o'，但是能匹配 \"food\" 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。例如，'o{2,}' 不能匹配 \"Bob\" 中的 'o'，但能匹配 \"foooood\" 中的所有 o。'o{1,}' 等价于 'o+'。'o{0,}' 则等价于 'o*'。 {n,m} m 和 n 均为非负整数，其中n Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/openapi.html":{"url":"blog/规范/openapi.html","title":"Openapi","keywords":"","body":" OpenAPI Specification v3.1.0 https://spec.openapis.org/oas/latest.html github https://github.com/OAI/OpenAPI-Specification/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/readme.html":{"url":"blog/规范/readme.html","title":"Readme","keywords":"","body":"规范 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/resful api design.html":{"url":"blog/规范/resful api design.html","title":"Resful Api Design","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/规范/代码走读.html":{"url":"blog/规范/代码走读.html","title":"代码走读","keywords":"","body":"阅读范围 检查业务逻辑 操作顺序， 如：删除操作，先处理业务，再删除库 异常场景是否考虑完全 网络中断异常 多使用场景的，是否列举全面 依赖的第三方服务暂停或升级 goroutine 是否存在内存泄漏 发送到一个没有接受者的channel 从没有发送者的channel中接受数据 传递尚未初始化的channel goroutine 泄露不仅仅是因为 channel 的错误使用造成的。泄露的原因也可能是 I/O 操作上的堵塞，例如发送请求到 API 服务器，而没有使用超时。另一种原因是，程序可以单纯地陷入死循环中 是否存在过多的重复代码，可以抽象 并发操作是否安全 map 并发写要加锁 数据库记录修改需要使用带判断的原子操作修改 数据库操作 最大连接数设置 失败从连接 失败后是否需要回滚操作 插入前数据库重复条目检查 删除前关联应用是否已经删除 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/":{"url":"blog/设计模式/","title":"设计模式","keywords":"","body":"前言 设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。 使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。 毫无疑问，设计模式于己于他人于系统都是多赢的，设计模式使代码编制真正工程化，设计模式是软件工程的基石，如同大厦的一块块砖石一样。 项目中合理的运用设计模式可以完美的解决很多问题，每种模式在现在中都有相应的原理来与之对应，每一个模式描述了一个在我们周围不断重复发生的问题，以及该问题的核心解决方案，这也是它能被广泛应用的原因。 一、设计模式的分类 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代器模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 本文档转自：https://github.com/senghoo/golang-design-pattern Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/readme.html":{"url":"blog/设计模式/readme.html","title":"Readme","keywords":"","body":"前言 设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。 使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。 毫无疑问，设计模式于己于他人于系统都是多赢的，设计模式使代码编制真正工程化，设计模式是软件工程的基石，如同大厦的一块块砖石一样。 项目中合理的运用设计模式可以完美的解决很多问题，每种模式在现在中都有相应的原理来与之对应，每一个模式描述了一个在我们周围不断重复发生的问题，以及该问题的核心解决方案，这也是它能被广泛应用的原因。 一、设计模式的分类 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代器模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 本文档转自：https://github.com/senghoo/golang-design-pattern Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/":{"url":"blog/设计模式/创建模型/","title":"创建模型","keywords":"","body":"创建者模型 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/readme.html":{"url":"blog/设计模式/创建模型/readme.html","title":"Readme","keywords":"","body":"创建者模型 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/创建者模型.html":{"url":"blog/设计模式/创建模型/创建者模型.html","title":"创建者模型","keywords":"","body":"创建者模式 将一个复杂对象的构建分离成多个简单对象的构建组合 builder.go package builder //Builder 是生成器接口 type Builder interface { Part1() Part2() Part3() } type Director struct { builder Builder } // NewDirector ... func NewDirector(builder Builder) *Director { return &Director{ builder: builder, } } //Construct Product func (d *Director) Construct() { d.builder.Part1() d.builder.Part2() d.builder.Part3() } type Builder1 struct { result string } func (b *Builder1) Part1() { b.result += \"1\" } func (b *Builder1) Part2() { b.result += \"2\" } func (b *Builder1) Part3() { b.result += \"3\" } func (b *Builder1) GetResult() string { return b.result } type Builder2 struct { result int } func (b *Builder2) Part1() { b.result += 1 } func (b *Builder2) Part2() { b.result += 2 } func (b *Builder2) Part3() { b.result += 3 } func (b *Builder2) GetResult() int { return b.result } builder_test.go package builder import \"testing\" func TestBuilder1(t *testing.T) { builder := &Builder1{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != \"123\" { t.Fatalf(\"Builder1 fail expect 123 acture %s\", res) } } func TestBuilder2(t *testing.T) { builder := &Builder2{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != 6 { t.Fatalf(\"Builder2 fail expect 6 acture %d\", res) } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/单例模式.html":{"url":"blog/设计模式/创建模型/单例模式.html","title":"单例模式","keywords":"","body":"单例模式 wiki百科: 单例模式，也叫单子模式，是一种常用的软件设计模式。在应用这个模式时，单例对象的类必须保证只有一个实例存在。许多时候整个系统只需要拥有一个的全局对象，这样有利于我们协调系统整体的行为。比如在某个服务器程序中，该服务器的配置信息存放在一个文件中，这些配置数据由一个单例对象统一读取，然后服务进程中的其他对象再通过这个单例对象获取这些配置信息。这种方式简化了在复杂环境下的配置管理。 单例模式要实现的效果就是，对于应用单例模式的类，整个程序中只存在一个实例化对象 go并不是一种面向对象的语言，所以我们使用结构体来替代 有几种方式: 懒汉模式 饿汉模式 双重检查锁机制 下面拆分讲解： 懒汉模式 构建一个示例结构体 type example struct { name string } 设置一个私有变量作为每次要返回的单例 var instance *example 写一个可以获取单例的方法 func GetExample() *example { // 存在线程安全问题，高并发时有可能创建多个对象 if instance == nil { instance = new(example) } return instance } test func main() { s := GetExample() s.name = \"第一次赋值单例模式\" fmt.Println(s.name) s2 := GetExample() fmt.Println(s2.name) } 懒汉模式存在线程安全问题，在第3步的时候，如果有多个线程同时调用了这个方法， 那么都会检测到instance为nil,就会创建多个对象，所以出现了饿汉模式... 饿汉模式 与懒汉模式类似，不再多说，直接上代码 // 构建一个结构体，用来实例化单例 type example2 struct { name string } // 声明一个私有变量，作为单例 var instance2 *example2 // init函数将在包初始化时执行，实例化单例 func init() { instance2 = new(example2) instance2.name = \"初始化单例模式\" } func GetInstance2() *example2 { return instance2 } func main() { s := GetInstance2() fmt.Println(s.name) } 饿汉模式将在包加载的时候就创建单例对象，当程序中用不到该对象时，浪费了一部分空间 和懒汉模式相比，更安全，但是会减慢程序启动速度 双重检查机制 懒汉模式存在线程安全问题，一般我们使用互斥锁来解决有可能出现的数据不一致问题 所以修改上面的GetInstance() 方法如下: var mux Sync.Mutex func GetInstance() *example { mux.Lock() defer mux.Unlock() if instance == nil { instance = &example{} } return instance } 如果这样去做，每一次请求单例的时候，都会加锁和减锁，而锁的用处只在于解决对象初始化的时候可能出现的并发问题 当对象被创建之后，加锁就失去了意义，会拖慢速度，所以我们就引入了双重检查机制（Check-lock-Check）, 也叫DCL(Double Check Lock), 代码如下: func GetInstance() *example { if instance == nil { // 单例没被实例化，才会加锁 mux.Lock() defer mux.Unlock() if instance == nil { // 单例没被实例化才会创建 instance = &example{} } } return instance } 这样只有当对象未初始化的时候，才会又加锁和减锁的操作 但是又出现了另一个问题：每一次访问都要检查两次，为了解决这个问题，我们可以使用golang标准包中的方法进行原子性操作: import \"sync\" import \"sync/atomic\" var initialized uint32 func GetInstance() *example { // 一次判断即可返回 if atomic.LoadUInt32(&initialized) == 1 { return instance } mux.Lock() defer mux.Unlock() if initialized == 0 { instance = &example{} atomic.StoreUint32(&initialized, 1) // 原子装载 } return instance } 以上代码只需要经过一次判断即可返回单例，但是golang标准包中其实给我们提供了相关的方法: sync.Once的Do方法可以实现在程序运行过程中只运行一次其中的回调，所以最终简化的代码如下: type example3 struct { name string } var instance3 *example3 var once sync.Once func GetInstance3() *example3 { once.Do(func() { instance3 = new(example3) instance3.name = \"第一次赋值单例\" }) return instance3 } func main() { e1 := GetInstance3() fmt.Println(e1.name) e2 := GetInstance3() fmt.Println(e2.name) } 单例模式是开发中经常用到的设计模式，我在制作自己的web框架 silsuer/bingo 的时候 在环境变量控制、配置项控制等位置都用到了这种模式。 想把所有设计模式使用golang实现一遍，开了个新坑silsuer/golang-design-patterns, 这是第一篇，以后会陆续更新，需要请自取～ 此文章的源码都在这个仓库中： golang设计模式 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/原型模式.html":{"url":"blog/设计模式/创建模型/原型模式.html","title":"原型模式","keywords":"","body":"原型模式 原型模式使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。 原型模式配合原型管理器使用，使得客户端在不知道具体类的情况下，通过接口管理器得到新的实例，并且包含部分预设定配置。 prototype.go package prototype //Cloneable 是原型对象需要实现的接口 type Cloneable interface { Clone() Cloneable } type PrototypeManager struct { prototypes map[string]Cloneable } func NewPrototypeManager() *PrototypeManager { return &PrototypeManager{ prototypes: make(map[string]Cloneable), } } func (p *PrototypeManager) Get(name string) Cloneable { return p.prototypes[name] } func (p *PrototypeManager) Set(name string, prototype Cloneable) { p.prototypes[name] = prototype } prototype_test.go package prototype import \"testing\" var manager *PrototypeManager type Type1 struct { name string } func (t *Type1) Clone() Cloneable { tc := *t return &tc } type Type2 struct { name string } func (t *Type2) Clone() Cloneable { tc := *t return &tc } func TestClone(t *testing.T) { t1 := manager.Get(\"t1\") t2 := t1.Clone() if t1 == t2 { t.Fatal(\"error! get clone not working\") } } func TestCloneFromManager(t *testing.T) { c := manager.Get(\"t1\").Clone() t1 := c.(*Type1) if t1.name != \"type1\" { t.Fatal(\"error\") } } func init() { manager = NewPrototypeManager() t1 := &Type1{ name: \"type1\", } manager.Set(\"t1\", t1) } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/工厂方法.html":{"url":"blog/设计模式/创建模型/工厂方法.html","title":"工厂方法","keywords":"","body":"工厂方法模式 工厂方法模式使用子类的方式延迟生成对象到子类中实现。 Go中不存在继承 所以使用匿名组合来实现 工厂方法建议使用特殊的工厂方法代替对象构造函数的直接使用 factorymethod.go package factorymethod //Operator 是被封装的实际类接口 type Operator interface { SetA(int) SetB(int) Result() int } //OperatorFactory 是工厂接口 type OperatorFactory interface { Create() Operator } //OperatorBase 是Operator 接口实现的基类，封装公用方法 type OperatorBase struct { a, b int } //SetA 设置 A func (o *OperatorBase) SetA(a int) { o.a = a } //SetB 设置 B func (o *OperatorBase) SetB(b int) { o.b = b } //PlusOperatorFactory 是 PlusOperator 的工厂类 type PlusOperatorFactory struct{} func (PlusOperatorFactory) Create() Operator { return &PlusOperator{ OperatorBase: &OperatorBase{}, } } //PlusOperator Operator 的实际加法实现 type PlusOperator struct { *OperatorBase } //Result 获取结果 func (o PlusOperator) Result() int { return o.a + o.b } //MinusOperatorFactory 是 MinusOperator 的工厂类 type MinusOperatorFactory struct{} func (MinusOperatorFactory) Create() Operator { return &MinusOperator{ OperatorBase: &OperatorBase{}, } } //MinusOperator Operator 的实际减法实现 type MinusOperator struct { *OperatorBase } //Result 获取结果 func (o MinusOperator) Result() int { return o.a - o.b } factorymethod_test.go package factorymethod import \"testing\" func compute(factory OperatorFactory, a, b int) int { op := factory.Create() op.SetA(a) op.SetB(b) return op.Result() } func TestOperator(t *testing.T) { var ( factory OperatorFactory ) factory = PlusOperatorFactory{} if compute(factory, 1, 2) != 3 { t.Fatal(\"error with factory method pattern\") } factory = MinusOperatorFactory{} if compute(factory, 4, 2) != 2 { t.Fatal(\"error with factory method pattern\") } } 乍看之下， 这种更改可能毫无意义： 我们只是改变了程序中调用构造函数的位置而已。 但是， 仔细想一下， 现在你可以在子类中重写工厂方法， 从而改变其创建产品的类型。 但有一点需要注意:仅当这些产品具有共同的基类或者接口时， 子类才能返回不同类型的产品， 同时基类中的工厂方法还应将其返回类型声明为这一共有接口 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/抽象工厂.html":{"url":"blog/设计模式/创建模型/抽象工厂.html","title":"抽象工厂","keywords":"","body":"抽象工厂模式 抽象工厂模式用于生成产品族的工厂，所生成的对象是有关联的。 如果抽象工厂退化成生成的对象无关联则成为工厂函数模式。 比如本例子中使用RDB和XML存储订单信息，抽象工厂分别能生成相关的主订单信息和订单详情信息。 如果业务逻辑中需要替换使用的时候只需要改动工厂函数相关的类就能替换使用不同的存储方式了。 abstractfactory.go package abstractfactory import \"fmt\" //OrderMainDAO 为订单主记录 type OrderMainDAO interface { SaveOrderMain() } //OrderDetailDAO 为订单详情纪录 type OrderDetailDAO interface { SaveOrderDetail() } //DAOFactory DAO 抽象模式工厂接口 type DAOFactory interface { CreateOrderMainDAO() OrderMainDAO CreateOrderDetailDAO() OrderDetailDAO } //RDBMainDAP 为关系型数据库的OrderMainDAO实现 type RDBMainDAO struct{} //SaveOrderMain ... func (*RDBMainDAO) SaveOrderMain() { fmt.Print(\"rdb main save\\n\") } //RDBDetailDAO 为关系型数据库的OrderDetailDAO实现 type RDBDetailDAO struct{} // SaveOrderDetail ... func (*RDBDetailDAO) SaveOrderDetail() { fmt.Print(\"rdb detail save\\n\") } //RDBDAOFactory 是RDB 抽象工厂实现 type RDBDAOFactory struct{} func (*RDBDAOFactory) CreateOrderMainDAO() OrderMainDAO { return &RDBMainDAO{} } func (*RDBDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return &RDBDetailDAO{} } //XMLMainDAO XML存储 type XMLMainDAO struct{} //SaveOrderMain ... func (*XMLMainDAO) SaveOrderMain() { fmt.Print(\"xml main save\\n\") } //XMLDetailDAO XML存储 type XMLDetailDAO struct{} // SaveOrderDetail ... func (*XMLDetailDAO) SaveOrderDetail() { fmt.Print(\"xml detail save\") } //XMLDAOFactory 是RDB 抽象工厂实现 type XMLDAOFactory struct{} func (*XMLDAOFactory) CreateOrderMainDAO() OrderMainDAO { return &XMLMainDAO{} } func (*XMLDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return &XMLDetailDAO{} } abstractfactory_test.go package abstractfactory func getMainAndDetail(factory DAOFactory) { factory.CreateOrderMainDAO().SaveOrderMain() factory.CreateOrderDetailDAO().SaveOrderDetail() } func ExampleRdbFactory() { var factory DAOFactory factory = &RDBDAOFactory{} getMainAndDetail(factory) // Output: // rdb main save // rdb detail save } func ExampleXmlFactory() { var factory DAOFactory factory = &XMLDAOFactory{} getMainAndDetail(factory) // Output: // xml main save // xml detail save } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/创建模型/简单工厂.html":{"url":"blog/设计模式/创建模型/简单工厂.html","title":"简单工厂","keywords":"","body":"简单工厂模式 go 语言没有构造函数一说，所以一般会定义NewXXX函数来初始化相关类。 NewXXX 函数返回接口时就是简单工厂模式，也就是说Golang的一般推荐做法就是简单工厂。 在这个simplefactory包中只有API 接口和NewAPI函数为包外可见，封装了实现细节。 simple.go代码 package simplefactory import \"fmt\" //API is interface type API interface { Say(name string) string } //NewAPI return Api instance by type func NewAPI(t int) API { if t == 1 { return &hiAPI{} } else if t == 2 { return &helloAPI{} } return nil } //hiAPI is one of API implement type hiAPI struct{} //Say hi to name func (*hiAPI) Say(name string) string { return fmt.Sprintf(\"Hi, %s\", name) } //HelloAPI is another API implement type helloAPI struct{} //Say hello to name func (*helloAPI) Say(name string) string { return fmt.Sprintf(\"Hello, %s\", name) } simple_test.go代码 package simplefactory import \"testing\" //TestType1 test get hiapi with factory func TestType1(t *testing.T) { api := NewAPI(1) s := api.Say(\"Tom\") if s != \"Hi, Tom\" { t.Fatal(\"Type1 test fail\") } } func TestType2(t *testing.T) { api := NewAPI(2) s := api.Say(\"Tom\") if s != \"Hello, Tom\" { t.Fatal(\"Type2 test fail\") } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/":{"url":"blog/设计模式/结构型模型/","title":"结构型模型","keywords":"","body":"结构型模型 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/readme.html":{"url":"blog/设计模式/结构型模型/readme.html","title":"Readme","keywords":"","body":"结构型模型 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/享元模式.html":{"url":"blog/设计模式/结构型模型/享元模式.html","title":"享元模式","keywords":"","body":"享元模式 享元模式从对象中剥离出不发生改变且多个实例需要的重复数据，独立出一个享元，使多个对象共享，从而节省内存以及减少对象数量。 使用场景： 程序需要生成数量巨大的相识对象 这将耗尽目标设备的所有内存 对象中包含可抽取且能在多个对象间共享的重复状态 角色： ①享元接口（Flyweight）：是一个接口，该接口定义了享元对外公开其内部数据的方法，以及享元接收外部数据的方法； ②具体享元（Concrete Flyweight）：实现享元接口的类，该类的实例称作为享元对象或简称享元。具体享元类的成员变量为享元对象的内部状态，显然，对象的内部状态必须与所处的周围环境无关。即要保证使用享元对象的应用程序无法更改享元的内部状态，只有这样才能使得享元对象在系统中被共享，因为享元对象是用来共享的，所以不能允许用户各自的使用具体享元来创建对象，这样就无法达到共享的目的，因为不同用户用具体享元类创建的对象显然是不同的，所以具体享元类的构造方法必须是private的，其目的是不允许用户程序直接使用，具体享元类来创建享元对象，创建和管理享元对象有享元工厂负责； ③享元工厂（Flyweight Factory）：享元工厂是一个类，该类的实例负责创建和管理享元对象，用户或其他对象必须请求享元工厂为它得到一个享元对象。享元工厂可以通过一个散列表，也称作共享池来管理享元对象，当用户程序或其他若干个对象向享元工厂请求一个享元对象时，如果想让工厂的散列表中已有这样的享元对象，享元工厂就提供这个享元对象给请求者，否则就创建一个享元对象，添加到散列表中，同时将该享元对象提供给请求者，显然当若干个用户或对象，请求享元工厂提供一个享元对象时，第一个用户获得该享元对象的时间可能慢一些，但是后记的用户会比较快的获得这个对象，可以使用单列模式来设计享元工厂，即让系统中只有一个享元工厂的实例，另外，为了让享元工厂能生成享元对象，需要将具体享元类作为享元工厂的内部类。 实现方式： 将需要改写为享元的类成员变量拆分为两个部分： 内在状态：包含不变的、可在许多对象中重复使用的数据的成员变量 外在状态：包含每个对象各自不同的情景数据的成员变量 保留类中表示内在状态的成员变量， 并将其属性设置为不可修改。 这些变量仅可在构造函数中获得初始数值。 找到所有使用外在状态成员变量的方法， 为在方法中所用的每个成员变量新建一个参数， 并使用该参数代替成员变量。 你可以有选择地创建工厂类来管理享元缓存池， 它负责在新建享元时检查已有的享元。 如果选择使用工厂， 客户端就只能通过工厂来请求享元， 它们需要将享元的内在状态作为参数传递给工厂。 客户端必须存储和计算外在状态 （情景） 的数值， 因为只有这样才能调用享元对象的方法。 为了使用方便， 外在状态和引用享元的成员变量可以移动到单独的情景类中。 享元模式的优缺点： 优点： ①使用享元可以节省内存的开销，特别适合处理大量细粒度对象，这些对象的许多属性值是相同的，而且一旦创建则不允许修改； ②享元模式中的享元可以使用方法的参数接受外部的状态中的数据，但外部状态数据不会干扰到享元中的内部数据，这就使享元可以在不同的环境中被共享； 缺点： ①使得系统更加复杂，需要分离出内部状态和外部状态，这使得程序逻辑复杂化； ②为了使对象可以共享，享元模式需要将享元对象的状态外部化，而读取外部状态使得运行时间变长。 flyweight.go package flyweight import \"fmt\" type ImageFlyweightFactory struct { maps map[string]*ImageFlyweight } var imageFactory *ImageFlyweightFactory func GetImageFlyweightFactory() *ImageFlyweightFactory { if imageFactory == nil { imageFactory = &ImageFlyweightFactory{ maps: make(map[string]*ImageFlyweight), } } return imageFactory } func (f *ImageFlyweightFactory) Get(filename string) *ImageFlyweight { image := f.maps[filename] if image == nil { image = NewImageFlyweight(filename) f.maps[filename] = image } return image } type ImageFlyweight struct { data string } func NewImageFlyweight(filename string) *ImageFlyweight { // Load image file data := fmt.Sprintf(\"image data %s\", filename) return &ImageFlyweight{ data: data, } } func (i *ImageFlyweight) Data() string { return i.data } type ImageViewer struct { *ImageFlyweight } func NewImageViewer(filename string) *ImageViewer { image := GetImageFlyweightFactory().Get(filename) return &ImageViewer{ ImageFlyweight: image, } } func (i *ImageViewer) Display() { fmt.Printf(\"Display: %s\\n\", i.Data()) } flyweight_test.go package flyweight import \"testing\" func ExampleFlyweight() { viewer := NewImageViewer(\"image1.png\") viewer.Display() // Output: // Display: image data image1.png } func TestFlyweight(t *testing.T) { viewer1 := NewImageViewer(\"image1.png\") viewer2 := NewImageViewer(\"image1.png\") if viewer1.ImageFlyweight != viewer2.ImageFlyweight { t.Fail() } } 文档更新时间: 2020-08-24 11:33 作者：kuteng https://refactoringguru.cn/design-patterns/flyweight https://cloud.tencent.com/developer/article/1782461?from=article.detail.1782462 - Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/代理模式.html":{"url":"blog/设计模式/结构型模型/代理模式.html","title":"代理模式","keywords":"","body":"代理模式 代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。 代理模式的常见用法有 虚代理 COW代理 远程代理 保护代理 Cache 代理 防火墙代理 同步代理 智能指引 等。。。 proxy.go package proxy type Subject interface { Do() string } type RealSubject struct{} func (RealSubject) Do() string { return \"real\" } type Proxy struct { real RealSubject } func (p Proxy) Do() string { var res string // 在调用真实对象之前的工作，检查缓存，判断权限，实例化真实对象等。。 res += \"pre:\" // 调用真实对象 res += p.real.Do() // 调用之后的操作，如缓存结果，对结果进行处理等。。 res += \":after\" return res } proxy_test.go package proxy import \"testing\" func TestProxy(t *testing.T) { var sub Subject sub = &Proxy{} res := sub.Do() if res != \"pre:real:after\" { t.Fail() } } 文档更新时间: 2020-08-24 11:18 作者：kuteng代理模式 代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。 代理模式的常见用法有 虚代理 COW代理 远程代理 保护代理 Cache 代理 防火墙代理 同步代理 智能指引 等。。。 proxy.go package proxy type Subject interface { Do() string } type RealSubject struct{} func (RealSubject) Do() string { return \"real\" } type Proxy struct { real RealSubject } func (p Proxy) Do() string { var res string // 在调用真实对象之前的工作，检查缓存，判断权限，实例化真实对象等。。 res += \"pre:\" // 调用真实对象 res += p.real.Do() // 调用之后的操作，如缓存结果，对结果进行处理等。。 res += \":after\" return res } proxy_test.go package proxy import \"testing\" func TestProxy(t *testing.T) { var sub Subject sub = &Proxy{} res := sub.Do() if res != \"pre:real:after\" { t.Fail() } } 文档更新时间: 2020-08-24 11:18 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/外观模式.html":{"url":"blog/设计模式/结构型模型/外观模式.html","title":"外观模式","keywords":"","body":"外观模式 API 为facade 模块的外观接口，大部分代码使用此接口简化对facade类的访问。 facade模块同时暴露了a和b 两个Module 的NewXXX和interface，其它代码如果需要使用细节功能时可以直接调用。 facade.go package facade import \"fmt\" func NewAPI() API { return &apiImpl{ a: NewAModuleAPI(), b: NewBModuleAPI(), } } //API is facade interface of facade package type API interface { Test() string } //facade implement type apiImpl struct { a AModuleAPI b BModuleAPI } func (a *apiImpl) Test() string { aRet := a.a.TestA() bRet := a.b.TestB() return fmt.Sprintf(\"%s\\n%s\", aRet, bRet) } //NewAModuleAPI return new AModuleAPI func NewAModuleAPI() AModuleAPI { return &aModuleImpl{} } //AModuleAPI ... type AModuleAPI interface { TestA() string } type aModuleImpl struct{} func (*aModuleImpl) TestA() string { return \"A module running\" } //NewBModuleAPI return new BModuleAPI func NewBModuleAPI() BModuleAPI { return &bModuleImpl{} } //BModuleAPI ... type BModuleAPI interface { TestB() string } type bModuleImpl struct{} func (*bModuleImpl) TestB() string { return \"B module running\" } facade_test.go package facade import \"testing\" var expect = \"A module running\\nB module running\" // TestFacadeAPI ... func TestFacadeAPI(t *testing.T) { api := NewAPI() ret := api.Test() if ret != expect { t.Fatalf(\"expect %s, return %s\", expect, ret) } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/桥模式.html":{"url":"blog/设计模式/结构型模型/桥模式.html","title":"桥模式","keywords":"","body":"桥接模式 桥接模式分离抽象部分和实现部分。使得两部分独立扩展。 桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。 策略模式使抽象部分和实现部分分离，可以独立变化。 bridge.go package bridge import \"fmt\" type AbstractMessage interface { SendMessage(text, to string) } type MessageImplementer interface { Send(text, to string) } type MessageSMS struct{} func ViaSMS() MessageImplementer { return &MessageSMS{} } func (*MessageSMS) Send(text, to string) { fmt.Printf(\"send %s to %s via SMS\", text, to) } type MessageEmail struct{} func ViaEmail() MessageImplementer { return &MessageEmail{} } func (*MessageEmail) Send(text, to string) { fmt.Printf(\"send %s to %s via Email\", text, to) } type CommonMessage struct { method MessageImplementer } func NewCommonMessage(method MessageImplementer) *CommonMessage { return &CommonMessage{ method: method, } } func (m *CommonMessage) SendMessage(text, to string) { m.method.Send(text, to) } type UrgencyMessage struct { method MessageImplementer } func NewUrgencyMessage(method MessageImplementer) *UrgencyMessage { return &UrgencyMessage{ method: method, } } func (m *UrgencyMessage) SendMessage(text, to string) { m.method.Send(fmt.Sprintf(\"[Urgency] %s\", text), to) } bridge_test.go package bridge func ExampleCommonSMS() { m := NewCommonMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via SMS } func ExampleCommonEmail() { m := NewCommonMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via Email } func ExampleUrgencySMS() { m := NewUrgencyMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via SMS } func ExampleUrgencyEmail() { m := NewUrgencyMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via Email } 文档更新时间: 2020-08-24 11:34 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/组合模式.html":{"url":"blog/设计模式/结构型模型/组合模式.html","title":"组合模式","keywords":"","body":"组合模式 组合模式统一对象和对象集，使得使用相同接口使用对象和对象集。 组合模式常用于树状结构，用于统一叶子节点和树节点的访问，并且可以用于应用某一操作到所有子节点。 composite.go package composite import \"fmt\" type Component interface { Parent() Component SetParent(Component) Name() string SetName(string) AddChild(Component) Print(string) } const ( LeafNode = iota CompositeNode ) func NewComponent(kind int, name string) Component { var c Component switch kind { case LeafNode: c = NewLeaf() case CompositeNode: c = NewComposite() } c.SetName(name) return c } type component struct { parent Component name string } func (c *component) Parent() Component { return c.parent } func (c *component) SetParent(parent Component) { c.parent = parent } func (c *component) Name() string { return c.name } func (c *component) SetName(name string) { c.name = name } func (c *component) AddChild(Component) {} func (c *component) Print(string) {} type Leaf struct { component } func NewLeaf() *Leaf { return &Leaf{} } func (c *Leaf) Print(pre string) { fmt.Printf(\"%s-%s\\n\", pre, c.Name()) } type Composite struct { component childs []Component } func NewComposite() *Composite { return &Composite{ childs: make([]Component, 0), } } func (c *Composite) AddChild(child Component) { child.SetParent(c) c.childs = append(c.childs, child) } func (c *Composite) Print(pre string) { fmt.Printf(\"%s+%s\\n\", pre, c.Name()) pre += \" \" for _, comp := range c.childs { comp.Print(pre) } } composite_test.go package composite func ExampleComposite() { root := NewComponent(CompositeNode, \"root\") c1 := NewComponent(CompositeNode, \"c1\") c2 := NewComponent(CompositeNode, \"c2\") c3 := NewComponent(CompositeNode, \"c3\") l1 := NewComponent(LeafNode, \"l1\") l2 := NewComponent(LeafNode, \"l2\") l3 := NewComponent(LeafNode, \"l3\") root.AddChild(c1) root.AddChild(c2) c1.AddChild(c3) c1.AddChild(l1) c2.AddChild(l2) c2.AddChild(l3) root.Print(\"\") // Output: // +root // +c1 // +c3 // -l1 // +c2 // -l2 // -l3 } 文档更新时间: 2020-08-24 11:33 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/装饰模式.html":{"url":"blog/设计模式/结构型模型/装饰模式.html","title":"装饰模式","keywords":"","body":"装饰模式 装饰模式使用对象组合的方式动态改变或增加对象行为。 Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。 使用匿名组合，在装饰器中不必显式定义转调原对象方法。 decorator.go package decorator type Component interface { Calc() int } type ConcreteComponent struct{} func (*ConcreteComponent) Calc() int { return 0 } type MulDecorator struct { Component num int } func WarpMulDecorator(c Component, num int) Component { return &MulDecorator{ Component: c, num: num, } } func (d *MulDecorator) Calc() int { return d.Component.Calc() * d.num } type AddDecorator struct { Component num int } func WarpAddDecorator(c Component, num int) Component { return &AddDecorator{ Component: c, num: num, } } func (d *AddDecorator) Calc() int { return d.Component.Calc() + d.num } decorator_test.go package decorator import \"fmt\" func ExampleDecorator() { var c Component = &ConcreteComponent{} c = WarpAddDecorator(c, 10) c = WarpMulDecorator(c, 8) res := c.Calc() fmt.Printf(\"res %d\\n\", res) // Output: // res 80 } 文档更新时间: 2020-08-24 11:33 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/结构型模型/适配器模式.html":{"url":"blog/设计模式/结构型模型/适配器模式.html","title":"适配器模式","keywords":"","body":"适配器模式 适配器模式用于转换一种接口适配另一种接口。 实际使用中Adaptee一般为接口，并且使用工厂函数生成实例。 在Adapter中匿名组合Adaptee接口，所以Adapter类也拥有SpecificRequest实例方法，又因为Go语言中非入侵式接口特征，其实Adapter也适配Adaptee接口。 adapter.go package adapter //Target 是适配的目标接口 type Target interface { Request() string } //Adaptee 是被适配的目标接口 type Adaptee interface { SpecificRequest() string } //NewAdaptee 是被适配接口的工厂函数 func NewAdaptee() Adaptee { return &adapteeImpl{} } //AdapteeImpl 是被适配的目标类 type adapteeImpl struct{} //SpecificRequest 是目标类的一个方法 func (*adapteeImpl) SpecificRequest() string { return \"adaptee method\" } //NewAdapter 是Adapter的工厂函数 func NewAdapter(adaptee Adaptee) Target { return &adapter{ Adaptee: adaptee, } } //Adapter 是转换Adaptee为Target接口的适配器 type adapter struct { Adaptee } //Request 实现Target接口 func (a *adapter) Request() string { return a.SpecificRequest() } adapter_test.go package adapter import \"testing\" var expect = \"adaptee method\" func TestAdapter(t *testing.T) { adaptee := NewAdaptee() target := NewAdapter(adaptee) res := target.Request() if res != expect { t.Fatalf(\"expect: %s, actual: %s\", expect, res) } } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/编程思想/gin route design.html":{"url":"blog/设计模式/编程思想/gin route design.html","title":"Gin Route Design","keywords":"","body":" type ResultRaw interface { GetResponse(int) Response ResMsg() string HasError() bool Error() error } type Operator interface { Run() ResultRaw SetAccount(ctx *gin.Context) } type Action func(p Operator) // HandleOperator 操作接口operator func HandleOperator(ctx *gin.Context, o Operator, action Action) { if err := Bind(ctx, o); err != nil { return } action(o) } // ListSourceCodeRepo // @Summary list source code repo // @Router /delivery-center/v2/code-repos [GET] // @Success 200 {object} app.Response // @Security ApiKeyAuth // @Tags source code repo func ListSourceCodeRepo(ctx *gin.Context) { o := &v2.CodeRepoList{} app.HandleOperator(ctx, o, func(o app.Operator) { app.HandleServiceResult(ctx, e.ListSourceCodeRepoFail, o.Run()) }) } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/编程思想/开闭原则.html":{"url":"blog/设计模式/编程思想/开闭原则.html","title":"开闭原则","keywords":"","body":"面向对象的编程思维理解interface [toc] 一、 interface接口 interface 是GO语言的基础特性之一。可以理解为一种类型的规范或者约定。它跟java，C# 不太一样，不需要显示说明实现了某个接口，它没有继承或子类或“implements”关键字，只是通过约定的形式，隐式的实现interface 中的方法即可。因此，Golang 中的 interface 让编码更灵活、易扩展。 如何理解go 语言中的interface ？ 只需记住以下三点即可： interface 是方法声明的集合 任何类型的对象实现了在interface 接口中声明的全部方法，则表明该类型实现了该接口。 interface 可以作为一种数据类型，实现了该接口的任何对象都可以给对应的接口类型变量赋值。 注意： 　　a. interface 可以被任意对象实现，一个类型/对象也可以实现多个 interface 　　b. 方法不能重载，如 eat(), eat(s string) 不能同时存在 package main import \"fmt\" type Phone interface { call() } type NokiaPhone struct { } func (nokiaPhone NokiaPhone) call() { fmt.Println(\"I am Nokia, I can call you!\") } type ApplePhone struct { } func (iPhone ApplePhone) call() { fmt.Println(\"I am Apple Phone, I can call you!\") } func main() { var phone Phone phone = new(NokiaPhone) phone.call() phone = new(ApplePhone) phone.call() } 上述中体现了interface接口的语法，在main函数中，也体现了多态的特性。 同样一个phone的抽象接口，分别指向不同的实体对象，调用的call()方法，打印的效果不同，那么就是体现出了多态的特性。 二、 面向对象中的开闭原则 2.1 平铺式的模块设计 那么作为interface数据类型，他存在的意义在哪呢？ 实际上是为了满足一些面向对象的编程思想。我们知道，软件设计的最高目标就是高内聚，低耦合。那么其中有一个设计原则叫开闭原则。什么是开闭原则呢，接下来我们看一个例子： package main import \"fmt\" //我们要写一个类,Banker银行业务员 type Banker struct { } //存款业务 func (this *Banker) Save() { fmt.Println( \"进行了 存款业务...\") } //转账业务 func (this *Banker) Transfer() { fmt.Println( \"进行了 转账业务...\") } //支付业务 func (this *Banker) Pay() { fmt.Println( \"进行了 支付业务...\") } func main() { banker := &Banker{} banker.Save() banker.Transfer() banker.Pay() } 代码很简单，就是一个银行业务员，他可能拥有很多的业务，比如Save()存款、Transfer()转账、Pay()支付等。那么如果这个业务员模块只有这几个方法还好，但是随着我们的程序写的越来越复杂，银行业务员可能就要增加方法，会导致业务员模块越来越臃肿。 这样的设计会导致，当我们去给Banker添加新的业务的时候，会直接修改原有的Banker代码，那么Banker模块的功能会越来越多，出现问题的几率也就越来越大，假如此时Banker已经有99个业务了，现在我们要添加第100个业务，可能由于一次的不小心，导致之前99个业务也一起崩溃，因为所有的业务都在一个Banker类里，他们的耦合度太高，Banker的职责也不够单一，代码的维护成本随着业务的复杂正比成倍增大。 2.2 开闭原则设计 那么，如果我们拥有接口, interface这个东西，那么我们就可以抽象一层出来，制作一个抽象的Banker模块，然后提供一个抽象的方法。 分别根据这个抽象模块，去实现支付Banker（实现支付方法）,转账Banker（实现转账方法） 如下： 那么依然可以搞定程序的需求。 然后，当我们想要给Banker添加额外功能的时候，之前我们是直接修改Banker的内容，现在我们可以单独定义一个股票Banker(实现股票方法)，到这个系统中。 而且股票Banker的实现成功或者失败都不会影响之前的稳定系统，他很单一，而且独立。 所以以上，当我们给一个系统添加一个功能的时候，不是通过修改代码，而是通过增添代码来完成，那么就是开闭原则的核心思想了。所以要想满足上面的要求，是一定需要interface来提供一层抽象的接口的。 golang代码实现如下: package main import \"fmt\" //抽象的银行业务员 type AbstractBanker interface{ DoBusi() //抽象的处理业务接口 } //存款的业务员 type SaveBanker struct { //AbstractBanker } func (sb *SaveBanker) DoBusi() { fmt.Println(\"进行了存款\") } //转账的业务员 type TransferBanker struct { //AbstractBanker } func (tb *TransferBanker) DoBusi() { fmt.Println(\"进行了转账\") } //支付的业务员 type PayBanker struct { //AbstractBanker } func (pb *PayBanker) DoBusi() { fmt.Println(\"进行了支付\") } func main() { //进行存款 sb := &SaveBanker{} sb.DoBusi() //进行转账 tb := &TransferBanker{} tb.DoBusi() //进行支付 pb := &PayBanker{} pb.DoBusi() } 当然我们也可以根据AbstractBanker设计一个小框架 //实现架构层(基于抽象层进行业务封装-针对interface接口进行封装) func BankerBusiness(banker AbstractBanker) { //通过接口来向下调用，(多态现象) banker.DoBusi() } 那么main中可以如下实现业务调用: func main() { //进行存款 BankerBusiness(&SaveBanker{}) //进行存款 BankerBusiness(&TransferBanker{}) //进行存款 BankerBusiness(&PayBanker{}) } 再看开闭原则定义: 开闭原则:一个软件实体如类、模块和函数应该对扩展开放，对修改关闭。 简单的说就是在修改需求的时候，应该尽量通过扩展来实现变化，而不是通过修改已有代码来实现变化。 三、 接口的意义 好了，现在interface已经基本了解，那么接口的意义最终在哪里呢，想必现在你已经有了一个初步的认知，实际上接口的最大的意义就是实现多态的思想，就是我们可以根据interface类型来设计API接口，那么这种API接口的适应能力不仅能适应当下所实现的全部模块，也适应未来实现的模块来进行调用。 调用未来可能就是接口的最大意义所在吧，这也是为什么架构师那么值钱，因为良好的架构师是可以针对interface设计一套框架，在未来许多年却依然适用。 四、 面向对象中的依赖倒转原则 4.1 耦合度极高的模块关系设计 package main import \"fmt\" // === > 奔驰汽车 宝马汽车 司机张三 司机李四 我们来看上面的代码和图中每个模块之间的依赖关系，实际上并没有用到任何的interface接口层的代码，显然最后我们的两个业务 张三开奔驰, 李四开宝马，程序中也都实现了。但是这种设计的问题就在于，小规模没什么问题，但是一旦程序需要扩展，比如我现在要增加一个丰田汽车 或者 司机王五， 那么模块和模块的依赖关系将成指数级递增，想蜘蛛网一样越来越难维护和捋顺。 4.2 面向抽象层依赖倒转 如上图所示，如果我们在设计一个系统的时候，将模块分为3个层次，抽象层、实现层、业务逻辑层。那么，我们首先将抽象层的模块和接口定义出来，这里就需要了interface接口的设计，然后我们依照抽象层，依次实现每个实现层的模块，在我们写实现层代码的时候，实际上我们只需要参考对应的抽象层实现就好了，实现每个模块，也和其他的实现的模块没有关系，这样也符合了上面介绍的开闭原则。这样实现起来每个模块只依赖对象的接口，而和其他模块没关系，依赖关系单一。系统容易扩展和维护。 我们在指定业务逻辑也是一样，只需要参考抽象层的接口来业务就好了，抽象层暴露出来的接口就是我们业务层可以使用的方法，然后可以通过多态的线下，接口指针指向哪个实现模块，调用了就是具体的实现方法，这样我们业务逻辑层也是依赖抽象成编程。 我们就将这种的设计原则叫做依赖倒转原则。 来一起看一下修改的代码： package main import \"fmt\" // ===== > 抽象层 实现层 业务逻辑层 4.3 依赖倒转小练习 模拟组装2台电脑， --- 抽象层 ---有显卡Card 方法display，有内存Memory 方法storage，有处理器CPU 方法calculate --- 实现层层 ---有 Intel因特尔公司 、产品有(显卡、内存、CPU)，有 Kingston 公司， 产品有(内存3)，有 NVIDIA 公司， 产品有(显卡) --- 逻辑层 ---1. 组装一台Intel系列的电脑，并运行，2. 组装一台 Intel CPU Kingston内存 NVIDIA显卡的电脑，并运行 /* 模拟组装2台电脑 --- 抽象层 --- 有显卡Card 方法display 有内存Memory 方法storage 有处理器CPU 方法calculate --- 实现层层 --- 有 Intel因特尔公司 、产品有(显卡、内存、CPU) 有 Kingston 公司， 产品有(内存3) 有 NVIDIA 公司， 产品有(显卡) --- 逻辑层 --- 1. 组装一台Intel系列的电脑，并运行 2. 组装一台 Intel CPU Kingston内存 NVIDIA显卡的电脑，并运行 */ package main import \"fmt\" //------ 抽象层 ----- type Card interface{ Display() } type Memory interface { Storage() } type CPU interface { Calculate() } type Computer struct { cpu CPU mem Memory card Card } func NewComputer(cpu CPU, mem Memory, card Card) *Computer{ return &Computer{ cpu:cpu, mem:mem, card:card, } } func (this *Computer) DoWork() { this.cpu.Calculate() this.mem.Storage() this.card.Display() } //------ 实现层 ----- //intel type IntelCPU struct { CPU } func (this *IntelCPU) Calculate() { fmt.Println(\"Intel CPU 开始计算了...\") } type IntelMemory struct { Memory } func (this *IntelMemory) Storage() { fmt.Println(\"Intel Memory 开始存储了...\") } type IntelCard struct { Card } func (this *IntelCard) Display() { fmt.Println(\"Intel Card 开始显示了...\") } //kingston type KingstonMemory struct { Memory } func (this *KingstonMemory) Storage() { fmt.Println(\"Kingston memory storage...\") } //nvidia type NvidiaCard struct { Card } func (this *NvidiaCard) Display() { fmt.Println(\"Nvidia card display...\") } //------ 业务逻辑层 ----- func main() { //intel系列的电脑 com1 := NewComputer(&IntelCPU{}, &IntelMemory{}, &IntelCard{}) com1.DoWork() //杂牌子 com2 := NewComputer(&IntelCPU{}, &KingstonMemory{}, &NvidiaCard{}) com2.DoWork() } 上一篇：5、Golang三色标记+混合写屏障GC模式全分析下一篇：7、Golang中的Defer必掌握的7知识点 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/":{"url":"blog/设计模式/行为型模型/","title":"行为型模型","keywords":"","body":"行为型模型 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/readme.html":{"url":"blog/设计模式/行为型模型/readme.html","title":"Readme","keywords":"","body":"行为型模型 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/中介者模式.html":{"url":"blog/设计模式/行为型模型/中介者模式.html","title":"中介者模式","keywords":"","body":"中介者模式 中介者模式封装对象之间互交，使依赖变的简单，并且使复杂互交简单化，封装在中介者中。 例子中的中介者使用单例模式生成中介者。 中介者的change使用switch判断类型。 mediator.go package mediator import ( \"fmt\" \"strings\" ) type CDDriver struct { Data string } func (c *CDDriver) ReadData() { c.Data = \"music,image\" fmt.Printf(\"CDDriver: reading data %s\\n\", c.Data) GetMediatorInstance().changed(c) } type CPU struct { Video string Sound string } func (c *CPU) Process(data string) { sp := strings.Split(data, \",\") c.Sound = sp[0] c.Video = sp[1] fmt.Printf(\"CPU: split data with Sound %s, Video %s\\n\", c.Sound, c.Video) GetMediatorInstance().changed(c) } type VideoCard struct { Data string } func (v *VideoCard) Display(data string) { v.Data = data fmt.Printf(\"VideoCard: display %s\\n\", v.Data) GetMediatorInstance().changed(v) } type SoundCard struct { Data string } func (s *SoundCard) Play(data string) { s.Data = data fmt.Printf(\"SoundCard: play %s\\n\", s.Data) GetMediatorInstance().changed(s) } type Mediator struct { CD *CDDriver CPU *CPU Video *VideoCard Sound *SoundCard } var mediator *Mediator func GetMediatorInstance() *Mediator { if mediator == nil { mediator = &Mediator{} } return mediator } func (m *Mediator) changed(i interface{}) { switch inst := i.(type) { case *CDDriver: m.CPU.Process(inst.Data) case *CPU: m.Sound.Play(inst.Sound) m.Video.Display(inst.Video) } } mediator_test.go package mediator import \"testing\" func TestMediator(t *testing.T) { mediator := GetMediatorInstance() mediator.CD = &CDDriver{} mediator.CPU = &CPU{} mediator.Video = &VideoCard{} mediator.Sound = &SoundCard{} //Tiggle mediator.CD.ReadData() if mediator.CD.Data != \"music,image\" { t.Fatalf(\"CD unexpect data %s\", mediator.CD.Data) } if mediator.CPU.Sound != \"music\" { t.Fatalf(\"CPU unexpect sound data %s\", mediator.CPU.Sound) } if mediator.CPU.Video != \"image\" { t.Fatalf(\"CPU unexpect video data %s\", mediator.CPU.Video) } if mediator.Video.Data != \"image\" { t.Fatalf(\"VidoeCard unexpect data %s\", mediator.Video.Data) } if mediator.Sound.Data != \"music\" { t.Fatalf(\"SoundCard unexpect data %s\", mediator.Sound.Data) } } 文档更新时间: 2020-08-24 11:37 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/命令模式.html":{"url":"blog/设计模式/行为型模型/命令模式.html","title":"命令模式","keywords":"","body":"命令模式 命令模式本质是把某个对象的方法调用封装到对象中，方便传递、存储、调用。 示例中把主板单中的启动(start)方法和重启(reboot)方法封装为命令对象，再传递到主机(box)对象中。于两个按钮进行绑定： 第一个机箱(box1)设置按钮1(button1) 为开机按钮2(button2)为重启。 第二个机箱(box1)设置按钮2(button2) 为开机按钮1(button1)为重启。 从而得到配置灵活性。 除了配置灵活外，使用命令模式还可以用作： 批处理 任务队列 undo, redo 等把具体命令封装到对象中使用的场合 command.go package command import \"fmt\" type Command interface { Execute() } type StartCommand struct { mb *MotherBoard } func NewStartCommand(mb *MotherBoard) *StartCommand { return &StartCommand{ mb: mb, } } func (c *StartCommand) Execute() { c.mb.Start() } type RebootCommand struct { mb *MotherBoard } func NewRebootCommand(mb *MotherBoard) *RebootCommand { return &RebootCommand{ mb: mb, } } func (c *RebootCommand) Execute() { c.mb.Reboot() } type MotherBoard struct{} func (*MotherBoard) Start() { fmt.Print(\"system starting\\n\") } func (*MotherBoard) Reboot() { fmt.Print(\"system rebooting\\n\") } type Box struct { button1 Command button2 Command } func NewBox(button1, button2 Command) *Box { return &Box{ button1: button1, button2: button2, } } func (b *Box) PressButton1() { b.button1.Execute() } func (b *Box) PressButton2() { b.button2.Execute() } command_test.go package command func ExampleCommand() { mb := &MotherBoard{} startCommand := NewStartCommand(mb) rebootCommand := NewRebootCommand(mb) box1 := NewBox(startCommand, rebootCommand) box1.PressButton1() box1.PressButton2() box2 := NewBox(rebootCommand, startCommand) box2.PressButton1() box2.PressButton2() // Output: // system starting // system rebooting // system rebooting // system starting } 文档更新时间: 2020-08-24 11:40 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/备忘录模式.html":{"url":"blog/设计模式/行为型模型/备忘录模式.html","title":"备忘录模式","keywords":"","body":"备忘录模式 备忘录模式用于保存程序内部状态到外部，又不希望暴露内部状态的情形。 程序内部状态使用窄接口船体给外部进行存储，从而不暴露程序实现细节。 备忘录模式同时可以离线保存内部状态，如保存到数据库，文件等。 memento.go package memento import \"fmt\" type Memento interface{} type Game struct { hp, mp int } type gameMemento struct { hp, mp int } func (g *Game) Play(mpDelta, hpDelta int) { g.mp += mpDelta g.hp += hpDelta } func (g *Game) Save() Memento { return &gameMemento{ hp: g.hp, mp: g.mp, } } func (g *Game) Load(m Memento) { gm := m.(*gameMemento) g.mp = gm.mp g.hp = gm.hp } func (g *Game) Status() { fmt.Printf(\"Current HP:%d, MP:%d\\n\", g.hp, g.mp) } memento_test.go package memento func ExampleGame() { game := &Game{ hp: 10, mp: 10, } game.Status() progress := game.Save() game.Play(-2, -3) game.Status() game.Load(progress) game.Status() // Output: // Current HP:10, MP:10 // Current HP:7, MP:8 // Current HP:10, MP:10 } 文档更新时间: 2020-08-24 11:48 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/模板方法模式.html":{"url":"blog/设计模式/行为型模型/模板方法模式.html","title":"模板方法模式","keywords":"","body":"模板方法模式 模版方法模式使用继承机制，把通用步骤和通用方法放到父类中，把具体实现延迟到子类中实现。使得实现符合开闭原则。 如实例代码中通用步骤在父类中实现（准备、下载、保存、收尾）下载和保存的具体实现留到子类中，并且提供 保存方法的默认实现。 因为Golang不提供继承机制，需要使用匿名组合模拟实现继承。 此处需要注意：因为父类需要调用子类方法，所以子类需要匿名组合父类的同时，父类需要持有子类的引用。 templatemethod.go package templatemethod import \"fmt\" type Downloader interface { Download(uri string) } type template struct { implement uri string } type implement interface { download() save() } func newTemplate(impl implement) *template { return &template{ implement: impl, } } func (t *template) Download(uri string) { t.uri = uri fmt.Print(\"prepare downloading\\n\") t.implement.download() t.implement.save() fmt.Print(\"finish downloading\\n\") } func (t *template) save() { fmt.Print(\"default save\\n\") } type HTTPDownloader struct { *template } func NewHTTPDownloader() Downloader { downloader := &HTTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *HTTPDownloader) download() { fmt.Printf(\"download %s via http\\n\", d.uri) } func (*HTTPDownloader) save() { fmt.Printf(\"http save\\n\") } type FTPDownloader struct { *template } func NewFTPDownloader() Downloader { downloader := &FTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *FTPDownloader) download() { fmt.Printf(\"download %s via ftp\\n\", d.uri) } templatemethod_test.go package templatemethod func ExampleHTTPDownloader() { var downloader Downloader = NewHTTPDownloader() downloader.Download(\"http://example.com/abc.zip\") // Output: // prepare downloading // download http://example.com/abc.zip via http // http save // finish downloading } func ExampleFTPDownloader() { var downloader Downloader = NewFTPDownloader() downloader.Download(\"ftp://example.com/abc.zip\") // Output: // prepare downloading // download ftp://example.com/abc.zip via ftp // default save // finish downloading } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/状态模式.html":{"url":"blog/设计模式/行为型模型/状态模式.html","title":"状态模式","keywords":"","body":"状态模式 在状态模式中，对象的行为是依赖于它的状态（属性）。当对象与外部交互时，触发其内部状态迁移，从而使得对象的行为也随之发生改变，状态模式又称为状态机模式。这种类型的设计模式属于 行为型模式。 传统解决状态迁移而改变行为的解决方案，通常是将所有可能的状态考虑到，然后用 if...else.... 或 switch...case...对状态逐个判断，再进行不同情况的处理。但状态很多时，程序则变得很复杂，且增加状态时要添加新的判断，违背了 开闭原则，不利于扩展和维护。 状态模式可以很好地解决上面问题，可以用于消除多层次复杂的条件选择语句。 模式分析 状态模式的关键是引入了一个抽象类来专门表示对象的状态，这个类叫做 抽象状态类，而对象的每一种具体状态都继承该类，在重写的方法里实现自己状态的行为，包括各种状态迁移。 状态模式的解决思想是：当控制一个对象状态转换的条件表达式过于复杂时，把相关 判断逻辑 提取出来，放到一系列的状态类当中，这样可以把原来复杂的逻辑判断简单化。 状态模式描述了对象状态的变化以及对象如何在每一种状态下表现出不同的行为。 模式结构 状态模式包含的主要角色： 抽象状态(State)：抽象状态，用于封装环境对象(Context)中特定状态所对应的行为。可以是抽象类，也可以是接口，具体状态类继承这个父类。 具体状态(Concrete State)：具体状态，实现抽象状态类中的方法，方法里封装自己状态的行为。 环境(Context)：持有一个抽象状态类型的属性用于维护当前状态，定义一个方法，在方法里将与状态相关的操作委托托给当前状态对象来处理。 环境类实际上就是拥有状态的对象，有时可以充当 状态管理器(State Manager) 的角色，可以在环境中对状态进行切换操作。 区分策略模式 状态模式 从关注焦点区分 策略模式关注的焦点在于具体的某一个行为，准确的说是某一行为的具体执行过程。一般来说，即使拥有多种不同的执行过程， 但是执行的结果是相同的。就比如拿到一串数字进行排序，排序是一个行为，可以理解为类中的一个方法，这个方法是抽象的。 而不同的排序算法就是对排序行为的不同实现。不同的排序算法所耗费的内存空间和时间都不相同，但是最终的排序结果都是相同的。 这应该是策略模式的典型应用场景。 状态模式关注的焦点在于内部状态的改变而引起的行为的变化。即在不同的状体下，某一个行为的执行是不尽相同的。 不仅是行为过程不同，甚至是结果也会改变。比如在一个电商网站的某个商品页面，点击购买。如果用户是已登录状态， 那么就会跳转至订单结算页面；但如果是未登录状态，就会跳转到登录页面要求用户先登录。 从外部干涉区分 从干涉方式来看，策略模式中具体行为策略的改变是由调用方主动指定的，除此之外，没有其他因素会让具体的执行策略发生改变。 也就是对于某一个 context 对象而言，只有一个最合适的策略对象。也就是当我们指定了某个具体的排序算法后，如果不主动重新指定， 那么以后就会一直使用该算法进行排序，不会发生改变。context 内部策略的改变对于调用方是透明的，因为策略的改变是由调用方发起的。 而状态模式中状态对象的改变是不需要调用方主动干涉的，根据 context 对象相关属性的变化，就会引起 context 内部 state 对象的变化。 而与状态相关的方法都依赖于具体的状态对象。并且在执行了相关方法后，状态会自动发生改变。而这些状态的改变对于调用方是隐藏的， 调用只是想调用某个方法，但是这个方法在不同状态的执行结果，调用方是无法预测的。就好像上面商城的案例中，当你点击购买按钮的时候， 你并不知道是否一定会跳转到订单结算付款页面。 具体实现 抽象接口 public interface IState { void handle(Context context); } 具体状态类 public class ConcreteStateA implements IState { @Override public void handle(Context context) { System.out.println(\"当前状态：A\"); //状态 A 执行完后，迁移到状态 B context.setState(new ConcreteStateB()); } } public class ConcreteStateB implements IState { @Override public void handle(Context context) { System.out.println(\"当前状态：B\"); //状态 B 执行完后，迁移到状态 A context.setState(new ConcreteStateA()); } } 环境 Context，持有抽象状态类型属性，定义一个使用状态对象属性调用其行为的方法。 public class Context { private IState state; public Context() { } public Context(IState state) { this.state = state; } void handle() { // 注意:这里传入当前对象 state.handle(this); } public IState getState() { return state; } public Context setState(IState state) { this.state = state; return this; } } 客户端调用 public class StateMain { public static void main(String[] args) { Context context = new Context(new ConcreteStateA()); context.handle(); context.handle(); context.handle(); context.handle(); context.handle(); } } 输出结果 当前状态：A 当前状态：B 当前状态：A 当前状态：B 当前状态：A link: http://www.gxitsky.com/2019/10/20/designPatterns-10-State/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/策略模式.html":{"url":"blog/设计模式/行为型模型/策略模式.html","title":"策略模式","keywords":"","body":"策略模式 代码描述 就是不同的　结构体　实现　同一个接口, 在不同的场景下我创建相应的对象，　然后调用该对象的方法 定义一系列算法，让这些算法在运行时可以互换，使得分离算法，符合开闭原则。 应用场景 example-1 接口:　出版图书(method: 印刷) type PublicBook interface{ publishBook() } 对象　宋代毕升， 现代印刷厂　　(他们都重写了印刷方法) type BiSheng struct{ } func (BiSheng) publishBook(){ fmt.Println(\"活字印刷\") } type PrintingPlant struct{ } func (PrintingPlant) publishBook(){ fmt.Println(\"激光印刷\") } 策略类 /*策略类*/ type PrintContext struct { publicBook PublicBook } /*策略类操作方法*/ func (context PrintContext) PublicBook(){ context.publicBook.publishBook() } /*策略类构造函数*/ func NewPrintContext(publicBook PublicBook) *PrintContext{ return &PrintContext{ publicBook: publicBook, } } 使用场景: 宋代 ----->　创建毕升　-----> call method 印刷 现代 -----> 　创建现代印刷厂　-----> call method 印刷 func main(){ publishBook := NewPrintContext(BiSheng{}) publishBook.PublicBook() publishBook = NewPrintContext(PrintingPlant{}) publishBook.PublicBook() } 应用场景 example-2 strategy.go package strategy import \"fmt\" type Payment struct { context *PaymentContext strategy PaymentStrategy } type PaymentContext struct { Name, CardID string Money int } func NewPayment(name, cardid string, money int, strategy PaymentStrategy) *Payment { return &Payment{ context: &PaymentContext{ Name: name, CardID: cardid, Money: money, }, strategy: strategy, } } func (p *Payment) Pay() { p.strategy.Pay(p.context) } type PaymentStrategy interface { Pay(*PaymentContext) } type Cash struct{} func (*Cash) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by cash\", ctx.Money, ctx.Name) } type Bank struct{} func (*Bank) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by bank account %s\", ctx.Money, ctx.Name, ctx.CardID) } strategy_test.go package strategy func ExamplePayByCash() { payment := NewPayment(\"Ada\", \"\", 123, &Cash{}) payment.Pay() // Output: // Pay $123 to Ada by cash } func ExamplePayByBank() { payment := NewPayment(\"Bob\", \"0002\", 888, &Bank{}) payment.Pay() // Output: // Pay $888 to Bob by bank account 0002 } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/职责链模式.html":{"url":"blog/设计模式/行为型模型/职责链模式.html","title":"职责链模式","keywords":"","body":"职责链模式 职责链模式用于分离不同职责，并且动态组合相关职责。 Golang实现职责链模式时候，因为没有继承的支持，使用链对象包涵职责的方式，即： 链对象包含当前职责对象以及下一个职责链。 职责对象提供接口表示是否能处理对应请求。 职责对象提供处理函数处理相关职责。 同时可在职责链类中实现职责接口相关函数，使职责链对象可以当做一般职责对象是用。 chain.go package chain import \"fmt\" type Manager interface { HaveRight(money int) bool HandleFeeRequest(name string, money int) bool } type RequestChain struct { Manager successor *RequestChain } func (r *RequestChain) SetSuccessor(m *RequestChain) { r.successor = m } func (r *RequestChain) HandleFeeRequest(name string, money int) bool { if r.Manager.HaveRight(money) { return r.Manager.HandleFeeRequest(name, money) } if r.successor != nil { return r.successor.HandleFeeRequest(name, money) } return false } func (r *RequestChain) HaveRight(money int) bool { return true } type ProjectManager struct{} func NewProjectManagerChain() *RequestChain { return &RequestChain{ Manager: &ProjectManager{}, } } func (*ProjectManager) HaveRight(money int) bool { return money chain_test.go package chain func ExampleChain() { c1 := NewProjectManagerChain() c2 := NewDepManagerChain() c3 := NewGeneralManagerChain() c1.SetSuccessor(c2) c2.SetSuccessor(c3) var c Manager = c1 c.HandleFeeRequest(\"bob\", 400) c.HandleFeeRequest(\"tom\", 1400) c.HandleFeeRequest(\"ada\", 10000) c.HandleFeeRequest(\"floar\", 400) // Output: // Project manager permit bob 400 fee request // Dep manager permit tom 1400 fee request // General manager permit ada 10000 fee request // Project manager don't permit floar 400 fee request } 文档更新时间: 2020-08-24 11:51 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/观察者模式.html":{"url":"blog/设计模式/行为型模型/观察者模式.html","title":"观察者模式","keywords":"","body":"观察者模式 观察者模式用于触发联动。 一个对象的改变会触发其它观察者的相关动作，而此对象无需关心连动对象的具体实现。 obserser.go package observer import \"fmt\" type Subject struct { observers []Observer context string } func NewSubject() *Subject { return &Subject{ observers: make([]Observer, 0), } } func (s *Subject) Attach(o Observer) { s.observers = append(s.observers, o) } func (s *Subject) notify() { for _, o := range s.observers { o.Update(s) } } func (s *Subject) UpdateContext(context string) { s.context = context s.notify() } type Observer interface { Update(*Subject) } type Reader struct { name string } func NewReader(name string) *Reader { return &Reader{ name: name, } } func (r *Reader) Update(s *Subject) { fmt.Printf(\"%s receive %s\\n\", r.name, s.context) } obserser_test.go package observer func ExampleObserver() { subject := NewSubject() reader1 := NewReader(\"reader1\") reader2 := NewReader(\"reader2\") reader3 := NewReader(\"reader3\") subject.Attach(reader1) subject.Attach(reader2) subject.Attach(reader3) subject.UpdateContext(\"observer mode\") // Output: // reader1 receive observer mode // reader2 receive observer mode // reader3 receive observer mode } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/解释器模式.html":{"url":"blog/设计模式/行为型模型/解释器模式.html","title":"解释器模式","keywords":"","body":"解释器模式 解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。 解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。 对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以。 interpreter.go package interpreter import ( \"strconv\" \"strings\" ) type Node interface { Interpret() int } type ValNode struct { val int } func (n *ValNode) Interpret() int { return n.val } type AddNode struct { left, right Node } func (n *AddNode) Interpret() int { return n.left.Interpret() + n.right.Interpret() } type MinNode struct { left, right Node } func (n *MinNode) Interpret() int { return n.left.Interpret() - n.right.Interpret() } type Parser struct { exp []string index int prev Node } func (p *Parser) Parse(exp string) { p.exp = strings.Split(exp, \" \") for { if p.index >= len(p.exp) { return } switch p.exp[p.index] { case \"+\": p.prev = p.newAddNode() case \"-\": p.prev = p.newMinNode() default: p.prev = p.newValNode() } } } func (p *Parser) newAddNode() Node { p.index++ return &AddNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newMinNode() Node { p.index++ return &MinNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newValNode() Node { v, _ := strconv.Atoi(p.exp[p.index]) p.index++ return &ValNode{ val: v, } } func (p *Parser) Result() Node { return p.prev } interpreter_test.go package interpreter import \"testing\" func TestInterpreter(t *testing.T) { p := &Parser{} p.Parse(\"1 + 2 + 3 - 4 + 5 - 6\") res := p.Result().Interpret() expect := 1 if res != expect { t.Fatalf(\"expect %d got %d\", expect, res) } } 文档更新时间: 2020-08-24 11:49 作者：kuteng Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/访问者模式.html":{"url":"blog/设计模式/行为型模型/访问者模式.html","title":"访问者模式","keywords":"","body":"访问者模式 封装一些作用于某种数据结构中的各元素的操作，它可以在不改变这个数据结构的前提下定义作用于这些元素的新的操作。 在Gof中，也有关于为什么引入访问者模式的解释 访问者模式在设计跨类层结构的异构对象集合的操作时非常有用。 访问者模式的结构 visitor(抽象访问者)：抽象访问者为对象结构中每一个具体元素类concreteElement声明一个访问操作，从这个操作的名称或 参数类型可以清楚知道需要访问的具体元素的类型，具体访问者则需要实现这些操作方法，定义对这些元素的访问操作。 concreteVisitor(具体访问者)：具体访问者实现了抽象访问者声明的方法，每一饿操作作用于访问对象结构中一种类型的元素 element(抽象元素)： 一般是一个抽象类或接口，定义一个accept方法，该方法通常以一个抽象访问者作为参数。 concreteElement(具体元素)： 具体元素实现了Accept方法，在accept方法中调用访问者方法以便完成一个元素的操作。 objectStruct(对象结构): 对象结构是一个元素的集合，用于存放元素对象，且提供遍历其内部元素的方法。 访问者模式的具体实现 java 访问者 接口 元素A访问方法 元素B访问方法 。。。。 visitor public interface Visitor { void visitElementA(A a); void visitElementB(B b); } 具体的访问者 访问者A public class Va implements Visitor { public void visitElementA(A a) { System.out.println(\"visitor A ...print element a\"); } public void visitElementB(B a) { System.out.println(\"visitor A ....print element b\"); } } 访问者B public class Vb implements Visitor { public void visitElementA(A a) { System.out.println(\"visitor B ...print element a\"); } public void visitElementB(B b){ System.out.println(\"visitor B ...print element b\"); } } 被访问方法接口 public interface Visitable { void accept(Visitor visitor); } 被访问元素A public class A implements Visitable { @Override public void accept(Visitor visitor) { visitor.visit(this); } } 被访问元素B public class B implements Visitable { @Override public void accept(Visitor visitor) { visitor.visit(this); } } 对象结构 public class AB { private List visit = new ArrayList<>(); public void addVisit(Visitable visitable) { visit.add(visitable); } public void show(Visitor visitor) { for (Visitable visitable: visit) { visitable.accept(visitor); } } } 客户端 public class Client { static public void main(String[] args) { AB ab = new AB(); ab.addVisit(new A()); ab.addVisit(new B()); ab.show(new Va()); // 被访问者接受Va的访问 ab.show(new Vb()); // 被访问者接受Vb的访问 } } 访问者模式的具体实现 golang visitor.go package visitor import \"fmt\" type Customer interface { Accept(Visitor) } type Visitor interface { Visit(Customer) } type EnterpriseCustomer struct { name string } type CustomerCol struct { customers []Customer } func (c *CustomerCol) Add(customer Customer) { c.customers = append(c.customers, customer) } func (c *CustomerCol) Accept(visitor Visitor) { for _, customer := range c.customers { customer.Accept(visitor) } } func NewEnterpriseCustomer(name string) *EnterpriseCustomer { return &EnterpriseCustomer{ name: name, } } func (c *EnterpriseCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type IndividualCustomer struct { name string } func NewIndividualCustomer(name string) *IndividualCustomer { return &IndividualCustomer{ name: name, } } func (c *IndividualCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type ServiceRequestVisitor struct{} func (*ServiceRequestVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"serving enterprise customer %s\\n\", c.name) case *IndividualCustomer: fmt.Printf(\"serving individual customer %s\\n\", c.name) } } // only for enterprise type AnalysisVisitor struct{} func (*AnalysisVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"analysis enterprise customer %s\\n\", c.name) } } visitor_test.go package visitor func ExampleRequestVisitor() { c := &CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Accept(&ServiceRequestVisitor{}) // Output: // serving enterprise customer A company // serving enterprise customer B company // serving individual customer bob } func ExampleAnalysis() { c := &CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Accept(&AnalysisVisitor{}) // Output: // analysis enterprise customer A company // analysis enterprise customer B company } Q： 1.如果被访问元素增加，需要增加访问者接口方法，每个访问者子类都要修改 2.调用触发时机： 被访问者发起邀请， 访问者是参数， 被访问者遍历自己所有被访问元素列表。 link: https://juejin.im/entry/5ab4c3d65188251fc3293550 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/设计模式/行为型模型/迭代器模式.html":{"url":"blog/设计模式/行为型模型/迭代器模式.html","title":"迭代器模式","keywords":"","body":"迭代器模式 送代器模式用于使用相同方式送代不同类型集合或者隐藏集合类型的具体实现。 可以使用送代器模式使遍历同时应用送代策略，如请求新对象、过滤、处理对象等。 iterator.go package iterator import \"fmt\" type Aggregate interface { Iterator() Iterator } type Iterator interface { First() IsDone() bool Next() interface{} } type Numbers struct { start, end int } func NewNumbers(start, end int) *Numbers { return &Numbers{ start: start, end: end, } } func (n *Numbers) Iterator() Iterator { return &NumbersIterator{ numbers: n, next: n.start, } } type NumbersIterator struct { numbers *Numbers next int } func (i *NumbersIterator) First() { i.next = i.numbers.start } func (i *NumbersIterator) IsDone() bool { return i.next > i.numbers.end } func (i *NumbersIterator) Next() interface{} { if !i.IsDone() { next := i.next i.next++ return next } return nil } func IteratorPrint(i Iterator) { for i.First(); !i.IsDone(); { c := i.Next() fmt.Printf(\"%#v\\n\", c) } } iterator_test.go package iterator func ExampleIterator() { var aggregate Aggregate aggregate = NewNumbers(1, 10) IteratorPrint(aggregate.Iterator()) // Output: // 1 // 2 // 3 // 4 // 5 // 6 // 7 // 8 // 9 // 10 } Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/UML.html":{"url":"blog/软考/UML.html","title":"UML","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/english.html":{"url":"blog/软考/english.html","title":"English","keywords":"","body":"Hew out of the mountain of despair stone of hope and you can make your life splendid one. 在绝望中寻找希望,人生终将辉煌! Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/plan.html":{"url":"blog/软考/plan.html","title":"Plan","keywords":"","body":"Plan 4月25-4月30 书籍 每天2章节 课后练习 5月1日到3日 每天一篇论文 每天一份真题练习 论软件需求获取技术及应用 论需求分析方法及应用 论信息系统开发方法及应用 5月4日-5月5日 科目二 考试 5月5日-5月-14日 准备论文 论软件多层架构的设计 论软件的系统测试及应用 论企业应用集成 论基于构建的软件开发 论软件开发模型及应用 论企业信息集成技术及应用 论软件设计模式及应用 5月15日-5月18日 梳理错题集 5月19日-5月21日 梳理知识点 5月22日 - 5月27日 复习视频 练写字速度 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/专利.html":{"url":"blog/软考/专利.html","title":"专利","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/信息工程.html":{"url":"blog/软考/信息工程.html","title":"信息工程","keywords":"","body":" 业务需求分析 - 职能域分析 - 业务域定义 - 业务流程梳理 数据需求分析 用户视图收集 用户视图分组、分析 数据 系统功能建模 子系统定义 功能模块定义 程序单元定义 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/同步.html":{"url":"blog/软考/同步.html","title":"同步","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/开发模型.html":{"url":"blog/软考/开发模型.html","title":"开发模型","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/成本效益.html":{"url":"blog/软考/成本效益.html","title":"成本效益","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/数据库.html":{"url":"blog/软考/数据库.html","title":"数据库","keywords":"","body":"数据库 数据库系统DBS的组成：数据库、硬件、软件、人员 数据库管理系统DBMS的功能：数据定义、数据操作、数据库运行管理、数据库的建立维护 DBMS的分类： 1. 关系数据库系统RDBS 1. 面向对象的数据库系统OODBS 1. 对象关系数据库系统ORDBS 三级模式- 两级映像 内模式： 模式： E-R模型 数据结构、数据操作、数据的约束条件 函数依赖： 给定一个X，能唯一确定一个Y，就称X确定Y，或者说Y依赖于X，例如Y=X*X函数 函数依赖可以扩展成以下两种规则： Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/测试.html":{"url":"blog/软考/测试.html","title":"测试","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/系统可靠性.html":{"url":"blog/软考/系统可靠性.html","title":"系统可靠性","keywords":"","body":"计算机基础---可靠性计算 原文地址 【摘要】 可靠性计算主要涉及三种系统，即串联系统、并联系统和冗余系统，其中串联系统和并联系统的可靠性计算都非常简单，只要了解其概念，公式很容易记住。（1）串联系统 假设一个系统由n个子系统组成，当且仅当所有的子系统都能正常工作时，系统才能正常工作，这种系统称为串联系统，如图1-8所示。 设系统各个子系统的可靠性分别用R1,R2...Rn表示，则系统的可靠性R = R1R2R3...Rn。 如果... 可靠性计算主要涉及三种系统，即串联系统、并联系统和冗余系统，其中串联系统和并联系统的可靠性计算都非常简单，只要了解其概念，公式很容易记住。 （1）串联系统 假设一个系统由n个子系统组成，当且仅当所有的子系统都能正常工作时，系统才能正常工作，这种系统称为串联系统，如图1-8所示。 .png) 设系统各个子系统的可靠性分别用R1,R2...Rn表示，则系统的可靠性R = R1R2R3...Rn。 如果系统的各个子系统的失效率分别用λ1,λ2.....λn来表示，则系统的失效率λ=λ1+λ2+...+λn。 （2）并联系统 假如一个系统由n个子系统组成，只要有一个子系统能够正常工作，系统就能正常工作，如图1-9所示。 .png) 设系统各个子系统的可靠性分别用R1,R2...Rn表示，则系统的可靠性R = 1-(1-R1)(1-R2)...*(1-Rn)。 假如所有子系统的失效率均为λ ，则系统的失效率为μ ：.png) 在并联系统中只有一个子系统是真正需要的，其余n-1个子系统都被称为冗余子系统。该系统随着冗余子系统数量的增加，其平均无故障时间也会增加。 （3）模冗余系统 m模冗余系统由m个（m=2n+1为奇数）相同的子系统和一个表决器组成，经过表决器表决后，m个子系统中占多数相同结果的输出可作为系统的输出，如图1-10所示。 .png) 在m个子系统中，只有n+1个或n+1个以上的子系统能正常工作，系统就能正常工作并输出正确结果。假设表决器是完全可靠的，每个子系统的可靠性为R0，则m模冗余系统的可靠性为： .png) Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/网络.html":{"url":"blog/软考/网络.html","title":"网络","keywords":"","body":" 网络层协议： IP ICMP ARP RARP AKP UUCP 数据链路层 FDDI Ethernet Arpanet PDN SLIP PPP 4位版本 4为首部长度 8位服务类型 Socks 5 认证流程 auth request auth reply proxy request response data data end end Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/计算机基础.html":{"url":"blog/软考/计算机基础.html","title":"计算机基础","keywords":"","body":" 计算机组成 1.1硬件 组成： 运算器、控制器、存储器、输入设备、输出设备 主机：CPU（运算器、控制器）+ 主存储器 CPU：由运算器、控制器、寄存器和内部总线组成。 运算器： 控制器： 1.2 计算机指令 1.3 寻址方式 内存地址 数的表示 原码：一个数的正常二进制表示 反码：正数的反码即原码；负数的反码是在原码的基础上，除符号位外，其他各位按位取反。 补码：正数的补码即原码；负数的补码是在原码的基础上，除符号位外，其他各位按位取反，而后末尾+1，若有进位则产生进位。因此数值0的补码只有一种形式 +0=-0=000000000 移码：用作浮点运算的阶码，无论正负，都是将原码的补码首位取反后得到. ​ 符号表示：原码最高位代表正负号，且不参与计算；而其他编码最高位虽然也是代表正负号，但参与计数。 校验码 码距：就单个编码A：00而言，其码距为1，因为其中只需要改变一位就变成另一个编码。在两个编码中，从A码到B码转换所需要改变的位数称为码距，例如A：00要转换为B：11，码距为2。一般来说，码距越大，越利于纠错和检错。 4.1 奇偶校验码 ​ 4.2 循环冗余校验码 ​ CRC 原理是找出一个能整除多项式的编码，因此首先要将原始报文除以多项式，将所得的余数作为校验位加在原始报文之后，作为发送数据发给接收方。 ​ 使用CRC 编码，需要先约定一个生成多项式G(x)。 生成多项式的最高位和最低位必须是1. ​ 例：假设原始信息为10110， CRC的生成多项式为G(x)=x^4+x+1,求CRC校验码 ​ 1）在原始信息位后面添加0，假设生成多项式的阶为r，则在原始信息位后添加r个0 ​ 2）由多项式得到除数，多项式中x的幂指数存在的位置1，不存在的位置0。本题中x幂指数位0，1，4的变量都存在，而幂指数2，3的不存在，因此得到串10011。 ​ 3） ​ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/论文/Untitled.html":{"url":"blog/软考/论文/Untitled.html","title":"Untitled","keywords":"","body":"论文 考试时间120分钟。 论文是软考高级中经常出现不合格情况的题型，但如果自己有实际的工作经验，也有一定的理论基础，也认真准备了考试，达到45分合格线并不是一件很困难的事。 考试形式是给出4道论文题目，选其中一道题目来写论文，两个小时，手写在类似高考语文作文的答题卡上，一般要写到框框2200字以上。 通常来讲，论文给出的4道题目里面，有一道一般是最近两年比较火热的技术，如云原生、无服务架构等。 其它三道题的出题方向很固定，频次大致从高到低主要包括： 软件系统架构设计 软件系统建模 信息系统规划与集成 软件质量保证 软件可用性设计 软件安全性设计 需求管理 项目管理 结构 写作的结构也很固定，基本上是八股文，摘要几乎是有模板的。 摘要 交代自己做了项目，自己是做什么的（一般是架构师） 我在这个项目里面，用到了哪些与题目相关的技术 项目很成功，客户很开心，老板很开心 正文 我现在在哪里工作，是个啥职位（100字左右，注意数据脱敏，不要透露完全真实的项目名称和个人、公司信息，比如用某某代替） 我做了什么项目，业务背景和产品设计是怎么样的（300字左右，同样注意数据脱敏） 说说题目里面的技术或概念是什么（作为论点，300字左右） 项目中是怎么体现题目中的技术的（作为论据，也是整片论文的主体部分，1000字左右，举2到4个例子） 项目取得了怎么样的结果，有哪些细小的可以改进的点（结论，400字左右） 感叹人生，感叹社会（100字左右） 论文概述 理论 实践： ​ 启动准备阶段 ​ 实时和监控阶段 ​ 常规动作 ​ 问题描述和解决 总结 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/论文/设计模式.html":{"url":"blog/软考/论文/设计模式.html","title":"设计模式","keywords":"","body":"设计模式在工程项目中的应用 设计模式介绍 　　一般而言，一个设计模式具有模式名称、适用场景、解决方案和效果四个方面的基本要素。设计模式根据其目的可以分为创建型、结构性和行为型三种类型，创建型模式，主要负责对象的创建工作，程序在明确需要创建对象时，可以获得更大的灵活性。常用的创建型设计模式有单例模式、工厂方法、原型、构造器、抽象工厂5种。结构型模式，负责处理类或对象之间的关系，用于构件结构更加复杂庞大的系统，常用的结构型设计模式有适配器、桥接模式、享元模式、组合模式、外观模式、代理模式等七种模式。行为模式，主要任务是对类或对象如何交互以及为类和对象分配具体职责进行描述。常用的行为设计模式有观察者、状态、策略、备忘录、责任链、命令、中介者等11种模式。这些设计模式都是经过反复使用的成熟方法，对优化软件结构，提供软件质量具有指导意义 ​ 一般而言， 一个设计模式具有模式名称、使用场景、解决放哪和效果四个方面的基本要素。设计模式更具其目的可以分为创建型、结构型和行为型三种类型，创建型模式，主要负责对象的创建工作，程序在明确需要创建对象时，可以获得更大的灵活性。结构型模式，负责处理类和对象之间的关系，用于构建结构更加复杂庞大的系统，常用的结构型设计模式有适配器、桥接模式、享元模式、组合模式、外观模式、代理模式等七种模式。行为模式，主要任务是对类或对象如何交互以及类的对象分配具体职责进行描述。常用的行为设计模式有观察者、状态、策略、备忘录、责任链、命令、中介者等11种模式化 项目介绍 使用背景 工厂方法 ​ 在devops项目中，为了满足不同企业用户对代码仓库使用的多样性，我们需要对接gitlab，gitee，github，华为codehub等代码仓库。由于用户集成仓库类型具有不确定性，设计者无法确定具体实例化哪一个类。为了解决这个问题，设计者使用了工厂方法模式。首先定义一个代码仓库类接口也叫抽象产品GitInterface，接口内定义了不同类型仓库必须要实现的方法，然后再定义具体代码仓库类GitHub,GitLab,Gitee,CodeHub。其次定义抽象工厂类NewGitFactory和具体工厂类NewGithub，NewGitLab，NewGitee，NewCodeHub。对外使用，只需要执行GitInterface定义的方法，实例化对象，则根据用户传参调用具体工厂方法。后续扩展新的仓库类型，只需要定义增加具体工厂类，和具体代码仓库类，代码仓库类实现抽象产品接口即可。这种良好的扩展性设计，符合开闭原则。 建造者模式 在平时的开发中，我们需要经常使用HTTP调用工具类，该类使用的时候可能只需要一个路径参数，也可能需要Header、Body、Query、请求方式等各种组合参数。为了解决不同开发人员在使用上的代码风格一致性。该类在使用过程中需要额外添加限流，请求体上下文处理。设计者采用了建造者模式，首先定一个基类Request，为Request添加开发者可能需要用到属性，然后为每一个属性添加定义一个方法，并且返回其自身。通过该设计模式的使用，有效提提高了代码的整洁度。 访问者模式 devops项目中需要使用命令行，向服务端获取任务状态和提交任务，但是每次将请求数据提交给APIServer前需要对数据进行校验，添加注解，数据转化，数据对比等操作。不同的提交方式，数据处理手段不一样，每次处理完后，不希望原始数据结构被污染。设计者使用Build和Visitor模式 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/论文/项目管理.html":{"url":"blog/软考/论文/项目管理.html","title":"项目管理","keywords":"","body":"项目背景 ​ 时间： 2020年 ​ 项目名称： 容器云平台 ​ 项目范围：kuberntes 集群管理， 应用管理，应用日志收集，应用性能监控，公有云资源统管 ​ 项目金额：每年800万的研发投入 项目目标 ​ 交付成果： ​ 工期目标 ​ 成本目标 ​ 质量目标 ​ 重要干系人 WBS 组织结构 职责分配 进度里程碑 成本分析 风险分析 理论： 项目管理的核心为通过规划与控制来确保项目按时完成，合集包含七个过程，其中前六个位于规划过程组，第七个位于监控过程组 规划进度管理：为规划、编制、管理、执行 活动定义 活动排序 活动资源估算 估算历时估算 制定进度计划 控制进度 本项目实践 应用工具制定项目计划 监控项目进度，动态调整维护进度基准 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/设计模式.html":{"url":"blog/软考/设计模式.html","title":"设计模式","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/软件工程.html":{"url":"blog/软考/软件工程.html","title":"软件工程","keywords":"","body":"Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/软考/错题集.html":{"url":"blog/软考/错题集.html","title":"错题集","keywords":"","body":"错题集 选择题 计算机基础知识 多媒体技术 数据库 开发模型 成本计算 软件工程 软著、专利保护法 问答题 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-17 22:47:19 "},"blog/algorithm/宫水三叶-大神.html":{"url":"blog/algorithm/宫水三叶-大神.html","title":"宫水三叶 大神","keywords":"","body":"GitHub: https://github.com/SharingSource/LogicStack-LeetCode/wiki 知乎：https://www.zhihu.com/column/c_1339974226623885312 leetcode： https://leetcode.cn/u/ac_oier/ Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 09:06:09 "},"blog/algorithm/负雪明烛-大神.html":{"url":"blog/algorithm/负雪明烛-大神.html","title":"负雪明烛 大神","keywords":"","body":"https://blog.csdn.net/fuxuemingzhu/article/details/105183554 大家好，我是负雪明烛。今天跟大家聊一聊「LeetCode应该怎么刷？」这个话题。 我是大二的时候开始接触 LeetCode 的，那时候 LeetCode 只有 400 题，我本来打算每天做 1 个题，但是由于当时觉得题目太难没坚持下去。在大四考研后和研一研二的两年左右的时间，我刷了 800 多道题（现在已经900多），并且大多数题目做了 2~3 遍。那么我是怎么做到的呢？ 一、入门篇 刷题姿势 刚开刷 LeetCode 时遇到二叉树翻转，想了一天也没明白，当时无比痛苦。因为我的方法不对，我总想着在脑子里面想明白再写，在白纸上不停地模拟二叉树树翻转的每一步，还想着用本地 IDE 写个二叉树结构进行Debug，现在看来都是走了弯路。 大部分新手应该是只学过课本上的一些数据结构和算法的知识，还没有实际刷题经验，因此非常痛苦。 对于新人而言，不应该自己死抠一个题目，如果想了一会没有任何思路，就应该果断看别人怎么写的。在理解了别人的做法之后，再凭理解和记忆在 LeetCode 的代码框里敲一遍。 学习 = 学 + 习。知识是学出来的，不是在自己脑子里蹦出来的；学过之后，还要自己动手练习。新手要勇敢地、经常地学习别人的解法和答案，然后凭理解敲代码练习。只要度过刷题初期的痛苦，后面就会越刷越快。 基础知识 需要掌握常用的数据结构和算法的思想和适用场景。 学习基础知识，我推荐 《算法（第4版）》。这个书不用全部细看，可以只看重点，比如前面的 Java 知识不用看，数学推导部分可以不用看。 再推荐一本侯捷的 《STL源码剖析》，这本书对理解C++ STL有重大帮助，看了之后绝对会对数据结构和算法有更深的理解，我看完这本书之后感觉相见恨晚啊。 刷题顺序 合理的刷题顺序能降低难度，帮助我们在有限的时间里获得最快的成长。 LeetCode 现在将近 2000 道题，基本没有人能够全部刷完，而且对于参加面试者来说也没有必要刷特别多的题。许多人在面试前刷了 200 道题，基本够了；准备更充分的人，大概会刷 400 道题；能刷 600 道题目以上的，基本上国内公司的 Offer 都能收获到一大堆。 我推荐的刷题顺序的规则是： 按分类刷；每个分类从 Easy 到 Medium 顺序刷； 优先刷 树、链表、二分查找、DFS、BFS 等面试常考类型； 优先刷题号靠前的题目； 优先刷点赞较多的题目； 如果基本上能做到 Easy 题 100% 能做对，Medium 题经过思考或与面试官交流后能做对，基本就能拿到 Offer。在实际面试过程中，很少出 Hard 题，视能力刷。 跟别人学习 向别人学习是非常必要的。 1）看别人的题解 主要看别人在解决这个题目的思路是什么。无论这个题你会不会，都要看下别人的解法，或许有新收获。 推荐的博客作者有： 负雪明烛：5 年在 CSDN 上更新了 800多道题解，收获 160万 阅读。在中文力扣日更题解。 李威威：中文力扣的大 V，对力扣题目掌握很全面，写得题解非常详细，对题目举一反三。 花花酱：基本每个题都有博客和视频，强烈推荐看他的视频。 Grandyang：在博客园更新了几乎所有力扣题目，收获了 1200万 阅读。 最近我在刷中文版的力扣，题解区的答案质量非常高。比较推荐的博主有：力扣官方题解，负雪明烛，李威威，zerotrac，Krahets。 我恬不知耻地推荐一下自己（负雪明烛）的题解，我最近已经连续在中文力扣日更「每日一题」题解 20 天。最近利用动图帮助大家理清做题思路，点赞和阅读数都比较高。 除了题解区以外，如果想看博客上面的题解，可以用搜索引擎搜题目和博主名。想看负雪明烛的「two sum」题解，那么搜索方式就是在关键词之后加上「fuxuemingzhu」： 2）看别人的总结 这部分包括算法讲解、套路整理、刷题模板等。 「算法题 = 思路 + 模板」，思路需要通过看别人的解答以及讲解获得，模板就是做题的套路，既可以自己总结，也可以看别人总结好的。 比如负雪明烛的【LeetCode】代码模板，刷题必会。 也比如说 AlgoWiki ： 当然推荐每个人在做题的过程中都整理一份自己的总结，用自己的方式总结好知识点和模板。 做好笔记 写作过程能更好地帮助我们理清思路，也能帮助我们再做此题时快速想起以前的做法，还能见证我们自己的成长。 在五六年前我刚开始刷题时，就把每个做过的题目记录在CSDN上，现在我的博客浏览量已经将近 161万 了。 任何题，无论难度，我都记录题目、想法、代码。虽然经常写博客的时间比写题的时间还多，但是把自己的想法讲解一遍才是真的懂了，更方便了自己之后看、以及大家交流。经常看到自己几年前写的愚蠢代码，然后感叹自己确实有进步了。 在 B站 有个小姐姐讲了小白如何上手LeetCode，也演示了如何用 iPad 做笔记，值得一看。 程序媛分享 | LeetCode小白如何上手刷题？iPad学习方法 | 刷题清单 | 新手指南 | 刷题找工作 | IT类 交流和监督 刷题最大的障碍是自己。特别是新手，很可能由于刚接触 LeetCode 感觉太难就没有毅力坚持下去，导致半途而废。而且，刷题更重要的是坚持，做题的感觉都需要手感进行保持的。 所以，如果能有个组织交流和监督就好了。 我组织了「每日一题交流群」的活动，并且做了个网站 https://ojeveryday.com 来监督大家打卡。在网站上提交力扣个人主页就能进群，群里的规则是每天同步力扣的每日一题，然后大家交流做法。群里还会组织模拟面试、周末直播讲题等活动。由于进群前需要提交自己的 LeetCode 个人主页，并且群主管理严格，所以群里没有任何广告。刷题群已经持续将近一年，欢迎大家加入。 事实证明这种大家一起做同一道题目，并且一起交流讨论的氛围非常好。 二、提高篇 如果你已经过了小白的阶段，那么应该做些提高项目。 周赛 所谓周赛，就是每周日上午，LeetCode 组织的一场比赛，总共 4 道题，一般是 Easy 一道，Medium 两道，Hard 一道。中英文网站同时开始，题目相同。每隔一个周六晚上有双周赛，题目和周赛类似。往届竞赛也可以点击参加做练习模拟。 做周赛的目的是检验我们的学习成果，毕竟这些题目都是新的，就像考试一样。 不要担心自己做不出来，只要尽力而为就好了，我一般的目标是解决前三道，第 4 道 Hard 做不出来也没有心理负担。 参加完比赛之后，看下别人的解答，因为这几个题目都是自己苦思冥想过的，因此学习和进步地都挺快。 我最好的一次周赛成绩是全球 28 名，当看到自己的 id 显示在了全球排名的第一页，我非常兴奋，开心了一整天。 模拟面试 对于大多数人来说，刷题的目的是找工作，那最终就要参加面试。一个人做题的过程是缺乏交流的，实际面试中会有交流互动。因此，推荐在面试前参加一下模拟面试。 另外，哪怕不参加模拟面试，给别人讲一下做题的思路和代码的实现过程也是大有裨益的。 需要参加模拟面试的也可以进「每日一题交流群」，我邀请了力扣全站排名第一的 storm 来做模拟面试官。 三、最后 上文总结了我想到的「LeetCode应该怎么刷？」的方法，最重要的还是坚持二字。做时间的朋友，努力付出就一定有收获。如果觉得刷题困难，就多多学习，多多交流，不要半途而废。 最后，希望大家都能够通过刷 LeetCode 获得成长，拿到自己满意的 Offer。 期待你的点赞、关注、分享。 欢迎加入刷题群 目前已经近 2000 人加入了每日一题打卡群。加入方式是通过每日一题打卡网站，该网站每天都会同步力扣每日一题，这是个互相帮助、互相监督的算法题打卡网站，其地址是 https://www.ojeveryday.com/。 想加入千人刷题群的朋友，可以打开上面的链接地址，然后在左侧点击「加入组织」，提交力扣个人主页，即可进入刷题群。期待你早日加入。 关于作者 我是本文的作者是负雪明烛，毕业于北京邮电大学，目前就职于阿里巴巴。坚持刷算法题 5 年，共计刷了 800 多道算法题。做过的每个算法题都在 CSDN 上写题解博客，获得好评无数，CSDN 的累计阅读量已经 161万 次！博客地址是：https://blog.csdn.net/fuxuemingzhu 「负雪明烛」公众号是负雪明烛维护的一个算法题解公众号，致力于帮助大家刷题、找工作。欢迎关注。 2021 年 2 月 13 日 负雪明烛 更新于 北京。 ———————————————— 版权声明：本文为CSDN博主「负雪明烛」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/fuxuemingzhu/article/details/105183554 Copyright © caixisheng 2017 all right reserved，powered by Gitbook该文件修订时间： 2022-08-18 09:06:01 "}}