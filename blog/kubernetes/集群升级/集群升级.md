# Kuberntes 集群升级步骤



kubernetes 可以通过kubeadm工具实现平滑升级，但是每次升级只能按序递增一个大版本，比如1.17.xx 只能升级到1.18.x。本文脚本以为1.17.11 升级到1.18.0为例。



## 1. 查询可用k8s 版本

```
yum list --showduplicates kubeadm --disableexcludes=kubernetes
```



## 2. 获取集群配置文件

通过kubeadm 安装的集群，会自动将配置文件存放在configmap： kubeadm-config。在1.23 之前的版本， 可以通过kubeadm view 的方式查看。

方式一：

```bash
# kubeadm version < 1.23.x 该方法会弃用
kubeadm config view > upgrade-k8s.yaml
```



方式二：

```bash
kubectl -n kube-system get cm kubeadm-config --template={{.data.ClusterConfiguration}} > upgrade-k8s.yaml
```



## 3. 升级master 节点kubeadm/kubelet/kubectl组件

在所有节点（包括 master、worker 节点）执行安装升级命令

```
yum upgrade -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 
```





## 4. 创建升级用的配置文件

我们在拿到集群现有的配置后，需要修改版本。

如果你的kuberntes 镜像无法获取可以使用阿里云提供的镜像，地址：registry.aliyuncs.com/google_containers

```bash
sed -i 's/^kubernetesVersion:.*/kubernetesVersion: 1.18.0/g' upgrade-k8s.yaml
```



## 5. 执行kubeadm upgrade命令升级master节点

在 master 节点执行

```bash
# 查看配置文件差异 
kubeadm upgrade diff --config upgrade-k8s.yaml      
# 执行升级前试运行 
kubeadm upgrade apply --config upgrade-k8s.yaml --dry-run      
# 执行升级动作 
kubeadm upgrade apply --config upgrade-k8s.yaml
systemctl daemon-reload 
systemctl restart kubelet
```





## 6. 升级worker节点

```
yum upgrade -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 
systemctl daemon-reload 
systemctl restart kubelet
```



## 7.检查是否升级成功

```bash
kubectl get nodes
```



## 常见问题

**1. 版本1.21 coredns 镜像在二级目录下，如果只修改了镜像地址，会发生pull 镜像失败。**

```
k8s.gcr.io/kube-apiserver:v1.21.11
k8s.gcr.io/kube-controller-manager:v1.21.11
k8s.gcr.io/kube-scheduler:v1.21.11
k8s.gcr.io/kube-proxy:v1.21.11
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0
```



解决方案：

​	kubernetes/cmd/kubeadm/app/constants/constants.go

```
	// CoreDNSImageName specifies the name of the image for CoreDNS add-on
	CoreDNSImageName = "coredns/coredns"
```



朗澈公司 编译的kubeadm，支持证书100年。下载地址如下

```
wget -O /usr/bin/kubeadm https://lstack-qa.oss-cn-hangzhou.aliyuncs.com/kubeadm-1.17.2
wget -O /usr/bin/kubeadm https://lstack-qa.oss-cn-hangzhou.aliyuncs.com/kubeadm-1.21.0
wget -O /usr/bin/kubeadm https://lstack-qa.oss-cn-hangzhou.aliyuncs.com/kubeadm-1.22.8
```



**2. Croups 报错, docker 和 kubelet 都配置的是systemd,但是仍然报错**

```bash
InitBinary:docker-init ContainerdCommit:{ID:3df54a852345ae127d1fa3092b95168e4a88e2f8 Expected:3df54a852345ae127d1fa3092b95168e4a88e2f8} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: DefaultAddressPools:[] Warnings:[]}
3月 26 14:55:24 dev-slave-3 kubelet[31173]: E0326 14:55:24.502947   31173 server.go:294] "Failed to run kubelet" err="failed to run Kubelet: misconfiguration: kubelet cgroup driver: \"cgroupfs\" is different from docker cgroup driver: \"systemd\""
3月 26 14:55:24 dev-slave-3 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
3月 26 14:55:24 dev-slave-3 systemd[1]: Unit kubelet.service entered failed state.
3月 26 14:55:24 dev-slave-3 systemd[1]: kubelet.service failed.
```



​	解决方案一：

​    https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/

​	更新所有节点的 cgroup 驱动

对于集群中的每一个节点：

- 执行命令 `kubectl drain <node-name> --ignore-daemonsets`，以 [腾空节点](https://kubernetes.io/zh/docs/tasks/administer-cluster/safely-drain-node)
- 执行命令 `systemctl stop kubelet`，以停止 kubelet
- 停止容器运行时
- 修改容器运行时 cgroup 驱动为 `systemd`
- 在文件 `/var/lib/kubelet/config.yaml` 中添加设置 `cgroupDriver: systemd`
- 启动容器运行时
- 执行命令 `systemctl start kubelet`，以启动 kubelet
- 执行命令 `kubectl uncordon <node-name>`，以 [取消节点隔离](https://kubernetes.io/zh/docs/tasks/administer-cluster/safely-drain-node)

在节点上依次执行上述步骤，确保工作负载有充足的时间被调度到其他节点。

流程完成后，确认所有节点和工作负载均健康如常。

 

   解决方案二：

​	node 节点没有安装kubeadm，笔者在升级的时候先卸载了kubelet，kubeadm，后来只安装了kubelet和kubectl 导致一直报cgroups，后来重新执行了下命令

```bash
yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0
```

 ,问题解决。









